Authors,Title,Year,Source title,Cited by,Link,Abstract,Author Keywords,Index Keywords,EID
"Shin K.-W., Lim D.-J.","Model-based automatic test case generation for automotive embedded software testing",2018,"International Journal of Automotive Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030321599&doi=10.1007%2fs12239-018-0011-6&partnerID=40&md5=7234bfd7596be935c327f327ff321bf6","We propose a method to automatically generate software and hardware test cases from a UML model developed through a model-based development process. Where languages such as source-code languages are used within the model, input and expected values for each test case are generated using a custom parser. As a next step, unit test cases are combined to generate integration test cases using a bottom-up approach. Then these cases are converted into hardware test cases for approval testing of embedded systems, using XQuery and hardware mapping tables. We demonstrate this process by applying it to the power window switch module of a Hyundai Santa Fe vehicle. Our approach provides an automatic testing procedure for embedded systems developed by model-based methods, and generates test cases efficiently using a recombination of signals. In conclusion, our proposed method could help reduce the resources needed for test case generation from software to hardware. © 2018, The Korean Society of Automotive Engineers and Springer-Verlag GmbH Germany.","Approval testing; Hardware testing; Integration testing; Model-based development; Power window switch module; Software testing; Test case generation; Unified Modeling Language (UML)","Automatic testing; Embedded systems; Hardware; Integration testing; Modeling languages; Unified Modeling Language; Automatic test-case generations; Bottom up approach; Expected values; Model based development; Model-based method; Software and hardwares; Switch modules; Test case generation; Software testing",2-s2.0-85030321599
"Cuzzocrea A., Mumolo E., Tessarotto M., Grasso G.M., Amendola D.","XML-VM: An XML-based grid computing middleware",2018,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026313010&doi=10.1007%2f978-3-319-61566-0_32&partnerID=40&md5=edcc5b8b7bd72a07b2e3627981b4c596","This paper describes a novel distributing computing middleware named XML-VM. Its architecture is inspired by the ‘Grid Computing’ paradigm. The proposed system improves many characteristics of previous Grid systems, in particular the description of the distributed computation, the distribution of the code and the execution times. XML is a markup language commonly used to interchange arbitrary data over the Internet. The idea behind this work is to use XML to describe algorithms; XML documents are distributed by means of XML-RPC, interpreted and executed using virtual machines. XML-VM is an assembly-like language, coded in XML. Parsing of XML-VM programs is performed with a fast SAX parser for JAVA. XML-VM interpreter is coded in JAVA. Several algorithms are written in XML-VM and executed in a distributed environment. Representative experimental results are reported. © Springer International Publishing AG 2018.",,"Computer software; Distributed computer systems; Hypertext systems; Java programming language; Middleware; Virtual machine; XML; Distributed computations; Distributed environments; Distributing computing; Grid computing middleware; Grid systems; ITS architecture; XML-RPC; Grid computing",2-s2.0-85026313010
"Karan M., Šnajder J.","Paraphrase-focused learning to rank for domain-specific frequently asked questions retrieval",2018,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029527329&doi=10.1016%2fj.eswa.2017.09.031&partnerID=40&md5=d57abf557426716e0a665d97d155e8c8","A frequently asked questions (FAQ) retrieval system improves the access to information by allowing users to pose natural language queries over an FAQ collection. From an information retrieval perspective, FAQ retrieval is a challenging task, mainly because of the lexical gap that exists between a query and an FAQ pair, both of which are typically very short. In this work, we explore the use of supervised learning to rank to improve the performance of domain-specific FAQ retrieval. While supervised learning-to-rank models have been shown to yield effective retrieval performance, they require costly human-labeled training data in the form of document relevance judgments or question paraphrases. We investigate how this labeling effort can be reduced using a labeling strategy geared toward the manual creation of query paraphrases rather than the more time-consuming relevance judgments. In particular, we investigate two such strategies, and test them by applying supervised ranking models to two domain-specific FAQ retrieval data sets, showcasing typical FAQ retrieval scenarios. Our experiments show that supervised ranking models can yield significant improvements in the precision-at-rank-5 measure compared to unsupervised baselines. Furthermore, we show that a supervised model trained using data labeled via a low-effort paraphrase-focused strategy has the same performance as that of the same model trained using fully labeled data, indicating that the strategy is effective at reducing the labeling effort while retaining the performance gains of the supervised approach. To encourage further research on FAQ retrieval we make our FAQ retrieval data set publicly available. © 2017 Elsevier Ltd","Convolutional neural network; FAQ retrieval; LambdaMART; Learning to rank; ListNET; Question answering","Human form models; Information retrieval; Natural language processing systems; Neural networks; Supervised learning; Convolutional neural network; FAQ retrieval; LambdaMART; Learning to rank; ListNET; Question Answering; Search engines",2-s2.0-85029527329
"Zhang L., Wen M., Ashuri B.","BIM Log Mining: Measuring Design Productivity",2018,"Journal of Computing in Civil Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032629269&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000721&partnerID=40&md5=11205660a5d20cd767e7cea104417f96","There has been a long debate on how to measure design productivity. Compared to construction productivity, design productivity is much more difficult to measure because design is an iterative and innovative process. Today, with rapid extension of building information modeling (BIM) applications, tremendous volumes of design logs have been generated by design software systems, such as Autodesk Revit. A systematic approach composed of a detailed step-by-step procedure is developed to deeply mine design logs in order to monitor and measure the productivity of the design process. A pattern retrieval algorithm is proposed to identify the most frequent design sequential patterns in building design projects. A novel metric for measuring design productivity based on the discovered sequential patterns is put forward. A large data set of design logs, provided by a large international design firm, is used as a case study to demonstrate the feasibility and applicability of the developed approach. Results indicate that: (1) typically, each designer executes specific commands more than any other commands; for instance, it is shown for a designer that the accumulative frequency of three commands can reach up to 56.15% of the entire number of commands executed by the designer; (2) a particular sequential pattern of design commands (\""pick lines →\""trim/extend two lines or walls to make a corner→ \""finish sketch"") has been executed 2,219 times, accounting for 46.75% of instances associated with the top five discovered sequential patterns of design commands; (3) the identified sequential patterns can be used as a project control mean to detect outlier performers that may require additional attention from project leaders; and (4) productivity performance within the discovered sequential patterns varies significantly among different designers; for instance, one of the designers (designer #6 in the case study) is identified as the most productive designer in executing both Patterns I and II, whereas another designer (Designer #1) is found to be the most productive designer in executing both Patterns III and IV. It is also uncovered that designers, on average, spend less time running the most observed sequential patterns of design commands as they gain more experience. This research contributes: (1) to the body of knowledge by providing a novel approach to monitoring, measuring, and analyzing design productivity; and (2) to the state of practice by providing new insights into what additional design process information can be retrieved from Revit journal files. © 2017 American Society of Civil Engineers.","Building information modeling (BIM); Log data; Pattern discovery; Process mining; Productivity measurement","Application programs; Design; Edge detection; Information theory; Iterative methods; Productivity; Project management; Building Information Model - BIM; Log data; Pattern discovery; Process mining; Productivity measurements; Architectural design",2-s2.0-85032629269
"Fu Z., Wu X., Wang Q., Ren K.","Enabling Central Keyword-Based Semantic Extension Search over Encrypted Outsourced Data",2017,"IEEE Transactions on Information Forensics and Security",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028942136&doi=10.1109%2fTIFS.2017.2730365&partnerID=40&md5=f7abddedd5eae8b555ced4b84853c89a","In practice, search keywords have quite different importance when users take search operations. In addition, such keywords may have a certain grammatical relationship among them, which reflect the importance of keywords from the user's perspective intuitively. However, the existing search techniques regard the search keywords as independent and unrelated. In this paper, for the first time, we take the relation among query keywords into consideration and design a keyword weighting algorithm to show the importance of the distinction among them. By introducing the keyword weight to the search protocol design, the search results will be more in line with the user's demand. On top of this, we further design a novel central keyword semantic extension ranked scheme. By extending the central query keyword instead of all keywords, our scheme makes a good tradeoff between the search functionality and efficiency. To better express the relevance between queries and files, we further introduce the TF-IDF rule when building trapdoors and the index. In particular, our scheme supports both data set and keywords updates by using the sub-matrix technique. Our work first gives a basic idea for the design of the central keyword semantic extension ranked scheme, and then presents two secure searchable encryption schemes to meet different privacy requirements under two different threat models. Experiments on the real-world data set show that our proposed schemes are efficient, effective, and secure. © 2017 IEEE.","cloud computing; privacy protection; search; semantic search","Cloud computing; Cryptography; Search engines; Semantics; Servers; Algorithm design and analysis; Indexes; Keyword search; Privacy protection; Semantic search; Semantic Web",2-s2.0-85028942136
"Alves M., Carreira P., Costa A.A.","BIMSL: A generic approach to the integration of building information models with real-time sensor data",2017,"Automation in Construction",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030102878&doi=10.1016%2fj.autcon.2017.09.005&partnerID=40&md5=40fc77784eed557de3abebda985fb121","The surge of interest in digital building models combined with increasing sensorization of spaces is prompting an integration of Building Information Modelling (BIM) with real-time sensor data. However, current approaches reported in literature either remain theoretical or target very specific application domains, showing that there is no generic approach to assist in the creation of service and application software that combines sensor data with BIM models. The solution presented in this article addresses the engineering complexity associated with integrating sensor data with BIM by leveraging an advanced software engineering technique known as a Domain-Specific Language (DSL). We demonstrate also that the language herein proposed, named Building Information Modeling Sensor Language(BIMSL) provides substantial gains of expressiveness and ease of use in developing queries that process sensor data with complex conditions over BIM models. BIMSL is validated with experienced software developers according to a consistent evaluation protocol for DSLs focused on effectiveness, efficiency, satisfaction, and usability attributes. The results outperform the standard existing alternatives, indicating that our proposal contributes to reducing the human effort associated with integrating BIM with sensor data. © 2017 Elsevier B.V.","BIM sensor language; Building information modeling; Domain-specific language; Internet of things; Real-time data; Sensors","Application programs; Buildings; Computer programming languages; Data integration; Digital subscriber lines; Information theory; Internet of things; Modeling languages; Modems; Problem oriented languages; Sensors; Software engineering; BIM sensor language; Building Information Model - BIM; Building Information Modelling; Digital building models; Domain specific languages; Engineering techniques; Real-time data; Usability attributes; Architectural design",2-s2.0-85030102878
"Kazmi M., Schüller P., Saygın Y.","Improving scalability of inductive logic programming via pruning and best-effort optimisation",2017,"Expert Systems with Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021056454&doi=10.1016%2fj.eswa.2017.06.013&partnerID=40&md5=e412a6b9307af4cbfe10a79737fd105d","Inductive Logic Programming (ILP) combines rule-based and statistical artificial intelligence methods, by learning a hypothesis comprising a set of rules given background knowledge and constraints for the search space. We focus on extending the XHAIL algorithm for ILP which is based on Answer Set Programming and we evaluate our extensions using the Natural Language Processing application of sentence chunking. With respect to processing natural language, ILP can cater for the constant change in how we use language on a daily basis. At the same time, ILP does not require huge amounts of training examples such as other statistical methods and produces interpretable results, that means a set of rules, which can be analysed and tweaked if necessary. As contributions we extend XHAIL with (i) a pruning mechanism within the hypothesis generalisation algorithm which enables learning from larger datasets, (ii) a better usage of modern solver technology using recently developed optimisation methods, and (iii) a time budget that permits the usage of suboptimal results. We evaluate these improvements on the task of sentence chunking using three datasets from a recent SemEval competition. Results show that our improvements allow for learning on bigger datasets with results that are of similar quality to state-of-the-art systems on the same task. Moreover, we compare the hypotheses obtained on datasets to gain insights on the structure of each dataset. © 2017 Elsevier Ltd","Answer Set Programming; Chunking; Inductive logic programming; Natural Language Processing","Budget control; Computer circuits; Computer programming; Education; Logic programming; Natural language processing systems; Optimization; Answer set programming; Artificial intelligence methods; Back-ground knowledge; Chunking; Natural languages; Optimisation method; State-of-the-art system; Training example; Inductive logic programming (ILP)",2-s2.0-85021056454
"Lavbič D., Matek T., Zrnec A.","Recommender system for learning SQL using hints",2017,"Interactive Learning Environments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991457687&doi=10.1080%2f10494820.2016.1244084&partnerID=40&md5=63b7aac9e00739d289520c8fc9ab67d9","Today’s software industry requires individuals who are proficient in as many programming languages as possible. Structured query language (SQL), as an adopted standard, is no exception, as it is the most widely used query language to retrieve and manipulate data. However, the process of learning SQL turns out to be challenging. The need for a computer-aided solution to help users learn SQL and improve their proficiency is vital. In this study, we present a new approach to help users conceptualize basic building blocks of the language faster and more efficiently. The adaptive design of the proposed approach aids users in learning SQL by supporting their own path to the solution and employing successful previous attempts, while not enforcing the ideal solution provided by the instructor. Furthermore, we perform an empirical evaluation with 93 participants and demonstrate that the employment of hints is successful, being especially beneficial for users with lower prior knowledge. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","improving classroom teaching; Intelligent Tutoring Systems; interactive learning environments; programming and programming languages; recommender system; SQL learning",,2-s2.0-84991457687
"Luo L., Tong L., Zhou X., Mejino J.L.V., Jr, Ouyang C., Liu Y.","Evaluating the granularity balance of hierarchical relationships within large biomedical terminologies towards quality improvement",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032490258&doi=10.1016%2fj.jbi.2017.10.001&partnerID=40&md5=1ed10741ab1e1a0f7d8a91814e2561ae","Organizing the descendants of a concept under a particular semantic relationship may be rather arbitrarily carried out during the manual creation processes of large biomedical terminologies, resulting in imbalances in relationship granularity. This work aims to propose scalable models towards systematically evaluating the granularity balance of semantic relationships. We first utilize “parallel concepts set (PCS)” and two features (the length and the strength) of the paths between PCSs to design the general evaluation models, based on which we propose eight concrete evaluation models generated by two specific types of PCSs: single concept set and symmetric concepts set. We then apply those concrete models to the IS-A relationship in FMA and SNOMED CT's Body Structure subset, as well as to the Part-Of relationship in FMA. Moreover, without loss of generality, we conduct two additional rounds of applications on the Part-Of relationship after removing length redundancies and strength redundancies sequentially. At last, we perform automatic evaluation on the imbalances detected after the final round for identifying missing concepts, misaligned relations and inconsistencies. For the IS-A relationship, 34 missing concepts, 80 misalignments and 18 redundancies in FMA as well as 28 missing concepts, 114 misalignments and 1 redundancy in SNOMED CT were uncovered. In addition, 6,801 instances of imbalances for the Part-Of relationship in FMA were also identified, including 3,246 redundancies. After removing those redundancies from FMA, the total number of Part-Of imbalances was dramatically reduced to 327, including 51 missing concepts, 294 misaligned relations, and 36 inconsistencies. Manual curation performed by the FMA project leader confirmed the effectiveness of our method in identifying curation errors. In conclusion, the granularity balance of hierarchical semantic relationship is a valuable property to check for ontology quality assurance, and the scalable evaluation models proposed in this study are effective in fulfilling this task, especially in auditing relationships with sub-hierarchies, such as the seldom evaluated Part-Of relationship. © 2017 Elsevier Inc.","Biomedical terminology; FMA; Granularity balance; Quality assurance; SNOMED CT","Alignment; Concretes; Quality assurance; Redundancy; Semantics; Terminology; Automatic evaluation; Concrete evaluation; Creation process; Evaluation models; Project leaders; Quality improvement; Semantic relationships; SNOMED-CT; Quality control",2-s2.0-85032490258
"Yao J.-G., Wan X., Xiao J.","Recent advances in document summarization",2017,"Knowledge and Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016111879&doi=10.1007%2fs10115-017-1042-4&partnerID=40&md5=4c7f0ae434d81f374699372660b04315","The task of automatic document summarization aims at generating short summaries for originally long documents. A good summary should cover the most important information of the original document or a cluster of documents, while being coherent, non-redundant and grammatically readable. Numerous approaches for automatic summarization have been developed to date. In this paper we give a self-contained, broad overview of recent progress made for document summarization within the last 5 years. Specifically, we emphasize on significant contributions made in recent years that represent the state-of-the-art of document summarization, including progress on modern sentence extraction approaches that improve concept coverage, information diversity and content coherence, as well as attempts from summarization frameworks that integrate sentence compression, and more abstractive systems that are able to produce completely new sentences. In addition, we review progress made for document summarization in domains, genres and applications that are different from traditional settings. We also point out some of the latest trends and highlight a few possible future directions. © 2017, Springer-Verlag London.","Document summarization; Natural language generation; Natural language processing; Text mining",,2-s2.0-85016111879
"Kuşcu L., Sezer A.D.","Future prospects for gene delivery systems",2017,"Expert Opinion on Drug Delivery",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029175535&doi=10.1080%2f17425247.2017.1292248&partnerID=40&md5=8576dfbb0bff09f0e4d5b68203862f22","Introduction: Gene therapy is the challenging area of biotechnology. Despite its promise for critical diseases, it has serious safety and efficiency issues, particularly with regards to gene transfer systems. Areas covered: We examined the current situation with gene transfer systems and addressed problems this technology. We then searched patent applications about in the area from the Patentscope online system, the international patent database. We analyzed the data obtained to get a general idea about gene delivery systems designed for future use and assessed approaches for more efficient, safer and valid delivery systems. Expert opinion: When quality assurance terms are fulfilled, some of these issues (genetic changes, mutations) could be minimized during the production process. Modification of vectors for improving their efficiency and safety or development of alternative transfer systems could be the solutions for these problems. Gene transfer technologies are important for gene therapy and should demonstrate effective, target-specific and acceptable safety profiles. For this reason, searching for alternatives to current systems is a necessity. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","gene delivery systems; gene patents; Gene therapy; gene transfer","adeno associated virus vector; adenovirus vector; cell penetrating peptide; lentivirus vector; lipid; liposome; naked DNA; nanoparticle; plasmid DNA; polypeptide; poxvirus vector; retrovirus vector; virus vector; Alphavirus; cardiovascular disease; eye disease; gene delivery system; gene mutation; gene therapy; gene transfer; genetic disorder; hematologic disease; Herpes simplex virus; human; immune deficiency; infection; liver disease; malignant neoplasm; mental disease; metabolic disorder; nonhuman; nonviral gene delivery system; online system; orthopedics; polymerization; Poxviridae; Review; transposon; viral gene delivery system",2-s2.0-85029175535
"Kholghi M., Sitbon L., Zuccon G., Nguyen A.","Active learning reduces annotation time for clinical concept extraction",2017,"International Journal of Medical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026899122&doi=10.1016%2fj.ijmedinf.2017.08.001&partnerID=40&md5=5e4aafa2f9b6190a9eb0fd07c12f0e6c","Objective To investigate: (1) the annotation time savings by various active learning query strategies compared to supervised learning and a random sampling baseline, and (2) the benefits of active learning-assisted pre-annotations in accelerating the manual annotation process compared to de novo annotation. Materials and methods There are 73 and 120 discharge summary reports provided by Beth Israel institute in the train and test sets of the concept extraction task in the i2b2/VA 2010 challenge, respectively. The 73 reports were used in user study experiments for manual annotation. First, all sequences within the 73 reports were manually annotated from scratch. Next, active learning models were built to generate pre-annotations for the sequences selected by a query strategy. The annotation/reviewing time per sequence was recorded. The 120 test reports were used to measure the effectiveness of the active learning models. Results When annotating from scratch, active learning reduced the annotation time up to 35% and 28% compared to a fully supervised approach and a random sampling baseline, respectively. Reviewing active learning-assisted pre-annotations resulted in 20% further reduction of the annotation time when compared to de novo annotation. Discussion The number of concepts that require manual annotation is a good indicator of the annotation time for various active learning approaches as demonstrated by high correlation between time rate and concept annotation rate. Conclusion Active learning has a key role in reducing the time required to manually annotate domain concepts from clinical free text, either when annotating from scratch or reviewing active learning-assisted pre-annotations. © 2017 Elsevier B.V.","Active learning; Annotation time; Clinical free text; Concept extraction; Machine-assisted pre-annotation","Extraction; Active Learning; Annotation time; Concept extraction; Discharge summary; Domain concepts; Free texts; Manual annotation; Query strategies; Artificial intelligence; active learning; annotation; Article; coding; concept analysis; human; human experiment; learning; medical informatics; practice guideline; priority journal; simulation; supervised machine learning; task performance",2-s2.0-85026899122
"Moreo A., Castro J.L., Zurita J.M.","Towards portable natural language interfaces based on case-based reasoning",2017,"Journal of Intelligent Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014054791&doi=10.1007%2fs10844-017-0453-8&partnerID=40&md5=f123f830e809c7fc50754e5eeeacaf82","Natural Language Interfaces allow non-technical people to access information stored in Knowledge Bases keeping them unaware of the particular structure of the model or the underlying formal query language. Early research in the field was devoted to improve the performance of a particular system for a given Knowledge Base. Since adapting the system to new domains usually entailed considerable effort, investigating how to bring Portability to NLI became a new challenge. In this article, we investigate how Case-Based Reasoning could serve to assist the expert in porting the system so as to improve its retrieval performance. Our method HITS is based on a novel grammar learning algorithm combined with language acquisition techniques that exploit structural analogies. The learner (system) is able to engage the teacher (expert) with clarification dialogues to validate conjectures (hypotheses and deductions) about the language. Our method presents the following advantages: (i) the customization is naturally defined in the case-based cycle, (ii) the types of questions the system can deal with are not delimited in advance, and (iii) the system ‘reasons’ about precedent cases to deal with unseen questions. © 2017, Springer Science+Business Media New York.","Case-based reasoning; Natural language interfaces; Portable information systems","Knowledge based systems; Natural language processing systems; Query languages; Teaching; Based on case-based reasoning; Clarification dialogues; Knowledge base; Knowledge basis; Language acquisition; Natural language interfaces; Retrieval performance; Structural analogies; Case based reasoning",2-s2.0-85014054791
"Devezas J., Nunes S.","Information extraction for event ranking",2017,"OpenAccess Series in Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032640530&doi=10.4230%2fOASIcs.SLATE.2017.18&partnerID=40&md5=921964f886ec30749363e45fb0e444e0","Search engines are evolving towards richer and stronger semantic approaches, focusing on entity-oriented tasks where knowledge bases have become fundamental. In order to support semantic search, search engines are increasingly reliant on robust information extraction systems. In fact, most modern search engines are already highly dependent on a well-curated knowledge base. Nevertheless, they still lack the ability to e ectively and automatically take advantage of multiple heterogeneous data sources. Central tasks include harnessing the information locked within textual content by linking mentioned entities to a knowledge base, or the integration of multiple knowledge bases to answer natural language questions. Combining text and knowledge bases is frequently used to improve search results, but it can also be used for the query-independent ranking of entities like events. In this work, we present a complete information extraction pipeline for the Portuguese language, covering all stages from data acquisition to knowledge base population. We also describe a practical application of the automatically extracted information, to support the ranking of upcoming events displayed in the landing page of an institutional search engine, where space is limited to only three relevant events. We manually annotate a dataset of news, covering event announcements from multiple faculties and organic units of the institution. We then use it to train and evaluate the named entity recognition module of the pipeline. We rank events by taking advantage of identified entities, as well as partOf relations, in order to compute an entity popularity score, as well as an entity click score based on implicit feedback from clicks from the institutional search engine. We then combine these two scores with the number of days to the event, obtaining a final ranking for the three most relevant upcoming events. © José Devezas and Sérgio Nunes","Academic Events; Entity-Based Ranking; Knowledge Base Population; Named Entity Recognition; Relation Extraction","Artificial intelligence; Data acquisition; Data mining; Information analysis; Information retrieval; Information retrieval systems; Knowledge based systems; Natural language processing systems; Pipelines; Population statistics; Semantic Web; Semantics; Slate; Academic Events; Entity-Based Ranking; Knowledge base; Named entity recognition; Relation extraction; Search engines",2-s2.0-85032640530
"Bhati M., Rai R.","Nanotechnology and water purification: Indian know-how and challenges",2017,"Environmental Science and Pollution Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029518109&doi=10.1007%2fs11356-017-0066-3&partnerID=40&md5=4eafcafaae87591691d681616b05b6a1","Water contamination being ubiquitous problem across the world. A significant strata of population worldwide are still struggling to get drinkable water. This demand to develop technologies to provide clean water at affordable price is unveiling the need of rigorous research in this area. There are several technologies available for removal of persistent as well as emerging pollutants from water. Nanotechnology-based technology are providing the promising solution because of its extraordinary characteristics like large surface area, low cost maintenance and reuse, etc. During the past decade, there is an advancement in the field of nanotechnology and diligent efforts of researchers in achieving milestones in developing nanosorbents, nanostructured catalytic membranes, efficient photo catalysts, bioactive nanoparticles and new filtration regime. This article gives an overview of nanotechnology applications in water purification in India with an attempt to ponder indigenous technologies for implementation. A bibliometric approach is applied to bring the indigenous technologies available. In addition, we discuss some challenges associated with the development of convincing material and building water processing plants for purification of the wastewater. © 2017, Springer-Verlag GmbH Germany.","India; Nanotechnology; Patents; Water and wastewater; Water purification","absorption; catalyst; demand analysis; drinking water; filtration; membrane; nanotechnology; price dynamics; purification; research work; wastewater; water pollution; India",2-s2.0-85029518109
"Leblebici E., Anjorin A., Schürr A., Taentzer G.","Multi-amalgamated triple graph grammars: Formal foundation and application to visual language translation",2017,"Journal of Visual Languages and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973931496&doi=10.1016%2fj.jvlc.2016.03.001&partnerID=40&md5=8c50093f170fc6f70d06b9627c83d399","Visual languages (VLs) facilitate software development by not only supporting communication and abstraction, but also by generating various artifacts such as code and reports from the same high-level specification. VLs are thus often translated to other formalisms, in most cases with bidirectionality as a crucial requirement to, e.g., support re-engineering of software systems. Triple Graph Grammars (TGGs) are a rule-based language to specify consistency relations between two (visual) languages from which bidirectional translators are automatically derived. TGGs are formally founded but are also limited in expressiveness, i.e., not all types of consistency can be specified with TGGs. In particular, 1-to-n correspondence between elements depending on concrete input models cannot be described. In other words, a universal quantifier over certain parts of a TGG rule is missing to generalize consistency to arbitrary size. To overcome this, we transfer the well-known multi-amalgamation concept from algebraic graph transformation to TGGs, allowing us to mark certain parts of rules as repeated depending on the translation context. Our main contribution is to derive TGG-based translators that comply with this extension. Furthermore, we identify bad smells on the usage of multi-amalgamation in TGGs, prove that multi-amalgamation increases the expressiveness of TGGs, and evaluate our tool support. © 2016 Elsevier Ltd",,,2-s2.0-84973931496
"da Costa Cordeiro W.L., Marques J.A., Gaspary L.P.","Data Plane Programmability Beyond OpenFlow: Opportunities and Challenges for Network and Service Operations and Management",2017,"Journal of Network and Systems Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028999022&doi=10.1007%2fs10922-017-9423-2&partnerID=40&md5=aa2f9993efb28c7493d0932c8ef6a74f","SDN and OpenFlow reshaped the way we configure forwarding devices and determine network behavior, by offering an open interface upon which apps like routing, monitoring, etc. can be built. SDN/OpenFlow helped break network “ossification” and unleash evolution, by enabling one to effectively think networking from top-down. It evidenced, however, a subtle but important problem: networking evolution is also hampered by the inability to change switch behavior, which forces one to build systems constrained from the bottom up. This scenario can change dramatically with the re-emergence of programmable data planes, and languages like POF and P4. Although recent, these languages proved to be very influential, powering several contributions towards flexible and customizable forwarding devices. In this paper, we survey the literature on data plane programmability, from early concepts (like Active Networks) to state-of-the-art solutions. We then dive into research questions that more recent work (especially P4) have uncovered: how to deploy and manage custom switch programs in large-scale networks? Can we improve services like monitoring and security with programmable data planes? Can we make network operation more dependable? As another contribution, we organize and discuss opportunities and challenges with potential to influence research in the field, from the perspective of network and service operations and management. © 2017, Springer Science+Business Media, LLC.","FCAPS management; P4; POF; Programmable forwarding planes; SDN","Hardware; Network management; Forwarding planes; Large-scale network; Network behaviors; Network operations; Programmability; Research questions; Service operations; State of the art; Information management",2-s2.0-85028999022
"Mittal S., Gupta A., Joshi K.P., Pearce C., Joshi A.","A Question and Answering System for Management of Cloud Service Level Agreements",2017,"IEEE International Conference on Cloud Computing, CLOUD",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032206311&doi=10.1109%2fCLOUD.2017.92&partnerID=40&md5=21d5bf99bab542b2719b232e56377698","One of the key challenges faced by consumers is to efficiently manage and monitor the quality of cloud services. To manage service performance, consumers have to validate rules embedded in cloud legal contracts, such as Service Level Agreements (SLA) and Privacy Policies, that are available as text documents. Currently this analysis requires significant time and manual labor and is thus inefficient. We propose a cognitive assistant that can be used to manage cloud legal documents by automatically extracting knowledge (terms, rules, constraints) from them and reasoning over it to validate service performance. In this paper, we present this Question and Answering (Q&A) system that can be used to analyze and obtain information from the SLA documents. We have created a knowledgebase of Cloud SLAs from various providers which forms the underlying repository of our Q&A system. We utilized techniques from natural language processing and semantic web (RDF, SPARQL and Fuseki server) to build our framework. We also present sample queries on how a consumer can compute metrics such as service credit. © 2017 IEEE.",,"Cloud computing; Distributed database systems; Natural language processing systems; Semantic Web; Cloud services; Knowledge base; Legal contracts; Legal documents; Manual labors; Privacy policies; Service Level Agreements; Service performance; Outsourcing",2-s2.0-85032206311
"Sanna D., Rocchitta G., Serra M., Abbondio M., Serra P.A., Migheli R., De Luca L., Garribba E., Porcheddu A.","Synthesis of nitric oxide donors derived from piloty’s acid and study of their effects on dopamine secretion from PC12 cells",2017,"Pharmaceuticals",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029048060&doi=10.3390%2fph10030074&partnerID=40&md5=d92d967f58c8515d6a0972de358f769d","This study investigated the mechanisms and kinetics of nitric oxide (NO) generation by derivatives of Piloty’s acid (NO-donors) under physiological conditions. In order to qualitatively and quantitatively measure NO release, electron paramagnetic resonance (EPR) was carried out with NO spin trapping. In addition, voltammetric techniques, including cyclic voltammetry and constant potential amperometry, were used to confirm NO release from Piloty’s acid and its derivatives. The resulting data showed that Piloty’s acid derivatives are able to release NO under physiological conditions. In particular, electron-withdrawing substituents favoured NO generation, while electron-donor groups reduced NO generation. In vitro microdialysis, performed on PC12 cell cultures, was used to evaluate the dynamical secretion of dopamine induced by the Piloty’s acid derivatives. Although all the studied molecules were able to induce DA secretion from PC12, only those with a slow release of NO have not determined an autoxidation of DA itself. These results confirm that the time-course of NO-donors decomposition and the amount of NO released play a key role in dopamine secretion and auto-oxidation. This information could drive the synthesis or the selection of compounds to use as potential drugs for the therapy of Parkinson’s disease (PD). © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","In vitro microdialysis; Nitric oxide; Parkinson’s disease; Piloty’s acid","dopamine; nitric oxide; nitric oxide donor; piloty acid derivative; unclassified drug; amperometry; animal cell; Article; autooxidation; chemical reaction kinetics; cyclic potentiometry; decomposition; dopamine release; electron spin resonance; in vitro study; microdialysis; molecular dynamics; molecular mechanics; nonhuman; PC12 cell line; qualitative analysis; quantitative analysis; rat; spin trapping; synthesis",2-s2.0-85029048060
"Kreimeyer K., Foster M., Pandey A., Arya N., Halford G., Jones S.F., Forshee R., Walderhaug M., Botsis T.","Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review",2017,"Journal of Biomedical Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028620561&doi=10.1016%2fj.jbi.2017.07.012&partnerID=40&md5=ff4aa29c63bb09aa453ea731548c9fa1","We followed a systematic approach based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses to identify existing clinical natural language processing (NLP) systems that generate structured information from unstructured free text. Seven literature databases were searched with a query combining the concepts of natural language processing and structured data capture. Two reviewers screened all records for relevance during two screening phases, and information about clinical NLP systems was collected from the final set of papers. A total of 7149 records (after removing duplicates) were retrieved and screened, and 86 were determined to fit the review criteria. These papers contained information about 71 different clinical NLP systems, which were then analyzed. The NLP systems address a wide variety of important clinical and research tasks. Certain tasks are well addressed by the existing systems, while others remain as open challenges that only a small number of systems attempt, such as extraction of temporal information or normalization of concepts to standard terminologies. This review has identified many NLP systems capable of processing clinical free text and generating structured output, and the information collected and evaluated here will be important for prioritizing development of new approaches for clinical NLP. © 2017 Elsevier Inc.","Common data elements; Natural language processing; Review; Systematic","Data handling; Reviews; Clinical information; Common datum; Existing systems; Literature database; Structured information; Systematic; Systematic Review; Temporal information; Natural language processing systems; algorithm; data analysis; human; information retrieval; medical information system; natural language processing; priority journal; publication; Review; standardization; structure analysis; systematic review; word processing",2-s2.0-85028620561
"Vilares M., Trigo E.S., Gomez-Rodríguez C., Alonso M.A.","Language technologies for opinion analysis in social networks [Tecnologías de la lengua para analisis de opiniones en redes sociales]",2017,"Procesamiento de Lenguaje Natural",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028817755&partnerID=40&md5=a83feae9c17826247d05e5cbc1726768","The recent popularization of social media based on microtexts, among which Twitter stands out, has enabled a globalization of the expression of opinions. Although microtexts present some specific lexical and syntactic properties that differ from those of standard text, certain basic aspects of language must be respected so that they are intelligible. In this project, we propose to exploit this fact in order to improve the linguistic support for processing microtexts in our natural sphere of interest: the Spanish and Galician languages. To do so, it will be necessary to improve the performance of current parsing and analysis techniques on standard text, to design mechanisms so that models and methods effective for analyzing standard language can be adapted to microtexts, and to project effective models, methods and resources across languages. © 2017 Sociedad Espanola para el Procesamiento del Lenguaje Natural.","Opinion mining; Parsing; Sentiment analysis; Universal dependencies",,2-s2.0-85028817755
"Wang Y., Winter S., Ronald N.","How much is trust: The cost and benefit of ridesharing with friends",2017,"Computers, Environment and Urban Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021261195&doi=10.1016%2fj.compenvurbsys.2017.06.002&partnerID=40&md5=2523bc9c422da8978d392a32ec8df3d8","Ridesharing with social contacts (i.e., ‘friends’) is substantially more accepted than with strangers. However, limiting ridesharing to friends while rejecting strangers also reduces ride choices and increases detour costs. This work studies, from a theoretical perspective, whether the additional detour costs of limiting shared rides to social network contacts would be prohibitive. It proposes a social network based ridesharing algorithm with heterogeneous detour tolerances for varied social contacts. The theoretical matching rates and detour costs are compared in a simulation for three levels of social connectivity: travelling with direct contacts only, with direct and indirect contacts, or with anyone. The simulation allows for a systematic and comprehensive testing of system behaviour when varying the parameters of social network structure, detour tolerance, and spatial distribution of friendship. Results show that for a clustered friendship – the expected spatial distribution of a social network growing with a ridesharing network – ridesharing with friends does not cause significantly higher costs. Furthermore, the algorithm prioritising friends can substantially increase the matching of friends. An empirical study justifies the findings. © 2016 Elsevier Ltd","Agent-based simulation; Collaborative travel; Complex travel behaviour; Ridesharing; Social network; Trust","Costs; Social networking (online); Spatial distribution; Agent based simulation; Collaborative travel; Ride-sharing; Travel behaviour; Trust; Complex networks; algorithm; cost-benefit analysis; social network; spatial distribution; travel behavior",2-s2.0-85021261195
"Ai H.","Providing graduated corrective feedback in an intelligent computer-assisted language learning environment",2017,"ReCALL",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019614934&doi=10.1017%2fS095834401700012X&partnerID=40&md5=8b3b8c0681442646d5666b2100e81dda","Corrective feedback (CF), a response to linguistic errors made by second language (L2) learners, has received extensive scholarly attention in second language acquisition. While much of the previous research in the field has focused on whether CF facilitates or impedes L2 development, few studies have examined the efficacy of gradually modifying the explicitness or specificity of CF as a function of a learner's response to the feedback. Yet, the type and extent of CF needed by a learner, as suggested by Vygotsky (1978), sheds light on whether a learner is developing his or her abilities in a particular area and the ways in which they do it. This paper reports on a study that explores the design, effectiveness and learners' perception toward a graduated (Aljaafreh & Lantolf, 1994) approach to CF, i.e., feedback that progresses from very general and implicit to very specific and explicit, in an intelligent computer-assisted language learning (ICALL) environment. The results show that the graduated approach to CF is effective in helping learners to self-identify and self-correct a number of grammatical issues, although an onsite tutor provides necessary remedies when the ICALL system occasionally fails to do its part. Implications for CF research, particularly on the notion of individualized feedback, are also discussed. Copyright © European Association for Computer Assisted Language Learning 2017.","Chinese as a foreign language; corrective feedback; individualized feedback; intelligent computer-assisted language learning; sociocultural theory; the ba-construction",,2-s2.0-85019614934
"Zeldes A.","The GUM corpus: creating multilayer resources in the classroom",2017,"Language Resources and Evaluation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957605897&doi=10.1007%2fs10579-016-9343-x&partnerID=40&md5=1eca6ef1fede0960c8f4da6aae3daec4","This paper presents the methodology, design principles and detailed evaluation of a new freely available multilayer corpus, collected and edited via classroom annotation using collaborative software. After briefly discussing corpus design for open, extensible corpora, five classroom annotation projects are presented, covering structural markup in TEI XML, multiple part of speech tagging, constituent and dependency parsing, information structural and coreference annotation, and Rhetorical Structure Theory analysis. Layers are inspected for annotation quality and together they coalesce to form a richly annotated corpus that can be used to study the interactions between different levels of linguistic description. The evaluation gives an indication of the expected quality of a corpus created by students with relatively little training. A multifactorial example study on lexical NP coreference likelihood is also presented, which illustrates some applications of the corpus. The results of this project show that high quality, richly annotated resources can be created effectively as part of a linguistics curriculum, opening new possibilities not just for research, but also for corpora in linguistics pedagogy. © 2016, Springer Science+Business Media Dordrecht.","Classroom annotation; Coreference; Information structure; Multilayer corpora; Parsing; Treebank",,2-s2.0-84957605897
"Gonzalez-Lara M.F., Sifuentes-Osornio J., Ostrosky-Zeichner L.","Drugs in Clinical Development for Fungal Infections",2017,"Drugs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028324043&doi=10.1007%2fs40265-017-0805-2&partnerID=40&md5=c58db75f6781cc4462e3bfe9ca7c82a8","Despite increasing rates of invasive fungal infections being reported globally, only a single antifungal drug has been approved during the last decade. Resistance, toxicity, drug interactions and restricted routes of administration remain unresolved issues. This review focuses on new antifungal compounds which are currently in various clinical phases of development. We discuss two azoles with a tetrazole moiety that allows selective activity against the fungal CYP: VT-1161 for Candida infections and VT-1129 for cryptococcal meningoencephalitis. We also discuss two glucan synthesis inhibitors: CD101, an echinocandin with an increased half-life, and SCY-078 with oral bioavailability and increased activity against echinocandin-resistant isolates. Among the polyenes, we discuss MAT023, an encochleated amphotericin B formulation that allows oral administration. Two novel classes of antifungal drugs are also described: glycosylphosphatidylinositol inhibitors, and the leading drug APX001, which disrupt the integrity of the fungal wall; and the orotomides, inhibitors of pyrimidine synthesis with the leading drug F901318. Finally, a chitin synthesis inhibitor and progress on human monoclonal antifungal antibodies are discussed. © 2017, Springer International Publishing AG.",,"amphotericin B; antifungal agent; apx 001; efungumab; f 901318; fungal enzyme; glycosyltransferase inhibitor; mat 2203; monoclonal antibody; monoclonal antibody 18B7; nikkomycin Z; pyrrole derivative; quilseconazole; rezafungin; scy 078; sterol 14alpha demethylase; unclassified drug; vt 1161; antifungal therapy; Article; candidiasis; cryptococcal meningitis; drug development; drug formulation; drug mechanism; enzyme inhibition; human; multicenter study (topic); mycosis; nonhuman; phase 1 clinical trial (topic); phase 2 clinical trial (topic); randomized controlled trial (topic)",2-s2.0-85028324043
"Volk M., Bosse S., Turowski K.","Providing clarity on big data technologies: A structured literature review",2017,"Proceedings - 2017 IEEE 19th Conference on Business Informatics, CBI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029438529&doi=10.1109%2fCBI.2017.26&partnerID=40&md5=f76d2996b9ecdb94e84323b377901da4","The success of big data projects depends heavily on the ability to decide which technological expertise and architectures are required in such a context. However, there is confusion in scientific discourse regarding the systemization of big data technologies which exacerbates this problem. Therefore, in this paper, a structured literature review is conducted to assess the current state of the art and give an overview about big data technology classifications. It can be stated that only very limited approaches for classifications exist, which are partially incomplete or address only single aspects of big data technologies, such as the used database type. Conducting this literature review, investigations have also been made to derive possible starting points for extending existing or developing new classification approaches, which can form the basis of a future big data classification framework. Such a concept could be used to improve the decision-support for planning and managing big data projects. However, a common understanding would require unambiguous definitions of the term technology and various other expressions, which are used in the literature as a synonym today. © 2017 IEEE.","Big Data Technologie; Classification; Decision Support; Literature Review","Classification (of information); Decision support systems; Classification approach; Data classification; Data technologies; Decision supports; Literature reviews; Scientific discourse; State of the art; Big data",2-s2.0-85029438529
"Jiang M., Shang J., Cassidy T., Ren X., Kaplan L.M., Hanratty T.P., Han J.","MetaPAD: Meta pattern discovery from massive text corpora",2017,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029103619&doi=10.1145%2f3097983.3098105&partnerID=40&md5=e5501feb897a8fd2529c8cc03c268eea","Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets - their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction. © 2017 ACM.",,"Information management; Context-Aware; Dependency parsing; High quality; Pattern discovery; Pattern quality; Segmentation methods; Text corpora; Textual patterns; Data mining",2-s2.0-85029103619
"Narayana S., Sivaraman A., Nathan V., Goyal P., Arun V., Alizadeh M., Jeyakumar V., Kim C.","Language-directed hardware design for network performance monitoring",2017,"SIGCOMM 2017 -  Proceedings of the 2017 Conference of the ACM Special Interest Group on Data Communication",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029441157&doi=10.1145%2f3098822.3098829&partnerID=40&md5=6ffc2cade9067ead5a71b8b8b8a28ce2","Network performance monitoring today is restricted by existing switch support for measurement, forcing operators to rely heavily on endpoints with poor visibility into the network core. Switch vendors have added progressively more monitoring features to switches, but the current trajectory of adding specific features is unsustainable given the ever-changing demands of network operators. Instead, we ask what switch hardware primitives are required to support an expressive language of network performance questions. We believe that the resulting switch hardware design could address a wide variety of current and future performance monitoring needs. We present a performance query language, Marple, modeled on familiar functional constructs like map, filter, groupby, and zip. Marple is backed by a new programmable key-value store primitive on switch hardware. The key-value store performs flexible aggregations at line rate (e.g., a moving average of queueing latencies per flow), and scales to millions of keys. We present a Marple compiler that targets a P4-programmable software switch and a simulator for highspeed programmable switches. Marple can express switch queries that could previously run only on end hosts, while Marple queries only occupy a modest fraction of a switch's hardware resources. © 2017 ACM.","Network hardware; Network measurement; Network programming","Computer programming; Computer software; Convolutional codes; Network performance; Program compilers; Query languages; Query processing; Flexible aggregation; Future performance; Hardware resources; Network measurement; Network performance monitoring; Network programming; Programmable software; Programmable switches; Hardware",2-s2.0-85029441157
"Da San Martino G., Romeo S., Barŕon-Cedeño A., Joty S., Marquez L., Moschitti A., Nakov P.","Cross-language question re-ranking",2017,"SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029385783&doi=10.1145%2f3077136.3080743&partnerID=40&md5=c77f0e9f059d65997151f1b6bb1ed6a3","We study how to find relevant questions in community forums when the language of the newquestions is different from that of the existing questions in the forum. In particular, we explore the Arabic-English language pair. We compare a kernel-based system with a feed-forward neural network in a scenario where a large parallel corpus is available for training a machine translation system, bilingual dictionaries, and cross-language word embeddings. We observe that both approaches degrade the performance of the system when working on the translated text, especially the kernel-based system, which depends heavily on a syntactic kernel. We address this issue using a cross-language tree kernel, which compares the original Arabic tree to the English trees of the related questions. We show that this kernel almost closes the performance gap with respect to the monolingual system. On the neural network side, we use the parallel corpus to train cross-language embeddings, which we then use to represent the Arabic input and the English related questions in the same space.The results also improve to close to those of the monolingual neural network. Overall, the kernel system shows a better performance compared to the neural network in all cases. © 2017 ACM.","Community Question Answering; Cross-language Approaches; Distributed Representations; Kernel-based Methods; Neural Networks; Question Retrieval","Forestry; Information retrieval; Linguistics; Neural networks; Community question answering; Cross languages; Distributed representation; Kernel based methods; Question Retrieval; Translation (languages)",2-s2.0-85029385783
"Ababou N., Mazroui A., Belehbib R.","Parsing Arabic Nominal sentences using context free grammar and fundamental rules of classical grammar",2017,"International Journal of Intelligent Systems and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026729619&doi=10.5815%2fijisa.2017.08.02&partnerID=40&md5=d04c6e278a78178833df5abeee3935b8","This work falls within the framework of the Arabic natural language processing. We are interested in parsing Arabic texts. Existing parsers generate parse trees that give an idea about the structure of the sentence without considering the syntactic functions specific to the Arabic language. Thus, the results are still insufficient in terms of syntactic information. The system we have developed in this article takes into consideration all these syntactic functions. This system begins with a morphological analysis in the context. Then, it uses a CFG grammar to extract the phrases and ends by exploiting the formalism of unification grammar and traditional grammar to combine these phrases and generate the final sentence structure. © 2017 MECS.","Arabic phrase; Grammar; Parser; POS tagger; Syntactic functions; Syntax tree",,2-s2.0-85026729619
"Plovie B., Yang Y., Guillaume J., Dunphy S., Dhaenens K., Van Put S., Vandecasteele B., Vervust T., Bossuyt F., Vanfleteren J.","Arbitrarily Shaped 2.5D Circuits using Stretchable Interconnects Embedded in Thermoplastic Polymers",2017,"Advanced Engineering Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018569005&doi=10.1002%2fadem.201700032&partnerID=40&md5=dbb801557fc3cc427430228ed0cbbc17","A method to fabricate thermoplastically deformable electronic circuits is presented, with the intent of achieving low-cost 2.5D free-form rigid smart objects. This by utilizing existing flexible circuit technology based stretchable circuits, in combination with thermoplastic materials. After fabricating the circuit in a flat state, a thermoforming step shapes the device by heating it beyond its glass transition temperature, and pushing it against a mold. Preliminary tests show the feasibility to fabricate simple circuits using off-the-shelf circuit components; showing a minimal decrease in conductivity of the polyimide supported copper-based interconnects. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim",,"Glass transition; Reinforced plastics; Thermoforming; Timing circuits; Circuit components; Copper-based; Deformable electronics; Flexible circuit; Simple circuits; Smart objects; Thermoplastic materials; Thermoplastic polymer; Flexible electronics",2-s2.0-85018569005
"Utomo F.S., Suryana N., Azmi M.S.","Question answering system: A review on question analysis, document processing, and answer extraction techniques",2017,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026777588&partnerID=40&md5=f703cc13b423c73f1db4bd86726076a7","Question Answering System could automatically provide an answer to a question posed by human in natural languages. This system consists of question analysis, document processing, and answer extraction module. Question Analysis module has task to translate query into a form that can be processed by document processing module. Document processing is a technique for identifying candidate documents, containing answer relevant to the user query. Furthermore, answer extraction module receives the set of passages from document processing module, then determine the best answers to user. Challenge to optimize Question Answering framework is to increase the performance of all modules in the framework. The performance of all modules that has not been optimized has led to the less accurate answer from question answering systems. Based on this issues, the objective of this study is to review the current state of question analysis, document processing, and answer extraction techniques. Result from this study reveals the potential research issues, namely morphology analysis, question classification, and term weighting algorithm for question classification. © 2005 – ongoing JATIT & LLS.","Information retrieval; Natural language processing; Question analysis; Question answering",,2-s2.0-85026777588
"Cabaleiro B., Peñas A., Manandhar S.","Grounding proposition stores for question answering over linked data",2017,"Knowledge-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018939241&doi=10.1016%2fj.knosys.2017.04.016&partnerID=40&md5=52a63cf40f4ba66e2a794ef049e4c2d6","Grounding natural language utterances into semantic representations is crucial for tasks such as question answering and knowledge base population. However, the importance of the lexicons that are central to this mapping remains unmeasured because question answering systems are evaluated as end-to-end systems. This article proposes a methodology to enable a standalone evaluation of grounding natural language propositions into semantic relations by fixing all the components of a question answering system other than the lexicon itself. Thus, we can explore different configurations trying to conclude which are the ones that contribute better to improve overall system performance. Our experiments show that grounding accounts with close to 80% of the system performance without training, whereas training supposes a relative improvement of 7.6%. Finally we show how lexical expansion using external linguistic resources can consistently improve the results from 0.8% up to 2.5%. © 2017 Elsevier B.V.","Grounding; Linked data; Question answering; Semantic parsing","Artificial intelligence; Data handling; Electric grounding; Knowledge based systems; Semantics; Linguistic resources; Linked datum; Natural languages; Question Answering; Question answering systems; Semantic parsing; Semantic relations; Semantic representation; Natural language processing systems",2-s2.0-85018939241
"López Condori R.E., Salgueiro Pardo T.A.","Opinion summarization methods: Comparing and extending extractive and abstractive approaches",2017,"Expert Systems with Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013168475&doi=10.1016%2fj.eswa.2017.02.006&partnerID=40&md5=553cacd231e4b5e983d20af0808331e0","In the last years, the opinion summarization task has gained much importance because of the large amount of online information and the increasing interest in learning the user evaluation about products, services, companies, and people. Although there are many works in this area, there is room for improvement, as the results are far from ideal. In this paper, we present our investigations to generate extractive and abstractive summaries of opinions. We study some well-known methods in the area and compare them. Besides using these methods, we also develop new methods that consider the main advantages of the ones before. We evaluate them according to three traditional summarization evaluation measures: informativeness, linguistic quality, and utility of the summary. We show that we produce interesting results and that our methods outperform some methods from literature. © 2017 Elsevier Ltd","Aspect-based approach; Extractive and abstractive summarization; Opinion summarization","Information systems; Mathematical models; Aspect-based approach; Evaluation measures; Extractive and abstractive summarizations; Informative ness; Large amounts; nocv1; On-line information; Opinion summarization; User evaluations; Quality control",2-s2.0-85013168475
"Chuqiao Y., Bessmertny I.A.","Shallow syntactic analysis of Chinese texts",2017,"3rd IEEE International Conference on ",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027515875&doi=10.1109%2fCIACT.2017.7977287&partnerID=40&md5=2c332af4c1352b31a324544ca6f35217","The paper considers a problem of automatic processing of natural language Chinese texts. One of the pressing tasks in this area is automatic fact acquisition from text documents by a query because existing automatic translators are useless at this task. The goal of the work is direct extraction of facts from the text in the original language without its translation. The suggested approach consists of syntactic analysis of sentences with subsequent matching of parts of speech found with a formalized query in the form of subject-object-predicate. A distinctive feature of the proposed algorithm of syntactic analysis is the absence of phase of segmentation into words for the sequence of hieroglyphs that make up the sentences. The bottleneck at this task is a dictionary because the correct interpretation of a phrase can be impossible when a word is absent in the dictionary. To eliminate this problem, we propose to identify a sentence model by function words while limitedness of the dictionary could be compensated by an automatic building of a subject area thesaurus and a dictionary of common words using statistical processing of a document corpus. The suggested approach was approved on a small topic area with a limited dictionary where it demonstrates its robustness. The analysis of temporal characteristics of the developed algorithm was carried out as well. As the proposed algorithm uses a naive inference, the parsing speed at real tasks could be unacceptable low, and this should become a subject for further research. © 2017 IEEE.","Chinese texts; function words; grammatical model; NLP; shallow parsing","Artificial intelligence; Context free grammars; Inference engines; Translation (languages); Automatic processing; Automatic translators; Chinese text; Function words; Grammatical models; Shallow parsing; Statistical processing; Temporal characteristics; Syntactics",2-s2.0-85027515875
"Fatima A., Ghazi A., Luca C.","Semantic graph from free-Text",2017,"Proceedings - 2017 International Conference on Optimization of Electrical and Electronic Equipment, OPTIM 2017 and 2017 Intl Aegean Conference on Electrical Machines and Power Electronics, ACEMP 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027708788&doi=10.1109%2fOPTIM.2017.7975124&partnerID=40&md5=80c7233cd5f8264ba300f895ecf16ac4","The amount of data still stored in HTML documents or in a semi-structured format is considerably vast even if an alternative, structured data or linked data, exists. The migration of the data to a structured format is not easy and the current search engines can't reach all types of data with one query. This paper introduces a new technique to convert free-Text into semantic graphs that can be then translated to any RDF format. The proposed system also generates ontology-based keywords for the text, which are tagged with related ontologies. These keywords could help the search engines to identify semantics of unstructured text. © 2017 IEEE.","Free-Text; Ontologies; Semantic graph","Electric machinery; Electronic equipment; Graphic methods; Ontology; Oscillators (electronic); Power electronics; Semantic Web; Semantics; Free texts; HTML documents; Linked datum; Ontology-based; Semantic graphs; Semi-structured; Structured data; Unstructured texts; Search engines",2-s2.0-85027708788
"Bila W.C., Mariano R.M.S., Silva V.R., dos Santos M.E.S.M., Lamounier J.A., Ferriolli E., Galdino A.S.","Applications of deuterium oxide in human health",2017,"Isotopes in Environmental and Health Studies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011632738&doi=10.1080%2f10256016.2017.1281806&partnerID=40&md5=2cca962c178f02b46481d31b9d4d3f7d","The main aim goal of this review was to gather information about recent publications related to deuterium oxide (D2O), and its use as a scientific tool related to human health. Searches were made in electronic databases Pubmed, Scielo, Lilacs, Medline and Cochrane. Moreover, the following patent databases were consulted: EPO (Espacenet patent search), USPTO (United States Patent and Trademark Office) and Google Patents, which cover researches worldwide related to innovations using D2O. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Application research; deuterium oxide; human health; hydrogen-2; isotope biochemistry; isotope effects; patents; stable isotope pharmaceuticals","biochemistry; deuterium; drug; innovation; public health; stable isotope; Syringa; deuterium oxide; biotechnology; chemistry; human; medical research; procedures; Biomedical Research; Biotechnology; Deuterium Oxide; Humans",2-s2.0-85011632738
"Tompson P., Ananiadou S.","Extracting gene-disease relations from text to support biomarker discovery",2017,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025443922&doi=10.1145%2f3079452.3079472&partnerID=40&md5=5631996ed8d26e24427000446955fb13","The biomedical literature constitutes a rich source of evidence to support the discovery of biomarkers. However, locating evidence in huge volumes of text can be difficult, as typical keyword queries cannot account for the meaning and structure of text. Text mining (TM) methods carry out automated semantic analysis of documents, to facilitate structured searching that can more precisely match users' information needs. We describe our TM approach to the detection of sentence-level associations between genes and diseases, as a first step towards developing a sophisticated search system targeted at locating biomarker evidence in the literature. We vary the sophistication of our detection methodology according to sentence complexity, using either co-occurring mentions of genes and diseases, or linguistic patterns obtained using evidence from approximately 1 million biomedical abstracts. We demonstrate that this method can detect associations more successfully than applying a single technique, with an accuracy that compares highly favourably to related efforts. We also show that the identified relations can complement those detected using alternative approaches. © 2017 Copyright is held by the owner/author(s).","Biomarkers; Dependency grammar; Gene-disease relations; Text mining","Data mining; Genes; Semantics; Syntactics; Text processing; Bio-marker discovery; Biomedical abstracts; Biomedical literature; Dependency grammar; Linguistic patterns; Semantic analysis; Structure of text; Text mining; Biomarkers",2-s2.0-85025443922
"Nawab R.M.A., Stevenson M., Clough P.","An IR-Based Approach Utilizing Query Expansion for Plagiarism Detection in MEDLINE",2017,"IEEE/ACM Transactions on Computational Biology and Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029589475&doi=10.1109%2fTCBB.2016.2542803&partnerID=40&md5=54ea3a22b4750cc13323b6b51f72d735","The identification of duplicated and plagiarized passages of text has become an increasingly active area of research. In this paper, we investigate methods for plagiarism detection that aim to identify potential sources of plagiarism from MEDLINE, particularly when the original text has been modified through the replacement of words or phrases. A scalable approach based on Information Retrieval is used to perform candidate document selection-the identification of a subset of potential source documents given a suspicious text-from MEDLINE. Query expansion is performed using the ULMS Metathesaurus to deal with situations in which original documents are obfuscated. Various approaches to Word Sense Disambiguation are investigated to deal with cases where there are multiple Concept Unique Identifiers (CUIs) for a given term. Results using the proposed IR-based approach outperform a state-of-The-Art baseline based on Kullback-Leibler Distance. © 2004-2012 IEEE.","extrinsic plagiarism detection; information retrieval; medline; Natural language processing; query expansion; umls metathesaurus","Information retrieval; Intellectual property; Document selection; Kullback-Leibler distance; Medline; Plagiarism detection; Query expansion; UMLS metathesaurus; Unique identifiers; Word Sense Disambiguation; Natural language processing systems",2-s2.0-85029589475
"Vanhove T., Sebrechts M., Van Seghbroeck G., Wauters T., Volckaert B., De Turck F.","Data transformation as a means towards dynamic data storage and polyglot persistence",2017,"International Journal of Network Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018726454&doi=10.1002%2fnem.1976&partnerID=40&md5=7dfd457ec78cfab5d48a3334e9f11c7e","Legacy applications have been built around the concept of storing their data in one relational data store. However, with the current differentiation in data store technologies as a consequence of the NoSQL paradigm, new and possibly more performant storage solutions are available to all applications. The concept of dynamic storage makes sure that application data are always stored in the most optimal data store at a given time to increase application performance. Additionally, polyglot persistence aims to push this performance even further by storing each different data type of an application in the data store technology best suited for it. To get legacy applications into dynamic storage and polyglot persistence, schema and data transformations between data store technologies are needed. This usually infers application redesigns as well to support the new data stores. This paper proposes such a transformation approach through a canonical model. It is based on the Lambda architecture to ensure no application downtime is needed during the transformation process, and after the transformation, the application can continue to query in the original query language, thus requiring no application code changes. Copyright © 2017 John Wiley & Sons, Ltd.","big data; data transformation; dynamic data storage; lambda architecture; polyglot persistence","Big data; Data storage equipment; Digital storage; Memory architecture; Query languages; Application codes; Application performance; Canonical modeling; Data transformation; Dynamic data; Legacy applications; Polyglot persistence; Transformation process; Metadata",2-s2.0-85018726454
"Lev-Tov H.","How microneedles can change cutaneous drug delivery - Small needles make a big difference",2017,"JAMA Dermatology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024113694&doi=10.1001%2fjamadermatol.2017.1039&partnerID=40&md5=98312eabbc0af2c12328de87f081a495",[No abstract available],,"aminolevulinic acid; actinic keratosis; drug penetration; Editorial; human; hypodermic needle; incubation time; microneedle; microtechnology; permeability barrier; photodynamic therapy; priority journal; skin absorption; skin water loss; transdermal drug delivery system",2-s2.0-85024113694
"Nair P.M., Pandya S.G., Dallo S.F., Reddoch K.M., Montgomery R.K., Pidcoke H.F., Cap A.P., Ramasubramanian A.K.","Platelets stored at 4°C contribute to superior clot properties compared to current standard-of-care through fibrin-crosslinking",2017,"British Journal of Haematology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020220016&doi=10.1111%2fbjh.14751&partnerID=40&md5=a2c51ab595a87fcfae3a093769b5a396","Currently, platelets for transfusion are stored at room temperature (RT) for 5–7 days with gentle agitation, but this is less than optimal because of loss of function and risk of bacterial contamination. We have previously demonstrated that cold (4°C) storage is an attractive alternative because it preserves platelet metabolic reserves, in vitro responses to agonists of activation, aggregation and physiological inhibitors, as well as adhesion to thrombogenic surfaces better than RT storage. Recently, the US Food and Drug Administration clarified that apheresis platelets stored at 4°C for up to 72 h may be used for treating active haemorrhage. In this work, we tested the hypothesis that cold-stored platelets contribute to generating clots with superior mechanical properties compared to RT-stored platelets. Rheological studies demonstrate that the clots formed from platelets stored at 4°C for 5 days are significantly stiffer (higher elastic modulus) and stronger (higher critical stress) than those formed from RT-stored platelets. Morphological analysis shows that clot fibres from cold-stored platelets were denser, thinner, straighter and with more branch points or crosslinks than those from RT-stored platelets. Our results also show that the enhanced clot strength and packed structure is due to cold-induced plasma factor XIII binding to platelet surfaces, and the consequent increase in crosslinking. © 2017 John Wiley & Sons Ltd","clot strength; factor XIII; refrigeration; rheology; ultrastructure","blood clotting factor 13; cytochalasin; eptifibatide; thrombin; analytical parameters; Article; blood clotting; blood clotting test; cell structure; cell surface; clot network structure; clot strength; clot ultrastructure; comparative study; controlled study; critical stress; cross linking; cryopreservation; fibrin crosslinking; flow kinetics; hematological parameters; human; human cell; in vitro study; normal human; polymerization; priority journal; protein binding; relative crosslinking density; room temperature; scanning electron microscopy; storage temperature; storage time; thrombocyte activation; thrombocyte adhesion; thrombocyte aggregation; thrombocyte preservation; thrombocytopheresis; time; viscous modulus; Young modulus",2-s2.0-85020220016
"Brogueira G., Batista F., Carvalho J.P.","A smart system for twitter corpus collection, management and visualization",2017,"International Journal of Technology and Human Interaction",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019119190&doi=10.4018%2fIJTHI.2017070102&partnerID=40&md5=f3cc1226662358c895adc9290e02451c","Social networks have become popular and are now becoming an alternate mean of communication, used to share information on various topics, ranging from politics or sports to simple aspects of everyday life. Twitter messages (tweets) are shared in real time and are essentially public, making them a useful source of information for areas such as tourism, marketing, health, and safety. This paper describes an information system that involves the creation and storage of a corpus of tweets, written in European Portuguese and published within the Portuguese territory. The system also involves a REST API that allows access to the stored information, and a web-based dashboard that makes it possible to analyze and visualize indicators concerning the stored data. Copyright © 2017, IGI Global.","Geolocation; Information system; MongoDB; Social networks; Twitter","Digital storage; Information systems; Geolocations; MongoDB; Real time; Smart System; Twitter; Web based; Social networking (online)",2-s2.0-85019119190
"Shriver D., Elbaum S., Stolee K.T.","At the end of synthesis: narrowing program candidates",2017,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Results Track, ICSE-NIER 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026739903&doi=10.1109%2fICSE-NIER.2017.7&partnerID=40&md5=fe0bc4f56ce7b6fc25c3a27c48b83ff6","Program synthesis is succeeding in supporting the generation of programs within increasingly complex domains. The use of weaker specifications, such as those consisting of input/output examples or test cases, has helped to fuel the success of program synthesis by lowering adoption barriers. Yet, employing weaker specifications has the side effect of generating a potentially large number of candidate programs. This was not a problem for simpler and smaller program domains, but it is becoming evident that differentiating among many synthesized programs is a challenge that needs addressing. We sketch an approach to mitigate this challenge, requiring less effort from the user while automatically identifying inputs that can differentiate clusters of synthesized programs. The approach has the potential to more cost-effectively narrow the space of candidate programs in a range of synthesis applications. © 2017 IEEE.","input generation; program synthesis; pruning","Software engineering; Software testing; Specifications; Adoption barriers; Complex domains; input generation; Input/output; Program synthesis; pruning; Side effect; Test case; Application programs",2-s2.0-85026739903
"Krallinger M., Rabal O., Lourenço A., Oyarzabal J., Valencia A.","Information retrieval and text mining technologies for chemistry",2017,"Chemical Reviews",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022005598&doi=10.1021%2facs.chemrev.6b00851&partnerID=40&md5=99102ef3228da76a3c50f5c60ede412a","Efficient access to chemical information contained in scientific literature, patents, technical reports, or the web is a pressing need shared by researchers and patent attorneys from different chemical disciplines. Retrieval of important chemical information in most cases starts with finding relevant documents for a particular chemical compound or family. Targeted retrieval of chemical documents is closely connected to the automatic recognition of chemical entities in the text, which commonly involves the extraction of the entire list of chemicals mentioned in a document, including any associated information. In this Review, we provide a comprehensive and in-depth description of fundamental concepts, technical implementations, and current technologies for meeting these information demands. A strong focus is placed on community challenges addressing systems performance, more particularly CHEMDNER and CHEMDNER patents tasks of BioCreative IV and V, respectively. Considering the growing interest in the construction of automatically annotated chemical knowledge bases that integrate chemical information and biological data, cheminformatics approaches for mapping the extracted chemical names into chemical structures and their subsequent annotation together with text mining applications for linking chemistry with biological information are also presented. Finally, future trends and current challenges are highlighted as a roadmap proposal for research in this emerging field. © 2017 American Chemical Society.",,"Character recognition; Electronic document exchange; Indicators (chemical); Information retrieval; Patents and inventions; Automatic recognition; Biological information; Chemical information; Current technology; Fundamental concepts; Scientific literature; Systems performance; Technical implementation; Data mining; chemistry; data mining; documentation; procedures; technology; Chemistry; Data Mining; Documentation; Technology",2-s2.0-85022005598
"Chen H., Trouve A., Murakami K.J., Fukuda A.","Semantic image retrieval for complex queries using a knowledge parser",2017,"Multimedia Tools and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021082907&doi=10.1007%2fs11042-017-4932-2&partnerID=40&md5=e4226c0325b82ed66554cfafc8b4a6d3","In order to improve the retrieval accuracy of image retrieval systems, research focus has been shifted from designing sophisticated low-level feature extraction algorithms to combining image retrieval processing with rich semantics and knowledge-based methods. In this paper, we aim at improving text-based image retrieval for complex natural language queries by using a semantic parser (Knowledge Parser or K-Parser). From text written in natural language, the K-parser extracts a graphical semantic representation of the objects involved, their properties as well as their relations. We analyze both the image textual captions and the natural language queries with the K-parser. As a technical solution, we leverage RDF in two ways: first, we store the parsed image captions as RDF triples; second, we translate image queries into SPARQL queries. When applied to the Flickr8k dataset with a set of 16 custom queries, we notice that the K-parser exhibits some biases that negatively affect the accuracy of the queries. We propose two techniques to address the weaknesses: (1) we introduce a set of rules to transform the output of K-parser and fix some basic, recurrent parsing mistakes that occur on the captions of Flickr8k; (2) we leverage two popular commonsense knowledge databases, ConceptNet and WordNet, to raise the accuracy of queries on broad concepts. Using those two techniques, we can fix most of the initial retrieval errors, and accurately execute our set of 16 queries on the Flickr8k dataset. © 2017 Springer Science+Business Media, LLC","Commonsense knowledge; Image retrieval; K-parser; Object retrieval; RDF","Computational linguistics; Information retrieval; Knowledge based systems; Knowledge management; Natural language processing systems; Search engines; Semantic Web; Semantics; Commonsense knowledge; Image retrieval systems; Knowledge-based methods; Natural language queries; Object retrieval; Semantic image retrieval; Semantic representation; Text-based image retrievals; Image retrieval",2-s2.0-85021082907
"Jia R., Heck L., Hakkani-Tür D., Nikolov G.","Learning concepts through conversations in spoken dialogue systems",2017,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023768147&doi=10.1109%2fICASSP.2017.7953253&partnerID=40&md5=d816956c0d0cbd71ba3062ab5086b57a","Spoken dialogue systems must be able to recover gracefully from unexpected user inputs. In many cases, these unexpected utterances may be within the scope of the system, but include previously unseen phrases that the system cannot interpret. In this work, we augment a spoken dialogue system with the ability to learn about new concepts by conversing with the user in natural language. We present a novel model that detects phrases corresponding to such concepts, using information from a neural slotfiller as well as syntactic cues. The system then prompts the user for a definition of the detected phrases, and uses these definitions to re-parse the original utterance. We demonstrate significant gains by learning from the user, compared to a baseline system. © 2017 IEEE.","interactive learning; Spoken dialogue systems",,2-s2.0-85023768147
"Bastani O., Sharma R., Aiken A., Liang P.","Synthesizing program input grammars",2017,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024493993&doi=10.1145%2f3062341.3062349&partnerID=40&md5=5af4bf5cfa2bdb7311fc8ebab1038798","We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers. © 2017 ACM.","Fuzzing; Grammar synthesis","Computer programming languages; Context free grammars; Fuzzy inference; Inference engines; Black boxes; Fuzzing; Grammar inference; Test program; Context free languages",2-s2.0-85024493993
"Subbarao E.C.","IBM Fellows - Indian presence",2017,"Current Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020281805&doi=10.18520%2fcs%2fv112%2fi11%2f2208-2211&partnerID=40&md5=a313c6c465f652d69efc8231a7cb92ee","Bright researchers with impressive output on a sustained basis need to be identified by clear criteria, encouraged and rewarded with freedom to pursue their interest. How IBM does this is described and naming them IBM Fellows. A remarkable number of IBM fellows is of Indian origin. A road map for progressive Indian industry to encourage innovative research by bright Indians working in India is elaborated as an urgent, worthwhile pursuit to put India and Indian industry on the global innovation map.","IBM Fellows; Indian presence; Research and development; Science and technology",,2-s2.0-85020281805
"Ritter D., Dann J., May N., Rinderle-Ma S.","Industry paper: Hardware accelerated application integration processing",2017,"DEBS 2017 - Proceedings of the 11th ACM International Conference on Distributed Event-Based Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023209536&doi=10.1145%2f3093742.3093911&partnerID=40&md5=97c6b379b9277cdf8797ceb03b974bb0","The growing number of (cloud) applications and devices massively increases the communication rate and volume pushing integration systems to their (throughput) limits. While the usage of modern hardware like Field Programmable Gate Arrays (FPGAs) led to low latency when employed for query and event processing, application integration adds yet unexplored processing opportunities. In this industry paper, we explore how to program integration semantics (e. g., message routing and transformation) in form of Enterprise Integration Patterns (EIP) on top of an FPGA, thus complementing the existing research on FPGA data processing. We focus on message routing, re-define the EIP for stream processing and propose modular hardware implementations as templates that are synthesized to circuits. For our real-world ""connected car"" scenario (i. e., composed patterns), we discuss common and new optimizations especially relevant for hardware integration processes. Our experimental evaluation shows competitive throughput compared to modern general-purpose CPUs and discusses the results. © 2017 Association for Computing Machinery.",,"Computer hardware description languages; Data handling; Field programmable gate arrays (FPGA); Integration; Metadata; Program processors; Semantics; Software architecture; Application integration; Enterprise integration patterns; Experimental evaluation; General purpose CPUs; Hardware integrations; Hardware-accelerated; Integration systems; Program integration; Hardware",2-s2.0-85023209536
"Li Y., Katsipoulakis N.R., Chandramouli B., Goldstein J., Kossmann D.","Mison: A fast JSON parser for data analytics",2017,"Proceedings of the VLDB Endowment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029569980&partnerID=40&md5=18c79fed387604c01781050e28823845","The growing popularity of the JSON format has fueled increased interest in loading and processing JSON data within analytical data processing systems. However, in many applications, JSON parsing dominates performance and cost. In this paper, we present a new JSON parser called Mison that is particularly tailored to this class of applications, by pushing down both projection and filter operators of analytical queries into the parser. To achieve these features, we propose to deviate from the traditional approach of building parsers using finite state machines (FSMs). Instead, we follow a two-level approach that enables the parser to jump directly to the correct position of a queried field without having to perform expensive tokenizing steps to find the field. At the upper level, Mison speculatively predicts the logical locations of queried fields based on previously seen patterns in a dataset. At the lower level, Mison builds structural indices on JSON data to map logical locations to physical locations. Unlike all existing FSM-based parsers, building structural indices converts control flow into data flow, thereby largely eliminating inherently unpredictable branches in the program and exploiting the parallelism available in modern processors. We experimentally evaluate Mison using representative real-world JSON datasets and the TPC-H benchmark, and show that Mison produces significant performance benefits over the best existing JSON parsers; in some cases, the performance improvement is over one order of magnitude. © 2017 VLDB Endowment.",,"Benchmarking; Indexing (materials working); Location; Program processors; Analytical data processing; Analytical queries; Building structural; Performance benefits; Physical locations; Structural indices; Traditional approaches; Two-level approach; Data handling",2-s2.0-85029569980
"Li X., Wang Q.-X., Jin Z.","Description Reinforcement Based Code Search",2017,"Ruan Jian Xue Bao/Journal of Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027348123&doi=10.13328%2fj.cnki.jos.005226&partnerID=40&md5=847a4359f341d4bf89468f88537dc772","Effectively searching code for specific programming task from code base has become an important research field of software engineering. This paper presents a description reinforcement based code search (DERECS) approach. DERECS first builds a code- description pair corpus, analyzes both code and its natural language description, and extracts features about method calls and code structure. DERECS reinforces the description of code based on the method calls and code structure features, reduces the gaps between code snippet and natural language query, and expands the search scope. Evaluation is conducted against real-world queries, and the results show DERECS is significantly better than SNIFF and Krugle. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Code description reinforcement; Code feature extraction; Code search; Code-description pair; Natural language process","Natural language processing systems; Reinforcement; Software engineering; Code search; Code structure; Code-description pair; Natural language process; Natural language queries; Natural languages; Programming tasks; Research fields; Codes (symbols)",2-s2.0-85027348123
"Moon T.-J., Chih M.-Y., Shah D.V., Yoo W., Gustafson D.H.","Breast Cancer Survivors' Contribution to Psychosocial Adjustment of Newly Diagnosed Breast Cancer Patients in a Computer-Mediated Social Support Group",2017,"Journalism and Mass Communication Quarterly",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020881722&doi=10.1177%2f1077699016687724&partnerID=40&md5=d0ed2e7ceb5e6bccd93229626884ef1d","This study investigated the role of breast cancer survivors in a computer-mediated social support (CMSS) group for women with breast cancer. Applying a computer-aided content analytic method, the present study examined the differences in support provision between survivors and newly diagnosed patients. This study further investigated the impacts of survivor-provided social support on psychosocial adjustment of newly diagnosed patients. The results revealed that, compared with newly diagnosed patients, breast cancer survivors provided more emotional and informational support. Receiving emotional support from survivors contributed to an improvement in the quality of life and the depression of patients. The effects of survivor-provided informational support were not significant. © Association for Education in Journalism & Mass Communication.","breast cancer survivor; CMSS groups; psychosocial adjustment; social support","analytical method; cancer; psychology; quality of life; social network; womens health",2-s2.0-85020881722
"Janse P., Sharp R., Surdeanuy M., Clarkz P.","Framing QA as building and ranking intersentence answer justifications",2017,"Computational Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021780119&doi=10.1162%2fCOLI_a_00287&partnerID=40&md5=5ceb0e4ea8d6d6d50fdf6ba94a42d41a","We propose a question answering (QA) approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats justification quality as a latent variable. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44% of the questions correctly, where the top justifications for 57% of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong baseline, and show that information aggregation is key to addressing the information need in complex questions. © 2017 Association for Computational Linguistics.",,,2-s2.0-85021780119
"Fu C., Yang D., Zhang X., Hu H.","An approach to translating OCL invariants into OWL 2 DL axioms for checking inconsistency",2017,"Automated Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011840306&doi=10.1007%2fs10515-017-0210-9&partnerID=40&md5=8cf3dba90ad89009dcf719b24d237406","Checking the design specification for contradictions at the early phase of the software development process is crucial to ensure that the design is implementable. However, the high expressivity of OCL makes manual inconsistency checking a difficult task. In addition, the developers cannot detect these problems by OCL itself due to its lack of automated reasoning support. We investigate an approach to translating OCL invariants into OWL 2 DL axioms. We do this where the OCL expression contained in an invariant is converted to the corresponding OWL 2 DL class expression in a compositional way. Our approach covers the OCL expressions including four: PrimaryExp, RelationalExp, LogicalExp and IfExp types. Considering the distinction between the CWA and OWA, we achieve correct translation from RelationalExp using closure axiom. Also, we employ an ontology design pattern to overcome the limitations of OWL 2 DL expressivity when translating IfExp. Then inconsistency checking is done through description logic reasoning by the OWL 2 DL high-performance reasoner. We construct an inductive proof to establish the correctness of our translation approach. Moreover, we evaluate our approach using the implemented TUCO tool prototype. © 2017, Springer Science+Business Media New York.","Description logic; OCL inconsistency checking; Ontology design pattern; OWL 2 DL","Birds; Computer circuits; Data description; Formal languages; Ontology; Software engineering; Automated reasoning; Description logic; Design specification; Inconsistency checking; Ontology design; OWL 2 DL; Reasoner; Software development process; Software design",2-s2.0-85011840306
"Li Y., Rafiei D.","Natural language data management and interfaces: Recent development and open challenges",2017,"Proceedings of the ACM SIGMOD International Conference on Management of Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021236781&doi=10.1145%2f3035918.3054783&partnerID=40&md5=b1b4dfd30d57829dd52e1235bffaf2a0","The volume of natural language text data has been rapidly increasing over the past two decades, due to factors such as the growth of the Web, the low cost associated to publishing and the progress on the digitization of printed texts. This growth combined with the proliferation of natural language systems for search and retrieving information provides tremendous opportunities for studying some of the areas where database systems and natural language processing systems overlap. This tutorial explores two more relevant areas of overlap to the database community: (1) managing natural language text data in a relational database, and (2) developing natural language interfaces to databases. The tutorial presents state-of-the-art methods, related systems, research opportunities and challenges covering both areas. © 2017 ACM.",,"Information management; Search engines; Database community; Natural language interfaces to database; Natural language systems; Natural language text; Natural languages; Relational Database; Research opportunities; State-of-the-art methods; Natural language processing systems",2-s2.0-85021236781
"Jin Z., Anderson M.R., Cafarella M., Jagadish H.V.","Foofah: Transforming data by example",2017,"Proceedings of the ACM SIGMOD International Conference on Management of Data",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021248011&doi=10.1145%2f3035918.3064034&partnerID=40&md5=f79bf62c7808d9cf080d7dd2dc81877c","Data transformation is a critical first step in modern data analysis: before any analysis can be done, data from a variety of sources must be wrangled into a uniform format that is amenable to the intended analysis and analytical software package. This data transformation task is tedious, time-consuming, and often requires programming skills beyond the expertise of data analysts. In this paper, we develop a technique to synthesize data transformation programs by example, reducing this burden by allowing the analyst to describe the transformation with a small input-output example pair, without being concerned with the transformation steps required to get there. We implemented our technique in a system, Foofah, that efficiently searches the space of possible data transformation operations to generate a program that will perform the desired transformation. We experimentally show that data transformation programs can be created quickly with Foofah for a wide variety of cases, with 60% less user effort than the well-known Wrangler system. © 2017 ACM.","A∗ algorithm; Data transformation; Heuristic; Program synthesis; Programming by example","Heuristic programming; Input output programs; Search engines; Analytical software; Data analysts; Data transformation; Heuristic; Program synthesis; Programming by Example; Programming skills; Small inputs; Metadata",2-s2.0-85021248011
"Rawassizadeh R., Dobbins C., Nourizadeh M., Ghamchili Z., Pazzani M.","A natural language query interface for searching personal information on smartwatches",2017,"2017 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2017",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019999271&doi=10.1109%2fPERCOMW.2017.7917645&partnerID=40&md5=d291b49be2fd121614185b01be3420e9","Currently, personal assistant systems, run on smartphones and use natural language interfaces. However, these systems rely mostly on the web for finding information. Mobile and wearable devices can collect an enormous amount of contextual personal data such as sleep and physical activities. These information objects and their applications are known as quantified-self, mobile health or personal informatics, and they can be used to provide a deeper insight into our behavior. To our knowledge, existing personal assistant systems do not support all types of quantified-self queries. In response to this, we have undertaken a user study to analyze a set of 'textual questions/queries' that users have used to search their quantified-self or mobile health data. Through analyzing these questions, we have constructed a light-weight natural language based query interface - including a text parser algorithm and a user interface - to process the users' queries that have been used for searching quantified-self information. This query interface has been designed to operate on small devices, i.e. smartwatches, as well as augmenting the personal assistant systems by allowing them to process end users' natural language queries about their quantified-self data. © 2017 IEEE.","Natural Language Interface; Quantified Self; Query; Smartwatch","Human computer interaction; mHealth; Natural language processing systems; Query processing; Ubiquitous computing; User interfaces; Wearable computers; Natural language interfaces; Natural language queries; Personal assistants; Personal informatics; Personal information; Quantified Self; Query; Smartwatch; Search engines",2-s2.0-85019999271
"Hu J., Wang X.","Parallel processing technology of image virtual dimension based on unified computing device architecture",2017,"Agro Food Industry Hi-Tech",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020850971&partnerID=40&md5=18b685247e993beac88fdc971283455d","Hyper spectral image has the characteristics of large amount of data and high latitude, which undoubtedly increases the difficulty of image processing. The band selection method can improve the image processing, but the number of selected bands must be determined in advance. Therefore, the image virtual dimension was adopted, and the problem of the number of band selection was solved. And the parallel algorithm of virtual dimension based on unified device architecture was designed and implemented. The results show that compared with the traditional C algorithm, the parallel algorithm based on the unified device architecture has a significant speedup effect.","CUDA; Image processing; Virtual dimension","Parallel algorithms; Parallel architectures; Spectroscopy; Band selection; Computing devices; CUDA; Device architectures; High Latitudes; Hyper-spectral images; Parallel-processing technology; Virtual dimension; Image processing",2-s2.0-85020850971
"Cao Z., Hu Y., Yu Q., Lu Y., Wu D., Zhou A., Ma W., Xia Y., Liu C., Loos K.","Facile Fabrication, Structures, and Properties of Laser-Marked Polyacrylamide/Bi2O3 Hydrogels",2017,"Advanced Engineering Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012013280&doi=10.1002%2fadem.201600826&partnerID=40&md5=e1cfde121a24bc6e91aaa70148bb8e3e","Laser marking of wet and soft polyacrylamide/bismuth oxide (PAM/Bi2O3) hydrogel materials can be achieved by simply embedding Bi2O3 particles inside PAM hydrogels and a subsequent laser treatment with a Nd: YAG laser beam at 1064 nm. In comparison with the pure hydrogel, the marking properties of the laser-marked PAM/Bi2O3 hydrogel samples vary as the Bi2O3 content increase from 0.17 to 3.0 wt% and as the laser power increase from 23.5 to 47.0 W. SEM, XPS, XRD, TGA, Raman spectroscopy, water contact angle and mechanical property tests are performed to characterize the laser-marked PAM/Bi2O3 hydrogels. The results herein indicate that the marking contrast and visual appearance on the surface of the PAM/Bi2O3 hydrogels after laser irradiation are mainly due to the decomposition of Bi2O3 to black bismuth metal. © 2017 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim",,"Laser beams; Neodymium lasers; Facile fabrication; Laser marking; Laser power; Laser treatment; Nd:YAG laser beams; Visual appearance; Water contact angle; Hydrogels",2-s2.0-85012013280
"Tang X.","Lexeme-based collexeme analysis with DepCluster",2017,"Corpus Linguistics and Linguistic Theory",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018848798&doi=10.1515%2fcllt-2015-0007&partnerID=40&md5=3a639bd8b7437f5d4f3a6de7e5ca4610","This paper introduces a tool for lexeme-based collexeme analysis. The tool uses cluster analysis to generate the typical constructions of a given lexeme and computes the collostruction strength of the constructions. These two functions enable the tool to facilitate efficient studies of lexeme-construction interactions in large-scale data. As a case study, the paper examines the lexeme ""cause"". It shows that the tool provides strong statistical evidence that confirms earlier findings about the negative semantic prosody of the lexeme. In addition, the collexeme analyses with the tool show that the lexeme is typically used in attitudinal constructions. The case study demonstrates that the tool can enhance the efficiency, comprehensiveness and granularity in lexeme-based collexeme analysis. © 2017 Walter de Gruyter GmbH, Berlin/Boston 2017.","attitudinal construction; cluster; collexeme analysis; Dependency Grammar; semantic prosody",,2-s2.0-85018848798
"Omar C., Voysey I., Hilton M., Sunshine J., Goues C.L., Aldrich J., Hammer M.A.","Toward semantic foundations for program editors",2017,"Leibniz International Proceedings in Informatics, LIPIcs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019566137&doi=10.4230%2fLIPIcs.SNAPL.2017.11&partnerID=40&md5=12f513d1749b35021c896b9f4814ea0e","Programming language definitions assign formal meaning to complete programs. Programmers, however, spend a substantial amount of time interacting with incomplete programs - programs with holes, type inconsistencies and binding inconsistencies - using tools like program editors and live programming environments (which interleave editing and evaluation). Semanticists have done comparatively little to formally characterize (1) the static and dynamic semantics of incomplete programs; (2) the actions available to programmers as they edit and inspect incomplete programs; and (3) the behavior of editor services that suggest likely edit actions to the programmer. This paper serves as a vision statement for a research program that seeks to develop these ""missing"" semantic foundations. Our hope is that these contributions, which will take the form of a series of simple formal calculi equipped with a tractable metatheory, will guide the design of a variety of current and future interactive programming tools, much as various lambda calculi have guided modern language designs. Our own research will apply these principles in the design of Hazel, an experimental live lab notebook programming environment designed for data science tasks. We plan to co-design the Hazel language with the editor so that we can explore concepts such as edit-time semantic conflict resolution mechanisms and mechanisms that allow library providers to install library-specific editor services. © Cyrus Omar, Ian Voysey, Michael Hilton, Joshua Sunshine, Claire Le Goues, Jonathan Aldrich, and Matthew A. Hammer; licensed under Creative Commons License CC-BY.","Live programming; Program editors; Program prediction; Type systems","Biomineralization; Computer programming; Pathology; Dynamic semantic; Program editors; Programming environment; Programming tools; Research programs; Semantic conflict; Semantic foundation; Type systems; Semantics",2-s2.0-85019566137
"Balaur I., Mazein A., Saqi M., Lysenko A., Rawlings C.J., Auffray C.","Recon2Neo4j: Applying graph database technologies for managing comprehensive genome-scale networks",2017,"Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019066321&doi=10.1093%2fbioinformatics%2fbtw731&partnerID=40&md5=f5a95168aca05dcfc118e3fb0355cc4d","The goal of this work is to offer a computational framework for exploring data from the Recon2 human metabolic reconstruction model. Advanced user access features have been developed using the Neo4j graph database technology and this paper describes key features such as efficient management of the network data, examples of the network querying for addressing particular tasks, and how query results are converted back to the Systems Biology Markup Language (SBML) standard format. The Neo4j-based metabolic framework facilitates exploration of highly connected and comprehensive human metabolic data and identification of metabolic subnetworks of interest. A Java-based parser component has been developed to convert query results (available in the JSON format) into SBML and SIF formats in order to facilitate further results exploration, enhancement or network sharing. Availability and Implementation: The Neo4j-based metabolic framework is freely available from: https://diseaseknowledgebase.etriks.org/metabolic/browser/. The Java code files developed for this work are available from the following url: https://github.com/ibalaur/MetabolicFramework. © The Author 2016. Published by Oxford University Press.",,"biological model; computer graphics; database management system; factual database; genetics; genome; human; metabolism; software; Computer Graphics; Database Management Systems; Databases, Factual; Genome; Humans; Metabolic Networks and Pathways; Models, Biological; Software",2-s2.0-85019066321
"Ye Y., Wagner M.M., Cooper G.F., Ferraro J.P., Su H., Gesteland P.H., Haug P.J., Millett N.E., Aronis J.M., Nowalk A.J., Ruiz V.M., Pineda A.L., Shi L., Van Bree R., Ginter T., Tsui F.","A study of the transferability of influenza case detection systems between two large healthcare systems",2017,"PLoS ONE",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017025499&doi=10.1371%2fjournal.pone.0174970&partnerID=40&md5=0d8027ef2c9a848c263892ab4a1cd12c","Objectives: This study evaluates the accuracy and transferability of Bayesian case detection systems (BCD) that use clinical notes from emergency department (ED) to detect influenza cases. Methods: A BCD uses natural language processing (NLP) to infer the presence or absence of clinical findings from ED notes, which are fed into a Bayesain network classifier (BN) to infer patients' diagnoses. We developed BCDs at the University of Pittsburgh Medical Center (BCDUPMC) and Intermountain Healthcare in Utah (BCDIH). At each site, we manually built a rule-based NLP and trained a Bayesain network classifier from over 40,000 ED encounters between Jan. 2008 and May. 2010 using feature selection, machine learning, and expert debiasing approach. Transferability of a BCD in this study may be impacted by seven factors: development (source) institution, development parser, application (target) institution, application parser, NLP transfer, BN transfer, and classification task. We employed an ANOVA analysis to study their impacts on BCD performance. Results: Both BCDs discriminated well between influenza and non-influenza on local test cases (AUCs &gt; 0.92). When tested for transferability using the other institution's cases, BCDUPMC discriminations declined minimally (AUC decreased from 0.95 to 0.94, p&lt;0.01), and BCDIH discriminations declined more (from 0.93 to 0.87, p&lt;0.0001). We attributed the BCDIH decline to the lower recall of the IH parser on UPMC notes. The ANOVA analysis showed five significant factors: development parser, application institution, application parser, BN transfer, and classification task. Conclusion: We demonstrated high influenza case detection performance in two large healthcare systems in two geographically separated regions, providing evidentiary support for the use of automated case detection from routinely collected electronic clinical notes in national influenza surveillance. The transferability could be improved by training Bayesian network classifier locally and increasing the accuracy of the NLP parser. © 2017 Ye et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Article; automation; Bayesain network classifier; Bayesian case detection; classifier; disease classification; electronic medical record; health care system; human; influenza; information processing; machine learning; natural language processing; United States; adolescent; adult; aged; Bayes theorem; child; decision support system; electronic health record; health care delivery; hospital emergency service; infant; Influenza, Human; machine learning; middle aged; newborn; preschool child; reproducibility; technology; young adult; Adolescent; Adult; Aged; Bayes Theorem; Child; Child, Preschool; Decision Support Techniques; Delivery of Health Care; Electronic Health Records; Emergency Service, Hospital; Humans; Infant; Infant, Newborn; Influenza, Human; Machine Learning; Middle Aged; Natural Language Processing; Reproducibility of Results; Technology Transfer; Young Adult",2-s2.0-85017025499
"Nguyen N.T.H., Soto A.J., Kontonatsios G., Batista-Navarro R., Ananiadou S.","Constructing a biodiversity terminological inventory",2017,"PLoS ONE",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017630031&doi=10.1371%2fjournal.pone.0175277&partnerID=40&md5=e436134b2659d16924866b6e8aa01d14","The increasing growth of literature in biodiversity presents challenges to users who need to discover pertinent information in an efficient and timely manner. In response, text mining techniques offer solutions by facilitating the automated discovery of knowledge from large textual data. An important step in text mining is the recognition of concepts via their linguistic realisation, i.e., terms. However, a given concept may be referred to in text using various synonyms or term variants, making search systems likely to overlook documents mentioning less known variants, which are albeit relevant to a query term. Domain-specific terminological resources, which include term variants, synonyms and related terms, are thus important in supporting semantic search over large textual archives. This article describes the use of text mining methods for the automatic construction of a large-scale biodiversity term inventory. The inventory consists of names of species, amongst which naming variations are prevalent. We apply a number of distributional semantic techniques on all of the titles in the Biodiversity Heritage Library, to compute semantic similarity between species names and support the automated construction of the resource. With the construction of our biodiversity term inventory, we demonstrate that distributional semantic models are able to identify semantically similar names that are not yet recorded in existing taxonomies. Such methods can thus be used to update existing taxonomies semi-automatically by deriving semantically related taxonomic names from a text corpus and allowing expert curators to validate them. We also evaluate our inventory as a means to improve search by facilitating automatic query expansion. Specifically, we developed a visual search interface that suggests semantically related species names, which are available in our inventory but not always in other repositories, to incorporate into the search query. An assessment of the interface by domain experts reveals that our query expansion based on related names is useful for increasing the number of relevant documents retrieved. Its exploitation can benefit both users and developers of search engines and text mining applications. © 2017 Nguyen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"biodiversity; human; information center; inheritance; mining; model; nomenclature; search engine; species; algorithm; data mining; library; procedures; semantics; Algorithms; Biodiversity; Data Mining; Libraries; Search Engine; Semantics; Terminology as Topic",2-s2.0-85017630031
"Gardent C., Perez-Beltrachini L.","A statistical, grammar-based approach to microplanning",2017,"Computational Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017185084&doi=10.1162%2fCOLI_a_00273&partnerID=40&md5=580d5342f6952fe555c9d4c2a2a2b7af","Although there has been much work in recent years on data-driven natural language generation, little attention has been paid to the fine-grained interactions that arise during microplanning between aggregation, surface realization, and sentence segmentation. In this article, we propose a hybrid symbolic/statistical approach to jointly model the constraints regulating these interactions. Our approach integrates a small handwritten grammar, a statistical hypertagger, and a surface realization algorithm. It is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence. We evaluate our approach in several ways. A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage. Results from a human study indicate that users find the output of this hybrid statistic/symbolic system more fluent than both a template-based and a purely symbolic grammar-based approach. Finally, we illustrate by means of examples that our approach can account for various factors impacting aggregation, sentence segmentation, and surface realization. © 2017 Association for Computational Linguistics.",,,2-s2.0-85017185084
"Mezghanni I.B., Gargouri F.","Deriving ontological semantic relations between Arabic compound nouns concepts",2017,"Journal of King Saud University - Computer and Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015973882&doi=10.1016%2fj.jksuci.2017.03.001&partnerID=40&md5=f5fea017169a726d10671f3eb1102530","Legal ontologies have proved their increasingly substantial role in representing, processing and retrieving legal information. By using the knowledge modeled by such ontologies in form of concepts and relations, it is possible to reason over the semantic content of legal documents. Supporting (semi-) automatically the development of ontologies from text is commonly referred to as ontology learning from text. The learning process includes learning of the concepts that will form the ontology and learning of the semantic relations among them. In this paper, we present a new approach for expliciting the semantic relations between Arabic compound nouns concepts. The originality of this work is twofold. Firstly, the technique of inferring relations is based on exploiting the internal structure of the compounds using a defined set of domain-and language-independent rules according to their different structures, on the one hand, and on studying prepositions semantics specifying the inferred relations applying a gamification mechanism that collects human votes, on the other hand. Secondly, relying on the compounds set described by both binary (structural positions in which there are written) and relational attributes (the deduced relations), we used a “Relational Concept Analysis” (RCA) technique, as an adaptation of “Formal Concept Analysis” (FCA), for the construction of interconnected lattices that we transformed into ontological concepts and relations which can be either taxonomic or transversal. Experiments carried out on Arabic legal dataset showed that the proposed approach reached encouraging performance through achieving high precision and recall scores. This performance affects positively the retrieval results of legal documents based on a powerful ontology, which presents our main objective. © 2017 The Authors","Arabic compound nouns; Compound structure; FCA; Gamification; RCA; Semantic relations derivation",,2-s2.0-85015973882
"Tripodi R., Pelillo M.","A game-theoretic approach to word sense disambiguation",2017,"Computational Linguistics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017177340&doi=10.1162%2fCOLI_a_00274&partnerID=40&md5=1e77cea5c29900d5bbf71a13359cb9d8","This article presents a new model for word sense disambiguation formulated in terms of evolutionary game theory, where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes. The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose. We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices. With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory, maintaining the textual coherence. The model is based on two ideas: Similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them. The article provides an in-depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory, which is illustrated by an example. The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state-of-the-art systems. The results show that our model outperforms state-of-the-art algorithms and can be applied to different tasks and in different scenarios. © 2017 Association for Computational Linguistics.",,,2-s2.0-85017177340
"Singh R., Solar-Lezama A.","SWAPPER: A framework for automatic generation of formula simplifiers based on conditional rewrite rules",2017,"Proceedings of the 16th Conference on Formal Methods in Computer-Aided Design, FMCAD 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017503165&doi=10.1109%2fFMCAD.2016.7886678&partnerID=40&md5=735296f447f8b2182607f27f9ff991d1","This paper addresses the problem of creating simplifiers for logic formulas based on conditional term rewriting. In particular, the paper focuses on a program synthesis application where formula simplifications have been shown to have a significant impact. We show that by combining machine learning techniques with constraint-based synthesis, it is possible to synthesize a formula simplifier fully automatically from a corpus of representative problems, making it possible to create formula simplifiers tailored to specific problem domains. We demonstrate the benefits of our approach for synthesis benchmarks from the SyGuS competition and automated grading. © 2016 FMCAD Inc.",,"Application programs; Formal methods; Grading; Learning systems; Automated grading; Automatic Generation; Conditional term rewriting; Constraint-based; Logic formulas; Machine learning techniques; Program synthesis; Specific problems; Computer aided design",2-s2.0-85017503165
"Onuean A., Gim J., Jang Y., Jung H.","Study on extracting implicit patterns of patent data based on timeline",2017,"2017 9th International Conference on Knowledge and Smart Technology: Crunching Information of Everything, KST 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017513394&doi=10.1109%2fKST.2017.7886132&partnerID=40&md5=9d1cfa4a8382bffb4d9ca3fcd0778686","Patent documents are provide a significant source of knowledge about future technologies. Many attempts have been conducted to mine important knowledge from patents to analyze new technology trends. In this paper, we will to analyze implicit knowledge derived from the patents dataset of Big Data domain from KIPRIS. Keywords that occur in the title of patents are classified into three categories: Approach, Goal Object, and Goal Predicate, in order to create a model of relations of title patterns. The same keywords found on the timeline interval will be analyzed and illustrated in the patent pattern which are able to depict the relationship of goals and approaches of the patents occurred in different time interval. As a result, implicit trends and knowledge related to of specific keywords of technology reflect of each time gap can be obtained. Search result using 'Goal object, Goal predicate and Approach' pattern query is also found efficient and meet the user enquiry related technologies in timeline. © 2017 IEEE.","Big data; Implicit; Patent; Patent pattern; Timeline","Patents and inventions; Future technologies; Implicit; Implicit knowledge; Patent; Patent pattern; Technology trends; Three categories; Timeline; Big data",2-s2.0-85017513394
"Brandizi M., Melnichuk O., Bild R., Kohlmayer F., Rodriguez-Castro B., Spengler H., Kuhn K.A., Kuchinke W., Ohmann C., Mustonen T., Linden M., Nyrönen T., Lappalainen I., Brazma A., Sarkans U.","Orchestrating differential data access for translational research: a pilot implementation",2017,"BMC Medical Informatics and Decision Making",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016010348&doi=10.1186%2fs12911-017-0424-6&partnerID=40&md5=1a17ef7bff2c02a5ef74b6f34e6088c3","Background: Translational researchers need robust IT solutions to access a range of data types, varying from public data sets to pseudonymised patient information with restricted access, provided on a case by case basis. The reason for this complication is that managing access policies to sensitive human data must consider issues of data confidentiality, identifiability, extent of consent, and data usage agreements. All these ethical, social and legal aspects must be incorporated into a differential management of restricted access to sensitive data. Methods: In this paper we present a pilot system that uses several common open source software components in a novel combination to coordinate access to heterogeneous biomedical data repositories containing open data (open access) as well as sensitive data (restricted access) in the domain of biobanking and biosample research. Our approach is based on a digital identity federation and software to manage resource access entitlements. Results: Open source software components were assembled and configured in such a way that they allow for different ways of restricted access according to the protection needs of the data. We have tested the resulting pilot infrastructure and assessed its performance, feasibility and reproducibility. Conclusions: Common open source software components are sufficient to allow for the creation of a secure system for differential access to sensitive data. The implementation of this system is exemplary for researchers facing similar requirements for restricted access data. Here we report experience and lessons learnt of our pilot implementation, which may be useful for similar use cases. Furthermore, we discuss possible extensions for more complex scenarios. © 2017 The Author(s).","Biomedical Data; Clinical Data; Data Access; Health Data Protection; Translational Research","feasibility study; human; identity; reproducibility; scientist; software; translational research; biobank; computer security; information processing; medical research; pilot study; standards; translational research; Biological Specimen Banks; Biomedical Research; Computer Security; Datasets as Topic; Humans; Pilot Projects; Translational Medical Research",2-s2.0-85016010348
"Zhao X., Xing Z., Kabir M.A., Sawada N., Li J., Lin S.-W.","HDSKG: Harvesting domain specific knowledge graph from content of webpages",2017,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018390307&doi=10.1109%2fSANER.2017.7884609&partnerID=40&md5=704c96478ef3d1476d162d7ed34719fc","Knowledge graph is useful for many different domains like search result ranking, recommendation, exploratory search, etc. It integrates structural information of concepts across multiple information sources, and links these concepts together. The extraction of domain specific relation triples (subject, verb phrase, object) is one of the important techniques for domain specific knowledge graph construction. In this research, an automatic method named HDSKG is proposed to discover domain specific concepts and their relation triples from the content of webpages. We incorporate the dependency parser with rule-based method to chunk the relations triple candidates, then we extract advanced features of these candidate relation triples to estimate the domain relevance by a machine learning algorithm. For the evaluation of our method, we apply HDSKG to Stack Overflow (a Q&A website about computer programming). As a result, we construct a knowledge graph of software engineering domain with 35279 relation triples, 44800 concepts, and 9660 unique verb phrases. The experimental results show that both the precision and recall of HDSKG (0.78 and 0.7 respectively) is much higher than the openIE (0.11 and 0.6 respectively). The performance is particularly efficient in the case of complex sentences. Further more, with the self-training technique we used in the classifier, HDSKG can be applied to other domain easily with less training data. © 2017 IEEE.","Dependency Parse; Knowledge Graph; openIE; Stack Overflow; Structural Information Extraction","Artificial intelligence; Classification (of information); Computer programming; Learning algorithms; Learning systems; Reengineering; Software engineering; Syntactics; Websites; Dependency Parse; Knowledge graphs; openIE; Stack overflow; Structural information extractions; Data mining",2-s2.0-85018390307
"Yang Y., Xiao L., Tian J.","Transmission of clinical information based on HL7 CDA standard",2017,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016998764&doi=10.1109%2fICSESS.2016.7883222&partnerID=40&md5=7a6a3a6b90a0b52e868959d5f2b8b3f3","At present, the phenomenon of electronic medical institutions is becoming more and more popular. In order to provide or get better medical services, the demand for exchange and sharing of clinical information is also increasing. However, due to the heterogeneity of the medical system, the problem of data format is not uniform, which has seriously hindered the delivery of clinical information, so that the patient's clinical information can not be fully utilized. In this paper, we apply the information model of HL7 CDA standard to package clinical information documents, use XML language to achieve a simple CDA document and its format output. At the same time, through the JDBC and XML parser to achieve the automatic generation and analysis of CDA documents. By encapsulating the CDA document in the HL7 message, the transmission of clinical information is realized. © 2016 IEEE.","CDA; HL7; the transmission of clinical information","Societies and institutions; Software engineering; XML; Automatic Generation; Clinical information; Information Modeling; Medical institutions; Medical services; Medical systems; XML languages; XML parser; Medical information systems",2-s2.0-85016998764
"Dhamdhere K., McCurley K.S., Nahmias R., Sundararajan M., Yan Q.","Analyza: Exploring data with conversation",2017,"International Conference on Intelligent User Interfaces, Proceedings IUI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016450730&doi=10.1145%2f3025171.3025227&partnerID=40&md5=cc3fd1a62f803b5c6f5d98863930181e","We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The first is a question-And-Answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza's ability to democratize access to data. We discuss the key design decisions in implementing this system. For instance, how to mix structured and natural language modalities, how to use conversation to disambiguate and simplify querying, how to rely on the ""semantics"" of the data to compensate for the lack of syntactic structure, and how to efficiently curate the data.","Exploratory data analysis; Natural language","Query processing; Semantics; Syntactics; Coding skills; Design decisions; Exploratory data analysis; Large sales; Natural languages; Real-world system; Syntactic structure; User interfaces",2-s2.0-85016450730
"Landsiedel C., Rieser V., Walter M., Wollherr D.","A review of spatial reasoning and interaction for real-world robotics",2017,"Advanced Robotics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009948912&doi=10.1080%2f01691864.2016.1277554&partnerID=40&md5=3e0bbba565c6842ee78c26bec1805471","Truly universal helper robots capable of coping with unknown, unstructured environments must be capable of spatial reasoning, i.e. establishing geometric relations between objects and locations, expressing those in terms understandable by humans. It is therefore desirable that spatial and semantic environment representations are tightly interlinked. 3D robotic mapping and the generation of consistent metric representations of space are highly useful for navigation and exploration, but they do not capture symbol-level information about the environment. This is, however, essential for reasoning, and enables interaction via natural language, which is arguably the most common and natural communication channel used and understood by humans. This article presents a review of research in three major fields relevant for this discussion of spatial reasoning and interaction. Firstly, dialogue systems are an integral part of modern approaches to situated human–robot interaction. Secondly, interactive robots must be equipped with environment representations and reasoning methods that are suitable for both navigation and task fulfillment, as well as for interaction with human partners. Thirdly, at the interface between these domains are systems that ground language in systemic environment representation and which allow the integration of information from natural language descriptions into robotic maps. For each of these areas, important approaches are outlined and relations between the fields are highlighted, and challenging applications as well as open problems are discussed. © 2017 Taylor & Francis and The Robotics Society of Japan.","environment modelling; natural language grounding; semantic mapping; Situated human–robot interaction; spatial reasoning","Mapping; Modeling languages; Robotics; Robots; Semantics; Speech processing; Environment modelling; Natural languages; Robot interactions; Semantic mapping; Spatial reasoning; Human robot interaction",2-s2.0-85009948912
"Iswarya P., Radha V.","Adapting hybrid machine translation techniques for cross-language text retrieval system",2017,"Journal of Engineering Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014726724&partnerID=40&md5=fd7d29b8271351f57fbdf5fdacf23027","This research work aims in developing Tamil to English Cross - language text retrieval system using hybrid machine translation approach. The hybrid machine translation system is a combination of rule based and statistical based approaches. In an existing word by word translation system there are lot of issues and some of them are ambiguity, Out-of-Vocabulary words, word inflections, and improper sentence structure. To handle these issues, proposed architecture is designed in such a way that, it contains Improved Part-of-Speech tagger, machine learning based morphological analyser, collocation based word sense disambiguation procedure, semantic dictionary, and tense markers with gerund ending rules, and two pass transliteration algorithm. From the experimental results it is clear that the proposed Tamil Query based translation system achieves significantly better translation quality over existing system, and reaches 95.88% of monolingual performance. © School of Engineering, Taylor’s University.","Ambiguity; Hybrid machine translation; Monolingual; Translation",,2-s2.0-85014726724
"De Sanctis M., Trubiani C., Cortellessa V., Di Marco A., Flamminj M.","A model-driven approach to catch performance antipatterns in ADL specifications",2017,"Information and Software Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006269717&doi=10.1016%2fj.infsof.2016.11.008&partnerID=40&md5=d317e7531fe748b12d1a4d11099bd4b1","Context: While the performance analysis of a software architecture is a quite well-assessed task nowadays, the issue of interpreting the performance results for providing feedback to software architects is still very critical. Performance antipatterns represent effective instruments to tackle this issue, because they document common mistakes leading to performance problems as well as their solutions. Objective: Up today performance antipatterns have been only studied in the context of software modeling languages like UML, whereas in this manuscript our objective is to catch them in the context of ADL-based software architectures to investigate their effectiveness. Method: We have implemented a model-driven approach that allows the automatic detection of four performance antipatterns in Æmilia, that is a stochastic process algebraic ADL for performance-aware component-oriented modeling of software systems. Results: We evaluate the approach by applying it to three case studies in different application domains. Experimental results demonstrate the effectiveness of our approach to support the performance improvement of ADL-based software architectures. Conclusion: We can conclude that the detection of performance antipatterns, from the earliest stages of software development, represents an effective instrument to tackle the issue of identifying flaws and improving system performance. © 2016","Architecture description languages; Model-driven engineering; Performance antipatterns; Software performance analysis; Æmilia ADL","Algebra; Modeling languages; Random processes; Software architecture; Stochastic models; Stochastic systems; Telecommunication links; Anti-patterns; Architecture description languages; Component-oriented models; Model driven approach; Model-driven Engineering; Performance analysis; Software modeling languages; Software performance; Software design",2-s2.0-85006269717
"Chen D., Zhang Y.-D., Wei W., Wang S.-X., Huang R.-B., Li X.-L., Qu B.-B., Jiang S.","Efficient vulnerability detection based on an optimized rule-checking static analysis technique",2017,"Frontiers of Information Technology and Electronic Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015928002&doi=10.1631%2fFITEE.1500379&partnerID=40&md5=65bb5e5d78ed2ebc40653c2426ec3a7b","Static analysis is an efficient approach for software assurance. It is indicated that its most effective usage is to perform analysis in an interactive way through the software development process, which has a high performance requirement. This paper concentrates on rule-based static analysis tools and proposes an optimized rule-checking algorithm. Our technique improves the performance of static analysis tools by filtering vulnerability rules in terms of characteristic objects before checking source files. Since a source file always contains vulnerabilities of a small part of rules rather than all, our approach may achieve better performance. To investigate our technique’s feasibility and effectiveness, we implemented it in an open source static analysis tool called PMD and used it to conduct experiments. Experimental results show that our approach can obtain an average performance promotion of 28.7% compared with the original PMD. While our approach is effective and precise in detecting vulnerabilities, there is no side effect. © 2017, Journal of Zhejiang University Science Editorial Office and Springer-Verlag Berlin Heidelberg.","Performance improvement; Rule-based static analysis; Software quality; Software validation","Computer software selection and evaluation; Open source software; Quality control; Software design; Software engineering; Analysis techniques; Performance improvement; Performance requirements; Rule based; Software development process; Software Quality; Software validation; Vulnerability detection; Static analysis",2-s2.0-85015928002
"Kranjc J., Orač R., Podpečan V., Lavrač N., Robnik-Šikonja M.","ClowdFlows: Online workflows for distributed big data mining",2017,"Future Generation Computer Systems",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988896840&doi=10.1016%2fj.future.2016.07.018&partnerID=40&md5=3c70ffaf6ec99acf7fc631effc64a3bd","The paper presents a platform for distributed computing, developed using the latest software technologies and computing paradigms to enable big data mining. The platform, called ClowdFlows, is implemented as a cloud-based web application with a graphical user interface which supports the construction and execution of data mining workflows, including web services used as workflow components. As a web application, the ClowdFlows platform poses no software requirements and can be used from any modern browser, including mobile devices. The constructed workflows can be declared either as private or public, which enables sharing the developed solutions, data and results on the web and in scientific publications. The server-side software of ClowdFlows can be multiplied and distributed to any number of computing nodes. From a developer's perspective the platform is easy to extend and supports distributed development with packages. The paper focuses on big data processing in the batch and real-time processing mode. Big data analytics is provided through several algorithms, including novel ensemble techniques, implemented using the map-reduce paradigm and a special stream mining module for continuous parallel workflow execution. The batch mode and real-time processing mode are demonstrated with practical use cases. Performance analysis shows the benefit of using all available data for learning in distributed mode compared to using only subsets of data in non-distributed mode. The ability of ClowdFlows to handle big data sets and its nearly perfect linear speedup is demonstrated. © 2016 Elsevier B.V.","Batch processing; Big data; Cloud computing; Data mining platform; Map-reduce; Scientific workflows","Application programs; Batch data processing; Cloud computing; Data handling; Data mining; Distributed computer systems; Graphical user interfaces; Mobile devices; User interfaces; Web services; World Wide Web; Data mining platforms; Distributed development; Map-reduce; Performance analysis; Scientific publications; Scientific workflows; Software requirements; Software technology; Big data",2-s2.0-84988896840
"Zhao H., Cai D., Xin Y., Wang Y., Jia Z.","A hybrid model for Chinese Spelling check",2017,"ACM Transactions on Asian and Low-Resource Language Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017161889&doi=10.1145%2f3047405&partnerID=40&md5=53874921fcf29638484e80cf70716b16","Spelling check for Chinese has more challenging difficulties than that for other languages. A hybrid model for Chinese spelling check is presented in this article. The hybrid model consists of three components: one graph-based model for generic errors and two independently trained models for specific errors. In the graph model, a directed acyclic graph is generated for each sentence, and the single-source shortest-path algorithm is performed on the graph to detect and correct general spelling errors at the same time. Prior to that, two types of errors over functional words (characters) are first solved by conditional random fields: the confusion of (at) (pinyin is zai in Chinese), (again, more, then) (pinyin: zai) and (of) (pinyin: de), (-ly, adverb-forming particle) (pinyin: de), and (so that, have to) (pinyin: de). Finally, a rule-based model is exploited to distinguish pronoun usage confusion: (she) (pinyin: ta), (he) (pinyin: ta), and some other common collocation errors. The proposed model is evaluated on the standard datasets released by the SIGHAN Bake-off shared tasks, giving state-of-the-art results. © 2017 ACM 2375-4699/2017/03-ART21 $15.00.","Chinese spelling check; Conditional random field; Graph model; Hybrid model; Rule-based model","Directed graphs; Errors; Graphic methods; Image segmentation; Random errors; Random processes; Conditional random field; Graph model; Hybrid model; Rule-based models; Spelling checks; Graph theory",2-s2.0-85017161889
"Huang L., May J., Pan X., Ji H., Ren X., Han J., Zhao L., Hendler J.A.","Liberal entity extraction: Rapid construction of fine-grained entity typing systems",2017,"Big Data",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016399563&doi=10.1089%2fbig.2017.0012&partnerID=40&md5=0651d310f737a8b6cfdabb39069ac029","The ability of automatically recognizing and typing entities in natural language without prior knowledge (e.g., predefined entity types) is a major challenge in processing such data. Most existing entity typing systems are limited to certain domains, genres, and languages. In this article, we propose a novel unsupervised entity-typing framework by combining symbolic and distributional semantics. We start from learning three types of representations for each entity mention: general semantic representation, specific context representation, and knowledge representation based on knowledge bases. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework does not rely on any annotated data, predefined typing schema, or handcrafted features; therefore, it can be quickly adapted to a new domain, genre, and/or language. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework. © 2017, Mary Ann Liebert, Inc.","fine-grained entity typing; Liberal Information Extraction; multi-level entity mention and representation; unsupervised learning",,2-s2.0-85016399563
"An C., Huang J., Chang S., Huang Z.","Question similarity modeling with bidirectional long short-term memory neural network",2017,"Proceedings - 2016 IEEE 1st International Conference on Data Science in Cyberspace, DSC 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016791663&doi=10.1109%2fDSC.2016.13&partnerID=40&md5=a799aa1a607a9b26cea043fe4780c6ee","Modeling sentence similarity all along is a challengeable task in the field of natural language processing (NLP), since ambiguity and variability of linguistic expression. Specifically, in the field of community question answering (CQA), homologous hotspot is focusing on question retrieval. To get the most similar question compared with user's query, we proposed a question model building with Bidirectional Long Short-Term Memory (BLSTM) neural networks, which as well can be used in other fields, such as sentence similarity computation, paraphrase detection, question answering and so on. We evaluated our model in labeled Yahoo! Answers data, and results show that our method achieves significant improvement over existing methods without using external resources, such as WordNet or parsers. © 2016 IEEE.","Community question answering; Long short term memory; Question retrieval; Sentence similarity","Brain; Computers; Modeling languages; Natural language processing systems; Syntactics; Community question answering; External resources; Linguistic expressions; NAtural language processing; Question Answering; Question retrieval; Sentence similarity; Similarity models; Long short-term memory",2-s2.0-85016791663
"Zhou Q.-Y., Zheng D.-Q., Zhao T.-J.","SLDP: Sequence learning dependency parsing model using long short-term memory",2017,"Proceedings - International Conference on Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021057747&doi=10.1109%2fICMLC.2016.7860886&partnerID=40&md5=fe27d4e37fc703e08fcee0740200fa0c","Recent work on neural network models shows success in dependency parsing. In this paper, we present a sequence learning dependency parsing (SLDP) model using long short-term memory for shift-reduce parser. A feed-forward neural network is used to build greedy model from rich local features. With the features extracted by the local model, we further train a long short-term memory (LSTM) model optimized for global parsing sequences. Our model has the capability of learning not only atomic feature combinations automatically but also the long distance dependent information for dependency parsing. Experiments on English Penn Treebank show that our SLDP model significantly outperforms the baseline, achieving 90.7% unlabeled attachment score and 89.0% labeled attachment score. © 2016 IEEE.","Dependency parsing; Long short-term memory; Natural language processing; Neural networks; Syntactic parsing","Artificial intelligence; Brain; Cybernetics; Education; Learning algorithms; Learning systems; Natural language processing systems; Neural networks; Syntactics; Dependency parsing; Feature combination; Local feature; Local model; Neural network model; Sequence learning; Syntactic parsing; Treebanks; Long short-term memory",2-s2.0-85021057747
"Hoque M.M., Quaresma P.","A content-aware hybrid architecture for answering questions from open-domain texts",2017,"19th International Conference on Computer and Information Technology, ICCIT 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016190636&doi=10.1109%2fICCITECHN.2016.7860212&partnerID=40&md5=520f988f0c7442d1a7399d528134527e","The current work blends the different paradigms of Question Answering systems and presents a content-aware hybrid architecture for an open-domain factoid questions. It combines a knowledge-based, information extraction-based and a web-based approach in a pipelined architecture to construct an answer to a question keeping the context and discourse of the question in view. The proposed semantic-aware hybrid architecture was compared with other QA systems designed over standard benchmark data. The work has shown enough potential in terms of accuracy and time domain complexity and can be used effectively as a semantic understanding-based QA system. © 2016 IEEE.","Entity detection; Hybrid architecture; Knowledge-based system; Pipelined architecture; Question answering system; Semantic core; Semantic parsing; Text-knowledge","Artificial intelligence; Data mining; Natural language processing systems; Pipeline processing systems; Semantics; Entity detection; Hybrid architectures; Pipelined architecture; Question answering systems; Semantic parsing; Text-knowledge; Knowledge based systems",2-s2.0-85016190636
"Velikovich L.","Semantic model for fast tagging of word lattices",2017,"2016 IEEE Workshop on Spoken Language Technology, SLT 2016 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016016199&doi=10.1109%2fSLT.2016.7846295&partnerID=40&md5=e4e2a254ab71fe5c7bbe86ea51c7c371","This paper introduces a semantic tagger that inserts tags into a word lattice, such as one produced by a real-time large-vocabulary speech recognition system. Benefits of such a tagger include the ability to rescore speech recognition hypotheses based on this metadata, as well as providing rich annotations to clients downstream. We focus on the domain of spoken search queries and voice commands, which can be useful for building an intelligent assistant. We explore a method to distill a pre-existing very large named entity disambiguation (NED) model into a lightweight tagger. This is accomplished by constructing a joint distribution of tagged n-grams from a supervised training corpus, then deriving a conditional distribution for a given lattice. With 300 tagging categories, the tagger achieves a precision of 88.2% and recall of 93.1% on 1-best paths in speech recognition lattices with 2.8ms median latency. © 2016 IEEE.","Knowledge distillation; Lattice rescoring; Named entity disambiguation; Word lattice","Computational linguistics; Deep neural networks; Distillation; Semantics; Conditional distribution; Intelligent assistants; Joint distributions; Large vocabulary speech recognition; Lattice rescoring; Named entity disambiguations; Supervised trainings; Word lattice; Speech recognition",2-s2.0-85016016199
"Chen Y.-N., Hakanni-Tür D., Tur G., Celikyilmaz A., Guo J., Deng L.","Syntax or semantics? Knowledge-guided joint semantic frame parsing",2017,"2016 IEEE Workshop on Spoken Language Technology, SLT 2016 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016048546&doi=10.1109%2fSLT.2016.7846288&partnerID=40&md5=a993beb200211741bb2ad691e05a5842","Spoken language understanding (SLU) is a core component of a spoken dialogue system, which involves intent prediction and slot filling and also called semantic frame parsing. Recently recurrent neural networks (RNN) obtained strong results on SLU due to their superior ability of preserving sequential information over time. Traditionally, the SLU component parses semantic frames for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper proposes to apply knowledge-guided structural attention networks (K-SAN), which additionally incorporate non-flat network topologies guided by prior knowledge, to a language understanding task. The model can effectively figure out the salient substructures that are essential to parse the given utterance into its semantic frame with an attention mechanism, where two types of knowledge, syntax and semantics, are utilized. The experiments on the benchmark Air Travel Information System (ATIS) data and the conversational assistant Cortana data show that 1) the proposed K-SAN models with syntax or semantics outperform the state-of-the-art neural network based results, and 2) the improvement for joint semantic frame parsing is more significant, because the structured information provides rich cues for sentence-level understanding, where intent prediction and slot filling can be mutually improved. © 2016 IEEE.","Deep learning; Joint semantic frame parsing; Knowledge-guided structural attention networks; Spoken dialogue system; Spoken language understanding","Advanced traveler information systems; Automata theory; Deep learning; Recurrent neural networks; Semantics; Speech processing; Speech recognition; Language understanding; Linguistic properties; Recurrent neural network (RNN); Semantic frames; Sequential information; Spoken dialogue system; Spoken language understanding; Structured information; Syntactics",2-s2.0-85016048546
"Iftikhar A., Iftikhar E., Mehmood M.K.","Domain specific query generation from natural language text",2017,"2016 6th International Conference on Innovative Computing Technology, INTECH 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015267584&doi=10.1109%2fINTECH.2016.7845105&partnerID=40&md5=a553f18fff503602888e7532a81bf7e1","This paper presents an approach to automate the generation of Structure Query Language from Natural Language Text. Software requirements specifications are most important part of Natural Language Processing, as little mistake in this phase results in absurd software design. Software Specifications are used in software industry. When we automatic translate these Natural Language Text into Structured Query Text we find Many issues because Software Specification is not an independent sentence they have many module related to each other. so when we translate these English text we have found many issues such as discourse, semantic and negation problem. The evaluation method for Natural Language texts is to test against a list of sentences, each of which is paired with yes or no. For this case I have study Natural Language text and their problems in my MS thesis. What Natural Language Texts are and what issues are found when we automatic translate these Texts into SQL. We used Stanford dependency parser for text translation. © 2016 IEEE.","discourse; natural language processing; semantic; Structure Query Language","Gears; Query languages; Semantics; Software design; Software engineering; Specifications; Syntactics; Translation (languages); discourse; NAtural language processing; Natural language text; Software industry; Software requirements specifications; Software Specification; Structure query languages; Structured queries; Natural language processing systems",2-s2.0-85015267584
"Joseph J., Panicker J.R., Meera M.","An efficient natural language interface to XML database",2017,"Proceedings - 2016 International Conference on Information Science, ICIS 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016000029&doi=10.1109%2fINFOSCI.2016.7845328&partnerID=40&md5=5b0ce02b8be3f12163954507fdf09d76","In this era databases are used in many fields like banking, human resources, universities etc. Everyone needs to deal with databases for the extraction of required data. But it is very difficult for a common user to retrieve information from the database using database query languages. Access any kind of data from database using natural language like English is a convenient and easy method instead of using formal query languages such as XQuery, SQL etc. The proposed system can accept English language sentences and then it is translated into an XQuery expression. This XQuery statement can be evaluated against an XML database. This query translation is done by mapping the tokens in the dependency parse tree of the natural language query into the XQuery fragments. The existing systems in this field work without handling the multi- sentence queries, yes-no queries and wh-queries. This paper introduces a novel architecture which is capable of translating a wide range of natural language queries into formal database queries. © 2016 IEEE.","Natural Language Interface; Natural Language Processing; XML Database; XQuery","Natural language processing systems; Query processing; Search engines; Translation (languages); XML; Database query language; Natural language interfaces; NAtural language processing; Natural language queries; Novel architecture; XML database; XQuery; XQuery expressions; Query languages",2-s2.0-85016000029
"Li C., Gu J.","A SQL transformation model of MongoDB based on ANTLR",2017,"Xibei Gongye Daxue Xuebao/Journal of Northwestern Polytechnical University",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015828517&partnerID=40&md5=d3558594ec8788de933bd7a70dc6712a","As a typical storage and management model of non relational data, NoSQL(not only structured query language) technology has become one of the hot research topics in recent years, with the development of big data tides. Through the analysis of the characteristics of MongoDB, which is a well-known NoSQL database, in order to solve the problem of the lack of SQL(structured query language) access interface, this paper presents a SQL transformation model of MongoDB based on ANTLR, and focuses on its hierarchical structure, SQL parser, semantic conversion and the operation mechanism. A prototype system is designed and developed. The effect of this model is verified, and the expected targets are achieved. The outstanding contribution of this research is that it can reduce the learning cost of software developers, and improve the efficiency of development. © 2017, Editorial Board of Journal of Northwestern Polytechnical University. All right reserved.","ANTLR; Application programming interfaces(API); Big data; MongoDB; NoSQL; SQL; Time delay; Transformation model","Application programming interfaces (API); Digital storage; Information management; Metadata; Query languages; Query processing; Research and development management; Semantics; Time delay; ANTLR; Hierarchical structures; Hot research topics; MongoDB; NoSQL; Semantic conversion; Structured Query Language; Transformation model; Big data",2-s2.0-85015828517
"Hills M., Klint P., Vinju J.J.","Enabling PHP software engineering research in Rascal",2017,"Science of Computer Programming",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971597188&doi=10.1016%2fj.scico.2016.05.003&partnerID=40&md5=02995143c05a14ce8108caf722d85092","Today, PHP is one of the most popular programming languages, and is commonly used in the open source community and in industry to build large application frameworks and web applications. In this paper, we discuss our ongoing work on PHP AiR, a framework for PHP Analysis in Rascal. PHP AiR is focused especially on program analysis and empirical software engineering, and is being used actively and effectively in work on evaluating PHP feature usage and system evolution, on program analysis for refactoring and security validation, and on source code metrics. We describe the requirements and design decisions for PHP AiR, summarize current research using PHP AiR, discuss lessons learned, and briefly sketch future work. © 2016 Elsevier B.V.","Dynamic languages; Empirical software engineering; Meta-programming; PHP; Program analysis","Computational linguistics; Computer programming; Object oriented programming; Open source software; Software engineering; Application frameworks; Design decisions; Dynamic languages; Empirical Software Engineering; Meta Programming; Open source communities; Program analysis; Source code metrics; Engineering research",2-s2.0-84971597188
"Jiang L., Zhao Z.","Grammar-aware parallelization for scalable XPath querying",2017,"Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014433333&doi=10.1145%2f3018743.3018772&partnerID=40&md5=8a64b236017b8abaff63b0f00f1ac656","Semi-structured data emerge in many domains, especially in web analytics and business intelligence. However, querying such data is inherently sequential due to the nested structure of input data. Existing solutions pessimistically enumerate all execution paths to circumvent dependencies, yielding sub-optimal performance and limited scalability. This paper presents GAP, a parallelization scheme that, for the first time, leverages the grammar of the input data to boost the parallelization efficiency. GAP leverages static analysis to infer feasible execution paths for specific contexts based on the grammar of the semi-structured data. It can eliminate unnecessary paths without compromising the correctness. In the absence of a pre-defined grammar, GAP switches into a speculative execution mode and takes potentially incomplete grammar extracted either from prior inputs. Together, the dual-mode GAP reduces the execution paths from all paths to a minimum, therefore maximizing the parallelization efficiency and scalability. The benefits of path elimination go beyond reducing extra computation - it also enables the use of more efficient data structures, which further improves the efficiency. An evaluation on a large set of standard benchmarks with diverse queries shows that GAP yields significant efficiency increase and boosts the speedup of the state-of-the-art from 2.9X to 17.6X on a 20-core machine for a set of 200 queries. © 2017 ACM.","Grammar; Parallelization; XML; XPath","Input output programs; Parallel programming; Scalability; Static analysis; XML; Efficient data structures; Grammar; Parallelization efficiency; Parallelizations; Semi structured data; Speculative execution; Sub-optimal performance; XPath; Efficiency",2-s2.0-85014433333
"Marginean A.N., Eniko K.","Towards lexicalization of DBpedia ontology with unsupervised learning and semantic role labeling",2017,"Proceedings - 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013887174&doi=10.1109%2fSYNASC.2016.048&partnerID=40&md5=fb63ab9667b2b39623121472793cd35d","Filling the gap between natural language expressions and ontology concepts or properties is the new trend in Semantic Web. Ontology lexicalization introduces a new layer of lexical information for ontology properties and concepts. We propose a method based on unsupervised learning for the extraction of the potential lexical expressions of DBpedia propertiesfrom Wikipedia text corpus. It is a resource-driven approach that comprises three main steps. The first step consists of the extraction of DBpedia triples for the aimed property followed by the extraction of Wikipedia articles describing the resources from these triples. In the second step, sentences mostly related to the property are extracted from the articles and they are analyzed with a Semantic Role Labeler resulting in a set of SRL annotated trees. In the last step, clusters of expressions are built using spectral clustering based on the distances between the SRL trees. The clusters with the least variance are considered to be relevant for the lexical expressions of the property. © 2016 IEEE.","DBpedia; ontology lexicalization; semantic role labeling; semantic similarity; spectral clustering","Clustering algorithms; Extraction; Natural language processing systems; Ontology; Dbpedia; Lexicalization; Semantic role labeling; Semantic similarity; Spectral clustering; Unsupervised learning",2-s2.0-85013887174
"Gonçalves B.","Show me the material evidence-Initial experiments on evaluating hypotheses from user-generated multimedia data",2017,"Proceedings - 2016 IEEE International Symposium on Multimedia, ISM 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015254604&doi=10.1109%2fISM.2016.24&partnerID=40&md5=0c314260b9a570947115c94c8ba5283f","Subjective questions such as 'does neymar dive', or 'is clinton lying', or 'is trump a fascist', are popular queries to web search engines, as can be seen by autocompletion suggestions on Google, Yahoo and Bing. In the era of cognitive computing, beyond search, they could be handled as hypotheses issued for evaluation. Our vision is to leverage on unstructured data and metadata of the rich user-generated multimedia that is often shared as material evidence in favor or against hypotheses in social media platforms. In this paper we present two preliminary experiments along those lines and discuss challenges for a cognitive computing system that collects material evidence from user-generated multimedia towards aggregating it into some form of collective decision on the hypothesis. © 2016 IEEE.","Cognitive computing; Material evidence; Social media hypothesis management; User-generated multimedia","Search engines; Cognitive Computing; Cognitive computing systems; Collective decision; Hypothesis management; Multimedia data; Social media platforms; Unstructured data; User-generated; Social networking (online)",2-s2.0-85015254604
"Vizza P., Guzzi P.H., Veltri P., Cascini G.L., Curia R., Sisca L.","GIDAC: A prototype for bioimages annotation and clinical data integration",2017,"Proceedings - 2016 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013287169&doi=10.1109%2fBIBM.2016.7822663&partnerID=40&md5=2f4e66f1aac43f65ecc0050a8de36b60","The analysis of bioimages and their correlated clinical patient information allows to investigate specific diseases and define the corresponding medical protocols. To perform a correct diagnosis and apply a precise therapy, bioimages must be collected and studied together with others relevant data as well as laboratory results, medical annotations and patient history. Today, the management of these data is performed by single systems inside hospital departments that often do not provide dedicated data integration platforms among different departments as well as different health structures to exchange of relevant clinical information. Also, images cannot be annotated or enriched by physicians to trace temporal studies for patients or even among patients with similar diseases. In this contribution, we report the results of a research project called GIDAC (standing for Gestione Integrata DAti Clinici) that aims to define a general purpose framework for the bioimages management and annotations as well as clinical data view and integration in a simple-to-use information system. The proposed framework does not substitute any existing clinical information system but is able in gathering and integrating data by using a XML-based module. The novelty also consists in allowing annotations on DICOM images by means of simple user-interface to take trace of changes intra images as well as comparisons among patients. This system supports oncologists in the management of DICOM images from different devices (e.g., ecograph or PACS) to extract relevant information necessary to query (annotate) images and study similar clinical cases. © 2016 IEEE.",,"Bioinformatics; Computer aided diagnosis; Data integration; Diagnosis; Information systems; Medical applications; Medical information systems; Patient treatment; Search engines; User interfaces; Clinical information; Clinical information system; General purpose framework; Health structures; Integration platform; Medical annotation; Medical protocols; Patient information; Information management",2-s2.0-85013287169
"Tahr K.H.","A global approach for determining protein function",2017,"Proceedings - 2016 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013270499&doi=10.1109%2fBIBM.2016.7822681&partnerID=40&md5=56f233237eed5de3b72c2e22f6326318","Proteins and amino acid residues mentioned in biomedical texts associated with an un-annotated protein p can be considered as characteristics terms for p. They are highly predictive of the potential functions of p. Similarly, proteins and amino acid residues mentioned in biomedical texts associated with proteins annotated with a functional category f can be considered as characteristics terms of f. We introduce in this paper an information extraction system called DPFG that predicts the functions of an un-annotated protein p by representing p and each functional category f by a vector of weights. Each weight reflects the degree of association between a characteristic term and p (or a characteristic term and f). First, DPFG constructs a network, whose nodes represent the different functional categories and edges the interrelationships between them, which are determined based on their vectors' similarities. Then, it determines the functions of p by employing random walks with restarts on the mentioned network. The walker is the vector of p. Finally, p will be assigned the functional categories of the nodes in the network that are visited the most by the walker. We evaluated the quality of DPFG by comparing it experimentally with two other systems. Results showed marked improvement. © 2016 IEEE.","Biomedical text; Information extraction; Protein annotation; Protein function prediction; Text mining","Amino acids; Bioinformatics; Data mining; Information analysis; Information retrieval; Information retrieval systems; Natural language processing systems; Amino acid residues; Biomedical text; Degree of association; Information extraction systems; Potential function; Protein annotation; Protein function prediction; Text mining; Proteins",2-s2.0-85013270499
"Olney W., Hill E., Thurber C., Lemma B.","Part of speech tagging Java method names",2017,"Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013119842&doi=10.1109%2fICSME.2016.80&partnerID=40&md5=6506e0547a7eac4738459dc98fab73ff","Numerous software engineering tools for evolution and comprehension, including code search, comment generation, and analyzing bug reports, make use of part-of-speech (POS) information. However, many POS taggers are developed for, and trained on, natural language. In this paper, we investigate the accuracy of 9 POS taggers on over 200 source code identifiers taken from method names in open source Java programs. The set of taggers includes traditional POS taggers for English as well as some tuned to source code identifiers. Our results indicate that taggers tailored for source code are significantly more effective. © 2016 IEEE.",,"Codes (symbols); Computer programming languages; Computer software; Computer software maintenance; Java programming language; Natural language processing systems; Open source software; Open systems; Search engines; Software engineering; Java methods; Java program; Natural languages; Open sources; Part Of Speech; Part of speech tagging; Software engineering tools; Source codes; Computational linguistics",2-s2.0-85013119842
"McMinn P., Wright C.J., Kinneer C., McCurdy C.J., Camara M., Kapfhammer G.M.","SchemaAnalyst: Search-based test data generation for relational database schemas",2017,"Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013080990&doi=10.1109%2fICSME.2016.93&partnerID=40&md5=e5c82faa2fda08a19eca53d9ab253e3c","Data stored in relational databases plays a vital role in many aspects of society. When this data is incorrect, the services that depend on it may be compromised. The database schema is the artefact responsible for maintaining the integrity of stored data. Because of its critical function, the proper testing of the database schema is a task of great importance. Employing a search-based approach to generate high-quality test data for database schemas, SchemaAnalyst is a tool that supports testing this key software component. This presented tool is extensible and includes both an evaluation framework for assessing the quality of the generated tests and full-featured documentation. In addition to describing the design and implementation of SchemaAnalyst and overviewing its efficiency and effectiveness, this paper coincides with the tool's public release, thereby enhancing practitioners' ability to test relational database schemas. © 2016 IEEE.",,"Computer software maintenance; Critical functions; Design and implementations; Evaluation framework; High Quality Test; Its efficiencies; Relational Database; Search-based test data generations; Software component; Software testing",2-s2.0-85013080990
"Liu S., Sun J., Xiao H., Wadhwa B., Dong J.S., Wang X.","Improving Quality of Use Case Documents through Learning and User Interaction",2017,"Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012050580&doi=10.1109%2fICECCS.2016.021&partnerID=40&md5=e14ad90d01e4ef0a84e4e36c32b8aa48","Use cases are widely used to capture user requirements based on interactions between different roles in the system. They are mostly documented in natural language and sometimes aided with graphical illustrations in the form of use case diagrams. Use cases serve as an important means to communicate among stakeholders, requirement engineers and system engineers as they are easy to understand and are produced early in the software development process. Having high quality use cases are beneficial in many ways, e.g., in avoiding inconsistency/incompleteness in requirements, in guiding system design, in generating test cases. In this work, we propose an approach to improve the quality of use cases using techniques including natural language processing and machine learning. The central idea is to discover potential problems in use cases through active learning and human interaction and provide feedbacks in natural language. We conduct user studies with a real-world use case document. The results show that our method is helpful in improving use cases with a reasonable amount of user interaction. © 2016 IEEE.","L; NLP; Use Case","Artificial intelligence; Learning algorithms; Learning systems; Software design; Software engineering; Graphical illustrations; Human interactions; NAtural language processing; Natural languages; Potential problems; Software development process; System engineers; User requirements; Natural language processing systems",2-s2.0-85012050580
"Sharoff S.","Corpus and systemic functional linguistics",2017,"The Routledge Handbook of Systemic Functional Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021304650&doi=10.4324%2f9781315413891&partnerID=40&md5=a95acba28a8fceac3146948c88e60352",[No abstract available],,,2-s2.0-85021304650
"Sahoo D., Balabantaray R., Phukon M., Saikia S.","Aspect based multi-document summarization",2017,"Proceeding - IEEE International Conference on Computing, Communication and Automation, ICCCA 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011081912&doi=10.1109%2fCCAA.2016.7813838&partnerID=40&md5=46e3a5a6d4cf4f05b18cc04cee60c322","Multi-document summarization is useful when a user deals with a group of heterogeneous documents and wants to compile the important information present in the collection, or there is a group of homogeneous documents, taken out from a large corpus as a result of a query. We present an approach to automatic multi-document summarization that depends on clustering and sentence extraction. User provides a query, based on the query; documents that are relevant to the query are extracted from a document corpus containing documents from various domains. An n × n similarity matrix is created among the sentences having sentence level similarity in all extracted documents. Then clusters of similar sentences are formed using Markov clustering algorithm. In each cluster, each sentence is assigned five different weights 1. Chronological weight of sentence (Document level) 2. Position weight of sentence (position of sentence in the document) 3. Sentence weight (based on term weight) 4. Aspect based weight (sentence containing aspect words) and 5. Synonymy and Hyponym Weight. Then top ranked sentences having highest weight are extracted from each cluster and presented to user. © 2016 IEEE.","Aspect Weight; Chronological Weight; Clusteri; Positional Weigh; Summarization; Term Weigt","Computer programming; Computer science; Aspect Weight; Chronological Weight; Clusteri; Summarization; Term Weigt; Clustering algorithms",2-s2.0-85011081912
"Azizan A., Bakar Z.A., Noah S.A.","Query reformulation using ontology and keyword for durian web search",2017,"2016 3rd International Conference on Information Retrieval and Knowledge Management, CAMP 2016 - Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015962509&doi=10.1109%2fINFRKM.2016.7806342&partnerID=40&md5=74207c95937324bea0d5b7693e10a0a4","Query reformulation techniques based on ontological approach have been studied as a method to improve retrieval effectiveness. However, the evaluation of this techniques has primarily focused on comparing the technique with ontology and without ontology. The aim of this paper is to present, evaluate and compare the proposed technique in four different possibilities of reformulation. In this study we propose the combination of ontology terms and keywords from the query to reformulate new queries. The experimental result shows that reformulation using ontology terms alone has increases recall and decreases precision. However, better results were obtained when the ontology terms being combined with the query's keywords. © 2016 IEEE.","durian; ontology; query keyword; query reformulation; recall-precision","Information retrieval; Knowledge management; durian; Ontological approach; Ontology terms; query keyword; Query reformulation; recall-precision; Retrieval effectiveness; Web searches; Ontology",2-s2.0-85015962509
"Ali A.A., Saad S.","Term extraction and hierarchy induction method based on islamic dictionary",2017,"2016 3rd International Conference on Information Retrieval and Knowledge Management, CAMP 2016 - Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015959387&doi=10.1109%2fINFRKM.2016.7806345&partnerID=40&md5=a823ee28ffc493856f338f9afd8cc46b","A machine readable dictionary (MRD) is an electronic dictionary that enables query processing. One of the common processing tasks that has been widely applied is Concept Hierarchy Induction which aims at identifying concepts with its corresponding taxonomies. The existing concept hierarchy approaches for Islamic domain are using limited linguistic patterns. This study aims to propose an unsupervised concept hierarchy induction for the Islamic domain by extending the patterns and rules. In fact, Term Frequency-Inverse Document Frequency (TF-IDF) was carried out in order to identify the most frequently used concepts. Furthermore, two syntactical features were used including POS tagging and chunk parser in order to identify the tagging for each word (e.g. verb, noun, adjective, etc.) and extracting Noun Phrases (NP). Hence, the proposed extension patterns aim at utilize lexico-syntactic patterns to induce the concept hierarchy. That demonstrates the usefulness of extending patterns for the Islamic domain. © 2016 IEEE.","concept hierarchy; lexico-syntactic patterns; terminology extraction","Computational linguistics; Extraction; Information retrieval; Knowledge management; Natural language processing systems; Terminology; Text processing; Concept hierarchies; Electronic dictionaries; Induction method; Lexico-syntactic patterns; Linguistic patterns; Machine-readable dictionaries; Term frequencyinverse document frequency (TF-IDF); Terminology extraction; Syntactics",2-s2.0-85015959387
"Mozzherin D.Y., Myltsev A.A., Patterson D.J.","""gnparser"": A powerful parser for scientific names based on Parsing Expression Grammar",2017,"BMC Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020231250&doi=10.1186%2fs12859-017-1663-3&partnerID=40&md5=8f4f231bb72495ec888426fed110d223","Background: Scientific names in biology act as universal links. They allow us to cross-reference information about organisms globally. However variations in spelling of scientific names greatly diminish their ability to interconnect data. Such variations may include abbreviations, annotations, misspellings, etc. Authorship is a part of a scientific name and may also differ significantly. To match all possible variations of a name we need to divide them into their elements and classify each element according to its role. We refer to this as 'parsing' the name. Parsing categorizes name's elements into those that are stable and those that are prone to change. Names are matched first by combining them according to their stable elements. Matches are then refined by examining their varying elements. This two stage process dramatically improves the number and quality of matches. It is especially useful for the automatic data exchange within the context of ""Big Data"" in biology. Results: We introduce Global Names Parser (gnparser). It is a Java tool written in Scala language (a language for Java Virtual Machine) to parse scientific names. It is based on a Parsing Expression Grammar. The parser can be applied to scientific names of any complexity. It assigns a semantic meaning (such as genus name, species epithet, rank, year of publication, authorship, annotations, etc.) to all elements of a name. It is able to work with nested structures as in the names of hybrids. gnparser performs with ≈ 99% accuracy and processes 30 million name-strings/hour per CPU thread. The gnparser library is compatible with Scala, Java, R, Jython, and JRuby. The parser can be used as a command line application, as a socket server, a web-app or as a RESTful HTTP-service. It is released under an Open source MIT license. Conclusions: Global Names Parser (gnparser) is a fast, high precision tool for biodiversity informaticians and biologists working with large numbers of scientific names. It can replace expensive and error-prone manual parsing and standardization of scientific names in many situations, and can quickly enhance the interoperability of distributed biological information. © The Author(s) 2017.","Biodiversity; Biodiversity informatics; Names based cyberinfrastructure; Parser; Parsing Expression Grammar; Scala; Scientific name; Semantic parser","Big data; Biodiversity; Biology; Computational linguistics; Electronic data interchange; HTTP; Information dissemination; Java programming language; Open source software; Semantics; Cyber infrastructures; Informatics; Parser; Parsing expression grammars; Scala; Scientific name; Formal languages; biodiversity; biology; genus; grammar; human; human experiment; information science; language; licence; machine; nomenclature; publication; species; spelling; standardization; writing; biodiversity; computer interface; information science; Internet; nomenclature; Biodiversity; Informatics; Internet; Terminology as Topic; User-Computer Interface",2-s2.0-85020231250
"Marton J., Szárnyas G., Búr M.","Model-driven engineering of an OpenCypher engine: Using graph queries to compile graph queries",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030695671&doi=10.1007%2f978-3-319-68015-6_6&partnerID=40&md5=bccde8a5961bc13aa2845bd24f0143e0","Graph database systems are increasingly adapted for storing and processing heterogeneous network-like datasets. Many challenging applications with near real-time requirements—such as financial fraud detection, on-the-fly model validation and root cause analysis—can be formalised as graph problems and tackled with graph databases efficiently. However, as no standard graph query language has yet emerged, users are subjected to the possibility of vendor lock-in. The openCypher group aims to define an open specification for a declarative graph query language. However, creating an openCypher-compatible query engine requires significant research and engineering efforts. Meanwhile, model-driven language workbenches support the creation of domain-specific languages by providing high-level tools to create parsers, editors and compilers. In this paper, we present an approach to build a compiler and optimizer for openCypher using model-driven technologies, which allows developers to define declarative optimization rules. © 2017, Springer International Publishing AG.",,"Computer programming languages; Embedded systems; Engines; Heterogeneous networks; Problem oriented languages; Program compilers; Query languages; Query processing; Systems analysis; XML; Domain specific languages; Financial fraud detections; Graph query language; Language workbenches; Model validation; Model-driven Engineering; Optimization rules; Root cause analysis; High level languages",2-s2.0-85030695671
"Bhama P.R.K.S., Senthilkumar R., Varshinee P.","XQUICK: An efficient path-based XML storage scheme for fast query processing and update",2017,"Journal of Internet Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018844378&doi=10.6138%2fJIT.2017.18.2.20130801&partnerID=40&md5=6e56e0c2998d46010386e6361b67c514","Due to the inherent flexibility in both structure and semantics, XML documents are massive in nature. The ratio of the size of the XML document to the size of the text data in it is usually large. Apart from data values, the huge size of the XML document is contributed by its tree structure. The structure of the XML document tightly bounded with the data renders the original form of XML less efficient in terms of both time and space. The problem of designing a compressor for XML documents which facilitates both update and query operations has turned the attention of many. In this paper, we propose an efficient storage scheme for XML documents called XQUICK. XQUICK exploits the high regularity of XML documents to compress the tree structure. It also handles updates in an efficient manner with minimum space and time overhead. This paper also describes a novel path-based querying approach that supports fast querying. Additional mechanisms such as indexing are provided to elicit faster query processing. XQUICK can also be used in conjunction with standard parsers like DOM, SAX etc. Experimental results conform to the capabilities of proposed scheme.","Compact structure; Query; RFX; XML","Digital storage; Forestry; Indexing (materials working); Query processing; Semantics; Trees (mathematics); Compact structures; Efficient path; Inherent flexibility; Query; Query operations; Space and time; Storage schemes; Tree structures; XML",2-s2.0-85018844378
"Eldawy A., Sabek I., Elganainy M., Bakeer A., Abdelmotaleb A., Mokbel M.F.","Sphinx: Empowering impala for efficient execution of SQL queries on big spatial data",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028467434&doi=10.1007%2f978-3-319-64367-0_4&partnerID=40&md5=f1650833d90ed2bde7cd7bdf8202b606","This paper presents Sphinx, a full-fledged open-source system for big spatial data which overcomes the limitations of existing systems by adopting a standard SQL interface, and by providing a high efficient core built inside the core of the Apache Impala system. Sphinx is composed of four main layers, namely, query parser, indexer, query planner, and query executor. The query parser injects spatial data types and functions in the SQL interface of Sphinx. The indexer creates spatial indexes in Sphinx by adopting a two-layered index design. The query planner utilizes these indexes to construct efficient query plans for range query and spatial join operations. Finally, the query executor carries out these plans on big spatial datasets in a distributed cluster. A system prototype of Sphinx running on real datasets shows up-to three orders of magnitude performance improvement over plain-vanilla Impala, SpatialHadoop, and PostGIS. © Springer International Publishing AG 2017.",,"Open systems; Distributed clusters; Existing systems; Open source system; Real data sets; Spatial datasets; Spatial indexes; System prototype; Three orders of magnitude; Query processing",2-s2.0-85028467434
"Starc J., Mladenić D.","Joint learning of ontology and semantic parser from text",2017,"Intelligent Data Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009888114&doi=10.3233%2fIDA-150452&partnerID=40&md5=1f4581dc55a7abe11137777a183834ff","Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels - classes, instances, taxonomic and non-taxonomic relations. The approach was evaluated on the first sentences of Wikipedia pages describing people. © 2017 - IOS Press and the authors. All rights reserved.","context-free grammar; grammar induction; Ontology learning; semantic parsing","Context free grammars; Forestry; Formal languages; Ontology; Syntactics; Grammar induction; Joint learning; Ontology learning; Semantic parsing; Semantic tree; Semi-automatics; Wikipedia; Semantics",2-s2.0-85009888114
"Dietrich G., Ertl M., Fette G., Kaspar M., Krebs J., MaCkenrodt D., Störk S., Puppe F.","Extending the query language of a data warehouse for patient recruitment",2017,"Studies in Health Technology and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029576263&doi=10.3233%2f978-1-61499-808-2-152&partnerID=40&md5=f7c83120315604ba1e7a7da4e8d386e7","Patient recruitment for clinical trials is a laborious task, as many texts have to be screened. Usually, this work is done manually and takes a lot of time. We have developed a system that automates the screening process. Besides standard keyword queries, the query language supports extraction of numbers, time-spans and negations. In a feasibility study for patient recruitment from a stroke unit with 40 patients, we achieved encouraging extraction rates above 95% for numbers and negations and ca. 86% for time spans. © 2017 German Association for Medical Informatics, Biometry and Epidemiology (gmds) e.V. and IOS Press.","Clinical trials; Data Warehouse; Information extraction; Text queries","adult; clinical article; extraction; feasibility study; female; human; language; male; stroke unit",2-s2.0-85029576263
"El-Ansari A., Beni-Hssane A., Saadi M.","A multiple ontologies based system for answering natural language questions",2017,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989925738&doi=10.1007%2f978-3-319-46568-5_18&partnerID=40&md5=1d0dd87967e87126327980adff59ab28","Due to the massive growth of information on the Web, information retrieval systems come to play a more critical role. Most of these systems are based on content matching rather than the meaning, therefore the returned results are not always relevant to the user. To solve this problem, the next generation of information retrieval systems focus on the meaning of the user query and search data using ontologies that provide the vocabulary and structure associated with metadata. In this work we present a Question Answering system which combines multiple knowledge bases, with a Natural Language parser to transform questions into SPARQL queries or other query language. We demonstrate the feasibility to build such a semantic QA system and the accuracy and relevance of the returned results. © Springer International Publishing AG 2017.","Natural language processing; Ontology; Question answering system; Semantic Web","Artificial intelligence; Information retrieval; Information retrieval systems; Ontology; Query languages; Search engines; Semantic Web; Content matching; Knowledge basis; NAtural language processing; Natural language questions; Natural languages; Question answering systems; Sparql queries; User query; Natural language processing systems",2-s2.0-84989925738
"Shastry B., Leutner M., Fiebig T., Thimmaraju K., Yamaguchi F., Rieck K., Schmid S., Seifert J.-P., Feldmann A.","Static Program Analysis as a Fuzzing Aid",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032868095&doi=10.1007%2f978-3-319-66332-6_2&partnerID=40&md5=e03776e682c9c04e4e04875ac88ab361","Fuzz testing is an effective and scalable technique to perform software security assessments. Yet, contemporary fuzzers fall short of thoroughly testing applications with a high degree of control-flow diversity, such as firewalls and network packet analyzers. In this paper, we demonstrate how static program analysis can guide fuzzing by augmenting existing program models maintained by the fuzzer. Based on the insight that code patterns reflect the data format of inputs processed by a program, we automatically construct an input dictionary by statically analyzing program control and data flow. Our analysis is performed before fuzzing commences, and the input dictionary is supplied to an off-the-shelf fuzzer to influence input generation. Evaluations show that our technique not only increases test coverage by 10–15% over baseline fuzzers such as afl but also reduces the time required to expose vulnerabilities by up, to an order of magnitude. As a case study, we have evaluated our approach on two classes of network applications: nDPI, a deep packet inspection library, and tcpdump, a network packet analyzer. Using our approach, we have uncovered 15 zero-day vulnerabilities in the evaluated software that were not found by stand-alone fuzzers. Our work not only provides a practical method to conduct security evaluations more effectively but also demonstrates that the synergy between program analysis and testing can be exploited for a better outcome. © 2017, Springer International Publishing AG.","Fuzzing; Program analysis; Protocol parsers","Computer system firewalls; Data flow analysis; Deep packet inspection; Fuzzing; Network applications; Program analysis; Security evaluation; Software security; Static program analysis; Zero day vulnerabilities; Software testing",2-s2.0-85032868095
"Kumar C.N., Sreedevi M.","Hybrid query processing in reliable data extraction from deep web interfaces",2017,"International Journal of Pure and Applied Mathematics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029313475&partnerID=40&md5=5cf033cf33f7355b74cbea9f5c18f33f","The number of web pages available on the web is growing tremendously day to day. In this situation searching relevant information on the web according to the user perception is a hard task. A lot of relevant information is hidden behind various forms that integrate to undetermined databases containing high quality structured data. For effective data utilization, an extraction of deep web pages from web resources proposes Smart Crawler, for efficient harvesting the deep web. This has two stage environments for extracting effective deep web interfaces. Smart crawler follows only pre-query evaluation, analysis for data extraction from deep web interfaces. In this paper, we propose to develop MDL (Minimum Description Length) for combining both pre and post query procedures for classifying deep web interfaces to improve the accuracy of the page parser and the web form parser. Our experimental result achieves effective data extraction with the high rank of ability in data extraction.","Adaptive learning of data extraction; Deep web user interfaces; DOM; Object Model; Smart crawler",,2-s2.0-85029313475
"Khvalchik M., Kulkarni A.","Open-domain non-factoid question answering",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028668823&doi=10.1007%2f978-3-319-64206-2_33&partnerID=40&md5=373a6ce965833bbbf1a9d5fba1bcec7e","We present an end-to-end system for open-domain non-factoid question answering. We leverage the information on the ever-growing World Wide Web, and the capabilities of modern search engines to find the relevant information. Our QA system is composed of three components: (i) query formulation module (QFM) (ii) candidate answer generation module (CAGM) and (iii) answer selection module (ASM). A thorough empirical evaluation using two datasets demonstrates that the proposed approach is highly competitive. © Springer International Publishing AG 2017.","BLSTM; Learning to rank; Neural network; Question answering","Natural language processing systems; Neural networks; Query processing; BLSTM; Empirical evaluations; End-to-end systems; Factoid questions; Learning to rank; Query formulation; Question Answering; Three component; Search engines",2-s2.0-85028668823
"Khvalchik M., Pithyaachariyakul C., Kulkarni A.","Answering the hard questions",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021242302&doi=10.1007%2f978-3-319-59888-8_22&partnerID=40&md5=b55f535a55b15e9976a80fccd5d62441","We present an end-to-end system for open-domain non-factoid question-answering. To accomplish this we leverage the information on the ever-growing World Wide Web, and the capabilities of commercial search engines to find the relevant information. Our QA system is composed of three components: (i) query formulation module (QFM) (ii) candidate answer generation module (CAGM) and (iii) answer selection module (ASM). A thorough empirical evaluation using two datasets demonstrates that the proposed approach is highly competitive. © Springer International Publishing AG 2017.",,"Query processing; Empirical evaluations; End-to-end systems; Factoid questions; QA system; Query formulation; Three component; Search engines",2-s2.0-85021242302
"Wang C., Chiticariu L., Li Y.","Active learning for black-box semantic role labeling with neural factors",2017,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031945608&partnerID=40&md5=37c0fbb624972309f59854fa0b663714","Active learning is a useful technique for tasks for which unlabeled data is abundant but manual labeling is expensive. One example of such a task is semantic role labeling (SRL), which relies heavily on labels from trained linguistic experts. One challenge in applying active learning algorithms for SRL is that the complete knowledge of the SRL model is often unavailable, against the common assumption that active learning methods are aware of the details of the underlying models. In this paper, we present an active learning framework for black-box SRL models (i.e., models whose details are unknown). In lieu of a query strategy based on model details, we propose a neural query strategy model that embeds both language and semantic information to automatically learn the query strategy from predictions of an SRL model alone. Our experimental results demonstrate the effectiveness of both this new active learning framework and the neural query strategy model.",,"Learning algorithms; Semantics; Active Learning; Active learning methods; Active-learning algorithm; Manual labeling; Neural factors; Query strategies; Semantic information; Semantic role labeling; Artificial intelligence",2-s2.0-85031945608
"Liu Z., Huang D., Zhang Y., Zhang C.","A hierarchical iterative attention model for machine comprehension",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021727054&doi=10.1007%2f978-3-319-59569-6_43&partnerID=40&md5=c967883a383a887a68a91f5a00a2fe76","Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of Natural Language Processing, so reading comprehension of text is an important problem in NLP research. In this paper, we propose a novel Hierarchical Iterative Attention model (HIA), which constructs iterative alternating attention mechanism over tree-structured rather than sequential representations. The proposed HIA model continually refines its view of the query and document while aggregating the information required to answer a query, aiming to compute the attentions not only for the document but also the query side, which will benefit from the mutual information. Experimental results show that HIA has achieved significant state-of-the-art performance in public English datasets, such as CNN and Childrens Book Test datasets. Furthermore, HIA also outperforms state-of-the-art systems by a large margin in Chinese datasets, including People Daily and Childrens Fairy Tale datasets, which are recently released and the first Chinese reading comprehension datasets. © Springer International Publishing AG 2017.","Chinese machine comprehension; Cloze-style reading comprehension; Hierarchical Iterative Attention; Machine comprehension; Tree-LSTM","Forestry; Information systems; Query processing; Attention mechanisms; Chinese machine comprehension; Hierarchical Iterative Attention; Mutual informations; Reading comprehension; State-of-the-art performance; State-of-the-art system; Tree-LSTM; Natural language processing systems",2-s2.0-85021727054
"Hakimov S., Jebbara S., Cimiano P.","AMUSE: Multilingual semantic parsing for question answering over linked data",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216547&doi=10.1007%2f978-3-319-68288-4_20&partnerID=40&md5=f32e2d0aff706ef53cba2e41b9447b26","The task of answering natural language questions over RDF data has received wide interest in recent years, in particular in the context of the series of QALD benchmarks. The task consists of mapping a natural language question to an executable form, e.g. SPARQL, so that answers from a given KB can be extracted. So far, most systems proposed are (i) monolingual and (ii) rely on a set of hard-coded rules to interpret questions and map them into a SPARQL query. We present the first multilingual QALD pipeline that induces a model from training data for mapping a natural language question into logical form as probabilistic inference. In particular, our approach learns to map universal syntactic dependency representations to a language-independent logical form based on DUDES (Dependency-based Underspecified Discourse Representation Structures) that are then mapped to a SPARQL query as a deterministic second step. Our model builds on factor graphs that rely on features extracted from the dependency graph and corresponding semantic representations. We rely on approximate inference techniques, Markov Chain Monte Carlo methods in particular, as well as Sample Rank to update parameters using a ranking objective. Our focus lies on developing methods that overcome the lexical gap and present a novel combination of machine translation and word embedding approaches for this purpose. As a proof of concept for our approach, we evaluate our approach on the QALD-6 datasets for English, German & Spanish. © Springer International Publishing AG 2017.","Factor graphs; Multilinguality; Probabilistic graphical models; QALD; Question answering","Mapping; Markov processes; Monte Carlo methods; Natural language processing systems; Query processing; Syntactics; Factor graphs; Multilinguality; Probabilistic graphical models; QALD; Question Answering; Semantic Web",2-s2.0-85032216547
"Tonon A., Cudré-Mauroux P., Blarer A., Lenders V., Motik B.","ArmaTweet: Detecting events by semantic tweet analysis",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019637337&doi=10.1007%2f978-3-319-58451-5_10&partnerID=40&md5=dd623c35f634a194a7029fa06f42bf96","Armasuisse Science and Technology, the R&D agency for the Swiss Armed Forces, is developing a Social Media Analysis (SMA) system to help detect events such as natural disasters and terrorist activity by analysing Twitter posts. The system currently supports only keyword search, which cannot identify complex events such as ‘politician dying’ or ‘militia terror act’ since the keywords that correctly identify such events are typically unknown. In this paper we present ArmaTweet, an extension of SMA developed in a collaboration between armasuisse and the Universities of Fribourg and Oxford that supports semantic event detection. Our system extracts a structured representation from the tweets’ text using NLP technology, which it then integrates with DBpedia and WordNet in an RDF knowledge graph. Security analysts can thus describe the events of interest precisely and declaratively using SPARQL queries over the graph. Our experiments show that ArmaTweet can detect many complex events that cannot be detected by keywords alone. © Springer International Publishing AG 2017.",,"Disasters; Search engines; Social networking (online); Knowledge graphs; Natural disasters; Science and Technology; Semantic event detection; Social media analysis; Sparql queries; Swiss Armed Forces; Terrorist activities; Semantic Web",2-s2.0-85019637337
"Goldstein I., Kopelowitz T., Lewenstein M., Porat E.","Conditional lower bounds for space/time tradeoffs",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025126711&doi=10.1007%2f978-3-319-62127-2_36&partnerID=40&md5=c2f09eddb7a9fba242cfb9119db0fe19","In recent years much effort has been concentrated towards achieving polynomial time lower bounds on algorithms for solving various well-known problems. A useful technique for showing such lower bounds is to prove them conditionally based on well-studied hardness assumptions such as 3SUM, APSP, SETH, etc. This line of research helps to obtain a better understanding of the complexity inside P. A related question asks to prove conditional space lower bounds on data structures that are constructed to solve certain algorithmic tasks after an initial preprocessing stage. This question received little attention in previous research even though it has potential strong impact. In this paper we address this question and show that surprisingly many of the well-studied hard problems that are known to have conditional polynomial time lower bounds are also hard when concerning space. This hardness is shown as a tradeoff between the space consumed by the data structure and the time needed to answer queries. The tradeoff may be either smooth or admit one or more singularity points. We reveal interesting connections between different space hardness conjectures and present matching upper bounds. We also apply these hardness conjectures to both static and dynamic problems and prove their conditional space hardness. We believe that this novel framework of polynomial space conjectures can play an important role in expressing polynomial space lower bounds of many important algorithmic problems. Moreover, it seems that it can also help in achieving a better understanding of the hardness of their corresponding problems in terms of time. © Springer International Publishing AG 2017.",,"Data structures; Parallel processing systems; Polynomial approximation; Polynomials; Query processing; Algorithmic problems; Dynamic problem; Hard problems; Lower bounds; Polynomial space; Polynomial-time; Singularity point; Upper Bound; Hardness",2-s2.0-85025126711
"Perez-Arriaga M.O., Estrada T., Abad-Mota S.","Table interpretation and extraction of semantic relationships to synthesize digital documents",2017,"DATA 2017 - Proceedings of the 6th International Conference on Data Science, Technology and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029413460&partnerID=40&md5=a85fc25c59e86901f0c5113eedfa954e","The large number of scientific publications produced today prevents researchers from analyzing them rapidly. Automated analysis methods are needed to locate relevant facts in a large volume of information. Though publishers establish standards for scientific documents, the variety of topics, layouts, and writing styles impedes the prompt analysis of publications. A single standard across scientific fields is infeasible, but common elements tables and text exist by which to analyze publications from any domain. Tables offer an additional dimension describing direct or quantitative relationships among concepts. However, extracting tables information, and unambiguously linking it to its corresponding text to form accurate semantic relationships are non-trivial tasks. We present a comprehensive framework to conceptually represent a document by extracting its semantic relationships and context. Given a document, our framework uses its text, and tables content and structure to identify relevant concepts and relationships. Additionally, we use the Web and ontologies to perform disambiguation, establish a context, annotate relationships, and preserve provenance. Finally, our framework provides an augmented synthesis for each document in a domain-independent format. Our results show that by using information from tables we are able to increase the number of highly ranked semantic relationships by a whole order of magnitude. © Copyright 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Information extraction; Information integration; Semantic analysis; Table understanding","Information analysis; Information retrieval; Query languages; Content and structure; Domain independents; Information integration; Scientific documents; Scientific publications; Semantic analysis; Semantic relationships; Table understanding; Semantics",2-s2.0-85029413460
"Atzeni M., Atzori M.","CodeOntology: RDF-ization of source code",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032190783&doi=10.1007%2f978-3-319-68204-4_2&partnerID=40&md5=b4a9581416850bcec47dd44a92366420","In this paper, we leverage advances in the Semantic Web area, including data modeling (RDF), data management and querying (JENA and SPARQL), to develop CodeOntology, a community-shared software framework supporting expressive queries over source code. The project consists of two main contributions: an ontology that provides a formal representation of object-oriented programming languages, and a parser that is able to analyze Java source code and serialize it into RDF triples. The parser has been successfully applied to the source code of OpenJDK 8, gathering a structured dataset consisting of more than 2 million RDF triples. CodeOntology allows to generate Linked Data from any Java project, thereby enabling the execution of highly expressive queries over source code, by means of a powerful language like SPARQL. © Springer International Publishing AG 2017.","Ontology; OWL; Programming languages; RDF; SPARQL","Codes (symbols); Computational linguistics; Computer programming; Computer programming languages; Information management; Java programming language; Object oriented programming; Ontology; Formal representations; Java source codes; Linked datum; RDF triples; Software frameworks; Source codes; SPARQL; Semantic Web",2-s2.0-85032190783
"Gardner M., Krishnamurthy J.","Open-vocabulary semantic parsing with both distributional statistics and formal knowledge",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030477030&partnerID=40&md5=ce8df5bce21d22ad40844cbaee8dfed7","Traditional semantic parsers map language onto compositional, executable queries in a fixed schema. This mapping allows them to effectively leverage the information contained in large, formal knowledge bases (KBs, e.g., Freebase) to answer questions, but it is also fundamentally limiting - these semantic parsers can only assign meaning to language that falls within the KB's manually-produced schema. Recently proposed methods for open vocabulary semantic parsing overcome this limitation by learning execution models for arbitrary language, essentially using a text corpus as a kind of knowledge base. However, all prior approaches to open vocabulary semantic parsing replace a formal KB with textual information, making no use of the KB in their models. We show how to combine the disparate representations used by these two approaches, presenting for the first time a semantic parser that (1) produces compositional, executable representations of language, (2) can successfully leverage the information contained in both a formal KB and a large corpus, and (3) is not limited to the schema of the underlying KB. We demonstrate significantly improved performance over state-of-the-art baselines on an open-domain natural language question answering task. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Knowledge based systems; Knowledge management; Natural language processing systems; Semantics; Arbitrary languages; Execution model; Formal knowledge; Knowledge base; Natural language questions; Semantic parsing; State of the art; Textual information; Computational linguistics",2-s2.0-85030477030
"Amin N., Rompf T.","LMS-Verify: Abstraction without regret for verified systems programming",2017,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015337662&doi=10.1145%2f3009837.3009867&partnerID=40&md5=233f6c07e237af5447fb8218bd2bc695","Performance critical software is almost always developed in C, as programmers do not trust high-level languages to deliver the same reliable performance. This is bad because low-level code in unsafe languages attracts security vulnerabilities and because development is far less productive, with PL advances mostly lost on programmers operating under tight performance constraints. Highlevel languages provide memory safety out of the box, but they are deemed too slow and unpredictable for serious system software. Recent years have seen a surge in staging and generative programming: the key idea is to use high-level languages and their abstraction power as glorified macro systems to compose code fragments in first-order, potentially domain-specific, intermediate languages, from which fast C can be emitted. But what about security? Since the end result is still C code, the safety guarantees of the high-level host language are lost. In this paper, we extend this generative approach to emit ACSL specifications along with C code. We demonstrate that staging achieves ""abstraction without regret"" for verification: we show how high-level programming models, in particular higher-order composable contracts from dynamic languages, can be used at generation time to compose and generate first-order specifications that can be statically checked by existing tools. We also show how type classes can automatically attach invariants to data types, reducing the need for repetitive manual annotations. We evaluate our system on several case studies that varyingly exercise verification of memory safety, overflow safety, and functional correctness. We feature an HTTP parser that is (1) fast (2) high-level: implemented using staged parser combinators (3) secure: with verified memory safety. This result is significant, as input parsing is a key attack vector, and vulnerabilities related to HTTP parsing have been documented in all widely-used web servers. © 2017 ACM.","Blame; Contracts; DSLs; Frama-C; LMS; Memory safety; Security; Verification","Abstracting; Automatic programming; C (programming language); Codes (symbols); Computational linguistics; Computer programming; Computer programming languages; Computer systems programming; Contracts; HTTP; Specifications; Verification; Blame; DSLs; Generative programming; High-level programming models; Memory safety; Performance constraints; Security; Security vulnerabilities; High level languages",2-s2.0-85015337662
"Verma Y., Jawahar C.V.","A support vector approach for cross-modal search of images and texts",2017,"Computer Vision and Image Understanding",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992364326&doi=10.1016%2fj.cviu.2016.10.001&partnerID=40&md5=1981af0a39576dbbd7fc6423cdac7af8","Building bilateral semantic associations between images and texts is among the fundamental problems in computer vision. In this paper, we study two complementary cross-modal prediction tasks: (i) predicting text(s) given a query image (“Im2Text”), and (ii) predicting image(s) given a piece of text (“Text2Im”). We make no assumption on the specific form of text; i.e., it could be either a set of labels, phrases, or even captions. We pose both these tasks in a retrieval framework. For Im2Text, given a query image, our goal is to retrieve a ranked list of semantically relevant texts from an independent text-corpus (i.e., texts with no corresponding images). Similarly, for Text2Im, given a query text, we aim to retrieve a ranked list of semantically relevant images from a collection of unannotated images (i.e., images without any associated textual meta-data). We propose a novel Structural SVM based unified framework for these two tasks, and show how it can be efficiently trained and tested. Using a variety of loss functions, extensive experiments are conducted on three popular datasets (two medium-scale datasets containing few thousands of samples, and one web-scale dataset containing one million samples). Experiments demonstrate that our framework gives promising results compared to competing baseline cross-modal search techniques, thus confirming its efficacy. © 2016","Cross-media analysis; Image description; Image search","Computer vision; Semantics; Cross-media; Image descriptions; Image search; Prediction tasks; Retrieval frameworks; Search technique; Semantic associations; Unified framework; Forecasting",2-s2.0-84992364326
"Rochim A.F., Sari R.F.","Evaluation of articles published in Mendeley and CrossRef in relation to the Google Scholar pages",2017,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011019555&partnerID=40&md5=5eaf6ed3e321405c960f29655f8d9166","This paper aims to show the performance of a researcher from their published articles. Our software crawled 10 (ten) most cited articles on the Google Scholar (GS), Mendeley and CrossRef with several of crawling methods. The method used in data retrieval is scrapping due to the limitations on the Application Programming Interface (API) provided by the Google search engine. To retrieve the Digital Object Identifier (DOI) data from Crossref, the API method has been used. In order to count the number of reader of paper on the Mendeley we used the API method. We used the R programming language, Python and Bash scripting shell. The operating system was based on Ubuntu 8.04 Linux and Mac OS. The Apache webserver were used to serve the website and we used the MySQL database to store the data. The database of MySQL is used for interfacing between R with the PHP language purposes. The Hypertext Preprocessor (PHP) is used for server-side scripting. Data was obtained by scrapping the best 10 articles from 100 Indonesia's scientists indexed on the GS. Firstly, the data samples (S') were obtained from the list of Indonesian scientists in Webometrics as the input of the GS scrapping. Secondly, the data resulted (S"") were used as the input of the Crossref's A query to obtain the DOI of each article. Finally, the DOIs were used as the input for the API query to get the number of the result to show the number of readers of each to article on Mendeley. The software produced can crawl the data from Google Scholar, Crossref and Mendeley reader count. © 2006-2017 Asian Research Publishing Network (ARPN). All rights reserved.","Google scholar and crossref; Mendeley; Reader count",,2-s2.0-85011019555
"Ponnalagu K.","Ontology-driven root-cause analytics for user-reported symptoms in managed IT systems",2017,"IBM Journal of Research and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016390673&doi=10.1147%2fJRD.2016.2629319&partnerID=40&md5=d838a3300e2b135f57b6dbe23bd2f146","Enterprise users of IT services seek real-time contextual insights during system-failure scenarios in both cloud-provisioned and legacy management systems. Current IT management systems mostly provide front-office automation support, such as ticket categorization and scheduling, using a generalized set of troubleshooting instructions. Therefore, in this paper, we propose an IT management system that provides real-time insights on user-perceived failures (e.g., 'Why is application not responding?') expressed in natural language texts. We achieve this through an underpinning of a knowledge graph that helps in discovering possible topology patterns comprising multiple interdependent systems for a specific purpose. Based on the detected list of topology patterns, the proposed system composes multiple debugging workflows to generate accurate operational insights. The user interactions are 'system agnostic' in nature and do not depend on the knowledge of the underlying system topology. This significantly augments the self-assist scenarios of end-users and front-office agents, before they engage with IT support teams. We demonstrate our proposed approach, as a cloud application with a natural language interface, using an experimental setup involving a standard ticket management system. © 1957-2012 IBM.","Debugging; Natural languages; Servers; Software systems; Topology; Uniform resource locators","Computer debugging; Human computer interaction; Legacy systems; Natural language processing systems; Office automation; Real time systems; Scheduling; Servers; Systems engineering; Topology; Websites; Cloud applications; Interdependent systems; Management systems; Natural language interfaces; Natural language text; Natural languages; Software systems; Underlying systems; Program debugging",2-s2.0-85016390673
"Srivastava S., Azaria A., Mitchell T.","Parsing natural language conversations using contextual cues",2017,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031909389&partnerID=40&md5=9cb56fed346ced09fe1ebb46a0d4439d","In this work, we focus on semantic parsing of natural language conversations. Most existing methods for semantic parsing are based on understanding the semantics of a single sentence at a time. However, understanding conversations also requires an understanding of conversational context and discourse structure across sentences. We formulate semantic parsing of conversations as a structured prediction task, incorporating structural features that model the 'flow of discourse' across sequences of utterances. We create a dataset for semantic parsing of conversations, consisting of 113 real-life sequences of interactions of human users with an automated email assistant. The data contains 4759 natural language statements paired with annotated logical forms. Our approach yields significant gains in performance over traditional semantic parsing.",,"Artificial intelligence; Semantics; Syntactics; Contextual cue; Discourse structure; Human users; Logical forms; Natural languages; Semantic parsing; Structural feature; Structured prediction; Context free grammars",2-s2.0-85031909389
"Van Meter A., Williams U., Zavala A., Kee J., Rebello E., Tsai J., Ifeanyi I., Ruiz J., Lim J., Owusu-Agyemang P.","Beat to Beat: A Measured Look at the History of Pulse Oximetry",2017,"Journal of Anesthesia History",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009188329&doi=10.1016%2fj.janh.2016.12.003&partnerID=40&md5=26ccb7dd9897e9fc58bf11b7546c94f1","It can be argued that pulse oximetry is the most important technological advancement ever made in monitoring the well-being and safety of patients undergoing anesthesia. Before its development, the physical appearance of the patient and blood gas analysis were the only methods of assessing hypoxemia in patients. The disadvantages of blood gas analysis are that it is not without pain, complications, and most importantly does not provide continuous, real-time data. Although it has become de rigueur to use pulse oximetry for every anesthetic, the road leading to pulse oximetry began long ago. © 2017 Anesthesia History Association",,,2-s2.0-85009188329
"Xiao C., Dymetman M., Gardent C.","Symbolic priors for RNN-based Semantic Parsing",2017,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031942127&partnerID=40&md5=b5cf6a18d0a4a7b6c1392e2ecda5f1cc","Seq2seq models based on Recurrent Neural Networks (RNNs) have recently received a lot of attention in the domain of Semantic Parsing for Question Answering. While in principle they can be trained directly on pairs (natural language utterances, logical forms), their performance is limited by the amount of available data. To alleviate this problem, we propose to exploit various sources of prior knowledge: the well-formedness of the logical forms is modeled by a weighted context-free grammar; the likelihood that certain entities present in the input utterance are also present in the logical form is modeled by weighted finite-state automata. The grammar and automata are combined together through an efficient intersection algorithm to form a soft guide (""background"") to the RNN. We test our method on an extension of the Overnight dataset and show that it not only strongly improves over an RNN baseline, but also outperforms non-RNN models based on rich sets of hand-crafted features.",,"Artificial intelligence; Automata theory; Context free grammars; Natural language processing systems; Semantics; Statistical tests; Intersection algorithms; Logical forms; Natural languages; Prior knowledge; Question Answering; Recurrent neural network (RNNs); Semantic parsing; Recurrent neural networks",2-s2.0-85031942127
"Li B., Yang X., Wang B., Cui W.","Efficiently mining high quality phrases from texts",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030452574&partnerID=40&md5=7a3684520c5b54bc4c6b5b8e8c051cf8","Phrase mining is a key research problem for semantic analysis and text-based information retrieval. The existing approaches based on NLP, frequency, and statistics cannot extract high quality phrases and the processing is also time consuming, which are not suitable for dynamic on-line applications. In this paper, we propose an efficient high-quality phrase mining approach (EQPM). To the best of our knowledge, our work is the first effort that considers both intra-cohesion and inter-isolation in mining phrases, which is able to guarantee appropriateness. We also propose a strategy to eliminate order sensitiveness, and ensure the completeness of phrases. We further design efficient algorithms to make the proposed model and strategy feasible. The empirical evaluations on four real data sets demonstrate that our approach achieved a considerable quality improvement and the processing time was 2.3× ~ 29× faster than the state-of-the-art works. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Semantics; Empirical evaluations; On-line applications; Processing time; Quality improvement; Research problems; Semantic analysis; State of the art; Text-based information; Quality control",2-s2.0-85030452574
"Argueta A., Chiang D.","Decoding with finite-state transducers on GPUs",2017,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021655423&partnerID=40&md5=3b831d7fe15e06631856af5532bbee65","Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUS) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST. © 2017 Association for Computational Linguistics.",,"Character recognition; Computational linguistics; Computer architecture; Computer graphics; Decoding; Finite automata; Hidden Markov models; Linguistics; Markov processes; Natural language processing systems; Program processors; Speech recognition; Transducers; Viterbi algorithm; Conditional random field; Finite state transducers; Forward / backward algorithms; GPU implementation; Morphological analysis; Named entity recognition; Part of speech tagging; Weighted finite automaton; Graphics processing unit",2-s2.0-85021655423
"Wang R.-Z., Zhan C.-D., Ling Z.-H.","Question answering with character-level lstm encoders and model-based data augmentation",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031418489&doi=10.1007%2f978-3-319-69005-6_25&partnerID=40&md5=17978e5032f696ae04c52c86d952e549","This paper presents a character-level encoder-decoder modeling method for question answering (QA) from large-scale knowledge bases (KB). This method improves the existing approach [9] from three aspects. First, long short-term memory (LSTM) structures are adopted to replace the convolutional neural networks (CNN) for encoding the candidate entities and predicates. Second, a new strategy of generating negative samples for model training is adopted. Third, a data augmentation strategy is applied to increase the size of the training set by generating factoid questions using another trained encoder-decoder model. Experimental results on the SimpleQuestions dataset and the Freebase5M KB demonstrates the effectiveness of the proposed method, which improves the state-of-the-art accuracy from 70.3% to 78.8% when augmenting the training set with 70,000 generated triple-question pairs. © Springer International Publishing AG 2017.","Encoder-Decoder; Knowledge base; Long short-term memory; Question answering","Brain; Computational linguistics; Decoding; Knowledge based systems; Linguistics; Long short-term memory; Natural language processing systems; Neural networks; Signal encoding; Convolutional neural network; Data augmentation; Encoder-decoder; Factoid questions; Knowledge base; Negative samples; Question Answering; State of the art; Big data",2-s2.0-85031418489
"Zhang J., Sun Y., Huang S., Nguyen C.-T., Wang X., Dai X., Chen J., Yu Y.","AGRA: An analysis-generation-ranking framework for automatic abbreviation from paper titles",2017,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031912995&partnerID=40&md5=50564aab5ca51681355ce016bc141a5c","People sometimes choose word-like abbreviations to refer to items with a long description. These abbreviations usually come from the descriptive text of the item and are easy to remember and pronounce, while preserving the key idea of the item. Coming up with a nice abbreviation is not an easy job, even for human. Previous assistant naming systems compose names by applying hand-written rules, which may not perform well. In this paper, we propose to view the naming task as an artificial intelligence problem and create a data set in the domain of academic naming. To generate more delicate names, we propose a three-step framework, including description analysis, candidate generation and abbreviation ranking, each of which is parameterized and optimizable. We conduct experiments to compare different settings of our framework with several analysis approaches from different perspectives. Compared to online or baseline systems, our framework could achieve the best results.",,"Artificial intelligence; Online systems; Analysis approach; Automatic abbreviation; Baseline systems; Candidate generation; Data set; Naming systems; Parameterized; Network function virtualization",2-s2.0-85031912995
"Huang H., Zhang Q., Huang X.","Mention recommendation for twitter with end-to-end memory network",2017,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031946434&partnerID=40&md5=b591ffbdc2e61fbb2ce3afe2582f7b17","In this study, we investigated the problem of recommending usernames when people attempt to use the ""@"" sign to mention other people in twitter-like social media. With the extremely rapid development of social networking services, this problem has received considerable attention in recent years. Previous methods have studied the problem from different aspects. Because most of Twitter-like microblogging services limit the length of posts, statistical learning methods may be affected by the problems of word sparseness and synonyms. Although recent progress in neural word embedding methods have advanced the state-of-the-art in many natural language processing tasks, the benefits of word embedding have not been taken into consideration for this problem. In this work, we proposed a novel end-to-end memory network architecture to perform this task. We incorporated the interests of users with external memory. A hierarchical attention mechanism was also applied to better consider the interests of users. The experimental results on a dataset we collected from Twitter demonstrated that the proposed method could outperform stateof-the-art approaches.",,"Artificial intelligence; Natural language processing systems; Network architecture; Attention mechanisms; Embedding method; Memory network architecture; Micro-blogging services; Social networking services; State of the art; State-of-the-art approach; Statistical learning methods; Social networking (online)",2-s2.0-85031946434
"Guo S., Zeng X., He S., Liu K., Zhao J.","Which is the effective way for Gaokao: Information retrieval or neural networks?",2017,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021650854&partnerID=40&md5=6eaf5c12def3feffc6c2d47cf0514d87","As one of the most important test of China, Gaokao is designed to be difficult enough to distinguish the excellent high school students. In this work, we detailed the Gaokao History Multiple Choice Questions( GKHMC) and proposed two different approaches to address them using various resources. One approach is based on entity search technique (IR approach), the other is based on text entailment approach where we specifically employ deep neural networks(NN approach). The result of experiment on our collected real Gaokao questions showed that they are good at different categories of questions, i.e. IR approach performs much better at entity questions(EQs) while NN approach shows its advantage on sentence questions(SQs). Our new method achieves state-of-the-art performance and show that it's indispensable to apply hybrid method when participating in the real-world tests. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Entity search; High school students; Hybrid method; Multiple choice questions; Real-world tests; State-of-the-art performance; Deep neural networks",2-s2.0-85021650854
"Lei X., Cai Y., Li Q., Xie H., Leung H.-F., Wang F.L.","Combining local and global features in supervised word sense disambiguation",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031396707&doi=10.1007%2f978-3-319-68786-5_10&partnerID=40&md5=2c84a289f2026b0725c0b656e5700abd","Word Sense Disambiguation (WSD) is a task to identify the sense of a polysemy in given context. Recently, word embeddings are applied to WSD, as additional input features of a supervised classifier. However, previous approaches narrowly use word embeddings to represent surrounding words of target words. They may not make sufficient use of word embeddings in representing different features like dependency relations, word order and global contexts (the whole document). In this work, we combine local and global features to perform WSD. We explore utilizing word embeddings to leverage word order and dependency features. We also use word embeddings to represent global contexts as global features. We conduct experiments to evaluate our methods and find out that our methods outperform the state-of-the-art methods on Lexical Sample WSD datasets. © 2017, Springer International Publishing AG.","Natural language processing; Word embeddings; Word sense disambiguation","Information systems; Systems engineering; Dependency relation; Embeddings; Global context; Global feature; Input features; State-of-the-art methods; Supervised classifiers; Word Sense Disambiguation; Natural language processing systems",2-s2.0-85031396707
"Yildiz T., Diri B., Yildirim S.","Turkish synonym identification from multiple resources: Monolingual corpus, mono/bilingual online dictionaries, and WordNet",2017,"Turkish Journal of Electrical Engineering and Computer Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017367768&doi=10.3906%2felk-1508-89&partnerID=40&md5=eb6eab3611290374c8d23ed1e030b94f","In this study, a model is proposed to determine synonymy by incorporating several resources. The model extracts the features from monolingual online dictionaries, a bilingual online dictionary, WordNet, and a monolingual Turkish corpus. Once it has built a candidate list, it determines the synonymy for a given word by means of those features. All these resources and the approaches are evaluated. Taking all features into account and applying machine learning algorithms, the model shows good performance of F-measure with 81.4%. The study contributes to the literature by integrating several resources and attempting the first corpus-driven synonym detection system for Turkish. © TÜBİTAK.","Corpus-based statistics; Dependency relations; Synonym","Learning systems; Ontology; Semantics; Candidate list; Corpus-based; Dependency relation; Detection system; F measure; Multiple resources; Online dictionaries; Synonym; Learning algorithms",2-s2.0-85017367768
"Hunter T., II","Advanced Microservices: A Hands-on Approach to Microservice Infrastructure and Tooling",2017,"Advanced Microservices: A Hands-on Approach to Microservice Infrastructure and Tooling",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032668245&doi=10.1007%2f978-1-4842-2887-6&partnerID=40&md5=431d566a7e50226f1fe6e7dad7ff771e","Use the many types of tools required to navigate and maintain a microservice ecosystem. This book examines what is normally a complex system of interconnected services and clarifies them one at a time, first examining theoretical requirements then looking at concrete tools, configuration, and workflows. Building out these systems includes many concerns such as containerization, container orchestration, build pipelines and continuous integration solutions, automated testing, service discovery, logging and analytics. You will examine each of these tools and understand how they can be combined within an organization. You will design an automated build pipeline from Pull Request to container deployment, understand how to achieve High Availability and monitor application health with Service Discovery, and learn how to collaborate with other teams, write documentation, and describe bugs. Covering use of Jenkins, Docker, Kubernetes, the ELK stack (Elasticsearch, Logstash, and Kibana), and StatsD and Grafana for analytics, you will build on your existing knowledge of Service-Oriented Architecture and gain an advanced, practical understanding of everything from infrastructure development to team collaboration. What You'll Learn Design an API to be convenient for developers to consume. Deploy dynamic instances of Microservices and allow then to discover each other. Track the health of a Microservice and be notified in case of degraded performance. Write effective documentation and communicate efficiently with other teams. Who This Book Is For Those who would like a better understanding of System Oriented Architecture. Those who would like to break a monolith into smaller Microservices. Those who are familiar with Microservices and would like a better understanding of peripheral technologies. © 2017 by Thomas Hunter II. All rights are reserved.",,,2-s2.0-85032668245
"Angluin D., Becerra-Bonache L.","A model of language learning with semantics and meaning-preserving corrections",2017,"Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992176763&doi=10.1016%2fj.artint.2016.10.002&partnerID=40&md5=9c029d81038bf0f749ed76b199f2301b","We present a computational model that takes into account semantics for language learning and allows us to model meaning-preserving corrections. The model is constructed with a learner and a teacher who interact in a sequence of shared situations by producing utterances intended to denote a unique object in each situation. We test our model with limited sublanguages of 10 natural languages exhibiting a variety of linguistic phenomena. The results show that learning to a high level of performance occurs after a reasonable number of interactions. Comparing the effect of a teacher who does no correction to that of a teacher who corrects whenever possible, we show that under certain conditions corrections can accelerate the rate of learning. We also define and analyze a simplified model of a probabilistic process of collecting corrections to help understand the possibilities and limitations of corrections in our setting. © 2016 Elsevier B.V.","Corrections; Grammar learning; Language learning; Semantics","Semantics; Speech recognition; Computational model; Corrections; Grammar learning; Language learning; Linguistic phenomena; Natural languages; Probabilistic process; Sublanguages; Teaching",2-s2.0-84992176763
"Roth D.","Incidental supervision: Moving beyond supervised learning",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030471204&partnerID=40&md5=4b238cc4a3a697b4a205b48271bca0d8","Machine Learning and Inference methods have become ubiquitous in our attempt to induce more abstract representations of natural language text, visual scenes, and other messy, naturally occurring data, and support decisions that depend on it. However, learning models for these tasks is difficult partly because generating the necessary supervision signals for it is costly and does not scale. This paper describes several learning paradigms that are designed to alleviate the supervision bottleneck. It will illustrate their benefit in the context of multiple problems, all pertaining to inducing various levels of semantic representations from text. In particular, we discuss (i) Response Driven Learning of models, a learning protocol that supports inducing meaning representations simply by observing the model's behavior in its environment, (ii) the exploitation of Incidental Supervision signals that exist in the data, independently of the task at hand, to learn models that identify and classify semantic predicates, and (iii) the use of weak supervision to combine simple models to support global decisions where joint supervision is not available. While these ideas are applicable in a range of Machine Learning driven fields, we will demonstrate it in the context of several natural language applications, from (cross-lingual) text classification, to Wikification, to semantic parsing. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Abstracting; Artificial intelligence; Classification (of information); Learning systems; Natural language processing systems; Semantics; Visual languages; Abstract representation; Learning protocols; Natural language applications; Natural language text; Naturally occurring; Semantic predicates; Semantic representation; Text classification; Text processing",2-s2.0-85030471204
"Schoormann T., Behrens D., Heid U., Knackstedt R.","Semi-automatic development of modelling techniques with computational linguistics methods - A procedure model and its application",2017,"Lecture Notes in Business Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022212974&doi=10.1007%2f978-3-319-59336-4_14&partnerID=40&md5=57cd9632418f2f8e1e267a34d2b46cca","In recent years, the number of domain-specific modelling techniques increased. Method engineering already provides text-based and semantic approaches which aim to unify constructs and allocate terminologies. As existing procedures are usually carried out manually, challenges arise such as reproducibility and standardization as well as ensuring quality. Hence, this paper aims to investigate how methods from the Computational Linguistics can be applied to automatically develop domain-specific modelling techniques in order to face these challenges. As a main result, we present a procedure model that was developed and applied in four iterations, recommend tools, methods and resources as well as reflect typical issues. © Springer International Publishing AG 2017.","Computational linguistics; Conceptual modelling; Method engineering; Procedure model; Text-Analytics","Information systems; Linguistics; Semantics; Conceptual modelling; Domain-specific modelling; Method engineering; Modelling techniques; Procedure modeling; Reproducibilities; Semantic approach; Text analytics; Computational linguistics",2-s2.0-85022212974
"Mangiatordi G.F., Trisciuzzi D., Alberga D., Denora N., Iacobazzi R.M., Gadaleta D., Catto M., Nicolotti O.","Novel chemotypes targeting tubulin at the colchicine binding site and unbiasing P-glycoprotein",2017,"European Journal of Medicinal Chemistry",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028376054&doi=10.1016%2fj.ejmech.2017.07.037&partnerID=40&md5=64093077a7783d43bb3fbbf33df5e466","Retrospective validation studies carried out on three benchmark databases containing a small fraction (that is 2.80%) of known tubulin binders permitted us to develop a computational platform very effective in selecting easier manageable subsets showing by far higher percentages of actives (about 25%). These studies relied on the hierarchical application of multilayer in silico screenings employing filters implying molecular shape similarity; a structure-based pharmacophore model and molecular docking campaigns. Building on this validated approach, we performed intensive prospective studies to screen a large chemical collection, including up to 3.7 millions of commercial compounds, to across an unexplored and patent space in the search of novel colchicine binding site inhibitors. Our investigation was successful in identifying a pool of 31 initial hits showing new molecular scaffolds (such as 4,5-dihydro-1H-pyrrolo[3,4-c]pyrazol-6-one and pyrazolo[1,5-a]pyrimidine). This panel of new hits resulted antiproliferative activity in the low μM range towards MCF-7 human breast cancer, HepG2 human liver cancer, HeLa human ovarian cancer and SHSY5Y human glioblastoma cell lines as well as interesting concentration-dependent inhibition of tubulin polymerization assessed through fluorescence polymerization assays. Unlike typical tubulin inhibitors, a satisfactorily low sensitivity towards P-gp was also measured in bi-directional transport studies across MDCKII-MDR1 cells for a selected subset of seven compounds. © 2017 Elsevier Masson SAS","Docking; P-gp; Tubulin; Virtual screening","1,2,4 triazole derivative; alpha tubulin; antimitotic agent; batabulin; beta tubulin; colchicine; cytotoxic agent; mivobulin isethionate; multidrug resistance protein; n acetylprolylhistidylserylcysteinylasparagine amide; n [2 [(4 hydroxyphenyl)amino] 3 pyridinyl] 4 methoxybenzenesulfonamide; paclitaxel; podophyllotoxin; protein inhibitor; pyrazole derivative; pyrimidine derivative; quinoline derivative; tn 16; tubulin; vinblastine; antineoplastic agent; colchicine; multidrug resistance protein; tubulin; antiproliferative activity; Article; binding site; concentration response; controlled study; cytotoxicity; drug structure; drug synthesis; female; fluorescence imaging; HeLa cell line; Hep-G2 cell line; human; human cell; hydrophobicity; MCF-7 cell line; MDCK-MDR1 cell line; microtubule assembly; molecular docking; pharmacophore; polymerization; retrospective study; SH-SY5Y cell line; validation study; antagonists and inhibitors; binding site; cell proliferation; chemical structure; chemistry; dose response; drug effects; drug screening; metabolism; structure activity relation; synthesis; tumor cell line; Antineoplastic Agents; Binding Sites; Cell Line, Tumor; Cell Proliferation; Colchicine; Dose-Response Relationship, Drug; Drug Screening Assays, Antitumor; Humans; Molecular Structure; P-Glycoprotein; Structure-Activity Relationship; Tubulin",2-s2.0-85028376054
"Fomichov V.A.","SK-languages as a powerful and flexible semantic formalism for the systems of cross-lingual intelligent information access",2017,"Informatica (Slovenia)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021812285&partnerID=40&md5=ca21b137e89fec5bfacba2c38cfb1d74","The first starting point of this paper is the broadly accepted idea of employing, as a promising methodology, an artificial semantic language-intermediary for the realization of automatic cross-lingual intelligent information access to natural language (NL) texts on the Web. The second one is the emergence in computational semantics during 2013-2016 of great interest in the semantic formalism (more exactly, notation) called Abstract Meaning Representation (AMR). This formalism was introduced in 2013 in an ACL publication by a group consisting of ten researchers from UK and USA. This paper shows that much broader prospects for creating semantic languages-intermediaries in comparison with AMR are opened by the theory of K-representations (TKR), developed by V. A. Fomichov. The basic mathematical model of TKR describes the regularities of NL structured meanings. The mathematical essence is that this model introduces a system consisting of ten partial operations on conceptual structures. Initial version of this model was published in 1996 in Informatica (Slovenia). The second version of the model (stated in a monograph released by Springer in 2010) defines a class of formal languages called SK-languages (standard knowledge languages). It is demonstrated that SK-languages allow us to simulate all expressive mechanisms of AMR. The advantages in comparison with AMR are, in particular, the possibilities to construct semantic representations of compound infinitive constructions (expressing goals, commitments, etc), of compound descriptions of notions and sets, and of complex discourses and knowledge pieces. .","Abstract meaning representation; Formal representation of semantic content; Semantic parsing; Theory of k-representations","Computation theory; Formal languages; Information retrieval; Natural language processing systems; Computational semantics; Conceptual structures; Intelligent information; Semantic content; Semantic parsing; Semantic representation; Structured meanings; Theory of k-representations; Semantics",2-s2.0-85021812285
"Verga P., Neelakantan A., Mccallum A.","Generalizing to unseen entities and entity pairs with row-less universal schema",2017,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021648133&partnerID=40&md5=a1f13cb7f120e0273e94c7b7ef26a933","Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types-not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Database systems; Knowledge based systems; Linguistics; Aggregation functions; Attention model; Binary relation; Matrix completion problems; Natural language database; Structured database; Textual patterns; Traditional models; Forecasting",2-s2.0-85021648133
"Ul Mustafa R., Nawaz M.S., Lali M.I.U., Zia T., Mehmood W.","Predicting the Cricket match outcome using crowd opinions on social networks: A comparative study of machine learning methods",2017,"Malaysian Journal of Computer Science",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013230607&partnerID=40&md5=50d29ccfc898c499a3a94f2300a63809","Social media has become a platform of first choice where one can express his/her feelings with freedom. The sports and matches being played are also discussed on social media such as Twitter. In this article, efforts are made to investigate the feasibility of using collective knowledge obtained from microposts posted on Twitter to predict the winner of a Cricket match. For predictions, we use three different methods that depend on the total number of tweets before the game for each team, fans sentiments toward each team and fans score predictions on Twitter. By combining these three methods, we classify winning team prediction in a Cricket game before the start of game. Our results are promising enough to be used for winning team forecast. Furthermore, the effectiveness of supervised learning algorithms is evaluated where Support Vector Machine (SVM) has shown advantage over other classifiers.","Cricket; Opinion mining; Pattern recognition; Sentimental analysis; Twitter",,2-s2.0-85013230607
"Dediu A.-H., Matos J.M., Martéın-Vide C.","Natural language processing, moving from rules to data",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018456292&doi=10.1007%2f978-3-319-55911-7_3&partnerID=40&md5=eccc7edc633f44efabc27f5b11f8e5b8","During the last decade, we assist to a major change in the direction that theoretical models used in natural language processing follow. We are moving from rule-based systems to corpus-oriented para-digms. In this paper, we analyze several generative formalisms together with newer statistical and data-oriented linguistic methodologies. We review existing methods belonging to deep or shallow learning applied in various subfields of computational linguistics. The continuous, fast improvements obtained by practical, applied machine learning techniques may lead us to new theoretical developments in the classic models as well. We discuss several scenarios for future approaches. © Springer International Publishing AG 2017.","Computational linguistics; Computational models; Machine translation; Speech processing methods","Computation theory; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Processing; Speech processing; Speech transmission; Applied machine learning; Classic models; Computational model; Machine translations; NAtural language processing; Processing method; Subfields; Theoretical development; Computational linguistics",2-s2.0-85018456292
"Vilares Ferro M., Darriba Bilbao V.M., Ribadas Pena F.J.","Modeling of learning curves with applications to POS tagging",2017,"Computer Speech and Language",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975450589&doi=10.1016%2fj.csl.2016.06.001&partnerID=40&md5=9fbca6de68747211f4c604d19188e09a","An algorithm to estimate the evolution of learning curves on the whole of a training data base, based on the results obtained from a portion and using a functional strategy, is introduced. We approximate iteratively the sought value at the desired time, independently of the learning technique used and once a point in the process, called prediction level, has been passed. The proposal proves to be formally correct with respect to our working hypotheses and includes a reliable proximity condition. This allows the user to fix a convergence threshold with respect to the accuracy finally achievable, which extends the concept of stopping criterion and seems to be effective even in the presence of distorting observations. Our aim is to evaluate the training effort, supporting decision making in order to reduce the need for both human and computational resources during the learning process. The proposal is of interest in at least three operational procedures. The first is the anticipation of accuracy gain, with the purpose of measuring how much work is needed to achieve a certain degree of performance. The second relates the comparison of efficiency between systems at training time, with the objective of completing this task only for the one that best suits our requirements. The prediction of accuracy is also a valuable item of information for customizing systems, since we can estimate in advance the impact of settings on both the performance and the development costs. Using the generation of part-of-speech taggers as an example application, the experimental results are consistent with our expectations. © 2016 Elsevier Ltd. All rights reserved.","Correctness; Functional sequences; Learning curves; POS tagging; Proximity criterion; Robustness","Algorithms; Decision making; Iterative methods; Learning systems; Robustness (control systems); Syntactics; Correctness; Functional sequences; Learning curves; PoS tagging; Proximity criterion; Computational linguistics",2-s2.0-84975450589
"Ruch P.","Text mining to support gene ontology curation and vice versa",2017,"Methods in Molecular Biology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994494266&doi=10.1007%2f978-1-4939-3743-1_6&partnerID=40&md5=762263c2c831a1a6493d4a884d06528c","In this chapter, we explain how text mining can support the curation of molecular biology databases dealing with protein functions. We also show how curated data can play a disruptive role in the developments of text mining methods. We review a decade of efforts to improve the automatic assignment of Gene Ontology (GO) descriptors, the reference ontology for the characterization of genes and gene products. To illustrate the high potential of this approach, we compare the performances of an automatic text categorizer and show a large improvement of +225 % in both precision and recall on benchmarked data. We argue that automatic text categorization functions can ultimately be embedded into a Question-Answering (QA) system to answer questions related to protein functions. Because GO descriptors can be relatively long and specific, traditional QA systems cannot answer such questions. A new type of QA system, socalled Deep QA which uses machine learning methods trained with curated contents, is thus emerging. Finally, future advances of text mining instruments are directly dependent on the availability of highquality annotated contents at every curation step. Databases workflows must start recording explicitly all the data they curate and ideally also some of the data they do not curate. © The Author(s) 2017.","Automatic text categorization; Data curation; Data stewardship; Databases; Gene ontology; Information storage and retrieval","accuracy; classification; data base; data mining; gene ontology; human; information processing; information retrieval; machine learning; molecular biology; ontology; protein function; recording",2-s2.0-84994494266
"Özgür A., Hur J., He Y.","The Interaction Network Ontology-supported modeling and mining of complex interactions represented with multiple keywords in biomedical literature",2016,"BioData Mining",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006380649&doi=10.1186%2fs13040-016-0118-0&partnerID=40&md5=f97bf3343d9692225fea3027640c5191","Background: The Interaction Network Ontology (INO) logically represents biological interactions, pathways, and networks. INO has been demonstrated to be valuable in providing a set of structured ontological terms and associated keywords to support literature mining of gene-gene interactions from biomedical literature. However, previous work using INO focused on single keyword matching, while many interactions are represented with two or more interaction keywords used in combination. Methods: This paper reports our extension of INO to include combinatory patterns of two or more literature mining keywords co-existing in one sentence to represent specific INO interaction classes. Such keyword combinations and related INO interaction type information could be automatically obtained via SPARQL queries, formatted in Excel format, and used in an INO-supported SciMiner, an in-house literature mining program. We studied the gene interaction sentences from the commonly used benchmark Learning Logic in Language (LLL) dataset and one internally generated vaccine-related dataset to identify and analyze interaction types containing multiple keywords. Patterns obtained from the dependency parse trees of the sentences were used to identify the interaction keywords that are related to each other and collectively represent an interaction type. Results: The INO ontology currently has 575 terms including 202 terms under the interaction branch. The relations between the INO interaction types and associated keywords are represented using the INO annotation relations: 'has literature mining keywords' and 'has keyword dependency pattern'. The keyword dependency patterns were generated via running the Stanford Parser to obtain dependency relation types. Out of the 107 interactions in the LLL dataset represented with two-keyword interaction types, 86 were identified by using the direct dependency relations. The LLL dataset contained 34 gene regulation interaction types, each of which associated with multiple keywords. A hierarchical display of these 34 interaction types and their ancestor terms in INO resulted in the identification of specific gene-gene interaction patterns from the LLL dataset. The phenomenon of having multi-keyword interaction types was also frequently observed in the vaccine dataset. Conclusions: By modeling and representing multiple textual keywords for interaction types, the extended INO enabled the identification of complex biological gene-gene interactions represented with multiple keywords. © 2016 The Author(s).","Gene regulation; Gene-gene interaction; INO; Interaction keywords; Interaction Network Ontology; Interaction types; Literature mining; LLL dataset; SciMiner","classifier; data mining; gene control; gene expression regulation; gene interaction; information retrieval; Interaction Network Ontology; medical literature; medical ontology; molecular interaction; priority journal; Review; software; standardization",2-s2.0-85006380649
"Qtaish A., Ahmad K.","XAncestor: An efficient mapping approach for storing and querying XML documents in relational database using path-based technique",2016,"Knowledge-Based Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994752265&doi=10.1016%2fj.knosys.2016.10.009&partnerID=40&md5=901d9ee2bbb4fcae6bb3dcf23a6439b9","XML has become a common language for data exchange on the Web, so it needs to be managed effectively. There are four central problems in XML data management: capture, storage, retrieval, and exchange. Even though numerous database systems are available, the relational database (RDB) is often used to store and query the content of XML documents. Therefore the processes of mapping from XML to RDB and vice versa occur frequently. Numerous researchers have proposed approaches to map hierarchically structured XML documents into the tabular format of a RDB. However, the previously developed approaches have faced problems in terms of storage and query response time. If the design of a RDB is inefficient, the number of join operations between tables increases when a query is executed, which affects the query response time. To overcome this limitation, this paper proposes a new mapping approach, known as XAncestor, which consists of two algorithms: an XML mapping algorithm (XtoDB) and a query mapping algorithm (XtoSQL). XtoDB maps XML documents to a fixed RDB with less storage space. XtoSQL translates XPath queries into corresponding SQL queries based on the constructed RDB in order to reduce the query response time i.e., the time taken to execute the translated SQL query. XAncestor is then developed as a prototype in order to test its effectiveness. The results of XAncestor are compared with those produced by five similar approaches. The comparison proves that XAncestor performs better than the previously developed approaches in terms of effectiveness and scalability. The correctness of XAncestor is also verified. The paper concludes with some recommendations for further work. © 2016","Model mapping approach; Query response time; RDB storage space; Relational database; XML","Conformal mapping; Digital storage; Electronic data interchange; Information management; Mapping; Query languages; Relational database systems; XML; Central problems; Common languages; Mapping algorithms; Model mappings; Query response time; Relational Database; Relational databases (RDB); Storage spaces; Query processing",2-s2.0-84994752265
"Bringmann K., Grandoni F., Saha B., Williams V.V.","Truly Sub-cubic Algorithms for Language Edit Distance and RNA-Folding via Fast Bounded-Difference Min-Plus Product",2016,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009357395&doi=10.1109%2fFOCS.2016.48&partnerID=40&md5=6073e287f4fd12f293669075def965d0","It is a major open problem whether the (min,+)-product of two n by n matrices has a truly sub-cubic time algorithm, as it is equivalent to the famous All-Pairs-Shortest-Paths problem (APSP) in n-vertex graphs. There are some restrictions of the (min,+)-product to special types of matrices that admit truly sub-cubic algorithms, each giving rise to a special case of APSP that can be solved faster. In this paper we consider a new, different and powerful restriction in which one matrix can be arbitrary, as long as the other matrix has 'bounded differences' in either its columns or rows, i.e. any two consecutive entries differ by only a small amount. We obtain the first truly sub-cubic algorithm for this Bounded Differences (min,+)-product (answering an open problem of Chan and Lewenstein). Our new algorithm, combined with a strengthening of an approach of L. Valiant for solving context-free grammar parsing with matrix multiplication, yields the first truly sub-cubic algorithms for the following problems: Language Edit Distance (a major problem in the parsing community), RNA-folding (a major problem in bioinformatics) and Optimum Stack Generation (answering an open problem of Tarjan). © 2016 IEEE.","Bounded differences; Fast matrix multiplication; Language edit distance; Min-plus matrix multiplication; RNA folding; Truly sub-cubic algorithm","Context free grammars; Context free languages; Formal languages; Graph theory; Matrix algebra; RNA; Bounded differences; Cubic algorithm; Edit distance; Fast matrix multiplication; MAtrix multiplication; RNA folding; Bioinformatics",2-s2.0-85009357395
"Rubidha Devi D., Venkatesan R., Raghuraman K.","Detection and preclusion of SQL injection in a distributed environment using contemporary approach",2016,"International Journal of Pharmacy and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018214429&partnerID=40&md5=6cd65987dff12e369c66d3b87ab57b71","SQL injection is a code injection technique; it has been a top most security threat to web applications and it mainly targets the databases that are accessible through a web application. Any attacker can embed an injected string into the original query, which poses a serious threat to web application security. Attacker exploits the vulnerability by inserting crafted SQL keywords and values, effectively altering the semantics of dynamic queries, and causing them to return expected results. Besides many tools and research prototypes are available, that tool doesn‘t detect and prevent the injection attacks accurately in a web application. The existing system supports only single web application SQL injection attacks. In this experiment, a novel approach is followed by developing a hybrid model that prevents brute force attack at the initial stage. If exploited, an encrypted query parser tool will block the attacker from grabbing data from the database. This approach can also be ported to other web application development platforms without requiring major modifications. The proposed system can protect attackers attempting to intrude from multiple Internet Protocols interacting with a database server, which is an advantage over existing methods. © 2016, International Journal of Pharmacy and Technology. All rights reserved.","Brute force; Fencing tool; Query parser; SQL injection; SQLIPDT; Tautological attack","data base; injection; internet protocol; model",2-s2.0-85018214429
"Song K., Lu H.","High-performance XML modeling of parallel queries based on MapReduce framework",2016,"Cluster Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987681398&doi=10.1007%2fs10586-016-0628-z&partnerID=40&md5=8faee537c4f092f4fbf5fcbaaaf8e66a","With the increasing of data at an incredible rate, the development of cloud computing technologies is of critical importance to the advances of researches. MapReduce is a widely adopted computing framework for data-intensive applications running on clusters. Traditional parallel XML parsing and indexing approaches are inadequate for processing large-scale XML datasets on clusters and; therefore, we propose an approach to exploit data parallelisms in XML processing using MapReduce in Hadoop. Our solution seamlessly integrates data storage, labeling, indexing, and parallel queries to process a massive amount of XML data. Specifically, we introduce an SDN labeling algorithm and a distributed hierarchical index using DHTs. More importantly, we design an advanced two phase MapReduce solution that is able to efficiently address the issues of labeling, indexing, and query processing on big XML data. The first MapReduce phase applies filtering, labeling, index building techniques, in which each DataNode performs elements labeling using a map function and a reduce function to merge and build indexes. In the second phase, local XML queries in multiple partitions are performed in parallel using index-table-enabled B-SLCA. Our experimental results show the efficiency and effectiveness of our proposed parallel XML data approach using MapReduce Framework. © 2016, Springer Science+Business Media New York.","B-SLCA; Big XML; Distributed programming; MapReduce; Parallel programming","Design; Digital storage; Indexing (materials working); Indexing (of information); Parallel programming; Building techniques; Cloud computing technologies; Computing frameworks; Data-intensive application; Distributed programming; Indexing approaches; Map-reduce; Mapreduce frameworks; XML",2-s2.0-84987681398
"Pazos R R.A., Aguirre L M.A., González B J.J., Martínez F J.A., Pérez O J., Verástegui O A.A.","Comparative study on the customization of natural language interfaces to databases",2016,"SpringerPlus",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964894264&doi=10.1186%2fs40064-016-2164-y&partnerID=40&md5=24ef8524d89c8bbf1827e7266d6ec894","In the last decades the popularity of natural language interfaces to databases (NLIDBs) has increased, because in many cases information obtained from them is used for making important business decisions. Unfortunately, the complexity of their customization by database administrators make them difficult to use. In order for a NLIDB to obtain a high percentage of correctly translated queries, it is necessary that it is correctly customized for the database to be queried. In most cases the performance reported in NLIDB literature is the highest possible; i.e., the performance obtained when the interfaces were customized by the implementers. However, for end users it is more important the performance that the interface can yield when the NLIDB is customized by someone different from the implementers. Unfortunately, there exist very few articles that report NLIDB performance when the NLIDBs are not customized by the implementers. This article presents a semantically-enriched data dictionary (which permits solving many of the problems that occur when translating from natural language to SQL) and an experiment in which two groups of undergraduate students customized our NLIDB and English language frontend (ELF), considered one of the best available commercial NLIDBs. The experimental results show that, when customized by the first group, our NLIDB obtained a 44.69 % of correctly answered queries and ELF 11.83 % for the ATIS database, and when customized by the second group, our NLIDB attained 77.05 % and ELF 13.48 %. The performance attained by our NLIDB, when customized by ourselves was 90 %. © 2016, Pazos R. et al.","Databases; Natural language interface; Natural language processing; Semantic modelling",,2-s2.0-84964894264
"Kaur D., Kalra S.","Five-tier barrier anti-phishing scheme using hybrid approach",2016,"Information Security Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981506570&doi=10.1080%2f19393555.2016.1215573&partnerID=40&md5=41ad1546af5e483518ce4819158bce03","Though hoaxing people to make financial benefits is an old idea, phishers have realized that social engineering tools for web attacks are relatively easy to execute and are highly profitable over the Internet. One of the threatening criminal activities is phishing, in which the phishers trap users into revealing their identities and financial information to a fraudulent website. Researchers have proposed a number of anti-phishing techniques based on blacklist, whitelist, and visual similarity, but the major disadvantage with such approaches is that they are slow techniques with high false positive rates. For robust detection of phishing attacks, this article uses fundamentals of heuristic factors and a whitelist. The article proposes a safeguard scheme referred as the five-tier barrier hybrid approach. Input to the five-tier barrier is a uniform resource locator (URL), and output of the application is a status of the page (“Secure Connection” representing a legitimate URL, “Phishing Alert” representing phishing URL, and “Query Page” representing that the webpage needs to be processed further/failure of JSoup connection). In comparison to a blacklist, the five-tier barrier is competent in detecting zero-hour phishing attacks, and it is much faster than visual similarity–based anti-phishing techniques. © 2016 Taylor & Francis.","Anti-phishing; footer links; webpage identity; webpage logo; whitelist","Internet; Websites; Anti-phishing; Criminal activities; False positive rates; Financial benefits; Financial information; footer links; Social engineering; whitelist; Computer crime",2-s2.0-84981506570
"Duan N.","Overview of the NLPCC-ICCPOL 2016 shared task: Open domain Chinese question answering",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004007006&doi=10.1007%2f978-3-319-50496-4_89&partnerID=40&md5=87b7f4fe6f0175e16a06796828604770","In this paper, we give the overview of the open domain Question Answering (or open domain QA) shared task in the NLPCC-ICCPOL 2016. We first review the background of QA, and then describe two open domain Chinese QA tasks in this year’s NLPCC-ICCPOL, including the construction of the benchmark datasets and the evaluation metrics. The evaluation results of submissions from participating teams are presented in the experimental part. © Springer International Publishing AG 2016.","Document-based QA; Knowledge-based QA; Question answering","Artificial intelligence; Computer science; Computers; Knowledge based systems; Speech recognition; Benchmark datasets; Chinese question answering; Document-based; Evaluation results; Knowledge based; nocv1; Open domain question answering; Participating teams; Question Answering; Knowledge based systems; Natural language processing systems",2-s2.0-85004007006
"Yang F., Gan L., Li A., Huang D., Chou X., Liu H.","Combining deep learning with information retrieval for question answering",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004107279&doi=10.1007%2f978-3-319-50496-4_86&partnerID=40&md5=cdbc88f602c3bc720a71da1abab33606","This paper presents a system which learns to answer single-relation questions on a broad range of topics from a knowledge base using a threelayered learning system. Our system first learning a Topic Phrase Detecting model based on a phrase-entities dictionary to detect which phrase is the topic phrase of the question. The second layer of the system learning several answer ranking models. The last layer re-ranking the scores from the output of the second layer and return the highest scored answer. Both convolutional neural networks (CNN) and information retrieval (IR) models are included in this models. Training our system using pairs of questions and structured representations of their answers, yields competitive results on the NLPCC 2016 KBQA share task. © Springer International Publishing AG 2016.","Deep learning; Information retrieval; Knowledge base; Question answering","Knowledge based systems; Neural networks; Information retrieval; Convolutional neural network; Deep learning; Knowledge base; Model-based OPC; Question Answering; Ranking model; Second layer; System learning; Information retrieval; Natural language processing systems",2-s2.0-85004107279
"Xie Z., Zeng Z., Zhou G., He T.","Knowledge base question answering based on deep learning models",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004007041&doi=10.1007%2f978-3-319-50496-4_25&partnerID=40&md5=0afb24d2a58bee1f4c36f16b28b0f4ea","This paper focuses on the task of knowledge-based question answering (KBQA). KBQA aims to match the questions with the structured semantics in knowledge base. In this paper, we propose a two-stage method. Firstly, we propose a topic entity extraction model (TEEM) to extract topic entities in questions, which does not rely on hand-crafted features or linguistic tools. We extract topic entities in questions with the TEEM and then search the knowledge triples which are related to the topic entities from the knowledge base as the candidate knowledge triples. Then, we apply Deep Structured Semantic Models based on convolutional neural network and bidirectional long short-term memory to match questions and predicates in the candidate knowledge triples. To obtain better training dataset, we use an iterative approach to retrieve the knowledge triples from the knowledge base. The evaluation result shows that our system achieves an AverageF1 measure of 79.57% on test dataset. © Springer International Publishing AG 2016.",,"Iterative methods; Knowledge based systems; Neural networks; Semantics; Statistical tests; Tunneling (excavation); Data mining; Convolutional neural network; Entity extractions; Evaluation results; Iterative approach; Long short term memory; Question Answering; Training dataset; Two-stage methods; Data mining; Natural language processing systems",2-s2.0-85004007041
"Wang C., Song Y., Roth D., Zhang M., Han J.","World knowledge as indirect supervision for document clustering",2016,"ACM Transactions on Knowledge Discovery from Data",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008881028&doi=10.1145%2f2953881&partnerID=40&md5=b9da262ac2fe28a83f57e7c83c679a23","One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then, the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this article, we provide an example of using world knowledge for domain-dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then, we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features. A preliminary version of this work appeared in the proceedings of KDD 2015 [Wang et al. 2015a]. This journal version has made several major improvements. First, we have proposed a new and general learning framework for machine learning with world knowledge as indirect supervision, where document clustering is a special case in the original paper. Second, in order to make our unsupervised semantic parsing method more understandable, we add several real cases from the original sentences to the resulting logic forms with all the necessary information. Third, we add details of the three semantic filtering methods and conduct deep analysis of the three semantic filters, by using case studies to show why the conceptualization-based semantic filter can produce more accurate indirect supervision. Finally, in addition to the experiment on 20 newsgroup data and Freebase, we have extended the experiments on clustering results by using all the combinations of text (20 newsgroup, MCAT, CCAT, ECAT) and world knowledge sources (Freebase, YAGO2). © 2016 ACM.","Document clustering; Heterogeneous information network; Knowledge base; Knowledge graph; World knowledge","Cluster analysis; Information retrieval; Information services; Knowledge based systems; Learning systems; Semantics; Syntactics; Document Clustering; Heterogeneous information; Knowledge base; Knowledge graphs; World knowledge; Clustering algorithms",2-s2.0-85008881028
"Nissim N., Moskovitch R., BarAd O., Rokach L., Elovici Y.","ALDROID: efficient update of Android anti-virus software using designated active learning methods",2016,"Knowledge and Information Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955561013&doi=10.1007%2fs10115-016-0918-z&partnerID=40&md5=be234dddc0ead1ead77e766c95f265be","Many new unknown malwares aimed at compromising smartphones are created constantly. These widely used smartphones are very dependent on anti-virus solutions due to their limited resources. To update the anti-virus signature repository, anti-virus vendors must deal with vast quantities of new applications daily in order to identify new unknown malwares. Machine learning algorithms have been used to address this task, yet they must also be efficiently updated on a daily basis. To improve detection and updatability, we introduce a new framework, “ALDROID” and active learning (AL) methods on which ALDROID is based. Our methods are aimed at selecting only new informative applications (benign and especially malicious), thus reducing the labeling efforts of security experts, and enable a frequent and efficient process of enhancing the framework’s detection model and Android’s anti-virus software. Results indicate that our AL methods outperformed other solutions including the existing AL method and heuristic engine. Our AL methods acquired the largest number and percentage of new malwares, while preserving the detection models’ detection capabilities (high TPR and low FPR rates). Specifically, our methods acquired more than double the amount of new malwares acquired by the heuristic engine and 6.5 times more malwares than the existing AL method. © 2016, Springer-Verlag London.","Acquisition; Active learning; Android; Anti-virus; Application; Detection; Malware",,2-s2.0-84955561013
"Beltagy I., Roller S., Cheng P., Erk K., Mooney R.J.","Representing meaning with a combination of logical and distributional models",2016,"Computational Linguistics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010800030&doi=10.1162%2fCOLI_a_00266&partnerID=40&md5=1f031096523fcf3d6a0e9f9a0bcabbdf","NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary. We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system:1 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logicbased and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.2. © 2017 Association for Computational Linguistics.",,,2-s2.0-85010800030
"Kern M.L., Park G., Eichstaedt J.C., Schwartz H.A., Sap M., Smith L.K., Ungar L.H.","Gaining insights from social media language: Methodologies and challenges",2016,"Psychological Methods",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004097351&doi=10.1037%2fmet0000091&partnerID=40&md5=b9d76996f36a16ce67eae60b0a7e98f0","Language data available through social media provide opportunities to study people at an unprecedented scale. However, little guidance is available to psychologists who want to enter this area of research. Drawing on tools and techniques developed in natural language processing, we first introduce psychologists to social media language research, identifying descriptive and predictive analyses that language data allow. Second, we describe how raw language data can be accessed and quantified for inclusion in subsequent analyses, exploring personality as expressed on Facebook to illustrate. Third, we highlight challenges and issues to be considered, including accessing and processing the data, interpreting effects, and ethical issues. Social media has become a valuable part of social life, and there is much we can learn by bringing together the tools of computer science with the theories and insights of psychology. © 2015 American Psychological Association.","Computational social science; Interdisciplinary collaboration; Linguistic analysis; Online behavior; Social media",,2-s2.0-85004097351
"Burgin M.","Theory of knowledge: Structures and processes",2016,"Theory of Knowledge: Structures and Processes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018672541&doi=10.1142%2f8893&partnerID=40&md5=0c45637d26884ca7a5cdb780fb3be5b3","This book aims to synthesize different directions in knowledge studies into a unified theory of knowledge and knowledge processes. It explicates important relations between knowledge and information. It provides the readers with understanding of the essence and structure of knowledge, explicating operations and process that are based on knowledge and vital for society. The book also highlights how the theory of knowledge paves the way for more advanced design and utilization of computers and networks. © 2017 by World Scientific Publishing Co. Pte. Ltd. All rights reserved.",,,2-s2.0-85018672541
"Narayana S., Sivaraman A., Nathan V., Alizadeh M., Walker D., Rexford J., Jeyakumar V., Kim C.","Hardware-software co-design for network performance measurement",2016,"HotNets 2016 - Proceedings of the 15th ACM Workshop on Hot Topics in Networks",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002194787&doi=10.1145%2f3005745.3005775&partnerID=40&md5=2d3e4315e3a3bdfe520d287e58d4c64f","Diagnosing performance problems in networks is important, for example to determine where packets experience high latency. However, existing diagnostic tools are constrained by limited switch mechanisms for measurement. As a result, operators use endpoint information to indirectly infer root causes for performance issues. Instead of designing piecemeal solutions to work around limited switch mechanisms, we believe that the right approach is to co-design language abstractions and switch hardware primitives for performance measurement. This approach provides confidence that the switch primitives are useful for a variety of existing and unanticipated use cases. We present a declarative query language that allows operators to ask a diverse set of network performance questions. We show that these queries can be implemented efficiently in switch hardware using a programmable key-value store primitive. Our preliminary evaluations show that our hardware design incurs modest additional chip area relative to existing switching chips, suggesting that it is a practical solution for network performance measurement.",,"Hardware; Network performance; Query languages; Declarative query languages; Key-value stores; Network performance measurement; Performance issues; Performance measurements; Performance problems; Practical solutions; Switch mechanism; Hardware-software codesign",2-s2.0-85002194787
"Zan T., Pacheco H., Ko H.-S., Hu Z.","BiFluX: A bidirectional functional update language for XML",2016,"Computer Software",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009966175&partnerID=40&md5=664523f5ed41a4950bc0891581feadad","Different XML formats are widely used for data exchange and processing, being often necessary to mutually convert between them. Standard XML transformation languages, like XSLT or XQuery, are unsatisfactory for this purpose since they require writing a separate transformation for each direction. Existing bidirec- tional transformation languages mean to cover this gap, by allowing programmers to write a single program that denotes both transformations. However, they often 1) induce a more cumbersome programming style than their traditionally unidirectional relatives, to establish the link between source and target formats, and 2) offer limited configurability, by making implicit assumptions about how modifications to both formats should be translated that may not be easy to predict. This paper proposes a bidirectional XML update language called BiFluX (BIdirectional FunctionaL Updates for XML), inspired by the Flux XML update language. Our language adopts a novel bidirectional programming by update paradigm, where a program succinctly and precisely describes how to update a source document with a target document in an intuitive way, such that there is a unique ""inverse"" source query for each update program. BiFluX extends Flux with bidirectional actions that describe the con- nection between source and target formats. We introduce a core BiFluX language, and translate it into a formally verified bidirectional update language BiGUL to guarantee a BiFluX program is well-behaved.",,"Data handling; Electronic data interchange; XML; Bi-directional programming; Configurability; Programming styles; Transformation languages; Update languages; XML format; XML transformation; XML update; Translation (languages)",2-s2.0-85009966175
"Kuppusamy K.S., Balaji V., Balamurugan K.","Personalized accessibility based Re-ranking of Search Engine Results (PARSER)",2016,"Proceedings of the 10th International Conference on Intelligent Systems and Control, ISCO 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007395875&doi=10.1109%2fISCO.2016.7726942&partnerID=40&md5=beafa9af57e6d3c54e4fcdff1624f61d","The relevancy and usability of the contents retrieved are the two major factors for deciding the utility value of web resources. In the case of persons with disabilities the usability aspect of the interface is solely dependent on the accessibility of the web contents. This paper proposes a model termed PARSER-Personalized Accessibility based Re-ranking of Search Engine Results which aims towards quantizing the optimality of the pages with respect to the user's personalized accessibility and information requirement context. The PARSER model proposes a metric termed PAS-Personalized Accessibility Score which computes the accessibility score of a given web page for a particular user profile. The model incorporates aggregation of three ranked lists derived by search engines, accessibility scores and personalized information requirement context, to build the re-ranked list. The interface proposed by the PARSER model is targeted towards reducing the information access barrier. © 2016 IEEE.","accessible information retrieval; personalized accessibility evaluation; web accessibility","Information retrieval; Intelligent systems; Transportation; Websites; Accessibility evaluation; Accessible information; Information access; Information requirement; Personalized information; Persons with disabilities; Search engine results; Web accessibility; Search engines",2-s2.0-85007395875
"Rompf T.","Reflections on LMS: Exploring front-end alternatives",2016,"SCALA 2016 - Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002235271&doi=10.1145%2f2998392.2998399&partnerID=40&md5=832c6e7a9b3c86fc7d4104e13a876751","Metaprogramming techniques to generate code at runtime in a general-purpose meta-language have seen a surge of interest in recent years, driven by the widening performance gap between high-level languages and emerging hardware platforms. In the context of Scala, the LMS (Lightweight Modular Staging) framework has contributed to ""abstraction without regret""-high-level programming without performance penalty-in a number of challenging domains, through runtime code generation and embedded compiler pipelines based on stacks of DSLs. Based on this experience, this paper crystallizes some of the design decisions of LMS and discusses potential alternatives, which maintain the underlying spirit but differ in implementation choices: specifically, strategies for realizing more flexible front-end embeddings using type classes instead of higher-kinded types, and strategies for type-safe metaprogramming with untyped intermediate representations.","Domain-specific languages; Intermediate representation; Multi-stage programming","Computer programming languages; Pipeline codes; Problem oriented languages; Program compilers; Domain specific languages; Hardware platform; High-level programming; Intermediate representations; Multi-stage programming; Performance gaps; Performance penalties; Run-time code generation; High level languages",2-s2.0-85002235271
"Memon K.R., Memon S., Memon B., Memon A.R., Shah S.M.Z.A.","Real time implementation of path planning algorithm with obstacle avoidance for autonomous vehicle",2016,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428857&partnerID=40&md5=7e0f481af62eed65c7c40823255c2bd6","Determination of a suitable path for a robot that is collision free between the initial and end positions through a workspace in the presence of obstacles is challenging for Autonomous Robot design. In this paper, we presenta prototype of an Autonomous Mobile Robot that finds the optimal path using Google navigation to navigate in a realtime environment. However, Google Maps or Google Navigation do not provide realtime obstacles at the current time, so it is also important to know about the obstacles in realtime and to avoid them. The Robot that we have designed consists of two sections, one is mobile Robot and other is workstation. The function of workstation is to interact with Google Maps and obtain multiple ways to reach the destination, it also finds optimal way from multiple ways and instruct Robot to follow that route. The Robot will be given a target location using GPS Coordinates and Robot has to find its way from current position to target by finding the optimal path and avoiding obstacles and finding its way even inside the building. © 2016 IEEE.","Autonomous; Google Navigation Implementation","Collision avoidance; Machine design; Mobile robots; Motion planning; Navigation; Real time control; Autonomous; Autonomous Mobile Robot; Autonomous Vehicles; Avoiding obstacle; Google Navigation Implementation; Path-planning algorithm; Real-time environment; Real-time implementations; Robots",2-s2.0-84997428857
"Argyros G., Stais I., Jana S., Keromytis A.D., Kiayias A.","SFADiff: Automated evasion attacks and fingerprinting using black-box differential automata learning",2016,"Proceedings of the ACM Conference on Computer and Communications Security",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995519560&doi=10.1145%2f2976749.2978383&partnerID=40&md5=5bcd3bed11144a6f26b404871653fdf7","Finding differences between programs with similar functionality is an important security problem as such differences can be used for fingerprinting or creating evasion attacks against security software like Web Application Firewalls (WAFs) which are designed to detect malicious inputs to web applications. In this paper, we present SFADiff, a black-box differential testing framework based on Symbolic Finite Automata (SFA) learning. SFADiff can automatically find differences between a set of programs with comparable functionality. Unlike existing differential testing techniques, instead of searching for each difference individually, SFADiff infers SFA models of the target programs using black-box queries and systematically enumerates the differences between the inferred SFA models. All differences between the inferred models are checked against the corresponding programs. Any difference between the models, that does not result in a difference between the corresponding programs, is used as a counterexample for further refinement of the inferred models. SFADiff's model-based approach, unlike existing differential testing tools, also support fully automated root cause analysis in a domain-independent manner. We evaluate SFADiff in three different settings for finding discrepancies between: (i) three TCP implementations, (ii) four WAFs, and (iii) HTML/JavaScript parsing implementations in WAFs and web browsers. Our results demonstrate that SFADiff is able to identify and enumerate the differences systematically and efficiently in all these settings. We show that SFADiff is able to find differences not only between different WAFs but also between different versions of the same WAF. SFADiff is also able to discover three previously-unknown differences between the HTML/Java-Script parsers of two popular WAFs (PHPIDS 0.7 and Expose 2.4.0) and the corresponding parsers of Google Chrome, Firefox, Safari, and Internet Explorer. We confirm that all these differences can be used to evade the WAFs and launch successful cross-site scripting attacks. © 2016 ACM.",,"Application programs; Computer system firewalls; Computer viruses; HTML; Testing; Web browsers; Cross Site Scripting Attacks; Differential testing; Domain independents; Internet explorers; Model based approach; Root cause analysis; Security problems; Web application firewalls; Black-box testing",2-s2.0-84995519560
"Sun X., Li C., Ren F.","Sentiment analysis for Chinese microblog based on deep neural networks with convolutional extension features",2016,"Neurocomputing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991256807&doi=10.1016%2fj.neucom.2016.02.077&partnerID=40&md5=fd6ffb8552755b84851c066cf3831b7f","Related research for sentiment analysis on Chinese microblog is aiming at the analysis procedure of posts. The length of short microblog text limits feature extraction of microblog. Tweeting is the process of communication with friends, so that microblog comments are important reference information for related post. A contents extension framework is proposed in this paper combining posts and related comments into a microblog conversation for features extraction. A novel convolutional auto encoder is adopted which can extract contextual information from microblog conversation as features for the post. A customized DNN (Deep Neural Network) model, which is stacked with several layers of RBM (Restricted Boltzmann Machine), is implemented to initialize the structure of neural network. The RBM layers can take probability distribution samples of input data to learn hidden structures for better high level features representation. A ClassRBM (Classification RBM) layer, which is stacked on top of RBM layers, is adopted to achieve the final sentiment classification label for the post. Experimental results show that, with proper structure and parameters, the performance of proposed DNN on sentiment classification is better than state-of-the-art surface learning models such as SVM or NB, which proves that the proposed DNN model is suitable for short-length document classification with the proposed feature dimensionality extension method. © 2016 Elsevier B.V.","ClassRBM; DNN; Microblog conversation; RBM; Sentiment analysis","Convolution; Data mining; Extraction; Feature extraction; Information retrieval systems; Probability distributions; ClassRBM; Contextual information; Deep neural networks; Document Classification; Micro-blog; Restricted boltzmann machine; Sentiment analysis; Sentiment classification; Classification (of information); analysis; Article; artificial neural network; classification; controlled study; conversation; data processing; deep neural network; feature dimensionality extension method; features extraction; information processing device; information retrieval; machine learning; measurement accuracy; microblog; priority journal; restricted boltzmann machine; sentiment analysis; software",2-s2.0-84991256807
"Wang X., Gulwani S., Singh R.","FIDEX: Filtering spreadsheet data using examples",2016,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995676363&doi=10.1145%2f2983990.2984030&partnerID=40&md5=6f3273be5bcf075870729b09f838b21a","Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples. There are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0:22s), using only 2:2 positive string instances and 2:7 negative string instances on average. © 2016 ACM.","Data filtering; Program synthesis; Programming by examples; Regular expressions","Computer systems programming; Digital subscriber lines; Human computer interaction; Spreadsheets; Computational model; Data filtering; Desired datum; Efficient synthesis; Negative examples; Program synthesis; Programming by Example; Regular expressions; Object oriented programming",2-s2.0-84995676363
"Li H., Xu F.","Question answering with DBpedia based on the dependency parser and entity-centric index",2016,"Proceedings - 2016 International Conference on Computational Intelligence and Applications, ICCIA 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994850866&doi=10.1109%2fICCIA.2016.10&partnerID=40&md5=d27f5e4648b945d672480870bf24972f","The emerging Linked Open Data provides an opportunity to answer the natural language question based on knowledge bases (KB). This study proposes an approach to question answering (QA) on the DBpedia dataset. After parsing the question by a dependency parser, we locate the entity mention and property mention with predefined templates. We propose an entity-centric indexing model to help search referent entities in KB. After obtaining the referent entities, we expand the property mention with WordNet and ConceptNet to find the referent properties of the returned entities. The values of the referent property are then considered the answer to the question. Evaluations are performed on DBpedia version 2015. Results show that our approach reaches 46% precision when the top-10 entities are returned in the final QA stage. The evaluation tests show that our approach is promising in dealing with QA in Linked Data. © 2016 IEEE.",,"Artificial intelligence; Computational linguistics; Dependency parser; Evaluation test; Indexing models; Knowledge basis; Linked datum; Linked open datum; Natural language questions; Question Answering; Natural language processing systems",2-s2.0-84994850866
"Setlur V., Battersby S.E., Tory M., Gossweiler R., Chang A.X.","Eviza: A natural language interface for visual analysis",2016,"UIST 2016 - Proceedings of the 29th Annual Symposium on User Interface Software and Technology",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995804143&doi=10.1145%2f2984511.2984588&partnerID=40&md5=259ee25e193b001375d539d58311b256","Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multimodal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis. © 2016 Copyright held by the owner/author(s).","Ambiguity; Natural language; Parser; Pragmatics; Probabilistic grammar; Visual data analysis; Visualization","Data visualization; Flow visualization; Knowledge based systems; Natural language processing systems; Query processing; Semantics; User interfaces; Visualization; Ambiguity; Natural languages; Parser; Pragmatics; Probabilistic grammars; Visual data analysis; Visual languages",2-s2.0-84995804143
"Keller F.","Jointly representing images and text: Dependency graphs, word senses, and multimodal embeddings",2016,"Iv and L-MM 2016 - Proceedings of the 2016 ACM Workshop on Vision and Language Integration Meets Multimedia Fusion, co-located with ACM Multimedia 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995495299&doi=10.1145%2f2983563.2986050&partnerID=40&md5=37c8249d1e88ab23911bd9ccd8bafbb1","The amount of image data available on the web is growing rapidly: on Facebook alone, 350 million new images are uploaded every day. Making sense of this data requires new ways of efficiently indexing, annotating, and querying such enormous collections. Research in computer vision has tackled this problem by developing algorithms for localizing and labeling objects in images. Object classification algorithms have been recently scaled up to work on thousands of object classes [8] based on the ImageNet database [2]. The next frontier in analyzing images is to go beyond classifying objects: to truly understand a visual scene, we need to identify how the objects in that scene relate to each other, which actions and events they are involved in, and ultimately recognize the intentions of the actors depicted in the scene. The key to achieving this goal is to develop methods for parsing images into structured representations. A number of approaches have recently been proposed in the literature, including Visual Dependency Representations [4], Scene Graphs [7], and Scene Description Graphs [1]. All of these models represent an image as a structured collection of objects, attributes, and relations between them. In this presentation, we will focus on Visual Dependency Representations (VDRs), the only approach to image structure that is explicitly multimodal. VDRs start from the observation that images typically do not exist in isolation, but co-occur with textual data such as comments, captions, or tags; well-established techniques exist for extracting structure from such textual data. The VDR model exploits this observation by positing an image structure that links objects through geometric relations. Text accompanying the image can be parsed into a syntactic dependency graph [9], and the two representations are aligned, yielding a multimodal graph (see Figure 1). Well-established synchronous parsing techniques from machine translation [11] can be applied to this task, and resulting VDRs are useful for image description and retrieval [5, 3, 10]. Parsing images into multimodal graph structures is an important step towards image understanding. However, for full understanding, representing the semantics of the image is also crucial. For example, the images in Figure 2 can all be described using the verb play (and presumably are assigned similar VDRs). However, a different meaning (verb sense) of play is evoked by each image. This has led to the new task of visual verb sense disambiguation [6]: given an image and a verb, assign the correct sense of the verb, i.e., the one that corresponds to the action depicted in the image. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, and multimodal embeddings. In this presentation, we will discuss how the two tasks of VDR parsing and visual verb disambiguation can be combined to yield more complete syntactico-semantic image representations, which can then underpin applications such as image retrieval, image description, and visual question answering. © 2016 Copyright held by the owner/author(s).","Dependency graphs; Image description; Image parsing; Language and vision; Word-sense disambiguation","Classification (of information); Computer vision; Data mining; Graphic methods; Image analysis; Image retrieval; Image segmentation; Natural language processing systems; Program processors; Query processing; Semantics; Syntactics; Dependency graphs; Image descriptions; Image parsing; Object classification; Syntactic dependencies; Unsupervised algorithms; Well-established techniques; Word Sense Disambiguation; Image processing",2-s2.0-84995495299
"Wani S., Wahiddin M.R., Sembok T.M.T.","Logico-linguistic Semantic Representation of Documents",2016,"Proceedings - 2016 IEEE 14th International Conference on Dependable, Autonomic and Secure Computing, DASC 2016, 2016 IEEE 14th International Conference on Pervasive Intelligence and Computing, PICom 2016, 2016 IEEE 2nd International Conference on Big Data Intelligence and Computing, DataCom 2016 and 2016 IEEE Cyber Science and Technology Congress, CyberSciTech 2016, DASC-PICom-DataCom-CyberSciTech 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995486618&doi=10.1109%2fDASC-PICom-DataCom-CyberSciTec.2016.135&partnerID=40&md5=415c650402d4fbaa81a4429da5c72fed","The knowledge behind the gigantic pool of data remains largely unextracted. Techniques such as ontology design, RDF representations, hpernym extraction, etc. have been used to represent the knowledge. However, the area of logic (FOPL) and linguistics (Semantics) has not been explored in depth for this purpose. Search engines suffer in extraction of specific answers to queries because of the absence of structured domain knowledge. The current paper deals with the design of formalism to extract and represent knowledge from the data in a consistent format. The application of logic and linguistics combined greatly eases and increases the precision of knowledge translation from natural language. The results clearly indicate the effectiveness of the knowledge extraction and representation methodology developed providing intelligence to machines for efficient analysis of data. The methodology helps machines to retrieve precise results in an efficient manner. © 2016 IEEE.","Al-Qur'an; knowledge extraction; Knowledge representation; knowledge translation; linguistics; logic; logico-linguistic formalism; semantics","Computation theory; Computer circuits; Extraction; Knowledge representation; Linguistics; Reconfigurable hardware; Search engines; Semantic Web; Semantics; Translation (languages); Efficient analysis; Knowledge extraction; Knowledge extraction and representations; Knowledge translation; logic; logico-linguistic formalism; Semantic representation of documents; Structured domain knowledge; Big data",2-s2.0-84995486618
"Fan H., Yang H., Ma Z., Liu J.","TwigStack-MR: An approach to distributed XML twig query using MapReduce",2016,"Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994556206&doi=10.1109%2fBigDataCongress.2016.79&partnerID=40&md5=b9760ba4f3ee1bcf1ff957f2302ed37a","Twig pattern query is the core operation of XML process, which directly affects the efficiency of XML data query. It is a challenge to manipulate massive XML data, especially on distributed cluster, such as how to effectively ensure the completeness and correctness of the query results, and minimize communication costs between the various machines. In this paper, we present TwigStack-MR, which simultaneously processes several twig pattern queries for a massive volume of XML data based on MapReduce framework. We first split the large scale XML data file into file-splits as input to the distributed storage system. Then we present the distributed twig algorithm, processing different subtrees of the document tree in parallel. Finally we use the MapReduce framework, full characteristics of distributed environments, to process twig query efficiently. The experimental results show that our approach is efficient and scalable on this issue. © 2016 IEEE.","Hadoop; MapReduce; Twig query; XML","Digital storage; Multiprocessing systems; Trees (mathematics); XML; Communication cost; Distributed clusters; Distributed environments; Distributed storage system; Hadoop; Map-reduce; Mapreduce frameworks; Twig queries; Big data",2-s2.0-84994556206
"Xu K., Cai H.","Data resource semantic support method research based on meta-data annotation",2016,"World Automation Congress Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994059924&doi=10.1109%2fWAC.2016.7583041&partnerID=40&md5=441e97d4ef55b0676daa7df3f37bd983","The concept of the cloud is no longer limited to providing remote hardware or platform support with the development of cloud computing technology. Instead, it encapsulates all aspects of the content in computer field into service invoked by others. As an important part of software, data resource has become core services to the cloud platform in areas such as collaborative manufacturing and collaborative service. Since these data are complex in structure, strongly independent but related to each other, it is necessary to annotate these data to improve their operability and make them able to be utilized more widely. This paper wants to design an annotation method based on word segmentation and semantics replacement aimed at metadata according to the characteristics of the meta-data itself. As a result, these data resource can then be understood by computers and further support later studies such as data fusion and semantic retrieval. © 2016 TSI Enterprise Inc (TSI Press).","annotation on meta-data; PSD; semantic web; word segmentation","Computational linguistics; Data fusion; Semantic Web; Annotation methods; Cloud computing technologies; Cloud platforms; Collaborative manufacturing; Collaborative services; Remote hardware; Semantic retrieval; Word segmentation; Metadata",2-s2.0-84994059924
"Arora C., Sabetzadeh M., Briand L., Zimmer F.","Extracting domain models from natural-language requirements: Approach and industrial evaluation",2016,"Proceedings - 19th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008449860&doi=10.1145%2f2976767.2976769&partnerID=40&md5=ad87efc806c15d90fdae267ebcc3c404","Domain modeling is an important step in the transition from natural-language requirements to precise specifications. For large systems, building a domain model manually is a laborious task. Several approaches exist to assist engineers with this task, whereby candidate domain model elements are automatically extracted using Natural Language Processing (NLP). Despite the existing work on domain model extraction, important facets remain under-explored: (1) there is limited empirical evidence about the usefulness of existing extraction rules (heuristics) when applied in industrial settings; (2) existing extraction rules do not adequately exploit the natural-language dependencies detected by modern NLP technologies; and (3) an important class of rules developed by the information retrieval community for information extraction remains unutilized for building domain models. Motivated by addressing the above limitations, we develop a domain model extractor by bringing together existing extraction rules in the software engineering literature, extending these rules with complementary rules from the information retrieval literature, and proposing new rules to better exploit results obtained from modern NLP dependency parsers. We apply our model extractor to four industrial requirements documents, reporting on the frequency of different extraction rules being applied. We conduct an expert study over one of these documents, investigating the accuracy and overall effectiveness of our domain model extractor. © 2016 ACM.","Case study research; Model extraction; Natural language processing; Natural-language requirements","Information retrieval; Modeling languages; Query languages; Search engines; Software engineering; Case study research; Industrial evaluations; Industrial requirements; Industrial settings; Model extraction; NAtural language processing; Natural language requirements; Overall effectiveness; Natural language processing systems",2-s2.0-85008449860
"Wei R., Kolovos D.S., Garcia-Dominguez A., Barmpis K., Paige R.F.","Partial loading of XMI models",2016,"Proceedings - 19th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008482366&doi=10.1145%2f2976767.2976787&partnerID=40&md5=f3f48056cc079bc7711a3ef38b374033","XML Metadata Interchange (XMI) is an OMG-standardised model exchange format, which is natively supported by the Eclipse Modeling Framework (EMF) and the majority of the modelling and model management languages and tools. Whilst XMI is widely supported, the XMI parser provided by EMF is inefficient in some cases where models are readonly (such as input models for model query, model-to-model transformation, etc) as it always requires loading the entire model into memory. In this paper we present a novel algorithm, and a prototype implementation (SmartSAX), which is capable of partially loading models persisted in XMI. SmartSAX oérs improved performance, in terms of loading time and memory footprint, over the default EMF XMI parser. We describe the algorithm in detail, and present benchmarking results that demonstrate the substantial improvements of the prototype implementation over the XMI parser provided by EMF. © 2016 ACM.","EMF; Partial model loading; XMI","Computational linguistics; Electric potential; Embedded systems; Modeling languages; Eclipse modeling framework; Memory footprint; Model management; Model to model transformation; Novel algorithm; Partial loading; Partial modeling; Prototype implementations; Loading",2-s2.0-85008482366
"Szárnyas G., Kovári Z., Salánki Á., Varró D.","Towards the characterization of realistic models: Evaluation of multidisciplinary graph metrics",2016,"Proceedings - 19th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008516505&doi=10.1145%2f2976767.2976786&partnerID=40&md5=adc3ec2752f80aa88ad1d60ae4b547f7","Custom generators of graph-based models are used in MDE for many purposes such as functional testing and performance benchmarking of modeling environments to ensure the correctness and scalability of tools. However, while existing generators may generate large models in increasing size, these models are claimed to be simple and synthetic, which hinders their credibility for industrial and research benchmarking purposes. But how to characterize a realistic model used in software and systems engineering? This question is investigated in the paper by collecting over 17 different widely used graph metrics taken from other disciplines (e.g. network theory) and evaluating them on 83 instance models originating from six modeling domains. Our preliminary results show that certain metrics are similar within a domain, but differ greatly between domains, which makes them suitable input for future instance model generators to derive more realistic models. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Domain models; Graph metrics; Network theory","Benchmarking; Circuit theory; Graphic methods; Industrial research; Domain model; Functional testing; Graph metrics; Graph-based models; Model generator; Modeling environments; Performance benchmarking; Software and systems engineerings; Graph theory",2-s2.0-85008516505
"Barrett D.P., Barbu A., Siddharth N., Siskind J.M.","Saying What You're Looking For: Linguistics Meets Video Search",2016,"IEEE Transactions on Pattern Analysis and Machine Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986267509&doi=10.1109%2fTPAMI.2015.2505297&partnerID=40&md5=72f8f59867463d449f8432aa0926ab66","We present an approach to searching large video corpora for clips which depict a natural-language query in the form of a sentence. Compositional semantics is used to encode subtle meaning differences lost in other approaches, such as the difference between two sentences which have identical words but entirely different meaning: The person rode the horse versus The horse rode the person. Given a sentential query and a natural-language parser, we produce a score indicating how well a video clip depicts that sentence for each clip in a corpus and return a ranked list of clips. Two fundamental problems are addressed simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, our approach uses the sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While most earlier work was limited to single-word queries which correspond to either verbs or nouns, we search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 2,627 naturally elicited sentential queries in 10 Hollywood movies. © 1979-2012 IEEE.","event recognition; language; object detection; Retrieval; sentential video retrieval; tracking; video","Object recognition; Semantics; Surface discharges; Syntactics; Event recognition; language; Retrieval; video; Video retrieval; Object detection",2-s2.0-84986267509
"Cao G.P., Thangapandian S., Son M., Kumar R., Choi Y.-J., Kim Y., Kwon Y.J., Kim H.-H., Suh J.-K., Lee K.W.","QSAR modeling to design selective histone deacetylase 8 (HDAC8) inhibitors",2016,"Archives of Pharmacal Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982315559&doi=10.1007%2fs12272-015-0705-5&partnerID=40&md5=4777a797958b4f05829a2e6ce44face4","HDAC8 inhibitors have become an attractive treatment for cancer. This study aimed to facilitate the identification of potential chemical scaffolds for the selective inhibition of histone deacetylase 8 (HDAC8) using in silico approaches. Non-linear QSAR classification and regression models of HDAC8 inhibitors were developed with support vector machine. Mean impact value-based sequential forward feature selection and grid search strategy were used for molecular descriptor selection and parameter optimization, respectively. The generated QSAR models were validated by leave-one-out cross validation and an external test set. The best QSAR classification model yielded 84 % of accuracy on the external test prediction and Matthews correlation coefficient is 0.69. The best QSAR regression model showed low root-mean-square error (0.63) and high squared correlation coefficient (0.53) for the test set. The validated QSAR models together with various drug-like properties, molecular docking and molecular dynamics simulation were sequentially used as a multi-step query in chemical database virtual screening. Finally, two hit compounds were discovered as new structural scaffolds which can be used for further in vitro and in vivo activity analyses. The strategy used in this study could be a promising computational strategy which can be utilized for other target drug design. © 2016, The Pharmaceutical Society of Korea.","Histone deacetylases 8; Molecular docking; Molecular dynamics simulation; QSAR; Support vector machine; Virtual screening","histone deacetylase 8; histone deacetylase inhibitor; HDAC8 protein, human; histone deacetylase; histone deacetylase inhibitor; repressor protein; accuracy; Article; chemical database; computer model; correlation coefficient; drug absorption; drug bioavailability; drug design; drug identification; drug screening; enzyme inhibition; human; molecular docking; molecular dynamics; nonhuman; quantitative structure activity relation; support vector machine; antagonists and inhibitors; chemistry; drug design; factual database; metabolism; molecular dynamics; Databases, Factual; Drug Design; Histone Deacetylase Inhibitors; Histone Deacetylases; Humans; Molecular Dynamics Simulation; Quantitative Structure-Activity Relationship; Repressor Proteins",2-s2.0-84982315559
"Polig R., Atasu K., Hagleitner C., Xu T., Nakayama A.","Annotation-based finite-state transducers on reconfigurable devices",2016,"FPL 2016 - 26th International Conference on Field-Programmable Logic and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994807408&doi=10.1109%2fFPL.2016.7577330&partnerID=40&md5=c000a221450c70d5c6bd6b29391777f0","With the ever growing amount of unstructured data, high-speed content analysis becomes ever more important. Enabling efficient search functions to help locate specific and relevant information hidden in this big data is a crucial task of today's enterprise systems, and can lead to valuable insights. A key component of content analysis systems are text parsers, which transform unstructured text data into structured information. Cascaded grammars offer a popular and powerful representation of text parsers by enabling the definition of more complex patterns in terms of simpler ones in a hierarchical fashion. This work presents a compilation framework to generate an optimized FPGA pipeline from a cascaded grammar description. We also describe the system integration and the way FPGA-based accelerators can be used as part of larger analysis tasks within Unstructured Information Management Application (UIMA) pipelines. We compare the performance of the hardware-accelerated system and a commercial software implementation using real-life UIMA pipelines from the healthcare domain. We show that the FPGA-accelerated system processes the parsing stage of a UIMA pipeline up to 31 times faster than the software implementation running on a high-end server, which results in an acceleration of up to 5 times for the complete pipeline. © 2016 EPFL.",,"Big data; Computer circuits; Field programmable gate arrays (FPGA); Pipeline processing systems; Pipelines; Reconfigurable hardware; Search engines; Speech recognition; Commercial software; Finite state transducers; Hardware-accelerated systems; Reconfigurable devices; Software implementation; Structured information; Unstructured information managements; Unstructured texts; Information management",2-s2.0-84994807408
"Goodall N.J.","Away from Trolley Problems and Toward Risk Management",2016,"Applied Artificial Intelligence",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997706130&doi=10.1080%2f08839514.2016.1229922&partnerID=40&md5=376c9f0c98ba59d74e6cdc396e7759dd","As automated vehicles receive more attention from the media, there has been an equivalent increase in the coverage of the ethical choices a vehicle may be forced to make in certain crash situations with no clear safe outcome. Much of this coverage has focused on a philosophical thought experiment known as the “trolley problem,” and substituting an automated vehicle for the trolley and the car’s software for the bystander. While this is a stark and straightforward example of ethical decision making for an automated vehicle, it risks marginalizing the entire field if it is to become the only ethical problem in the public’s mind. In this chapter, I discuss the shortcomings of the trolley problem, and introduce more nuanced examples that involve crash risk and uncertainty. Risk management is introduced as an alternative approach, and its ethical dimensions are discussed. © 2016 Virginia Department of Transportation.",,"Automation; Crashworthiness; Decision making; Philosophical aspects; Public risks; Risk management; Vehicles; Automated vehicles; Crash risk; Crash situations; Ethical decision making; Ethical problems; Thought experiments; Accidents",2-s2.0-84997706130
"Gradl T., Henrich A.","Extending data models by declaratively specifying contextual knowledge",2016,"DocEng 2016 - Proceedings of the 2016 ACM Symposium on Document Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991237398&doi=10.1145%2f2960811.2967147&partnerID=40&md5=b2da91b0ff874b19ad2a1bad279e41b8","The research data landscape of the arts and humanities is characterized by a high degree of heterogeneity. To improve interoperability, recent initiatives and research infrastructures are encouraging the use of standards and best practices. However, custom data models are often considered necessary to exactly reflect the requirements of a particular collection or research project. To address the needs of scholars in the arts and humanities for a composition of research data irrespective of the degree of structuredness and standardization, we propose a concept on the basis of formal languages, which facilitates declarative data modeling by respective domain experts. By identifying and defining grammatical patterns and deriving transformation functions, the structure of data is generated or extended in accordance with the particular context and needs of the domain.","DARIAH; Descriptive data modeling; Digital humanities; Language applications","Formal languages; Information analysis; Modeling languages; Best practices; Contextual knowledge; DARIAH; Digital humanities; Domain experts; Research data; Research infrastructure; Transformation functions; Metadata",2-s2.0-84991237398
"Doval Y., Gómez-Rodríguez C., Vilares J.","Spanish word segmentation through neural language models [Segmentación de palabras en español mediante modelos del lenguaje basados en redes neuronales]",2016,"Procesamiento de Lenguaje Natural",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986251723&partnerID=40&md5=268f2117671bc87de0ccd0e104ca89db","In social media platforms special tokens abound such as hashtags and mentions in which multiple words are written together without spacing between them; e.g. # leapyear or @ryanreynoldsnet. Due to the way this kind of texts are written, this word assembly phenomenon can appear with its opposite, word segmentation, affecting any token of the text and making it more difficult to perform analysis on them. In this work we show an algorithmic approach based on a language model -in this case a neural model- to solve the problem of the segmentation and assembly of words, in which we try to recover the standard spacing of the words that have suffered one of these transformations by adding or deleting spaces when necessary. The promising results indicate that after some further refinement of the language model it will be possible to surpass the state of the art. © 2016 Sociedad Española para el Procesamiento del Lenguaje Natural.","Neural language models; Spanish; Word assembly; Word segmentation",,2-s2.0-84986251723
"Xiong W., Wu Z., Li B., Gu Q., Yuan L., Hang B.","Inferring service recommendation from natural language API descriptions",2016,"Proceedings - 2016 IEEE International Conference on Web Services, ICWS 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990879559&doi=10.1109%2fICWS.2016.48&partnerID=40&md5=aa234267c3ce53037bba19d6e159b58d","Software reuse through Application Programming Interfaces (APIs) is a common practice in software development. It remains a big challenge to bridge the semantic gap between user expectations and application functionality with the development of Web-based services. This paper proposes a service recommendation approach via extracting semantic relationship from natural language API descriptions and inferring. To validate our approach, large-scale experiments are conducted based on a real-world accessible service repository, ProgrammableWeb. The results show the effectiveness of our proposed approach. © 2016 IEEE.","Natural language processing; Semantic graph; Service recommendation; Web-based service","Application programming interfaces (API); Application programs; Computer software reusability; Natural language processing systems; Semantic Web; Semantics; Software design; Websites; Large scale experiments; NAtural language processing; Natural languages; Semantic graphs; Semantic relationships; Service recommendations; Service repositories; Web-based service; Web services",2-s2.0-84990879559
"Hussein A.S.","Visualizing document similarity using n-grams and latent semantic analysis",2016,"Proceedings of 2016 SAI Computing Conference, SAI 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988869895&doi=10.1109%2fSAI.2016.7555994&partnerID=40&md5=15b7799520d21c894a9f186d0e6121f8","As the number of information resources and document quantity explodes, efficient tools with intuitive visualization capabilities desperately needed to assist users in conducting document similarity analysis and/or plagiarism detection tasks by discovering hidden relations among documents. This paper proposes a content-based method for document similarity analysis and visualization. The proposed method is based on modeling the relationship between documents and their n-gram phrases, which are generated from the normalized text, exploiting morphology analysis and lexical lookup. Resolving possible morphological ambiguities is carried out by tagging the words within the examined documents. Text indexing and stop-words removal are performed, employing a new technique that is efficient in dealing with multiple long documents. The examined documents' TF-IDF model is constructed using heuristic based pair-wise matching algorithm, considering lexical and syntactic changes. Then, the hidden associations between the documents and their unique n-gram phrases are investigated using Latent Semantic Analysis (LSA). Next, the pairwise document subset and similarity measures are derived from the Singular Value Decomposition (SVD) computations. Different visualization techniques are then applied on the SVD results to expose the hidden relations among the documents under consideration. As Arabic is one of the most morphological and complicated languages, this paper emphasizes Arabic documents similarity analysis and visualization. Various experiments were carried out revealing the strong capabilities of the proposed method in analyzing and visualizing literal and some types of intelligent similarities. © 2016 IEEE.","document visualization; Latent Semantic Analysis; natural language processing; plagiarism check; similarity estimation; Singular Value Decomposition; text mining; text-reuse","Computational linguistics; Data mining; Intellectual property; Natural language processing systems; Semantics; Visualization; Document visualization; Latent Semantic Analysis; NAtural language processing; Plagiarism checks; Similarity estimation; Text mining; text-reuse; Singular value decomposition",2-s2.0-84988869895
"Sathiya B., Geetha T.V.","Semantic querying based concept hierarchy construction for ontology learning",2016,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997107219&doi=10.1145%2f2980258.2980307&partnerID=40&md5=5ee3de1ffc7704eb52bc6c3fada5d477","The method of identifying a set of concepts (domain specific) and the relations among these concepts from text is called ontology learning. These relations can be taxonomical (hypernym and hyponym) or non-Taxonomical relations. The important step in ontology learning is to, extract the concepts representing the domain and building the concept hierarchy based on the taxonomical relations prevailing between them. The important resources of the text used for concept hierarchy construction are domain-specific corpus and vast text from web pages. The former resource is static and most likely outdated information, whereas the latter resource is uncertain. Therefore, to tackle these drawbacks, we have proposed a new two-level semantic query formation methodology which is based on lexical-syntactic patterns. It utilizes both the resources of text: corpus and web pages for automatic concept hierarchy construction by discovering the hypernyms and hyponyms. Specifically, it resolves the limited and static content problem of the corpus by utilizing the vast and current knowledge available from the web. Meanwhile, the uncertainty of the knowledge in the web is removed by adding two new contextual information to the semantic queries. From the experimental results, it is evident that the proposed concept hierarchy construction method achieved enhancement of 6.5%, 5.65% and 6.8% for metrics such as Precision, Recall, and FMeasure respectively. © 2016 ACM.","Artificial intelligence; Knowledge engineering; Pattern analysis; Semantic web; World wide web","Artificial intelligence; Information science; Knowledge engineering; Websites; World Wide Web; Concept hierarchies; Construction method; Contextual information; Ontology learning; Outdated information; Pattern analysis; Semantic querying; Syntactic patterns; Semantic Web",2-s2.0-84997107219
"Tseng H.-W., Zhao Q., Zhou Y., Gahagan M., Swanson S.","Morpheus: Creating Application Objects Efficiently for Heterogeneous Computing",2016,"Proceedings - 2016 43rd International Symposium on Computer Architecture, ISCA 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988409868&doi=10.1109%2fISCA.2016.15&partnerID=40&md5=19a20ed98fa61f0c44edcbb3b16f3a1d","In high performance computing systems, object deserialization can become a surprisingly important bottleneck - in our test, a set of general-purpose, highly parallelized applications spends 64% of total execution time deserializing data into objects. This paper presents the Morpheus model, which allows applications to move such computations to a storage device. We use this model to deserialize data into application objects inside storage devices, rather than in the host CPU. Using the Morpheus model for object deserialization avoids unnecessary system overheads, frees up scarce CPU and main memory resources for compute-intensive workloads, saves I/O bandwidth, and reduces power consumption. In heterogeneous, co-processor-equipped systems, Morpheus allows application objects to be sent directly from a storage device to a co-processor (e.g., a GPU) by peer-to-peer transfer, further improving application performance as well as reducing the CPU and main memory utilizations. This paper implements Morpheus-SSD, an SSD supporting the Morpheus model. Morpheus-SSD improves the performance of object deserialization by 1.66, reduces power consumption by 7%, uses 42% less energy, and speeds up the total execution time by 1.32. By using NVMe-P2P that realizes peer-to-peer communication between Morpheus-SSD and a GPU, Morpheus-SSD can speed up the total execution time by 1.39 in a heterogeneous computing platform. © 2016 IEEE.",,"Computer architecture; Digital storage; Distributed computer systems; Electric power utilization; Memory architecture; Virtual storage; Application objects; Application performance; Co-processors; Heterogeneous computing; High performance computing systems; Main memory; Peer-to-peer communications; Peer-to-peer transfer; Peer to peer networks",2-s2.0-84988409868
"Thanhdat N., Claudiu K.V., Zobia R., Lobont L.","Knowledge portal for Six Sigma DMAIC process",2016,"IOP Conference Series: Materials Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991211165&doi=10.1088%2f1757-899X%2f145%2f6%2f062011&partnerID=40&md5=9defd82297f25a947ad17b9e23cac416","Knowledge plays a crucial role in success of DMAIC (Define, Measure, Analysis, Improve, and Control) execution. It is therefore necessary to share and renew the knowledge. Yet, one problem arising is how to create a place where knowledge are collected and shared effectively. We believe that Knowledge Portal (KP) is an important solution for the problem. In this article, the works concerning with requirements and functionalities for KP are first reviewed. Afterwards, a procedure with necessary tools to develop and implement a KP for DMAIC (KPD) is proposed. Particularly, KPD is built on the basis of free and open-source content and learning management systems, and Ontology Engineering. In order to structure and store knowledge, tools such as Protégé, OWL, as well as OWL-RDF Parsers are used. A Knowledge Reasoner module is developed in PHP language, ARC2, MySQL and SPARQL endpoint for the purpose of querying and inferring knowledge available from Ontologies. In order to validate the availability of the procedure, a KPD is built with the proposed functionalities and tools. The authors find that the KPD benefits an organization in constructing Web sites by itself with simple steps of implementation and low initial costs. It creates a space of knowledge exchange and supports effectively collecting DMAIC reports as well as sharing knowledge created. The authors' evaluation result shows that DMAIC knowledge is found exactly with a high success rate and a good level of response time of queries. © Published under licence by IOP Publishing Ltd.","DMAIC; Knowledge Portal; Knowledge Sharing; Ontology; Six Sigma","Birds; Ontology; Open systems; Portals; Process engineering; Process monitoring; Six sigma; Work simplification; DMAIC; Evaluation results; Knowledge exchange; Knowledge portals; Knowledge-sharing; Learning management system; Ontology engineering; Requirements and functionalities; Knowledge management",2-s2.0-84991211165
"Ermakova L., Mothe J.","Document re-ranking based on topic-comment structure",2016,"Proceedings - International Conference on Research Challenges in Information Science",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987668061&doi=10.1109%2fRCIS.2016.7549352&partnerID=40&md5=1f14ec9372b6a23fff8029140ced6d55","This paper introduces a novel approach for document re-ranking in information retrieval based on topic-comment structure of texts. While most information retrieval models make the assumption that relevant documents are about the query and that aboutness can be captured considering bags of words only, we rather consider a more sophisticated analysis of discourse to capture document relevance by distinguishing the topic of a text from what is said about the topic (comment) in the text. The topic-comment structure of texts is extracted automatically from the first retrieved documents which are then re-ranked so that the top documents are the ones that share their topics with the query. The evaluation on TREC collections shows that the method significantly improves the retrieval performance. © 2016 IEEE.","comment; document re-ranking; Information retrieval; information structure; rheme; theme; topic","Data mining; Information science; comment; Information structures; Re-ranking; rheme; theme; topic; Information retrieval",2-s2.0-84987668061
"Sivaraman A., Cheung A., Budiu M., Kim C., Alizadeh M., Balakrishnan H., Varghese G., McKeown N., Licking S.","Packet transactions: High-level programming for line-rate switches",2016,"SIGCOMM 2016 - Proceedings of the 2016 ACM Conference on Special Interest Group on Data Communication",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986587169&doi=10.1145%2f2934872.2934900&partnerID=40&md5=91b149977bf1055e43cf7c3169ca0d29","Many algorithms for congestion control, scheduling, network measurement, active queue management, and traffic engineering require custom processing of packets in the data plane of a network switch. To run at line rate, these dataplane algorithms must be implemented in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built. This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chips. The key challenge is that many data-plane algorithms create and modify algorithmic state. To achieve line-rate programmability for stateful algorithms, we introduce the notion of a packet transaction: a sequential packet-processing code block that is atomic and isolated from other such code blocks. We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated chip-area overhead. © 2016 Copyright held by the owner/author(s).","Programmable switches; Stateful data-plane algorithms","C (programming language); Computer programming languages; Convolutional codes; Hardware; High level languages; Information management; Packet networks; Reconfigurable hardware; Scheduling; Switching circuits; Active Queue Management; Data planes; High-level programming; Imperative languages; Network measurement; Packet processing; Programmable switches; Traffic Engineering; Algorithms",2-s2.0-84986587169
"Tsujiuchi N., Ito A., Masuda A., Seki H., Takahashi H.","Developing evaluation model of tire pattern impact noise",2016,"Proceedings of the INTER-NOISE 2016 - 45th International Congress and Exposition on Noise Control Engineering: Towards a Quieter Future",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994639406&partnerID=40&md5=e65384ac76c4dd0a78ad16e5dad5f01c","Demand for noise-reduced tires has been increasing globally due to the European Union's tire noise regulation ECE R117. Therefore, it is crucial for tire manufacturers to comply and to improve the problem of tire noise. Tire noise consists of several noise sources. This paper focuses on tread impact noise because the noise generating mechanism of each tread pattern when contacting the road is unclear. It is important to clarify the mechanism of tread pattern and road impact noise in order to develop a noise-reduced tire. Thus, this paper focused on impact noise and aimed to construct an evaluation model. We made a tire with only one tread block on its circumference in order to examine the characteristics of impact noise. According to our experiment, there is a proportional relationship between sound pressure and acceleration of the tire tread surface. We also constructed a model from the simplified phenomenon of the impact between the tread block and the road to identify the maximum acceleration of the tire tread surface. Then, we compared the calculated values and the measured values. Our results showed that the calculated values qualitatively coincided with the measured values, therefore confirming the validity of our model. © 2016, German Acoustical Society (DEGA). All rights reserved.","Impact noise; Model analysis; Tire; Tire-road noise; Vehicle noise; Vibration","Acoustic noise measurement; Acoustic variables control; Automobile manufacture; Roads and streets; Transportation; Vibration analysis; Vibrations (mechanical); Impact noise; Model analysis; Tire-road noise; Vehicle noise; Vibration; Tires",2-s2.0-84994639406
"Li J., Li J., Fu X., Masud M.A., Huang J.Z.","Learning distributed word representation with multi-contextual mixed embedding",2016,"Knowledge-Based Systems",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973917891&doi=10.1016%2fj.knosys.2016.05.045&partnerID=40&md5=355d0b4cd73d8d7168d505a8856e7dd4","Learning distributed word representations has been a popular method for various natural language processing applications such as word analogy and similarity, document classification and sentiment analysis. However, most existing word embedding models only exploit a shallow slide window as the context to predict the target word. Because the semantic of each word is also influenced by its global context, as the distributional models usually induced the word representations from the global co-occurrence matrix, the window-based models are insufficient to capture semantic knowledge. In this paper, we propose a novel hybrid model called mixed word embedding (MWE) based on the well-known word2vec toolbox. Specifically, the proposed MWE model combines the two variants of word2vec, i.e.; SKIP-GRAM and CBOW, in a seamless way via sharing a common encoding structure, which is able to capture the syntax information of words more accurately. Furthermore, it incorporates a global text vector into the CBOW variant so as to capture more semantic information. Our MWE preserves the same time complexity as the SKIP-GRAM. To evaluate our MWE model efficiently and adaptively, we study our model on linguistic and application perspectives with both English and Chinese dataset. For linguistics, we conduct empirical studies on word analogies and similarities. The learned latent representations on both document classification and sentiment analysis are considered for application point of view of this work. The experimental results show that our MWE model is very competitive in all tasks as compared with the state-of-the-art word embedding models such as CBOW, SKIP-GRAM, and GloVe. © 2016 Elsevier B.V.","Distributed word representation; Natural language processing; Word embedding; Word2vec","Classification (of information); Computational linguistics; Data mining; Information retrieval systems; Linguistics; Semantics; Distributional models; Document Classification; NAtural language processing; Natural language processing applications; Semantic information; Word embedding; Word representations; Word2vec; Natural language processing systems",2-s2.0-84973917891
"Nguyen H., Patrick J.","Text mining in clinical domain: Dealing with noise",2016,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984945124&doi=10.1145%2f2939672.2939720&partnerID=40&md5=63b04a69142f4ad253cd654cabfbf150","Text mining in clinical domain is usually more difficult than general domains (e.g. newswire reports and scientific literature) because of the high level of noise in both the corpus and training data for machine learning (ML). A large number of unknown word, non-word and poor grammatical sentences made up the noise in the clinical corpus. Unknown words are usually complex medical vocabularies, misspellings, acronyms and abbreviations where unknown non-words are generally the clinical patterns including scores and measures. This noise produces obstacles in the initial lexical processing step as well as subsequent semantic analysis. Furthermore, the labelled data used to build ML models is very costly to obtain because it requires intensive clinical knowledge from the annotators. And even created by experts, the training examples usually contain errors and inconsistencies due to the variations in human annotators' attentiveness. Clinical domain also suffers from the nature of the imbalanced data distribution problem. These kinds of noise are very popular and potentially affect the overall information extraction performance but they were not carefully investigated in most presented health informatics systems. This paper introduces a general clinical data mining architecture which is potential of addressing all of these challenges using: automatic proof-reading process, trainable finite state pattern recogniser, iterative model development and active learning. The reportability classifier based on this architecture achieved 98.25% sensitivity and 96.14% specificity on an Australian cancer registry's held-out test set and up to 92% of training data provided for supervised ML was saved by active learning. © 2016 ACM.","Active learning; Clinical; Named-entity recognition; Natural languages processing; Text classification","Artificial intelligence; Character recognition; Classification (of information); Health care; Learning systems; Natural language processing systems; Semantics; Text processing; Active Learning; Clinical; Named entity recognition; Natural languages; Text classification; Data mining",2-s2.0-84984945124
"Curley S.S., Harang R.E.","Grammatical Inference and Machine Learning Approaches to Post-Hoc LangSec",2016,"Proceedings - 2016 IEEE Symposium on Security and Privacy Workshops, SPW 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008613563&doi=10.1109%2fSPW.2016.26&partnerID=40&md5=9579d3d57be5ec34f1900e0ebe26a590","Formal Language Theory for Security (LangSec) applies the tools of theoretical computer science to the problem of protocol design and analysis. In practice, most results have focused on protocol design, showing that by restricting the complexity of protocols it is possible to design parsers with desirable and formally verifiable properties, such as correctness and equivalence. When we consider existing protocols, however, many of these were not subjected to formal analysis during their design, and many are not implemented in a manner consistent with their formal documentation. Determining a grammar for such protocols is the first step in analyzing them, which places this problem in the domain of grammatical inference, for which a deep theoretical literature exists. In particular, although it has been shown that the higher level categories of the Chomsky hierarchy cannot be generically learned, it is also known that certain subcategories of that hierarchy can be effectively learned. In this paper, we summarize some theoretical results for inferring well-known Chomsky grammars, with special attention to context-free grammars (CFGs) and their generated languages (CFLs). We then demonstrate that, despite negative learnability results in the theoretical regime, we can use long short-term memory (LSTM) networks, a type of recurrent neural network (RNN) architecture, to learn a grammar for URIs that appear in Apache HTTP access logs for a particular server with high accuracy. We discuss these results in the context of grammatical inference, and suggest avenues for further research into learnability of a subgroup of the context-free grammars. © 2016 IEEE.",,"Complex networks; Computational grammars; Context free grammars; Formal languages; Learning systems; Recurrent neural networks; Formal documentation; Formal language theory; Grammatical inferences; Long short term memory; Machine learning approaches; Protocol design and analysis; Recurrent neural network (RNN); Theoretical computer science; Context free languages",2-s2.0-85008613563
"Kim J.-C., Jung H., Chung K.","Mining Based Urban Climate Disaster Index Service According to Potential Risk",2016,"Wireless Personal Communications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959080541&doi=10.1007%2fs11277-016-3212-1&partnerID=40&md5=2adbf4551f406170e908b3711b002373","As weather observation technology develops, natural disasters such as typhoon, earthquake, and heavy snow can be easily monitored as well as basic weather elements such as temperature, precipitation, wind, and air pressure. Advanced IT enables the statistical analysis of weather information and converged weather service. A variety of studies are being performed to analyze and utilize weather, temperature, humidity, etc. by using these IT convergence technology and weather observation technology. Meteorological Administration develops and provides the weather index to help the daily life of people by using weather elements. Influence of weather on life, industry and health is calculated by using indexes to provide weather index service. The weather index services are classified into life weather index, industry weather index and health weather index according to use. Weather indexes are correlated to each other as they are calculated by using common weather elements and advanced weather index service can be provided by analyzing these association patterns. The conventional service shows difference from the actual weather situation around the user as it is calculated by using weather information measured at the observation points. To improve this, personalized service can be provided by using context information-based ontology modeling and reasoning engine. This paper intends to propose a mining-based urban climate disaster index service according to potential risk. The proposed method constructs XML files provided by Meteorological Administration and Open Data Portal in the form of a tree by using the DOM parser and preprocesses it. Emerging risks are selected among socially important issues by using disaster-related keywords and early detected by using the previously developed WebBot. The collected weather indexes are normalized to construct weather index transactions. FP-Tree for mining is used to construct the weather index frequent pattern tree and extract association sets. Natural disaster risk, social disaster risk, and life safety risk are calculated from the extracted association sets. Urban climate disaster index is calculated by considering common elements among potential risks, weather information, disaster information, and emerging risk. Experimental application is tried to develop and verify its logical validity and effectiveness of urban climate disaster index monitoring. Therefore, urban climate disaster index service detects, predicts, and analyzes the trend of various risks such as disasters and safety accidents. It is also used for decision in disaster management in order to determine risks based on natural, man-made, and social disasters and predict future progress and direction of spread. Â© 2016, Springer Science+Business Media New York.","Disaster service; FP-Tree; Mining; Potential risk; Urban climate; Weather index","Atmospheric pressure; Disaster prevention; Information use; Mining; Safety engineering; Trees (mathematics); Convergence technologies; Experimental application; FP tree; Frequent Pattern Tree; Natural disaster risk; Potential risks; Urban climates; Weather index; Disasters",2-s2.0-84959080541
"Gregor D., Toral S., Ariza T., Barrero F., Gregor R., Rodas J., Arzamendia M.","A methodology for structured ontology construction applied to intelligent transportation systems",2016,"Computer Standards and Interfaces",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949497095&doi=10.1016%2fj.csi.2015.10.002&partnerID=40&md5=07f25ccc4f44879e15ddc141dc658e84","The number of computers installed in urban and transport networks has grown tremendously in recent years, also the local processing capabilities and digital networking currently available. However, the heterogeneity of existing equipment in the field of ITS (Intelligent Transportation Systems) and the large volume of information they handle, greatly hinder the interoperability of the equipment and the design of cooperative applications between devices currently installed in urban networks. While the dynamic discovery of information, composition and invocation of services through intelligent agents are a potential solution to these problems, all these technologies require intelligent management of information flows. In particular, it is necessary to wean these information flows of the technologies used, enabling universal interoperability between computers, regardless of the context in which they are located. The main objective of this paper is to propose a systematic methodology to create ontologies, using methods such as a semantic clustering algorithms for retrieval and representation of information. Using the proposed methodology, an ontology will be developed in the ITS domain. This ontology will serve as the basis of semantic information to a SS (Semantic Service) that allows the connection of new equipment to an urban network. The SS uses the CORBA standard as distributed communication architecture. © 2015 Elsevier B.V. All rights reserved.","Collaboration; Distributed systems; Intelligent transportation systems; Ontology; Statistical data analysis","Clustering algorithms; Common object request broker architecture (CORBA); Equipment; Intelligent agents; Intelligent systems; Intelligent vehicle highway systems; Interoperability; Middleware; Ontology; Search engines; Semantics; Transportation; Transportation routes; Collaboration; Cooperative applications; Distributed communication architecture; Distributed systems; Intelligent management; Intelligent transportation systems; Statistical data analysis; Systematic methodology; Urban transportation",2-s2.0-84949497095
"Ezeife C.I., Peravali B.","Comparative mining of B2C web sites by discovering web database schemas",2016,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989231088&doi=10.1145%2f2938503.2938522&partnerID=40&md5=d28eb114f0040a24ea66415727ae1d6c","Discovering potentially useful and previously unknown infor-mation or knowledge from heterogeneous web contents such as ""list all laptop prices from Walmart and Staples between 2013 and 2015 including make, type, screen size, CPU power, year of make"", would require the difficult task of finding the schema of web documents from different web pages, per-forming web content data integration, building their virtual or physical data warehouse integration before web content extraction and mining from the database. Wrappers that extract target information from web pages can be manual, semi-supervised or automatic systems. Automatic systems such as the WebOMiner system, use some data extraction techniques based on parsing the web page html source code into a document object model (DOM) tree, then traverse the DOM for pattern discovery. Some limitations of these exist-ing systems include using complicated matching techniques such as tree matching, Finite state automata, not yielding accurate results for complex queries such as historical and derived. This paper proposes building the WebOMiner S which uses web structure and content mining approaches on the DOM-tree html code to simplify and make more easily ex-tendable, the web data extraction process of theWebOMiner system. TheWebOMiner system is based on non-deterministic finite state automata (NFA) to recognize and extract web different types (e.g., text, image, links, and lists). The pro-posed WebOMiner S replaces the use of NFA of the We-bOMiner with a frequent structure finder algorithm which uses regular expression matching in Java xpath parser and methods (such as compile(),evaluate()) to dynamically dis-cover the most frequent structure (which is the most fre-quently repeated blocks in the html code represented as tags < divclass = "" "" >) in the Dom tree. This approach elim-inates the need for any supervised training or updating the wrapper for each new B2C web page making the approach simpler, more easily extendable and automated. © ACM 2016.","Automatic web data extraction; Data integration; Web content mining; Wrappers","Automata theory; Character recognition; Codes (symbols); Data integration; Data mining; Data warehouses; Extraction; Finite automata; Image matching; Information retrieval systems; Pipeline processing systems; Websites; World Wide Web; XML; Deterministic finite state automata; Document object model; Regular-expression matching; Supervised trainings; Web content extractions; Web content mining; Web data extraction; Wrappers; Trees (mathematics)",2-s2.0-84989231088
"Savenkov D., Agichtein E.","When a knowledge base is not enough: Question answering over knowledge bases with external text data",2016,"SIGIR 2016 - Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980378919&doi=10.1145%2f2911451.2911536&partnerID=40&md5=5eaea56dbf5ed2ebd0d9ef1b60d46f7c","One of the major challenges for automated question answering over Knowledge Bases (KBQA) is translating a natural language question to the Knowledge Base (KB) entities and predicates. Previous systems have used a limited amount of training data to learn a lexicon that is later used for question answering. This approach does not make use of other potentially relevant text data, outside the KB, which could supplement the available information. We introduce a new system, Text2KB, that enriches question answering over a knowledge base by using external text data. Specifically, we revisit different phases in the KBQA process and demonstrate that text resources improve question interpretation, candidate generation and ranking. Building on a state-of-the-art traditional KBQA system, Text2KB utilizes web search results, community question answering and general text document collection data, to detect question topic entities, map question phrases to KB predicates, and to enrich the features of the candidates derived from the KB. Text2KB significantly improves performance over the baseline KBQA method, as measured on a popular WebQuestions dataset. The results and insights developed in this work can guide future efforts on combining textual and structured KB data for question answering. © 2016 ACM.",,"Information retrieval; Knowledge based systems; Natural language processing systems; World Wide Web; Candidate generation; Community question answering; Knowledge base; Knowledge basis; Natural language questions; Question Answering; State of the art; Web search results; Search engines",2-s2.0-84980378919
"Li H., Lu Z.","Deep learning for information retrieval",2016,"SIGIR 2016 - Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980334593&doi=10.1145%2f2911451.2914800&partnerID=40&md5=fc6088927486141b6bd2bbc5d31ae2b4","Recent years have observed a significant progress in information retrieval and natural language processing with deep learning technologies being successfully applied into almost all of their major tasks. The key to the success of deep learning is its capability of accurately learning distributed representations (vector representations or structured arrangement of them) of natural language expressions such as sentences, and effectively utilizing the representations in the tasks. This tutorial aims at summarizing and introducing the results of recent research on deep learning for information retrieval, in order to stimulate and foster more significant research and development work on the topic in the future. The tutorial mainly consists of three parts. In the first part, we introduce the fundamental techniques of deep learning for natural language processing and information retrieval, such as word embedding, recurrent neural networks, and convolutional neural networks. In the second part, we explain how deep learning, particularly representation learning techniques, can be utilized in fundamental NLP and IR problems, including matching, translation, classification, and structured prediction. In the third part, we describe how deep learning can be used in specific application tasks in details. The tasks are search, question answering (from either documents, database, or knowledge base), and image retrieval. © 2016 ACM.","Deep learning; Image retrieval; Information retrieval; Question answering; Search","Computational linguistics; Image retrieval; Information retrieval; Knowledge based systems; Neural networks; Recurrent neural networks; Convolutional neural network; Deep learning; Distributed representation; Natural language expressions; NAtural language processing; Question Answering; Research and development; Search; Natural language processing systems",2-s2.0-84980334593
"Chatterjee K., Dvoák W., Henzinger M., Loitzenbauer V.","Model and Objective Separation with Conditional Lower Bounds: Disjunction is Harder than Conjunction",2016,"Proceedings - Symposium on Logic in Computer Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994627584&doi=10.1145%2f2933575.2935304&partnerID=40&md5=9068d5d96c68914a0880986515bc510d","Given a model of a system and an objective, the model-checking question asks whether the model satisfies the objective. We study polynomial-time problems in two classical models, graphs and Markov Decision Processes (MDPs), with respect to several fundamental -regular objectives, e.g., Rabin and Streett objectives. For many of these problems the best-known upper bounds are quadratic or cubic, yet no super-linear lower bounds are known. In this work our contributions are two-fold: First, we present several improved algorithms, and second, we present the first conditional super-linear lower bounds based on widely believed assumptions about the complexity of CNF-SAT and combinatorial Boolean matrix multiplication. A separation result for two models with respect to an objective means a conditional lower bound for one model that is strictly higher than the existing upper bound for the other model, and similarly for two objectives with respect to a model. Our results establish the following separation results: (1) A separation of models (graphs and MDPs) for disjunctive queries of reachability and Büchi objectives. (2) Two kinds of separations of objectives, both for graphs and MDPs, namely, (2a) the separation of dual objectives such as Streett/Rabin objectives, and (2b) the separation of conjunction and disjunction of multiple objectives of the same type such as safety, Büchi, and coBüchi. In summary, our results establish the first model and objective separation results for graphs and MDPs for various classical -regular objectives. Quite strikingly, we establish conditional lower bounds for the disjunction of objectives that are strictly higher than the existing upper bounds for the conjunction of the same objectives. © 2016 ACM.","Conditional lower bounds; Graph algorithms; Markov Decision processes; Model checking","Computer circuits; Graphic methods; Markov processes; Polynomial approximation; Reconfigurable hardware; Separation; Boolean matrix multiplication; Classical model; Graph algorithms; Lower bounds; Markov Decision Processes; Multiple-objectives; Polynomial-time problems; Reachability; Model checking",2-s2.0-84994627584
"Gil Y., Levy T.","Formal language recognition with the Java type checker",2016,"Leibniz International Proceedings in Informatics, LIPIcs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982830475&doi=10.4230%2fLIPIcs.ECOOP.2016.10&partnerID=40&md5=51064423a5a9f10ba660d6e1193c85b4","This paper is a theoretical study of a practical problem: the automatic generation of Java Fluent APIs from their specification. We explain why the problem's core lies with the expressive power of Java generics. Our main result is that automatic generation is possible whenever the specification is an instance of the set of deterministic context-free languages, a set which contains most ""practical"" languages. Other contributions include a collection of techniques and idioms of the limited meta-programming possible with Java generics, and an empirical measurement demonstrating that the runtime of the ""javac"" compiler of Java may be exponential in the program's length, even for programs composed of a handful of lines and which do not rely on overly complex use of generics. © Yossi Gil and Tomer Levy; licensed under Creative Commons License CC-BY.","Fluent API; Generic programming; Parser generators","Application programming interfaces (API); Computational linguistics; Computer software; Context free languages; Formal languages; Java programming language; Program compilers; Specifications; Automatic Generation; Deterministic-context-free-languages; Empirical measurement; Fluent API; Generic programming; Language recognition; Parser generators; Practical problems; Object oriented programming",2-s2.0-84982830475
"Minor J.M., Rickey L.M., Bergenstal R.M.","Recurrent Endocrine Cycles",2016,"Journal of Diabetes Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009288237&doi=10.1177%2f1932296816637622&partnerID=40&md5=b3455047360a09f10e21e0a2aaa3d213","Background: The chaotic nature of blood glucose creates a formidable clinical challenge for diabetes healthcare. The recent discovery of recurrent endocrine cycles offers the advantage of advanced-prediction (proactive) health care. Methods: Historical studies covering 111 patients and 1 subject collected several months of glucose readings and their daily metrics. Phase portraits and phase analytics can detect recurrent metric cycles and test their ability to anticipate serious glycemic conditions. Results: Recurrent patterns were detected having a rate of ∼7 days per complete cycle. Plots and risk models based on these cycles produced advanced alerts for acute glycemia, capturing greater than 96% of true-positive days with a 5% false-positive rate. Conclusions: This method can be implemented graphically and functionally within a BG monitoring system to warn doctors and patients of impending serious glycemic levels. © 2015 Diabetes Technology Society.","diabetes; dynamics; hyperglycemia; hypoglycemia; prediction; recurrent cycles","hemoglobin A1c; glucose blood level; blood glucose monitoring; clinical article; controlled study; endocrine system; glucose blood level; glycemic index; human; hyperglycemia; hypoglycemia; recurrence risk; Review; analysis; hyperglycemia; statistical model; Blood Glucose; Humans; Hyperglycemia; Logistic Models",2-s2.0-85009288237
"Yang S., Lu W., Zhang Z., Wei B., An W.","Amplifying scientific paper's abstract by leveraging data-weighted reconstruction",2016,"Information Processing and Management",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961327981&doi=10.1016%2fj.ipm.2015.12.014&partnerID=40&md5=76f5b44381c7573b2a95534c637a56a7","In this paper, we focus on the problem of automatically generating amplified scientific paper's abstract which represents the most influential aspects of scientific paper. The influential aspects can be illustrated by the target scientific paper's abstract and citation sentences discussing the target paper, which are provided in papers citing the target paper. In this paper, we extract representative sentences through data-weighted reconstruction approach(DWR) by jointly leveraging target scientific paper's abstract and citation sentences’ content and structure. In our study, we make two-folded contributions. Firstly, sentence's weight was learned by exploiting regularization for ranking on heterogeneous bibliographic network. Specially, Sentences-similar-Sentences relationship was identified by language modeling-based approach and added to the bibliographic network. Secondly, a data-weighted reconstruction objective function is optimized to select the most representative sentences which reconstructs the original sentence set with minimum error. In this process, sentences’ weight plays a critical role. Experimental evaluation over real dataset confirms the effectiveness of our approach. © 2016","Citation analysis; Data-weighted reconstruction; Document summarization; Scientific literature","Bibliographies; Natural language processing systems; Citation analysis; Content and structure; Document summarization; Experimental evaluation; Language model; Objective functions; Scientific literature; Scientific papers; Modeling languages",2-s2.0-84961327981
"Schulz S., De Pauw G., De Clercq O., Desmet B., Hoste V., Daelemans W., Macken L.","Multimodular text normalization of Dutch user-generated content",2016,"ACM Transactions on Intelligent Systems and Technology",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979071622&doi=10.1145%2f2850422&partnerID=40&md5=071d8c0eb28161826d5cb2e7c1395f60","As social media constitutes a valuable source for data analysis for a wide range of applications, the need for handling such data arises. However, the nonstandard language used on social media poses problems for natural language processing (NLP) tools, as these are typically trained on standard language material. We propose a text normalization approach to tackle this problem. More specifically, we investigate the usefulness of a multimodular approach to account for the diversity of normalization issues encountered in user-generated content (UGC). We consider three different types of UGC written in Dutch (SNS, SMS, and tweets) and provide a detailed analysis of the performance of the different modules and the overall system. We also apply an extrinsic evaluation by evaluating the performance of a part-of-speech tagger, lemmatizer, and named-entity recognizer before and after normalization.","Social media; Text normalization; User-generated content","Data handling; Natural language processing systems; Social networking (online); Multi-modular; Named entities; NAtural language processing; Part-of-speech tagger; Social media; Text normalizations; User generated content (UGC); User-generated content; Computational linguistics",2-s2.0-84979071622
"Duan Q., Wei F., Zhang L., Xiao X.","Automatic acquisition and classification system for agricultural network information based on Web data",2016,"Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975129714&doi=10.11975%2fj.issn.1002-6819.2016.12.025&partnerID=40&md5=37107f60763d156828e6e2aab8115a79","The purpose of this study is to obtain agricultural web information efficiently, and to provide users with personalized service through the integration of agricultural resources scattered in different sites and the fusion of heterogeneous environmental data. The research in this paper has improved some key information technologies, which are agricultural web data acquisition and extraction technologies, text classification based on support vector machine (SVM) and heterogeneous data collection based on the Internet of things (IOT). We first add quality target seed site into the system, and get website URL (uniform resource locator) and category information. The web crawler program can save original pages. The de-noised web page can be obtained through HTML parser and regular expressions, which create custom Node Filter objects. Therefore, the system builds a document object model (DOM) tree before digging out data area. According to filtering rules, the target data area can be identified from a plurality of data regions with repeated patterns. Next, the structured data can be extracted after property segmentation. Secondly, we construct linear SVM classification model, and realize agricultural text classification automatically. The procedures of our model include 4 steps. First of all, we use segment tool ICTCLAS to carry out the word segment and part-of-speech (POS) tagging, followed by combining agricultural key dictionary and document frequency adjustment rule to choose feature words, and building a feature vector and calculating inverse document frequency (IDF) weight value for feature words; lastly we design adaptive classifier of SVM algorithm. Finally, the perception data of different format collected by the sensor are transmitted to the designated server as the source data through the wireless sensor network. Relational database in accordance with specified acquisition frequency can be achieved through data conversion and data filtering. The key step of data conversion can be implemented on the basis of mapping rules between source data and target data. The mapping rules include 3 kinds of rules. The first is the source data directly corresponding to the target data; the second is that we create a temporary table, which corresponds to target table if they have same field name; and the third is converting perception data of XML (extensible markup language) type to relational database. Besides, data filtering is required to process abnormal values of the measured value beyond the sensor range. In this paper, unified modeling language (UML) is used to describe the agricultural network information automatic acquisition and classification system. User requirement analysis is described by the system's use case diagram. Web data extraction process is described by the system activity diagram. These help the system's key function implement of automatic information acquisition from Internet. The IOT data sharing module is implemented based on the proposed data conversion and filtering rules. The system can supply the services of on-time agricultural news, agricultural product prices, supply and demand information browsing query, real-time agricultural environment monitoring and personalized information statistics. The preliminary application shows that the agricultural network information automatic acquisition and classification system improves the accuracy of information extraction and text classification. The information acquisition accuracy rate for sample web sets is 98.2%, and the accuracy rate of text classification with rules is 92.5%. Compared with sequential minimal optimization (SMO), Bayesian, C4.5 decision tree and radial basis function (RBF) based SVM algorithm, linear SVM is more suitable for agricultural news classification. The system has high real-time performance and good user participation for IOT applications, which will expect to be applied to agricultural information integration and intelligent processing. © 2016, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Agriculture; Information; Information systems; Text processing; The Internet of things","Agricultural machinery; Agricultural products; Agriculture; Algorithms; Computational linguistics; Computer simulation languages; Data acquisition; Data handling; Data mining; Decision trees; Economics; Hypertext systems; Image retrieval; Information analysis; Information retrieval; Information systems; Internet; Internet of things; Inverse problems; Mapping; Modeling languages; Optimization; Query processing; Radial basis function networks; Search engines; Support vector machines; Telecommunication services; Text processing; Unified Modeling Language; Websites; Wireless sensor networks; XML; Agricultural environment monitoring; Agricultural informations; Information; Inverse Document Frequency; Radial Basis Function(RBF); Sequential minimal optimization; User requirement analysis; XML (extensible markup language); Classification (of information)",2-s2.0-84975129714
"Feki G., Fakhfakh R., Ben Ammar A., Ben Amar C.","Knowledge structures: Which one to use for the query disambiguation?",2016,"International Conference on Intelligent Systems Design and Applications, ISDA",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978374741&doi=10.1109%2fISDA.2015.7489166&partnerID=40&md5=f299bc09bb3b230de5d1e486fb9ee974","In this paper, we present a comparative study between some well-known knowledge structures, applied in the domain of textual query disambiguation, which are Wikipedia Miner, WordNet and BabelNet. We provide an idea about our proposed approach: online Wikipedia-based query disambiguation. Based on different types of the Wikipedia pages, the proposed approach shows promising results when testing a set of thirty ambiguous queries. © 2015 IEEE.","knowledge structures; online approach; query disambiguation; semantic databases; Wikipedia","Intelligent systems; Semantics; Systems analysis; Knowledge structures; online approach; Query disambiguations; Semantic database; Wikipedia; World Wide Web",2-s2.0-84978374741
"Seddiqui M.H., Hoque M.N., Rahman M.H.H.","Semantic annotation of Bangla news stream to record history",2016,"2015 18th International Conference on Computer and Information Technology, ICCIT 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978654130&doi=10.1109%2fICCITechn.2015.7488135&partnerID=40&md5=2c46fa2f04cebf9ee464bfdba595e073","Every day thousands of news articles are published in Bangla from several different sources on the web and this number is even increasing rapidly. On the contrary, the readers are often selective to read their desired news only. In this connection, classical Information Extraction (IE) techniques are used to query with keywords from unstructured or semi-structured news contents to fulfill partial requirements. However, they cannot interpret sequences of events, relation among entities, inference some unveiled facts to facilitate further human analysis. To achieve this goal, semantic technology adds formal structure and semantics to the news stream. In this paper, we propose a system to analyze Bangla news content to annotate especially things, people and places with semantic technology automatically by extracting what happened, when, where and who being involved in the news with the help of classical Natural Language Processing (NLP) techniques. Furthermore, we relate news of today with the previous news to accumulate information over time. We present our proposed system of annotating Bangla news semantically and experiment with SPARQL to inference integrated news from different sources over time and shows its effectiveness in querying specific information. © 2015 IEEE.",,"Information retrieval systems; Information services; Semantic Web; Semantics; Classical information; Formal structures; Human analysis; NAtural language processing; Semantic annotations; Semantic technologies; Semi-structured; Specific information; Natural language processing systems",2-s2.0-84978654130
"Zhang X., Breitinger F., Baggili I.","Rapid Android Parser for Investigating DEX files (RAPID)",2016,"Digital Investigation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961620852&doi=10.1016%2fj.diin.2016.03.002&partnerID=40&md5=f5847a8a241096323c52422431ef7cfb","Android malware is a well-known challenging problem and many researchers/vendors/practitioners have tried to address this issue through application analysis techniques. In order to analyze Android applications, tools decompress APK files and extract relevant data from the Dalvik EXecutable (DEX) files. To acquire the data, investigators either use decompiled intermediate code generated by existing tools, e.g., Baksmali or Dex2jar or write their own parsers/dissemblers. Thus, they either need additional time because of decompiling the application into an intermediate representation and then parsing text files, or they reinvent the wheel by implementing their own parsers. In this article, we present Rapid Android Parser for Investigating DEX files (RAPID) which is an open source and easy-to-use Java library for parsing DEX files. RAPID comes with well-documented APIs which allow users to query data directly from the DEX binary files. Our experiments reveal that RAPID outperforms existing approaches in terms of runtime efficiency, provides better reliability (does not crash) and can support dynamic analysis by finding critical offsets. Notably, the processing time for our sample set of 22.35 GB was only 1.5 h with RAPID while the traditional approaches needed about 23 h (parsing and querying). © 2016 Elsevier Ltd. All rights reserved.","Android malware; Dalvik EXecutable; Decompiler; DEX; Parsing android applications; Smali code/Baksmali","Computer crime; Malware; Open source software; Reliability analysis; Android applications; Android malware; Dalvik EXecutable; Decompiler; Smali code/Baksmali; Android (operating system)",2-s2.0-84961620852
"Jaya K., Gupta D.","Exploration of corpus augmentation approach for english-Hindi bidirectional statistical machine translation system",2016,"International Journal of Electrical and Computer Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979238760&doi=10.11591%2fijece.v6i3.8904&partnerID=40&md5=ccb385690ac770546b4262b2ade5c303","Even though lot of Statistical Machine Translation (SMT) research work is happening for English-Hindi language pair, there is no effort done to standardize the dataset. Each of the research work uses different number of sentences, datasets and parameters during various phases of translation resulting in varied translation output. So comparing these models, understand the result of these models, to get insight into corpus behavior for these models, regenerating the result of these research work becomes tedious. This necessitates the need for standardization of dataset and to identify the common parameter for the development of model. The main contribution of this paper is to introduce an approach to standardize the dataset and to identify the best parameter which in combination gives best performance. It also investigates a novel corpus augmentation approach to improve the translation quality of English-Hindi bidirectional statistical machine translation system. This model works well for the scarce resource without incorporating the external parallel data corpus of the underlying language. This experiment is carried out using Open Source phrase-based toolkit Moses. Indian Languages Corpora Initiative (ILCI) Hindi-English tourism corpus is used. With limited dataset, considerable improvement is achieved using the corpus augmentation approach for the English-Hindi bidirectional SMT system. Copyright © 2016 Institute of Advanced Engineering and Science. All rights reserved.","Corpus augmentation; Indian language; Machine translation; Moses SMT; OOV; Statistical machine translation",,2-s2.0-84979238760
"Bourhis P., Puppis G., Riveros C., Staworko S.","Bounded repairability for regular tree languages",2016,"ACM Transactions on Database Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978069657&doi=10.1145%2f2898995&partnerID=40&md5=f3dc6e909eb3193d701a789aeb4c9359","We study the problem of bounded repairability of a given restriction tree language R into a target tree language T. More precisely, we say that R is bounded repairable with respect to T if there exists a bound on the number of standard tree editing operations necessary to apply to any tree in R to obtain a tree in T. We consider a number of possible specifications for tree languages: bottom-up tree automata (on curry encoding of unranked trees) that capture the class of XML schemas and document type definitions (DTDs). We also consider a special case when the restriction language R is universal (i.e., contains all trees over a given alphabet). We give an effective characterization of bounded repairability between pairs of tree languages represented with automata. This characterization introduces two tools-synopsis trees and a coverage relation between them-allowing one to reason about tree languages that undergo a bounded number of editing operations. We then employ this characterization to provide upper bounds to the complexity of deciding bounded repairability and show that these bounds are tight. In particular, when the input tree languages are specified with arbitrary bottom-up automata, the problem is CONEXP-complete. The problem remains CONEXP-complete even if we use deterministic nonrecursive DTDs to specify the input languages. The complexity of the problem can be reduced if we assume that the alphabet, the set of node labels, is fixed: the problem becomes PSPACE-complete for nonrecursive DTDs and CONP-complete for deterministic nonrecursive DTDs. Finally, when the restriction tree language R is universal, we show that the bounded repairability problem becomes EXP-complete if the target language is specified by an arbitrary bottom-up tree automaton and becomes tractable (P-complete, in fact) when a deterministic bottom-up automaton is used. © 2016 ACM.","DTD; Edit distance; Repair; XML; XML schema","Automata theory; Characterization; Computational linguistics; Repair; XML; Coverage relations; Document type definition; Edit distance; Editing operations; PSPACE-complete; Regular tree languages; Target language; XML schemas; Forestry",2-s2.0-84978069657
"Ling W., Marujo L., Dyer C., Black A.W., Trancoso I.","Mining parallel corpora from Sina Weibo and Twitter",2016,"Computational Linguistics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975062870&doi=10.1162%2fCOLI_a_00249&partnerID=40&md5=f79d13ca475ba67746585775a6094c4e","Microblogs such as Twitter, Facebook, and Sina Weibo (China’s equivalent of Twitter) are a remarkable linguistic resource. In contrast to content from edited genres such as newswire, microblogs contain discussions of virtually every topic by numerous individuals in different languages and dialects and in different styles. In this work, we show that some microblog users post “self-translated” messages targeting audiences who speak different languages, either by writing the same message in multiple languages or by retweeting translations of their original posts in a second language. We introduce a method for finding and extracting this naturally occurring parallel data. Identifying the parallel content requires solving an alignment problem, and we give an optimally efficient dynamic programming algorithm for this. Using our method, we extract nearly 3M Chinese–English parallel segments from Sina Weibo using a targeted crawl of Weibo users who post in multiple languages. Additionally, from a random sample of Twitter, we obtain substantial amounts of parallel data in multiple language pairs. Evaluation is performed by assessing the accuracy of our extraction approach relative to a manual annotation as well as in terms of utility as training data for a Chinese–English machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yield substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content. © 2016 Association for Computational Linguistics.",,,2-s2.0-84975062870
"Solaimani M., Gopalan R., Khan L., Brandt P.T., Thuraisingham B.","Spark-Based Political Event Coding",2016,"Proceedings - 2016 IEEE 2nd International Conference on Big Data Computing Service and Applications, BigDataService 2016",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973641964&doi=10.1109%2fBigDataService.2016.30&partnerID=40&md5=4323b585735a4b20aa54619df976bfad","Political event data have been widely used to study international politics. Previously, natural text processing and event generation required a lot of human efforts. Today we have high computing infrastructure with advance NLP metadata to leverage those tiresome efforts. TABARI-an open source non distributed event-coding software-was an early effort to generate events from a large corpus. It uses a shallow parser to identify the political actors, but ignores semantics and relation among the sentences. PETRARCH, the successor of TABARI, encodes event data into «who-did-what-to-whom» format. It uses Stanford CoreNLP to parse sentences and a static CAMEO dictionary to encode the data. To build dynamic dictionaries, we need to analyze more metadata such as the token, Named Entity Recognition (NER), co-reference, and many more from parsed sentences. Although these tools can code modest amounts of source corpora into event data they are too slow and suffer scalability issues when we try to extract metadata from a single document. The situation gets worse for other languages like Spanish or Arabic. In this paper, we develop a novel distributed framework using Apache Spark, MongoDB, Stanford CoreNLP, and PETRARCH. It shows a distributed workflow by using Stanford CoreNLP to extract all the metadata (parse tree, tokens, lemma, etc.) from the news corpora of the Gigaword dataset and storing it to MongoDB. Then it uses PETRARCH to encode events from the metadata. The framework integrates both tools using distributed commodity hardware and reduces text processing time substantially with respect to a non-distributed architecture. We have chosen Spark over traditional distributed frameworks like MapReduce, Storm, Mahout. Spark has in-memory computation and lower processing time in both batch and stream processing compared to other options. © 2016 IEEE.","Apache Spark; MongoDB; PETRARCH; Political event; Stanford CoreNLP; TABARI","Codes (symbols); Encoding (symbols); Metadata; Open source software; Open systems; Semantics; Social sciences; Syntactics; Text processing; MongoDB; PETRARCH; Political events; Stanford; TABARI; Big data",2-s2.0-84973641964
"Karkar A., Ja'Am J.M.A., Foufou S., Sleptchenko A.","An e-learning mobile system to generate illustrations for Arabic text",2016,"IEEE Global Engineering Education Conference, EDUCON",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994578365&doi=10.1109%2fEDUCON.2016.7474551&partnerID=40&md5=f09a9bb864a297428c0b6eb1bc83fa76","Smart devices applications can assist children in improving their learning capabilities and comprehension skills. However most applications are built without taking into consideration the effective needs and background of Arab children and youth. They are somehow incompatible with their local environment. We propose in this paper an Arabic-based mobile educational system that displays illustrations automatically to characterize Arabic stories' contents. In order to generate these illustrations, different phases are carried out which include processing of Arabic texts, extraction of word-to-word relationships, building and accessing an educational ontology and usage of Internet search engines. The aim of our system is to improve the Arab children educational skills to grasp Arabic vocabulary and grammar using multimedia with a portable smart device which includes observation, comprehension, realization, and deduction. Children will then be able to continue learning outside the limited time of their schools and from any location with enabled Wi-Fi connectivity. Preliminary results show that the system enhances the learners' comprehension, deduction and realization. © 2016 IEEE.","Arabic Text Processing; Education; Educational Ontology; Mobile Learning; Multimedia","E-learning; Engineering education; Search engines; Text processing; Arabic text processing; Educational ontologies; Educational systems; Internet search engine; Learning capabilities; Local environments; Mobile Learning; Multimedia; Education",2-s2.0-84994578365
"Ferreira E., Masson A.R., Jabaian B., Lefèvre F.","Adversarial Bandit for online interactive active learning of zero-shot spoken language understanding",2016,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973364675&doi=10.1109%2fICASSP.2016.7472860&partnerID=40&md5=a98677bbe62abac8db056bc5e263e870","Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. The difficulty remains in the cost of collecting and annotating such data. Another point is the time for updating an existing model to a new domain. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Then, this framework has been extended to exploit user feedbacks to refine the zero-shot semantic parser parameters and increase its performance online. In this paper, we propose to drive this online adaptive process with a policy learnt using the Adversarial Bandit algorithm Exp3. We show, on the second Dialog State Tracking Challenge (DSTC2) datasets, that this proposition can optimally balance the cost of gathering valuable user feedbacks and the overall performance of the spoken language understanding module. © 2016 IEEE.","bandit problem; online adaptation; out-of-domain training data; Spoken language understanding; zero-shot learning",,2-s2.0-84973364675
"Desai A., Gulwani S., Hingorani V., Jain N., Karkare A., Marron M., Sailesh R., Roy S.","Program synthesis using natural language",2016,"Proceedings - International Conference on Software Engineering",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971435101&doi=10.1145%2f2884781.2884786&partnerID=40&md5=bff0d7714841565c0c781f979686c01b","Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks. We present a general framework for constructing program synthesizers that take natural language (NL) inputs and produce expressions in a target DSL. The framework takes as input a DSL definition and training data consisting of NL/DSL pairs. From these it constructs a synthesizer by learning optimal weights and classifiers (using NLP features) that rank the outputs of a keywordprogramming based translation. We applied our framework to three domains: repetitive text editing, an intelligent tutoring system, and flight information queries. On 1200+ English descriptions, the respective synthesizers rank the desired program as the top-1 and top-3 for 80% and 90% descriptions respectively. © 2016 ACM.",,"Computational linguistics; Computer aided instruction; Computer programming languages; Education computing; Natural language processing systems; Problem oriented languages; Domain specific languages; Flight information; Intelligent tutoring system; Natural languages; Optimal weight; Program synthesis; Text editing; Training data; Software engineering",2-s2.0-84971435101
"Ferro N., Silvello G.","Descendants, ancestors, children and parent: A set-based approach to efficiently address XPath primitives",2016,"Information Processing and Management",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960397642&doi=10.1016%2fj.ipm.2015.11.001&partnerID=40&md5=a0f7150587da0a72c39c7f2ff7af1be4","XML is a pervasive technology for representing and accessing semi-structured data. XPath is the standard language for navigational queries on XML documents and there is a growing demand for its efficient processing. In order to increase the efficiency in executing four navigational XML query primitives, namely descendants, ancestors, children and parent, we introduce a new paradigm where traditional approaches based on the efficient traversing of nodes and edges to reconstruct the requested subtrees are replaced by a brand new one based on basic set operations which allow us to directly return the desired subtree, avoiding to create it passing through nodes and edges. Our solution stems from the NEsted SeTs for Object hieRarchies (NEASTOR) formal model, which makes use of set-inclusion relations for representing and providing access to hierarchical data. We define in-memory efficient data structures to implement NESTOR, we develop algorithms to perform the descendants, ancestors, children and parent query primitives and we study their computational complexity. We conduct an extensive experimental evaluation by using several datasets: Digital archives (EAD collections), INEX 2009 Wikipedia collection, and two widely-used synthetic datasets (XMark and XGen). We show that NESTOR-based data structures and query primitives consistently outperform state-of-the-art solutions for XPath processing at execution time and they are competitive in terms of both memory occupation and pre-processing time. © 2015 Elsevier Ltd. All rights reserved.","Data structures; In-memory XPath processing; NESTOR; Set-based data models","Data structures; XML; Experimental evaluation; Inclusion relation; Navigational queries; NESTOR; Pervasive technologies; Semi structured data; Traditional approaches; XPath processing; Computational efficiency",2-s2.0-84960397642
"Desarkar M.S., Sarkar S., Mitra P.","Preference relations based unsupervised rank aggregation for metasearch",2016,"Expert Systems with Applications",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953431845&doi=10.1016%2fj.eswa.2015.12.005&partnerID=40&md5=36f737a1cd9623f197e6351514579ac9","Rank aggregation mechanisms have been used in solving problems from various domains such as bioinformatics, natural language processing, information retrieval, etc. Metasearch is one such application where a user gives a query to the metasearch engine, and the metasearch engine forwards the query to multiple individual search engines. Results or rankings returned by these individual search engines are combined using rank aggregation algorithms to produce the final result to be displayed to the user. We identify few aspects that should be kept in mind for designing any rank aggregation algorithm for metasearch. For example, generally equal importance is given to the input rankings while performing the aggregation. However, depending on the indexed set of web pages, features considered for ranking, ranking functions used etc. by the individual search engines, the individual rankings may be of different qualities. So, the aggregation algorithm should give more weight to the better rankings while giving less weight to others. Also, since the aggregation is performed when the user is waiting for response, the operations performed in the algorithm need to be light weight. Moreover, getting supervised data for rank aggregation problem is often difficult. In this paper, we present an unsupervised rank aggregation algorithm that is suitable for metasearch and addresses the aspects mentioned above. We also perform detailed experimental evaluation of the proposed algorithm on four different benchmark datasets having ground truth information. Apart from the unsupervised Kendall-Tau distance measure, several supervised evaluation measures are used for performance comparison. Experimental results demonstrate the efficacy of the proposed algorithm over baseline methods in terms of supervised evaluation metrics. Through these experiments we also show that Kendall-Tau distance metric may not be suitable for evaluating rank aggregation algorithms for metasearch. © 2015 Elsevier Ltd. All rights reserved.","Information retrieval; Metasearch; Rank aggregation","Algorithms; Bioinformatics; Information retrieval; Natural language processing systems; Websites; Aggregation algorithms; Experimental evaluation; Meta search engines; Metasearch; NAtural language processing; Performance comparison; Preference relation; Rank aggregation; Search engines",2-s2.0-84953431845
"Shih M.-H., Hsieh S.-K.","Word dependency sketch for Chinese language learning",2016,"Concentric: Studies in Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026472958&doi=10.6241%2fconcentric.ling.42.1.03&partnerID=40&md5=b4f69be32205a097a11ddef5e094f7fd","This article describes an approach to constructing a language resource through automatically sketching grammatical relations of words in an untagged corpus based on dependency parses. Compared to the handcrafted, rule-based Word Sketch Engine (Kilgarriff et al. 2004), this approach provides more details about the different syntagmatic usages of each word such as various types of modification a given word can undergo and other grammatical functions it can fulfill. As a way to properly evaluate the approach, we attempt to evaluate the auto-generated result in terms of the distributional thesaurus function, and compare this with items in an existing thesaurus. Our results have been tailored for the purpose of Chinese learning and, to the best of our knowledge, the resulting resource is the first of its kind in Chinese. We believe it will have a great impact on both Chinese corpus linguistics and Teaching Chinese as a Second Language (TCSL). © 2016, National Taiwan Normal University. All rights reserved.","Computational lexicography; Corpus linguistics; Dependency relation; Thesaurus",,2-s2.0-85026472958
"Son H.S., Kim R.Y.C.","A Method of Handling Metamodel Based on XML Database for SW Visulization",2016,"2016 International Conference on Platform Technology and Service, PlatCon 2016 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968538228&doi=10.1109%2fPlatCon.2016.7456776&partnerID=40&md5=648060e669a3000149cd930449efcd13","Software visualization shows to extract the architecture from source code. With software visualization, we can improve the software quality due to software modularization and also reconfiguration. To do this, we compose of parser, data analyzer, database management system, and visualizer like a tool-chain for realizing the software visualization. In this tool-chain, we develop the parser named xCodeParser that generate the OMG's Abstract Syntax Tree Metamodel (ASTM) for various language such as C, C++ or Java. In this paper, we propose the metamodel based XML databases to save the ASTM files into the repository. The proposed database is able to handle the data which organized metamodel based on XML database. Improving quality through realizing this visualization helps easily to develop high quality software, to increase reusability, and to reduce maintenance cost. © 2016 IEEE.","metamodel; Structured Query Language (SQL); SW quality; SW Visualization; XML databases","Chains; Computational linguistics; Computer software reusability; Computer software selection and evaluation; Data visualization; Information management; Java programming language; Modular construction; Query languages; Query processing; Reusability; Syntactics; Trees (mathematics); Visualization; XML; Abstract Syntax Trees; High-quality software; Meta model; Software modularization; Software Quality; Software visualization; Structured query languages; XML database; C++ (programming language)",2-s2.0-84968538228
"Reed C., Evans M.J., Di Carlo P., Lee J.D., Carpenter L.J.","Interferences in photolytic NO2 measurements: Explanation for an apparent missing oxidant?",2016,"Atmospheric Chemistry and Physics",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966349366&doi=10.5194%2facp-16-4707-2016&partnerID=40&md5=337fa65f2eb8f18984f056d62b8943c3","Measurement of NO2 at low concentrations (tens of ppts) is non-trivial. A variety of techniques exist, with the conversion of NO2 into NO followed by chemiluminescent detection of NO being prevalent. Historically this conversion has used a catalytic approach (molybdenum); however, this has been plagued with interferences. More recently, photolytic conversion based on UV-LED irradiation of a reaction cell has been used. Although this appears to be robust there have been a range of observations in low-NOx environments which have measured higher NO2 concentrations than might be expected from steady-state analysis of simultaneously measured NO, O3, jNO2, etc. A range of explanations exist in the literature, most of which focus on an unknown and unmeasured ""compound X"" that is able to convert NO to NO2 selectively. Here we explore in the laboratory the interference on the photolytic NO2 measurements from the thermal decomposition of peroxyacetyl nitrate (PAN) within the photolysis cell. We find that approximately 5% of the PAN decomposes within the instrument, providing a potentially significant interference. We parameterize the decomposition in terms of the temperature of the light source, the ambient temperature, and a mixing timescale (∼ 0.4 s for our instrument) and expand the parametric analysis to other atmospheric compounds that decompose readily to NO2 (HO2NO2, N2O5, CH3O2NO2, IONO2, BrONO2, higher PANs). We apply these parameters to the output of a global atmospheric model (GEOS-Chem) to investigate the global impact of this interference on (1) the NO2 measurements and (2) the NO2 : NO ratio, i.e. the Leighton relationship. We find that there are significant interferences in cold regions with low NOx concentrations such as the Antarctic, the remote Southern Hemisphere, and the upper troposphere. Although this interference is likely instrument-specific, the thermal decomposition to NO2 within the instrument's photolysis cell could give an at least partial explanation for the anomalously high NO2 that has been reported in remote regions. The interference can be minimized by better instrument characterization, coupled to instrumental designs which reduce the heating within the cell, thus simplifying interpretation of data from remote locations. © Author(s) 2016.",,"atmospheric chemistry; atmospheric modeling; concentration (composition); decomposition analysis; nitrogen oxides; oxidant; Southern Hemisphere; steady-state equilibrium",2-s2.0-84966349366
"Yerimbetova A.S., Murzin F.A., Batura T.V., Sagnayeva S.K., Semich D.F., Bakiyeva A.M.","Estimation of the degree of similarity of sentences in a natural language based on using the link grammar parser program system",2016,"Journal of Theoretical and Applied Information Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963575603&partnerID=40&md5=8120282e659c851443ebb6c73e3e5649","Our main goal is to construct the algorithms that can estimate the document relevance on the basis of the text structure analysis. It is important that this estimate should be based on the context of the search query and not limited only by keywords, their similarity or frequency. The semantic-syntactical relations between words built by the program system Link Grammar Parser (developed in Carnegie Mellon University, USA) can be used to solve these problems. They allow us to develop the methods of comparison of the sentences in a natural language and introduce certain measures of the closeness (similarity) between the sentences. These measures take into account both lexical and syntactic relations between words. Experiments with different types of sentences and links took almost one year. It was observed that there is no need to use too many links. First, the use of some links leads us to the analysis of diagrams which correspond badly to intuition and principles of classical linguistics, and it is not clear what we can do with them further. Second, there is also a complexity aspect. If there are fewer links, the algorithm works faster. Therefore, a compromise is necessary. The minimum variant giving good results is when only eight connectors (links) are used. One of the problems we solve in the current time is the development of a parser like Link Grammar Parser for Turkic languages most frequent in the Internet, such as Kazakh, Uzbek (Cyrillic and Roman alphabets), and Turkish. The results of our research are planned to be used in different information retrieval systems. © 2005 - 2016 JATIT & LLS. All rights reserved.","Link grammar parser; Natural language processing; Relevance; Syntactic analysis; Turkic languages",,2-s2.0-84963575603
"Raedt L.D., Kersting K., Natarajan S., Poole D.","Statistical Relational Artificial Intelligence: Logic, Probability, and Computation",2016,"Synthesis Lectures on Artificial Intelligence and Machine Learning",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962394044&doi=10.2200%2fS00692ED1V01Y201601AIM032&partnerID=40&md5=d216edc34d5f7294b9a3a5a1d66bb18c","An intelligent agent interacting with the real world will encounter individual people, courses, test results, drugs prescriptions, chairs, boxes, etc., and needs to reason about properties of these individuals and relations among them as well as cope with uncertainty. Uncertainty has been studied in probability theory and graphical models, and relations have been studied in logic, in particular in the predicate calculus and its extensions. @is book examines the foundations of combining logic and probability into what are called relational probabilistic models. It introduces representations, inference, and learning techniques for probability, logic, and their combinations. @e book focuses on two representations in detail: Markov logic networks, a relational extension of undirected graphical models and weighted first-order predicate calculus formula, and Problog, a probabilistic extension of logic programs that can also be viewed as a Turing-complete relational extension of Bayesian networks. © 2016 by Morgan and Claypool.","inductive logic programming; lifted inference; logic programming; machine learning; Markov logic networks; Probabilistic logic models; probabilistic programming; Problog; Prolog; relational probabilistic models; statistical relational learning","Artificial intelligence; Bayesian networks; Calculations; Computation theory; Computer circuits; Graphic methods; Learning systems; Logic programming; Markov processes; Probabilistic logics; Probability; PROLOG (programming language); Reconfigurable hardware; lifted inference; Markov logic networks; Probabilistic models; Probabilistic programming; Problog; Prolog; Statistical relational learning; Inductive logic programming (ILP)",2-s2.0-84962394044
"Balakrishna M., Werner S., Tatu M., Erekhinskaya T., Moldovan D.","K-Extractor: Automatic Knowledge Extraction for Hybrid Question Answering",2016,"Proceedings - 2016 IEEE 10th International Conference on Semantic Computing, ICSC 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968865958&doi=10.1109%2fICSC.2016.30&partnerID=40&md5=0e9e0c709a980137231d81ae22bafa4b","This paper describes an approach to integrate unstructured and structured data and provide a natural language query interface to the consolidated knowledge base. The approach is based on deep semantic representation expressed in RDF triples. Natural language questions are automatically converted into SPARQL queries executed against the RDF store. The approach is implemented in K-Extractor platform. Two domain collections are shown: antibiotic resistant bacteria and illicit drugs. © 2016 IEEE.","Natural Language Query; Question Answering; Resource Description Framework (RDF); Semantic Relations; SPARQL","Computational linguistics; Knowledge based systems; Semantic Web; Semantics; World Wide Web; Natural language queries; Question Answering; Resource description framework; Semantic relations; SPARQL; Natural language processing systems",2-s2.0-84968865958
"Tatu M., Balakrishna M., Werner S., Erekhinskaya T., Moldovan D.","Automatic Extraction of Actionable Knowledge",2016,"Proceedings - 2016 IEEE 10th International Conference on Semantic Computing, ICSC 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968813306&doi=10.1109%2fICSC.2016.29&partnerID=40&md5=d78eb456bc532f7fecd109901fb08848","In this paper, we describe K-Extractor, a powerful NLP framework that provides integrated and seamless access to structured and unstructured information with minimal effort. The K-Extractor converts natural language documents into a rich set of semantic triples that, not only, can be stored within an RDF semantic index, but also, can be queried using natural language questions, thus eliminating the need to manually formulate SPARQL queries. The K-Extractor greatly outperforms a free text search index-based question answering system. © 2016 IEEE.","Automatic Knowledge Extraction; Natural Language Processing (NLP); Resource Description Framework (RDF); SPARQL Interface","Artificial intelligence; Computational linguistics; Extraction; Semantic Web; Semantics; World Wide Web; Automatic extraction; Free-text search; Knowledge extraction; NAtural language processing; Natural language questions; Natural languages; Question answering systems; Resource description framework; Natural language processing systems",2-s2.0-84968813306
"Kuntarto G.P., Moechtar F.L., Santoso B.I., Gunawan I.P.","Comparative study between Part-of-Speech and statistical methods of text extraction in the tourism domain",2016,"2015 International Conference on Information Technology Systems and Innovation, ICITSI 2015 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966728751&doi=10.1109%2fICITSI.2015.7437675&partnerID=40&md5=6df362bc72139e0d9e9a12d516898690","In this paper, a comparison between two different text extraction methods is given, namely the linguistic (Part-of-Speech / POS) and statistical methods (Term Frequency Inverse Document Frequency / TF-IDF). Text extractions were performed as part of ontology population in the Indonesian tourism domain. This paper also contributes in creating a multimedia corpus from three different resources or websites of Balinese tourism domain. Performance of each method is evaluated by means of several relevance measures. It was found that the statistical method used gives higher relevance than the linguistic methods. We have analysed that this is due to the limitation of the reference terms used in the initial ontology from our previous research. © 2015 IEEE.","Bali tourism; linguistic method; ontology population; part of speech; statistical method; TF-IDF","Computational linguistics; Extraction; Inverse problems; Linguistics; Multimedia systems; Population statistics; Text processing; Bali tourism; Linguistic methods; Ontology Population; Part Of Speech; TF-IDF; Statistical methods",2-s2.0-84966728751
"Pouramini A., Naseri A.","The XMLization of a dependency treebank in CoNLL format for evaluating linguistic queries using XQuery",2016,"Conference Proceedings of 2015 2nd International Conference on Knowledge-Based Engineering and Innovation, KBEI 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971411341&doi=10.1109%2fKBEI.2015.7436191&partnerID=40&md5=d46b7b4f0aff4133af55f2a14704a39d","Treebanks are essential resources for both data-driven approaches to natural language processing (NLP) and empirical linguistic researches. Developing these resources is time- and cost-consuming and requires specialized expertise. Therefore, they should be designed to be reused for different purposes. Currently, there are several dependency treebanks for some languages which are annotated in CoNLL format. For some languages, such as Persian, they are the few available linguistic resources. These treebanks are more suitable for the input of data-driven parsers, and querying linguistic data in them is not easy. In recent years, XML has been widely used for formatting treebanks, and there are various tools available for querying and annotating a linguistic croups in this format. In this paper, we present a tool for converting a dependency treebank in CoNLL format to an appropriate XML format. We designed the XML scheme to be particularly suitable for writing linguistic queries in XQuery syntax. © 2015 IEEE.","depenency structure; Natural language processing; treebanks","Engineering research; Forestry; Knowledge based systems; Linguistics; Natural language processing systems; Syntactics; XML; Data driven; Data-driven approach; Linguistic data; Linguistic resources; NAtural language processing; Treebanks; XML format; XML schemes; Computational linguistics",2-s2.0-84971411341
"Xiao X., Yan R., Ye R., Li Q., Peng S., Jiang Y.","Detection and Prevention of Code Injection Attacks on HTML5-Based Apps",2016,"Proceedings - 2015 3rd International Conference on Advanced Cloud and Big Data, CBD 2015",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966671700&doi=10.1109%2fCBD.2015.48&partnerID=40&md5=f4e172814fa69fedc4827fe813778961","Security on mobile devices is becoming increasingly important. HTML5 are widely used to develop mobile applications due to its portability on multi platforms. However it is allowed to mix data and code together in web technology. HTML5-based applications are prone to suffer from code injection attacks that are similar to XSS. In this paper, at first, we introduce a more hidden type of code injection attacks, coding-based attacks. In the new type of code injection attacks, JavaScript code is encoded in a human-unreadable form. Then we use classification algorithms of machine learning to determine whether an app suffers from the code injection attack or not. The experimental result shows that the Precision of our detection method reaches 95.3%. Compare with the other method, our approach improves a lot in detection speed with the precision nearly unchanged. Furthermore, an improved access control model is proposed to mitigate the attack damage. In addition, filters are adopted to remove JavaScript code from data to prevent the attacks. The effectiveness and rationality have been validated through extensive simulations. © 2015 IEEE.","access control model; classification algorithm; code injection; Filter; machine learning","Access control; Artificial intelligence; Big data; High level languages; HTML; Learning algorithms; Learning systems; Mobile devices; Network security; Access control models; Classification algorithm; Code injection; Code injection attacks; Detection methods; Extensive simulations; Filter; Mobile applications; Codes (symbols)",2-s2.0-84966671700
"Shrestha N., Kukkonen-Harjula K.T., Verbeek J.H., Ijaz S., Hermans V., Bhaumik S.","Workplace interventions for reducing sitting at work",2016,"Cochrane Database of Systematic Reviews",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961926404&doi=10.1002%2f14651858.CD010912.pub3&partnerID=40&md5=ce52b4ed38baadf06b14c0f84b6d2197","Background: Office work has changed considerably over the previous couple of decades and has become sedentary in nature. Physical inactivity at workplaces and particularly increased sitting has been linked to increase in cardiovascular disease, obesity and overall mortality. Objectives: To evaluate the effects of workplace interventions to reduce sitting at work compared to no intervention or alternative interventions. Search methods: We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, EMBASE, CINAHL, OSH UPDATE, PsycINFO, Clinical trials.gov and the World Health Organization (WHO) International Clinical Trials Registry Platform (ICTRP) search portal up to 2 June, 2015. We also screened reference lists of articles and contacted authors to find more studies to include. Selection criteria: We included randomised controlled trials (RCTs), cluster-randomised controlled trials (cRCTs), and quasi-randomised controlled trials of interventions to reduce sitting at work. For changes of workplace arrangements, we also included controlled before-and-after studies (CBAs) with a concurrent control group. The primary outcome was time spent sitting at work per day, either self-reported or objectively measured by means of an accelerometer-inclinometer. We considered energy expenditure, duration and number of sitting episodes lasting 30 minutes or more, work productivity and adverse events as secondary outcomes. Data collection and analysis: Two review authors independently screened titles, abstracts and full-text articles for study eligibility. Two review authors independently extracted data and assessed risk of bias. We contacted authors for additional data where required. Main results: We included 20 studies, two cross-over RCTs, 11 RCTs, three cRCTs and four CBAs, with a total of 2180 participants from high income nations. The studies evaluated physical workplace changes (nine studies), policy changes (two studies), information and counselling (seven studies) and interventions from multiple categories (two studies). One study had both physical workplace changes and information and counselling components. We did not find any studies that had investigated the effect of periodic breaks or standing or walking meetings. Physical workplace changes A sit-stand desk alone compared to no intervention reduced sitting time at work per workday with between thirty minutes to two hours at short term (up to three months) follow-up (six studies, 218 participants, very low quality evidence). In two studies, sit-stand desks with additional counselling reduced sitting time at work in the same range at short-term follow-up (61 participants, very low quality evidence). One study found a reduction at six months' follow-up of -56 minutes (95% CI -101 to -12, very low quality evidence) compared to no intervention. Also total sitting time at work and outside work decreased with sit-stand desks compared to no intervention (MD -78 minutes, 95% CI -125 to -31, one study) as did the duration of sitting episodes lasting 30 minutes or more (MD -52 minutes, 95% CI -79 to -26, two studies). This is considerably less than the two to four hours recommended by experts. Sit-stand desks did not have a considerable effect on work performance, musculoskeletal symptoms or sick leave. It remains unclear if standing can repair the harms of sitting because there is hardly any extra energy expenditure. The effects of active workstations were inconsistent. Treadmill desks combined with counselling reduced sitting time at work (MD -29 minutes, 95% CI -55 to -2, one study) compared to no intervention at 12 weeks' follow-up. Pedalling workstations combined with information did not reduce inactive sitting at work considerably (MD -12 minutes, 95% CI -24 to 1, one study) compared to information alone at 16 weeks' follow-up. The quality of evidence was low for active workstations. Policy changes Two studies with 443 participants provided low quality evidence that walking strategies did not have a considerable effect on workplace sitting time at 10 weeks' (MD -16 minutes, 95% CI -54 to 23) or 21 weeks' (MD -17 minutes, 95% CI -58 to 25) follow-up respectively. Information and counselling Counselling reduced sitting time at work (MD -28 minutes, 95% CI -52 to -5, two studies, low quality evidence) at medium term (three months to 12 months) follow-up. Mindfulness training did not considerably reduce workplace sitting time (MD -2 minutes, 95% CI -22 to 18) at six months' follow-up and at 12 months' follow-up (MD -16 minutes, 95% CI -45 to 12, one study, low quality evidence). There was no considerable increase in work engagement with counselling. There was an inconsistent effect of computer prompting on sitting time at work. One study found no considerable effect on sitting at work (MD -17 minutes, 95% CI -48 to 14, low quality evidence) at 10 days' follow-up, while another study reported a significant reduction in sitting at work (MD -55 minutes, 95% CI -96 to -14, low quality evidence) at 13 weeks' follow-up. Computer prompts to stand reduced sitting at work by 14 minutes more (95% CI 10 to 19, one study) compared to computer prompts to step at six days' follow-up. Computer prompts did not change the number of sitting episodes that last 30 minutes or longer. Interventions from multiple categories Interventions combining multiple categories had an inconsistent effect on sitting time at work, with a reduction in sitting time at 12 weeks' (25 participants, very low quality evidence) and six months' (294 participants, low quality evidence) follow-up in two studies but no considerable effect at 12 months' follow-up in one study (MD -47.98, 95% CI -103 to 7, 294 participants, low quality evidence). Authors' conclusions: At present there is very low to low quality evidence that sit-stand desks may decrease workplace sitting between thirty minutes to two hours per day without having adverse effects at the short or medium term. There is no evidence on the effects in the long term. There were no considerable or inconsistent effects of other interventions such as changing work organisation or information and counselling. There is a need for cluster-randomised trials with a sufficient sample size and long term follow-up to determine the effectiveness of different types of interventions to reduce objectively measured sitting time at work. © 2016 The Cochrane Collaboration.",,"cardiovascular disease; cardiovascular mortality; Cinahl; Cochrane Library; Embase; energy expenditure; follow up; human; job performance; Medline; musculoskeletal disease; obesity; patient counseling; physical inactivity; PsycINFO; randomized controlled trial (topic); Review; self report; sitting; systematic review; workplace; world health organization; accelerometry; bioengineering; body posture; energy metabolism; epidemiology; statistics and numerical data; time factor; workplace; Accelerometry; Controlled Before-After Studies; Energy Metabolism; Human Engineering; Humans; Posture; Randomized Controlled Trials as Topic; Time Factors; Workplace",2-s2.0-84961926404
"Shin H., Ryu B.-G., Ryu W.-J., Lee G., Lee S.","Bringing bag-of-phrases to ODP-based text classification",2016,"2016 International Conference on Big Data and Smart Computing, BigComp 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964680330&doi=10.1109%2fBIGCOMP.2016.7425975&partnerID=40&md5=86c7eaa88208120ce1debb3a7b7de1ab","The Open Directory Project (ODP) is a large scale, high quality and publicly available web directory. Many studies and real-world applications build on an ODP-based classifier. However, existing approaches use traditional bag-of-words representation of text to develop an ODP-based classifier and words alone do not always provide atomic units of semantic meaning. In this paper, we propose a novel framework to better understand the semantic meaning of text by bringing bag-of-phrases to ODP-based text classification. The proposed method employs a syntactic tree to extract phrases from ODP and applies a phrase selection method to alleviate the high dimensionality problem of bag-of-phrases. The conducted evaluation results demonstrate that our approach outperforms the state-of-the-art methods in classification performance. © 2016 IEEE.","open directory project; syntactic structure; text classification; text mining","Big data; Classification (of information); Data mining; Semantics; Syntactics; Classification performance; Evaluation results; High dimensionality; Open directory projects; State-of-the-art methods; Syntactic structure; Text classification; Text mining; Text processing",2-s2.0-84964680330
"Lim C.-G.","A survey of temporal information extraction and language independent features",2016,"2016 International Conference on Big Data and Smart Computing, BigComp 2016",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964664574&doi=10.1109%2fBIGCOMP.2016.7425967&partnerID=40&md5=d78968d863ccbafeb96b1b5b0ad23317","Since there has been an explosive growth of online documents, the knowledge extraction from natural language texts becomes more important to complement limited human ability. In this paper, we introduce existing methods for temporal information extraction from input texts in two viewpoints. One is the researches about language-independent feature generation. Even though linguistic features have been generally utilized to extract time expressions, we need different features which are independent from language characteristics in order to overcome the boundary of particular language. Another is the researches about temporal information extraction based on the common patterns or knowledge bases. By summarizing and discussing existing researches, we can see the current research direction on this field to help better understanding of the temporal information extraction methods. © 2016 IEEE.",,"Big data; Computational linguistics; Information analysis; Information retrieval; Linguistics; Explosive growth; Feature generation; Knowledge extraction; Language independents; Linguistic features; Natural language text; On-line documents; Temporal information extraction; Data mining",2-s2.0-84964664574
"Chakraborty S., Raghuraman S., Rangan C.P.","A pairing-free, one round identity based authenticated key exchange protocol secure against memory-scrapers",2016,"Journal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962385436&partnerID=40&md5=ad0f0d1ef2e34968f773a6577aa3f3bb","Security of a key exchange protocol is formally established through an abstract game between a challenger and an adversary. In this game the adversary can get various information which are modeled by giving the adversary access to appropriate oracle queries. Empowered with all these information, the adversary will try to break the protocol. This is modeled by a test query which asks the adversary to distinguish between a session key of a fresh session from a random session key; properly guessing which correctly leads the adversary to win the game. In this traditional model of security the adversary sees nothing apart from the input/ output relationship of the algorithms. However, in recent past an adversary could obtain several additional information beyond what he gets to learn in these black box models of computation, thanks to the availability of powerful malwares. This data exfiltration due to the attacks of Memory Scraper/Ram-Scraper-type malwares is an emerging threat. In order to realistically capture these advanced classes of threats posed by such malwares we propose a new security model for identity-based authenticated key exchange (ID-AKE) which we call the Identity based Strong Extended Canetti Krawzyck (ID-seCK) model. Our security model captures leakages of intermediate values by appropriate oracle queries given to the adversary. Following this, we propose a round optimal (i.e., single round) ID-AKE protocol for two-party settings. Our design assumes a hybrid system equipped with a bare minimal Trusted Platform Module (TPM) that can only perform group exponentiations. One of the major advantages of our construction is that it does not involve any pairing operations, works in prime order group and have a tight security reduction to the Gap Diffie Hellman (GDH) problem under our new ID-seCK model. Our scheme also has the capability to handle active adversaries while most of the previous ID-AKE protocols are secure only against passive adversaries. The security of our protocol is proved in the Random Oracle (RO) model. © 2016, Innovative Information Science and Technology Research Group. All rights reserved.","Authenticated key exchange; ID-seCK model; Identity-based authenticated key exchange (ID-AKE); Intermediate values; Ram Scraper",,2-s2.0-84962385436
"Wu Q., Ma S., Liu Y.","Sub-event discovery and retrieval during natural hazards on social media data",2016,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958110819&doi=10.1007%2fs11280-015-0359-8&partnerID=40&md5=8c7d8c8c71da936281d8dd9f68c67589","Social media sites contain a considerable amount of data for natural calamities events, such as earthquakes, snowstorms, mud-rock flows. With the increasing amount of social media data, an important task is to discover and retrieve sub-events over time. Especially in emergency situations, rescue and relief activities can be enhanced by identifying and retrieving sub-events of a natural hazard event. However, the existing event detection techniques in news-related reports cannot effectively work for social media data due to the unstructured of social network data. In this paper, we propose a new natural hazard sub-events discovery model SED (Sub-Events Discovery), which adopts multifarious features to detect sub-events. Moreover, in order to retrieve the sub-events over a specific event, we introduce a novel SER (Sub-Event Retrieval) algorithm from time-stamped social media data. Our novel approach SER makes use of automatically obtained messages from external search engines in the entire process. For purpose of determining the periodical convergence time for natural hazard event, our method provides online sub-events retrieval and sub-events discovery to meet the further needs. Next the improved estimation standards with timestamp are utilized in our experiments to verify the effectiveness and efficiency of SED model and SER algorithm. © 2015, Springer Science+Business Media New York.","Natural hazard event; Social media data timeliness; Sub-event discovery; Sub-event retrieval","Hazards; Search engines; Convergence time; Discovery model; Effectiveness and efficiencies; Emergency situation; Natural hazard; Relief activities; Social media datum; Sub-events; Social networking (online)",2-s2.0-84958110819
"Hussain I., Csallner C., Grechanik M., Xie Q., Park S., Taneja K., Mainul Hossain B.M.","RUGRAT: Evaluating program analysis and testing tools and compilers with large generated random benchmark applications",2016,"Software - Practice and Experience",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957843707&doi=10.1002%2fspe.2290&partnerID=40&md5=bf795ec6423340a18846bcb87f646f84","Benchmarks are heavily used in different areas of computer science to evaluate algorithms and tools. In program analysis and testing, open-source and commercial programs are routinely used as benchmarks to evaluate different aspects of algorithms and tools. Unfortunately, many of these programs are written by programmers who introduce different biases, not to mention that it is very difficult to find programs that can serve as benchmarks with high reproducibility of results. We propose a novel approach for generating random benchmarks for evaluating program analysis and testing tools and compilers. Our approach uses stochastic parse trees, where language grammar production rules are assigned probabilities that specify the frequencies with which instantiations of these rules will appear in the generated programs. We implemented our tool for Java and applied it to generate a set of large benchmark programs of up to 5M lines of code each with which we evaluated different program analysis and testing tools and compilers. The generated benchmarks let us independently rediscover several issues in the evaluated tools. Copyright © 2014 John Wiley & Sons, Ltd.","benchmark application generator; benchmark applications; stochastic parse tree","Algorithms; Application programs; Benchmarking; Computer programming; Forestry; Formal languages; Java programming language; Open source software; Software testing; Stochastic systems; Assigned probability; Benchmark applications; Benchmark programs; Commercial projects; High reproducibility; Language grammar; Parse trees; Production rules; Program compilers",2-s2.0-84957843707
"Mezulis S., Sternberg M.J.E., Kelley L.A.","PhyreStorm: A Web Server for Fast Structural Searches Against the PDB",2016,"Journal of Molecular Biology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959553393&doi=10.1016%2fj.jmb.2015.10.017&partnerID=40&md5=443fc5713d3c38fbdf3632e5ba6796c5","The identification of structurally similar proteins can provide a range of biological insights, and accordingly, the alignment of a query protein to a database of experimentally determined protein structures is a technique commonly used in the fields of structural and evolutionary biology. The PhyreStorm Web server has been designed to provide comprehensive, up-to-date and rapid structural comparisons against the Protein Data Bank (PDB) combined with a rich and intuitive user interface. It is intended that this facility will enable biologists inexpert in bioinformatics access to a powerful tool for exploring protein structure relationships beyond what can be achieved by sequence analysis alone. By partitioning the PDB into similar structures, PhyreStorm is able to quickly discard the majority of structures that cannot possibly align well to a query protein, reducing the number of alignments required by an order of magnitude. PhyreStorm is capable of finding 93 ± 2% of all highly similar (TM-score > 0.7) structures in the PDB for each query structure, usually in less than 60 s. PhyreStorm is available at http://www.sbg.bio.ic.ac.uk/phyrestorm/. © 2015 The Authors.","one-vs-many; protein; structural alignment; structural search; TM-align","protein; protein; algorithm; amino acid sequence; Article; bioinformatics; computer interface; priority journal; Protein Data Bank; protein database; protein structure; sequence analysis; web browser; biology; chemistry; Internet; procedures; protein conformation; protein database; Computational Biology; Databases, Protein; Internet; Protein Conformation; Proteins",2-s2.0-84959553393
"Sharma S., Srinivas P.Y.K.L., Balabantaray R.C.","Sentiment analysis of code - Mix script",2016,"2015 International Conference on Computing and Network Communications, CoCoNet 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964889510&doi=10.1109%2fCoCoNet.2015.7411238&partnerID=40&md5=6ae2c54832c3aabcb1bff8d819fbac89","Due to the advent of social media and networking sites, people now have the opportunity to communicate with each other more easily and frequently than ever before. The analysis of the content of these communications can lead to numerous benefits to the governments and corporations across various industries. These will allow them in gauging the public sentiment on a multitude of items and issues, on which they can take necessary actions. However, to do so, one needs to decipher the content which is usually in the form of a complicated mix of multiple languages. In Indian social media, users often combine Romanized English with their mother tongue language for communications. In this paper, we have presented a number of techniques to identify the sentiment of text after normalizing the influence of multiple languages. © 2015 IEEE.","code mixed; devanagari; polarity; Romanized script; sentiment","Social networking (online); code mixed; devanagari; polarity; Romanized script; sentiment; Computational linguistics",2-s2.0-84964889510
"Diaz J.G.R., García N.J., Fuentes M.E.R., Leyva J.C.S., Martínez J.L.S., Medrano L.A.C.","Mobile agent for the building of automatic textual reports from web queries",2016,"2015 IEEE International Autumn Meeting on Power, Electronics and Computing, ROPEC 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964355131&doi=10.1109%2fROPEC.2015.7395119&partnerID=40&md5=e347b2f0ff157d8a4ab601c854dcb7a6","Traditionally, web searching in order to discover meaningful information is an action that obeys a typical way. A user launches a web query by means of a set of keywords directed toward a web searcher. Thus, it returns a list of web links (URLs) which must be read and then, analyzed by the user for finding out the required information. © 2015 IEEE.","agent; information retrieval; mobile app; report; web searching","Agents; Information retrieval; Web services; World Wide Web; Mobile app; report; Web links; Web searcher; Web-searching; Mobile agents",2-s2.0-84964355131
"Ta C.D.C., Thi T.P.","Identifying the queries' topics based-on computing domain ontology",2016,"2015 International Conference on Computing, Management and Telecommunications, ComManTel 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964786219&doi=10.1109%2fComManTel.2015.7394255&partnerID=40&md5=a15e3723196d23230902383f13eb9003","The World Wide Web (WWW) is a massive data repository that continues to grow daily. Most people in the world are familiar with the WWW and use it to find information on any domain. However, most of the search engines usually are based on the vocabulary of queries to find information, some of the results coming back from WWW are good, and the others are not good. To resolve this problem, a large sum of past search engines have relied on the methods related to lexical-syntactic analysis and the handcrafted knowledge sources; conversely, this paper proposes a method used to explore latent semantic features in order to identify topics of queries automatically based on the domain specific ontology. Besides, we select for our experiments with two real data sets. The first is the ACM Digital Library and the second is the queries, which are input made directly by users. The experimental results are evaluated by Precision, Recall measures. We provide details of the technical implementation and extensive experimental evaluation. © 2015 IEEE.","Domain Ontology; Information Extraction; TopicIdentification","Digital libraries; Information retrieval; Search engines; Semantics; Syntactics; World Wide Web; Domain ontologies; Domain-specific ontologies; Experimental evaluation; Knowledge sources; Syntactic analysis; Technical implementation; TopicIdentification; World wide webs (WWW); Data mining",2-s2.0-84964786219
[No author name available],"PEPM 2016 - Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, co-located with POPL 2016",2016,"PEPM 2016 - Proceedings of the 2016 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, co-located with POPL 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966549167&partnerID=40&md5=fc9f54cb8c9e8a7fea99a31f3efc6dd3","The proceedings contain 10 papers. The topics discussed include: practical, general parser combinators; operator precedence for data-dependent grammars; everything old is new again: quoted domain-specific languages; finally, safely-extensible and efficient language-integrated query; a constraint language for static semantic analysis based on scope graphs; BiGUL: a formally verified core language for putback-based bidirectional programming; removing runtime overhead for optimized object queries; staging generic programming; toward introducing binding-time analysis to MetaOCaml; and staging beyond terms: prospects and challenges.",,,2-s2.0-84966549167
"Thompson P., Batista-Navarro R.T., Kontonatsios G., Carter J., Toon E., McNaught J., Timmermann C., Worboys M., Ananiadou S.","Text mining the history of medicine",2016,"PLoS ONE",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954055651&doi=10.1371%2fjournal.pone.0144717&partnerID=40&md5=f0ba064705d0776b39870612a5f9839e","Historical text archives constitute a rich and diverse source of information, which is becoming increasingly readily accessible, due to large-scale digitisation efforts. However, it can be difficult for researchers to explore and search such large volumes of data in an efficient manner. Text mining (TM) methods can help, through their ability to recognise various types of semantic information automatically, e.g., instances of concepts (places, medical conditions, drugs, etc.), synonyms/variant forms of concepts, and relationships holding between concepts (which drugs are used to treat which medical conditions, etc.). TM analysis allows search systems to incorporate functionality such as automatic suggestions of synonyms of user-entered query terms, exploration of different concepts mentioned within search results or isolation of documents in which concepts are related in specific ways. However, applying TM methods to historical text can be challenging, according to differences and evolutions in vocabulary, terminology, language structure and style, compared to more modern text. In this article, we present our efforts to overcome the various challenges faced in the semantic analysis of published historical medical text dating back to the mid 19th century. Firstly, we used evidence from diverse historical medical documents from different periods to develop new resources that provide accounts of the multiple, evolving ways in which concepts, their variants and relationships amongst them may be expressed. These resources were employed to support the development of a modular processing pipeline of TM tools for the robust detection of semantic information in historical medical documents with varying characteristics. We applied the pipeline to two large-scale medical document archives covering wide temporal ranges as the basis for the development of a publicly accessible semanticallyoriented search system. The novel resources are available for research purposes, while the processing pipeline and its modules may be used and configured within the Argo TM platform. © 2016 Thompson et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Article; data mining; history of medicine; information processing; medical documentation; medical history; medical informatics; medical information system; medical record; medical terminology; text mining; history; semantics; Data Mining; History of Medicine; History, 19th Century; Semantics",2-s2.0-84954055651
"McLeod A., Steedman M.","HMM-Based Voice Separation of MIDI Performance",2016,"Journal of New Music Research",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959219715&doi=10.1080%2f09298215.2015.1136650&partnerID=40&md5=542facbac7ab2e189256aa7f7447bcf5","Voice separation is an important component of Music Information Retrieval (MIR). In this paper, we present an HMM which can be used to separate music performance data in the form of MIDI into monophonic voices. It works on two basic principles: that consecutive notes within a single voice will tend to occur on similar pitches, and that there are short (if any) temporal gaps between them. We also present an incremental algorithm which can perform inference on the model efficiently. We show that our approach achieves a significant improvement over existing approaches, when run on a corpus of 78 compositions by J.S. Bach, each of which has been separated into the gold standard voices suggested by the original score. We also show that it can be used to perform voice separation on live MIDI data without an appreciable loss in accuracy. The code for the model described here is available at https://github.com/apmcleod/voice-splitting. © 2016 Taylor & Francis.","music analysis; perception; software; voice",,2-s2.0-84959219715
"Wang I., Kahane S., Tellier I.","From built examples to attested examples: A syntax-based query system for non-specialists",2016,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation, PACLIC 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015978788&partnerID=40&md5=b4ce88b6d2830c81b0592f17a8e96bf3","Using queries to explore corpora is today routine practice not only among researchers in various fields with an empirical approach to discourse, but also among nonspecialists who use search engines or concordancers for language learning purposes. While keyword-based queries are quite common, non-specialists are less likely to explore syntactic constructions. Syntax-based queries usually require the use of regular expressions with grammatical words combined with morphosyntactic tags, meaning that users need to master both the query language of the tool and the tagset of the annotated corpus. However, non-specialists such as language learners may prefer to focus on the output rather than spend time and efforts mastering a query language. To address this shortcoming, we propose a methodology including a syntactic parser and using common similarity measures to compare sequences of automatically produced morphosyntactic tags.",,"Computer programming languages; Query languages; Query processing; Search engines; Empirical approach; Keyword-based; Language learning; Morpho-syntactic tags; Query systems; Regular expressions; Similarity measure; Syntactic parsers; Syntactics",2-s2.0-85015978788
"Batura T.V., Murzin F.A., Semich D.F., Sagnayeva S.K., Tazhibayeva S.Z., Bakiyev M.N., Yerimbetova A.S., Bakiyeva A.M.","Using the link grammar parser in the study of Turkic languages",2016,"Eurasian Journal of Mathematical and Computer Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018344008&partnerID=40&md5=f8a268faa8b82d98dc3c19fe47a6c609","Growing amount of information on the Internet and rapid development of social networks make the task of text processing increasingly actual. In this paper we propose an algorithm for the comparison of sentences and introduce certain measures of the closeness (similarity) between the sentences. The estimation of the relevance of documents should be based on the context of a search query and should not be limited only by keywords, their similarity or frequency. So proposed measures take into account lexical, syntactic and semantic relations between words. One of the problems we solve in the current time is the development of a parser like Link Grammar Parser for Turkic languages most frequent in the Internet, such as Kazakh, Uzbek (Cyrillic and Roman alphabets), and Turkish. The results of our research are planned to be used in different information retrieval systems.","Link grammar parser; Natural language processing; Relevance; Syntactic analysis; Turkic languages",,2-s2.0-85018344008
"Haas C., Riezler S.","A corpus and semantic parser for multilingual natural language querying of OpenStreetMap",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994086227&partnerID=40&md5=bb15ee1d43feee83e9900aed0321bc12","We present a corpus of 2,380 natural language queries paired with machine readable formulae that can be executed against world wide geographic data of the OpenStreetMap (OSM) database. We use the corpus to learn an accurate semantic parser that builds the basis of a natural language interface to OSM. Furthermore, we use response-based learning on parser feedback to adapt a statistical machine translation system for multilingual database access to OSM. Our framework allows to map fuzzy natural language expressions such as ""nearby"", ""north of"", or ""in walking distance"" to spatial polygons on an interactive map. Furthermore, it combines syntactic complexity and compositionality with a reasonable lexical variability of queries, making it an interesting new publicly available dataset for research on semantic parsing. ©2016 Association for Computational Linguistics.",,"Computer aided language translation; Computer software maintenance; Linguistics; Natural language processing systems; Query languages; Query processing; Semantics; Syntactics; Multilingual database; Natural language expressions; Natural language interfaces; Natural language queries; Natural languages; Semantic parsing; Statistical machine translation system; Syntactic complexity; Computational linguistics",2-s2.0-84994086227
"Szárnyas G.","Scalable graph query evaluation and benchmarking with realistic models",2016,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013487558&partnerID=40&md5=ebaca65740a3270d301aed8eadc9f093","Model queries are widely used in model-driven engineering toolchains: models are checked for errors with validation queries, model simulations and transformations require complex pattern matching, while injective mappings for views are defined with model queries. Efficient and scalable evaluation of complex queries on large models is a challenging task. To achieve scalable graph query evaluation, I identified key challenges such as the lack of credible benchmarks and difficulties of obtaining real models for performance testing. To address these challenges, my contributions target (1) distributed incremental graph queries, (2) a cross-technology benchmark for model validation, (3) characterization of realistic models, and (4) realistic models generation. © 2016, CEUR-WS. All rights reserved.","Benchmarking; Distributed queries; Model generation; Model validation","Pattern matching; Complex queries; Distributed query; Injective mapping; Model generation; Model simulation; Model validation; Model-driven Engineering; Performance testing; Benchmarking",2-s2.0-85013487558
"Yin P., Lu Z., Li H., Kao B.","Eural enquirer: Learning to query tables in natural language",2016,"IJCAI International Joint Conference on Artificial Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006128021&partnerID=40&md5=a59706d3c78080d75ab58289764cc115","We propose NEURAL ENQUIRER - a neural network architecture for answering natural language (NL) questions based on a knowledge base (KB) table. Unlike existing work on end-to-end training of semantic parsers [Pasupat and Liang, 2015; Neelakantan et al., 2015], NEURAL ENQUIRER is fully ""neuralized"": it finds distributed representations of queries and KB tables, and executes queries through a series of neural network components called ""executors"". Executors model query operations and compute intermediate execution results in the form of table annotations at different levels. NEURAL ENQUIRER can be trained with gradient descent, with which the representations of queries and the KB table are jointly optimized with the query execution logic. The training can be done in an end-to-end fashion, and it can also be carried out with stronger guidance, e.g., step-by-step supervision for complex queries. NEURAL ENQUIRER is one step towards building neural network systems that can understand natural language in real-world tasks. As a proof-of-concept, we conduct experiments on a synthetic QA task, and demonstrate that the model can learn to execute reasonably complex NL queries on small-scale KB tables.",,"Artificial intelligence; Knowledge based systems; Network architecture; Semantics; Complex queries; Distributed representation; Gradient descent; Natural languages; Neural network systems; Proof of concept; Query operations; Real-world task; Complex networks",2-s2.0-85006128021
"Krishnamurthy J.","Probabilistic models for learning a semantic parser lexicon",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994108622&partnerID=40&md5=8f6ff74959f406138d3d9d0392893baf","We introduce several probabilistic models for learning the lexicon of a semantic parser. Lexicon learning is the first step of training a semantic parser for a new application domain and the quality of the learned lexicon significantly affects both the accuracy and efficiency of the final semantic parser. Existing work on lexicon learning has focused on heuristic methods that lack convergence guarantees and require significant human input in the form of lexicon templates or annotated logical forms. In contrast, our probabilistic models are trained directly from question/answer pairs using EM and our simplest model has a concave objective that guarantees convergence to a global optimum. An experimental evaluation on a set of 4th grade science questions demonstrates that our models improve semantic parser accuracy (35-70% error reduction) and efficiency (4-25x more sentences per second) relative to prior work despite using less human input. Our models also obtain competitive results on GEO880 without any dataset-specific engineering. ©2016 Association for Computational Linguistics.",,"Efficiency; Heuristic methods; Linguistics; Semantics; Syntactics; Error reduction; Experimental evaluation; Global optimum; Logical forms; New applications; Probabilistic models; Computational linguistics",2-s2.0-84994108622
"Verbitskaia E., Grigorev S., Avdyukhin D.","Relaxed parsing of regular approximations of string-embedded languages",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978955012&doi=10.1007%2f978-3-319-41579-6_22&partnerID=40&md5=567d5a8bad3095c2cc6206c6a753cc81","We present a technique for syntax analysis of a regular set of input strings. This problem is relevant for the analysis of string-embedded languages when a host program generates clauses of embedded language at run time. Our technique is based on a generalization of RNGLR algorithm, which, inherently, allows us to construct a finite representation of parse forest for regularly approximated set of input strings. This representation can be further utilized for semantic analysis and transformations in the context of reengineering, code maintenance, program understanding etc. The approach in question implements relaxed parsing: non-recognized strings in approximation set are ignored with no error detection. © Springer International Publishing Switzerland 2016.","Parser generator; Parsing; RNGLR; String analysis; String-embedded languages","Computational linguistics; Information science; Semantics; Embedded Languages; Parser generators; Parsing; RNGLR; String analysis; Syntactics",2-s2.0-84978955012
"Beltagy I., Quirk C.","Improved semantic parsers for if-then statements",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011845716&partnerID=40&md5=4b3363625ebf95701ec23947ea22b170","Digital personal assistants are becoming both more common and more useful. The major NLP challenge for personal assistants is machine understanding: translating natural language user commands into an executable representation. This paper focuses on understanding rules written as If-Then statements, though the techniques should be portable to other semantic parsing tasks. We view understanding as structure prediction and show improved models using both conventional techniques and neural network models. We also discuss various ways to improve generalization and reduce overfitting: synthetic training data from paraphrase, grammar combinations, feature selection and ensembles of multiple systems. An ensemble of these techniques achieves a new state of the art result with 8% accuracy improvement. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Semantics; Accuracy Improvement; Conventional techniques; Machine understanding; Natural languages; Neural network model; Personal assistants; Structure prediction; Synthetic training data; Personal digital assistants",2-s2.0-85011845716
"Lin H., Wang Y., Zhang P., Wang W., Yue Y., Lin Z.","A rule based open information extraction method using cascaded finite-state transducer",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988023759&doi=10.1007%2f978-3-319-31750-2_26&partnerID=40&md5=2d52b4ad0c62f0d17e88fffce8de5e8c","In this paper, we present R-OpenIE, a rule based open information extraction method using cascaded finite-state transducer. R-OpenIE defines contextual constraint declarative rules to generate relation extraction templates, which frees from the influence of syntactic parser errors, and it uses cascaded finite-state transducer model to match the satisfied relational tuples. It is noted that R-OpenIE creates inverted index for each matched state during the matching process of cascaded finite-state transducer, which improves the efficiency of pattern matching. The experimental results have shown that our R-OpenIE can achieve good adaptability and efficiency for open information extraction. © Springer International Publishing Switzerland 2016.","Contextual constraint rule; Declarative definition; Knowledge base population; Relation extraction","Efficiency; Information analysis; Information retrieval; Information use; Knowledge based systems; Pattern matching; Syntactics; Transducers; Contextual constraints; Declarative definition; Finite state transducers; Information extraction methods; Inverted indices; Knowledge base; Relation extraction; Syntactic parsers; Data mining",2-s2.0-84988023759
"Kodabagi M.M., Angadi S.A.","A methodology for machine translation of simple sentences from Kannada to English language",2016,"Proceedings of the 2016 2nd International Conference on Contemporary Computing and Informatics, IC3I 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020053467&doi=10.1109%2fIC3I.2016.7917967&partnerID=40&md5=297102378e8217dd3f6e9c5ba39f52f1","In this paper, a new approach for machine translation of simple sentences from Kannada to English language is presented. Initially, the method performs lexical analysis to process input transliterated Kannada sentence to recognize and classify tokens using newly constructed source language lexicon KannadaWordNet. Then, newly designed recursive descent parser for the domain validates the transliterated Kannada sentence with the syntactic constructs/grammar rules. Further, the phrase mapper and output generation mechanisms are employed to obtain translated English sentence. The proposed methodology is evaluated on a data set containing transliterated Kannada sentences of text written in low resolution images of display boards of Karnataka government offices in India. The method achieves an average translation accuracy of 97.56%. The proposed method is robust and efficient in dealing with various issues such as construction of source lexicon, syntactic/semantic representation of language constructs, design of efficient parser, resolving differences between syntactic/semantic constructs of source and target language, construction of bilingual dictionary. The method also resolves word sense ambiguity, translation ambiguity, and provides accurate translation without errors. © 2016 IEEE.","Lexical Analysis; Lexicon; Machine Translation; Phrase Mapper; Recursive Descent Parser","Computational linguistics; Computer aided language translation; Natural language processing systems; Translation (languages); Lexical analysis; Lexicon; Machine translations; Phrase Mapper; Recursive Descent Parser; Syntactics",2-s2.0-85020053467
"Virk Z.S., Dua M.","An advanced web-based hindi language interface to database using machine learning approach",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978817926&doi=10.1007%2f978-3-319-40349-6_36&partnerID=40&md5=f8ff0089dc5ae0125f5db3569f74d10d","NLIDB (Natural Language Interface to Database) system is a key step to many areas like Database Mining, Search Engines, Medical Science, Artificial Intelligence etc. Existing literature reveals that Natural Language Interfaces were extensively studied from the 1970‘s to 1990‘s. The paper proposes a NLIDB system namely Advanced WB-HLIDB (Web based Hindi Language Interface to Database using Machine Learning Approach) that uses the natural language as Hindi language and is based on the Machine Learning technique. The main features that have been introduced are Web Based Graphical User Interface to the system along with the use of Clustering and Similarity functions. This paper discusses the use of Similarity approach instead of the ‘Like’ approach which had been in practice till recently. Various other components such as multiword selection, recognizing misspelled words, storing of successful queries, auto complete function, built in keywords etc. have also been implemented. © Springer International Publishing Switzerland 2016.","Data mining; Hindi shallow parser; HLIDB; KNN algorithm; Machine learning; NLIDB; Similarity functions; WB-HLIDB","Artificial intelligence; Computational linguistics; Data mining; Database systems; Graphical user interfaces; Human computer interaction; Natural language processing systems; Search engines; User interfaces; Websites; Hindi shallow parser; HLIDB; k-NN algorithm; NLIDB; Similarity functions; WB-HLIDB; Learning systems",2-s2.0-84978817926
"Zhang Y., He S., Liu K., Zhao J.","A joint model for question answering over multiple knowledge bases",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007158143&partnerID=40&md5=7c2e2750a327af7bb39e64e606b5110a","As the amount of knowledge bases (KBs) grows rapidly, the problem of question answering (QA) over multiple KBs has drawn more attention. The most significant distinction between multiple KB-QA and single KB-QA is that the former must consider the alignments between KBs. The pipeline strategy first constructs the alignments independently, and then uses the obtained alignments to construct queries. However, alignment construction is not a trivial task, and the introduced noises would be passed on to query construction. By contrast, we notice that alignment construction and query construction are interactive steps, and jointly considering them would be beneficial. To this end, we present a novel joint model based on integer linear programming (ILP), uniting these two procedures into a uniform framework. The experimental results demonstrate that the proposed approach outperforms state-of-the-art systems, and is able to improve the performance of both alignment construction and query construction. © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Alignment; Artificial intelligence; Integer programming; Construct queries; Integer Linear Programming; Knowledge basis; Knowledge basis (KBs); Query construction; Question Answering; State-of-the-art system; Uniform framework; Query processing",2-s2.0-85007158143
"Pinter Y., Reichart R., Szpektor I.","Syntactic parsing of web queries with question intent",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994184183&partnerID=40&md5=146e3e5744f53fa39340b109f9f24e04","Accurate automatic processing of Web queries is important for high-quality information retrieval from the Web. While the syntactic structure of a large portion of these queries is trivial, the structure of queries with question intent is much richer. In this paper we therefore address the task of statistical syntactic parsing of such queries. We first show that the standard dependency grammar does not account for the full range of syntactic structures manifested by queries with question intent. To alleviate this issue we extend the dependency grammar to account for segments - independent syntactic units within a potentially larger syntactic structure. We then propose two distant supervision approaches for the task. Both algorithms do not require manually parsed queries for training. Instead, they are trained on millions of (query, page title) pairs from the Community Question Answering (CQA) domain, where the CQA page was clicked by the user who initiated the query in a search engine. Experiments on a new treebank1 consisting of 5,000 Web queries from the CQA domain, manually parsed using the proposed grammar, show that our algorithms outperform alternative approaches trained on various sources: tens of thousands of manually parsed OntoNotes sentences, millions of unlabeled CQA queries and thousands of manually segmented CQA queries. ©2016 Association for Computational Linguistics.",,"Computational linguistics; Formal languages; Linguistics; Search engines; Automatic processing; Community question answering; Dependency grammar; High quality information; Syntactic parsing; Syntactic structure; Treebanks; Syntactics",2-s2.0-84994184183
"Van Woensel W., Casteleyn S.","A mobile query service for integrated access to large numbers of online semantic web data sources",2016,"Journal of Web Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956759136&doi=10.1016%2fj.websem.2015.10.002&partnerID=40&md5=499d141ef91310e63bc8988370a80b2d","From the Semantic Web's inception, a number of concurrent initiatives have given rise to multiple segments: large semantic datasets, exposed by query endpoints; online Semantic Web documents, in the form of RDF files; and semantically annotated web content (e.g., using RDFa), semantic sources in their own right. In various mobile application scenarios, online semantic data has proven to be useful. While query endpoints are most commonly exploited, they are mainly useful to expose large semantic datasets. Alternatively, mobile RDF stores are utilized to query local semantic data, but this requires the design-time identification and replication of relevant data. Instead, we present a mobile query service that supports on-the-fly and integrated querying of semantic data, originating from a largely unused portion of the Semantic Web, comprising online RDF files and semantics embedded in annotated webpages. To that end, our solution performs dynamic identification, retrieval and caching of query-relevant semantic data. We explore several data identification and caching alternatives, and investigate the utility of source metadata in optimizing these tasks. Further, we introduce a novel cache replacement strategy, fine-tuned to the described query dataset, and include explicit support for the Open World Assumption. An extensive experimental validation evaluates the query service and its alternative components. © 2015 Elsevier B.V. All rights reserved.","Cache replacement; Data caching; Data indexing; Data integration; Mobile computing; Open world assumption","Cache memory; Data integration; Mobile computing; World Wide Web; Cache replacement; Data caching; Dynamic identification; Experimental validations; Integrated querying; Mobile applications; Open world assumption; Semantic web documents; Semantic Web",2-s2.0-84956759136
"Seganti A., Kapłański P., Campo J.D.N., Cieśliński K., Koziołkiewicz J., Zarzycki P.","Asking data in a controlled way with ask data anything NQL",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978873011&doi=10.1007%2f978-3-319-41498-0_6&partnerID=40&md5=d08d264f748bb23e17163cdaa0d0ad0c","While to collect data, it is necessary to store it, to understand its structure it is necessary to do data-mining. Business Intelligence (BI) enables us to make intelligent, data-driven decisions by the mean of a set of tools that allows the creation of a potentially unlimited number of machine-generated, data-driven reports, which are calculated by a machine as a response to queries specified by humans. Natural Query Languages (NQLs) allow one to dig into data with an intuitive humanmachine dialogue. The current NQL-based systems main problems are the required prior learning phase for writing correct queries, understanding the linguistic coverage of the NQL and asking precise questions. Results: We have developed an NQL as well as an entire Natural Language Interface Database (NLIDB) that supports the user with BI queries with minimized disadvantages, namely Ask Data Anything. The core part - NQL parser - is a hybrid of CNL and the pattern matching approach with a prior error repair phase. Equipped with reasoning capabilities due to the intensive use of semantic technologies, our hybrid approach allows one to use very simple, keyword-based (even erroneous) queries as well as complex CNL ones with the support of a predictive editor. © Springer International Publishing Switzerland 2016.",,"Computational linguistics; Data mining; Natural language processing systems; Pattern matching; Query processing; Semantics; Data driven decision; Human-machine dialogue; Hybrid approach; Keyword-based; Learning phase; Natural language interfaces; Reasoning capabilities; Semantic technologies; Query languages",2-s2.0-84978873011
"de la Higuera C.","Learning grammars and automata with queries",2016,"Topics in Grammatical Inference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988640237&doi=10.1007%2f978-3-662-48395-4_3&partnerID=40&md5=ce8af52a46fb720966ec6da5e23902a2","When learning languages or grammars, an attractive alternative to using a large corpus is to learn by interacting with the environment. This can allow us to deal with situations where data is scarce or expensive, but testing or experimenting is possible. The situation, which arises in a number of fields, is formalised in a setting called active learning or query learning. By controlling better the information to which one has access, this setting provides us with a better understanding of the hardness of learning tasks. But the setting also allows us to solve practical learning situations, for which new algorithms are needed. © Springer-Verlag Berlin Heidelberg 2016.",,,2-s2.0-84988640237
"Uwagbole S.O., Buchanan W., Fan L.","Applied web traffic analysis for numerical encoding of SQL injection attack features",2016,"European Conference on Information Warfare and Security, ECCWS",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979266830&partnerID=40&md5=be41b9cdf28ad6a363f67aab387a7968","SQL Injection Attack (SQLIA) remains a technique used by a computer network intruder to pilfer an organisation's confidential data. This is done by an intruder re-crafting web form's input and query strings used in web requests with malicious intent to compromise the security of an organisation's confidential data stored at the back-end database. The database is the most valuable data source, and thus, intruders are unrelenting in constantly evolving new techniques to bypass the signature's solutions currently provided in Web Application Firewalls (WAF) to mitigate SQLIA. There is therefore a need for an automated scalable methodology in the pre-processing of SQLIA features fit for a supervised learning model. However, obtaining a ready-made scalable dataset that is feature engineered with numerical attributes dataset items to train Artificial Neural Network (ANN) and Machine Leaning (ML) models is a known issue in applying artificial intelligence to effectively address ever evolving novel SQLIA signatures. This proposed approach applies numerical attributes encoding ontology to encode features (both legitimate web requests and SQLIA) to numerical data items as to extract scalable dataset for input to a supervised learning model in moving towards a ML SQLIA detection and prevention model. In numerical attributes encoding of features, the proposed model explores a hybrid of static and dynamic pattern matching by implementing a Non-Deterministic Finite Automaton (NFA). This combined with proxy and SQL parser Application Programming Interface (API) to intercept and parse web requests in transition to the back-end database. In developing a solution to address SQLIA, this model allows processed web requests at the proxy deemed to contain injected query string to be excluded from reaching the target back-end database. This paper is intended for evaluating the performance metrics of a dataset obtained by numerical encoding of features ontology in Microsoft Azure Machine Learning (MAML) studio using Two-Class Support Vector Machines (TCSVM) binary classifier. This methodology then forms the subject of the empirical evaluation.","Azure Machine Learning; Input neurons; Numerical encoding; SQL Injection; SQLIA; Training data","Application programming interfaces (API); Artificial intelligence; Classification (of information); Database systems; Encoding (symbols); Learning systems; Network security; Neural networks; Numerical models; Pattern matching; Pipeline processing systems; Query languages; Query processing; Scalability; Supervised learning; Windows operating system; World Wide Web; Dynamic pattern matching; Input neurons; Nondeterministic finite automaton; SQL injection; SQLIA; Training data; Two-class support vector machines; Web application firewalls; Computer crime",2-s2.0-84979266830
"Saha D., Floratou A., Sankaranarayanan K., Minhas U.F., Mittal A.R., Özcan F.","ATHENA: An ontologydriven system for natural language querying over relational data stores",2016,"Proceedings of the VLDB Endowment",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013646447&partnerID=40&md5=2abe83fa29ec60d94d2d92200a9e1845","In this paper, we present ATHENA, an ontology-driven system for natural language querying of complex relational databases. Natural language interfaces to databases enable users easy access to data, without the need to learn a complex query language, such as SQL. ATHENA uses domain specific ontologies, which describe the semantic entities, and their relationships in a domain. We propose a unique two-stage approach, where the input natural language query (NLQ) is first translated into an intermediate query language over the ontology, called OQL, and subsequently translated into SQL. Our two-stage approach allows us to decouple the physical layout of the data in the relational store from the semantics of the query, providing physical independence. Moreover, ontologies provide richer semantic information, such as inheritance and membership relations, that are lost in a relational schema. By reasoning over the ontologies, our NLQ engine is able to accurately capture the user intent. We study the effectiveness of our approach using three different workloads on top of geographical (GEO), academic (MAS) and financial (FIN) data. ATHENA achieves 100% precision on the GEO and MAS workloads, and 99% precision on the FIN workload which operates on a complex financial ontology. Moreover, ATHENA attains 87.2%, 88.3%, and 88.9% recall on the GEO, MAS, and FIN workloads, respectively. © 2016 VLDB Endowment 2150-8097/16/08.",,"Fins (heat exchange); Natural language processing systems; Ontology; Orbits; Query languages; Semantics; Domain-specific ontologies; Membership relations; Natural language interfaces to database; Natural language queries; Relational Database; Relational schemas; Semantic information; Two stage approach; Translation (languages)",2-s2.0-85013646447
"Horridge M., Musen M.","Snap-SPARQL: A java framework for working with SPARQL and OWL",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964827969&doi=10.1007%2f978-3-319-33245-1_16&partnerID=40&md5=5dd3fe996fb4360a8710338abe10bb89","We present Snap-SPARQL, which is a Java framework for working with SPARQL and OWL. The framework includes a parser, axiom template API, SPARQL algebra implementation, and graphical user interface components for reading, processing and executing SPARQL queries under the SPARQL 1.1 OWL Entailment Regime. While the framework was originally designed to support the implementation of a SPARQL teaching aid in the form of a Protégé plugin, we believe that it is more generally useful and may be of interest to developers and researchers working on SPARQL 1.1 OWL entailment regime implementations and optimisations. The framework is open source and pluggable. © Springer International Publishing Switzerland 2016.",,"Application programming interfaces (API); Graphical user interfaces; Open source software; Semantic Web; User interfaces; Open sources; Optimisations; Plug-ins; SPARQL 1.1; Sparql queries; Teaching aids; User interface components; Birds",2-s2.0-84964827969
"Vijayarajan V., Dinakaran M., Tejaswin P., Lohani M.","A generic framework for ontology‑based information retrieval and image retrieval in web data",2016,"Human-centric Computing and Information Sciences",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006992445&doi=10.1186%2fs13673-016-0074-1&partnerID=40&md5=f36ef85071194a0f991449e854b129f7","In the internet era, search engines play a vital role in information retrieval from web pages. Search engines arrange the retrieved results using various ranking algorithms. Additionally, retrieval is based on statistical searching techniques or content-based information extraction methods. It is still difficult for the user to understand the abstract details of every web page unless the user opens it separately to view the web content. This key point provided the motivation to propose and display an ontologybased object-attribute-value (O-A-V) information extraction system as a web model that acts as a user dictionary to refine the search Keywords in the query for subsequent attempts. This first model is evaluated using various natural language processing (NLP) queries given as English sentences. Additionally, image search engines, such as Google Images, use content-based image information extraction and retrieval of web pages against the user query. To minimize the semantic gap between the image retrieval results and the expected user results, the domain ontology is built using image descriptions. The second proposed model initially examines natural language user queries using an NLP parser algorithm that will identify the subject-predicate-object (S-P-O) for the query. S-P-O extraction is an extended idea from the ontology-based O-A-V web model. Using this S-P-O extraction and considering the complex nature of writing SPARQL protocol and RDF query language (SPARQL) from the user point of view, the SPARQL auto query generation module is proposed, and it will auto generate the SPARQL query. Then, the query is deployed on the ontology, and images are retrieved based on the auto-generated SPARQL query. With the proposed methodology above, this paper seeks answers to following two questions. First, how to combine the use of domain ontology and semantics to improve information retrieval and user experience? Second, does this new unified framework improve the standard information retrieval systems? To answer these questions, a document retrieval system and an image retrieval system were built to test our proposed framework. The web document retrieval was tested against three key-words/bag-of-words models and a semantic ontology model. Image retrieval was tested on IAPR TC-12 benchmark dataset. The precision, recall and accuracy results were then compared against standard information retrieval systems using TREC_EVAL. The results indicated improvements over the standard systems. A controlled experiment was performed by test subjects querying the retrieval system in the absence and presence of our proposed framework. The queries were measured using two metrics, time and click-count. Comparisons were made on the retrieval performed with and without our proposed framework. The results were encouraging. © 2016 The Author(s).","Image retrieval; Information retrieval; Natural language processing; Ontology; SPARQL query",,2-s2.0-85006992445
"Martini R.G., Araújo C., Librelotto G.R., Henriques P.R.","A reduced CRM-compatible form ontology for the virtual emigration museum",2016,"Advances in Intelligent Systems and Computing",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961655030&doi=10.1007%2f978-3-319-31232-3_38&partnerID=40&md5=6a55c7c6d3a3594aed48c3d7888bd30a","In this paper we discuss the construction of a Reduced CRMcompatible form ontology for the virtual Emigration Museum based in the international standard for museum ontologies, CIDOC-CRM. To extract knowledge from the information of the virtual Emigration Museum when navigating through it, abstract data models should be used to conceptualize, the emigration documents stored in a relational database. In that way, resorting to an ontology (as abstract layer), the information contained in those documents can be accessed by the end-users (the museum visitors) to learn about the emigration phenomena. We also describe how we instantiate the ontology through a parser that automatically translates a plain text description of emigration data into RDF. Finally, we also discuss the choice of a triple storage system to save the RDF triples in order to enable the use of SPARQL to query the RDF data. © Springer International Publishing Switzerland 2016.","CIDOC-CRM; Emigration documents; Emigration museum; Ontology; RDF; Triplestore","Abstracting; Digital storage; Information systems; Museums; Ontology; Query processing; Abstract data; CIDOC CRM; Emigration documents; International standards; Museum visitor; Relational Database; Storage systems; Triple-store; Search engines",2-s2.0-84961655030
"Jizhou H., Zhao S., Shiqiang D., Wu H., Sun M., Haifeng W.","Generating recommendation evidence using translation model",2016,"IJCAI International Joint Conference on Artificial Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006094563&partnerID=40&md5=a28a170bddacab815153a4415b058d5d","Entity recommendation, providing entity suggestions relevant to the query that a user is searching for, has become a key feature of today's web search engine. Despite the fact that related entities are relevant to users' search queries, sometimes users cannot easily understand the recommended entities without evidences. This paper proposes a statistical model consisting of four sub-models to generate evidences for entities, which can help users better understand each recommended entity, and figure out the connections between the recommended entities and a given query. The experiments show that our method is domain independent, and can generate catchy and interesting evidences in the application of entity recommendation.",,"Search engines; Domain independents; Key feature; Related entities; Search queries; Statistical modeling; Submodels; Translation models; Artificial intelligence",2-s2.0-85006094563
"Tian R., Okazaki N., Inui K.","Learning semantically and additively compositional distributional representations",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011876438&partnerID=40&md5=be37e95762f675062349cf5dd968568a","This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.",,"Classification (of information); Computation theory; Computational linguistics; Formal methods; Linguistics; Semantics; Composition model; Compositional semantics; Formal Semantics; Relation classifications; Sentence completions; State of the art; Structured queries; Vectors",2-s2.0-85011876438
"Ameen S., Chung H., Han S.C., Kang B.H.","Open-domain question answering framework using wikipedia",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007189885&doi=10.1007%2f978-3-319-50127-7_55&partnerID=40&md5=580f0f727daed44ceb52ceade1bba4fc","This paper explores the feasibility of implementing a model for an open domain, automated question and answering framework that leverages Wikipedia’s knowledgebase. While Wikipedia implicitly comprises answers to common questions, the disambiguation of natural language and the difficulty of developing an information retrieval process that produces answers with specificity present pertinent challenges. However, observational analysis suggests that it is possible to discount the syntactical and lexical structure of a sentence in contexts where questions contain a specific target entity (words that identify a person, location or organisation) and that correspondingly query a property related to it. To investigate this, we implemented an algorithmic process that extracted the target entity from the question using CRF based named entity recognition (NER) and utilised all remaining words as potential properties. Using DBPedia, an ontological database of Wikipedia’s knowledge, we searched for the closest matching property that would produce an answer by applying standardised string matching algorithms including the Levenshtein distance, similar text and Dice’s coefficient. Our experimental results illustrate that using Wikipedia as a knowledgebase produces high precision for questions that contain a singular unambiguous entity as the subject, but lowered accuracy for questions where the entity exists as part of the object. © Springer International Publishing AG 2016.","Open-domain; Question answering; Wikipedia","Artificial intelligence; Query processing; Levenshtein distance; Named entity recognition; Observational analysis; Ontological database; Open domain question answering; Open-domain; Question Answering; Wikipedia; Natural language processing systems",2-s2.0-85007189885
"Cui W., Xiao Y., Wang H., Song Y., Hwang S.-W., Wang W.","KBQA: Learning question answering over QA corpora and knowledge bases",2016,"Proceedings of the VLDB Endowment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020425932&partnerID=40&md5=e2ddb2fcaaaace95b473e934a1e146c2","Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provided that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The challenge, however, is that a human can ask one question in many different ways. Previous approaches have natural limits due to their representations: Rule based approaches only understand a small set of ""canned"" questions, while keyword based or synonym based approaches cannot fully understand the questions. In this paper, we design a new kind of question representation: Templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city's population, we learn templates such as What's the population of $city?, How many people are there in $city?. We learned 27 million templates for 2782 intents. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary factoid questions. Furthermore, we expand predicates in RDF knowledge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efficiency over QALD benchmarks. © 2017. VLDB Endowment.",,"Bins; Knowledge based systems; Complex questions; Effectiveness and efficiencies; Factoid questions; Knowledge basis; Natural language questions; Question Answering; Rule-based approach; Structured queries; Natural language processing systems",2-s2.0-85020425932
"Om Prakash P.G., Jaya A.","Analyzing and predicting user behavior pattern from weblogs",2016,"International Journal of Applied Engineering Research",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992470207&partnerID=40&md5=593648dc1006b6b38212abbfff764676","A Weblogs contains series of transactions updated frequently, while users accessing the websites. It comprises of various entries like IP address, status code and number of bytes transferred, categories and time stamp. The user interest can be classified based on categories and attributes and it is helpful in identifying user behavior. The log query parser is to convert unstructured log to structured log based on user interest. The weblog data can be classified as successful and unsuccessful data. The aim of the research is to classify the data of success response and analyze the user navigation. The process of identifying user behavior consisting of data collection, query parser, pre-processing and pattern analysis that will help us to analyze and predict the user behavior in short time. This research work explores to analyze the user prediction, based on the user preference present in various levels that is captured from weblogs. © Research India Publications.","Data mining; Prediction accuracy; Traversal pattern; User behavior; User navigation; Web mining",,2-s2.0-84992470207
"Burns G.A., Hermjakob U., Ambite J.L.","Abstract meaning representations as linked data",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992597297&doi=10.1007%2f978-3-319-46547-0_2&partnerID=40&md5=11aa13bd011ac43dcb155862f19d1a41","The complex relationship between natural language and formal semantic representations can be investigated by the development of large, semantically-annotated corpora. The “Abstract Meaning Representation” (AMR) formulation describes the semantics of a whole sentence as a rooted, labeled graph, where nodes represent concepts/entities (such as PropBank frames and named entities) and edges represent relations between concepts (such as verb roles). AMRs have been used to annotate corpora of classic books, newstext and biomedical literature. Research on semantic parsers that generate AMRs from text is progressing rapidly. In this paper, we describe an AMR corpus as Linked Data (AMR-LD) and the techniques used to generate it (including an opensource implementation). We discuss the benefits of AMR-LD, including convenient analysis using SPARQL queries and ontology inferences enabled by embedding into the web of Linked Data, as well as the impact of semantic web representations directly derived from natural language. © Springer International Publishing AG 2016.","Abstract meaning representation; AMR; Biological pathways; Linked linguistic data; Sembank","Abstracting; Data handling; Natural language processing systems; Semantics; Syntactics; Biological pathways; Biomedical literature; Complex relationships; Formal Semantics; Linguistic data; Natural languages; Open source implementation; Sembank; Semantic Web",2-s2.0-84992597297
"Boley H.","The ruleML knowledge-interoperation hub",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978971220&doi=10.1007%2f978-3-319-42019-6_2&partnerID=40&md5=1f18d3087230fccd5b3406275a6aedbf","The RuleML knowledge-interoperation hub provides for syntactic/semantic representation and internal/external transformation of formal knowledge. The representation system permits the configuration of textbook and enriched Relax NG syntax as well as the association of syntax with semantics. The transformation tool suite includes serialized formatters (normalizers and compactifiers), polarized parsers and generators (the RuleML↔POSL tool and the RuleML→PSOA/PS generator and PSOA/PS→AST parser), as well as importers and exporters (the importer from Dexlog to Naf Datalog RuleML and the exporter from FOL RuleML languages to TPTP). An N3-PSOA-Flora knowledgeinteroperation use case is introduced for illustration. © Springer International Publishing Switzerland 2016.",,"Computational linguistics; Semantics; Syntactics; Datalog; Formal knowledge; Formatters; Interoperations; Relax NG; Transformation tools; Computer programming languages",2-s2.0-84978971220
"Ali A.A., Saad S.","Unsupervised concept hierarchy induction based on islamic glossary",2016,"ARPN Journal of Engineering and Applied Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978180088&partnerID=40&md5=c12fd297e7c604565bd48ac68ddde5a7","A machine-readable dictionary (MRD) is an electronic dictionary that enables query processing. One of the common processing tasks that has been widely applied is Concept Hierarchy Induction which aims at identifying concepts with its corresponding taxonomies such as named entities, synonyms and hyponyms. The Islamic domain contains a variety of concepts that are associated with numerous taxonomies. The existing concept hierarchy approaches for Islamic domain are using limited linguistic patterns. This study aims to propose an unsupervised concept hierarchy induction for the Islamic domain by extending the patterns and rules. In fact, Term Frequency-Inverse Document Frequency (TF-IDF) was carried out in order to identify the most frequently used concepts. Furthermore, two syntactical features were used including POS tagging and chunk parser in order to identify the tagging for each word (e.g. verb, noun, adjective, etc.) and extracting Noun Phrases (NP). Hence, the proposed extension patterns aim at utilize lexico-syntactic patterns to induce the concept hierarchy. The evaluation was performed using precision method by identifying the number of correctly extracted concepts and relation between them. Moreover, an expert review evaluation was performed by an expert in the Islamic domain. The experimental results showed that the proposed method achieved 82% precision. That demonstrates the usefulness of extending patterns for the Islamic domain. © 2006-2016 Asian Research Publishing Network (ARPN).","Concept hierarchy; Lexico-syntactic patterns; Ontology; Terminology extraction",,2-s2.0-84978180088
[No author name available],"9th International Conference on Genetic and Evolutionary Computing, ICGEC 2015",2016,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945925910&partnerID=40&md5=55c5dbc0be6739eefa71fa6df7f4208f","The proceedings contain 52 papers. The special focus in this conference is on Data Mining Techniques, QoS Control, Assessment in Networked Multimedia Applications, High Speed Computation and Applications in Information Systems. The topics include: An efficient solution for time-bound hierarchical key assignment scheme; a novel load balance algorithm for cloud computing; interference avoidance function research of spread spectrum system using composite sequence; an adaptive Kelly betting strategy for finite repeated games; a sanitization approach of privacy preserving utility mining; a modeling method of virtual terrain environment; method of founding focusing matrix for two-dimensional wideband signals; an efficient content searching method using transmission records with wasted queries reduction scheme in unstructured peer-to-peer networks; reliability specification of telecommunication networks based on the failure influence by using evolutional algorithm; trade-off relationship between operability and fairness in networked balloon bursting game using haptic interface devices; the effect of spatiotemporal tradeoff of picture patterns on QOE in multi-view video and audio IP transmission; detection of web application attacks with request length module and regex pattern analysis; a study on the effects of virtualization on mobile learning applications in private cloud; developing mobile application framework by using restful web service with JSON parser; problems on Gaussian normal basis multiplication for elliptic curve cryptosystem and auto-scaling mechanism for cloud resource management based on client-side turnaround time.",,,2-s2.0-84945925910
"Long R., Pasupat P., Liang P.","Simpler context-dependent logical forms via model projections",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011928335&partnerID=40&md5=dbc7d8c4e1e1e5e338cb359c39b199c9","We consider the task of learning a contextdependent mapping from utterances to denotations. With only denotations at training time, we must search over a combinatorially large space of logical forms, which is even larger with context-dependent utterances. To cope with this challenge, we perform successive projections of the full model onto simpler models that operate over equivalence classes of logical forms. Though less expressive, we find that these simpler models are much faster and can be surprisingly effective. Moreover, they can be used to bootstrap the full model. Finally, we collected three new contextdependent semantic parsing dataseis, and develop a new left-to-right parser.",,"Computational linguistics; Linguistics; Semantics; Context dependent; Full model; Large spaces; Logical forms; Semantic parsing; Training time; Via modeling; Equivalence classes",2-s2.0-85011928335
"Chen B., Sun L., Han X., An B.","Sentence rewriting for semantic parsing",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011824642&partnerID=40&md5=d3ece7033486e5cbcae5b6d038a00620","A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology. In this paper, we propose a sentence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form. Specifically, we propose two sentence-rewriting methods for two common types of mismatch: a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch. We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset - WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in Fl, and generates logical forms more accurately and parses sentences more robustly. © 2016 Association tor Computational Linguistics.",,"Computational linguistics; Context free grammars; Linguistics; Semantics; Base systems; Logical forms; Mismatch problems; Natural languages; New forms; Semantic parsing; Template based methods; Syntactics",2-s2.0-85011824642
"Hazber M.A.G., Li R., Xu G., Alalayah K.M.","An approach for automatically generating R2RML-based direct mapping from relational databases",2016,"Communications in Computer and Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981275461&doi=10.1007%2f978-981-10-2053-7_15&partnerID=40&md5=5da15667ff8309f0b3b77952ff48405d","For integrating relational databases (RDBs) into semantic web applications, the W3C RDB2RDF Working Group recommended two approaches, Direct Mapping (DM) and R2RML. The DM provides a set of mapping rules according to RDB schema, while the R2RML allows users to manually define mappings according to existing target ontology. The major problem to use R2RML is the effort for creating R2RML mapping documents manually. This may lead to appearance of many mistakes in the R2RML documents and requires domain experts. In this paper, we propose and implement an approach to generate an R2RML mapping documents automatically from RDB schema. The R2RML mapping reflects the behavior of the DM specification and allows any R2RML parser to generate a set of RDF triples from relational data. The input of generating approach is DBsInfo class that automatically generated from relational schema. An experimental prototype is developed and shows the effectiveness of our approach algorithms. © Springer Science+Business Media Singapore 2016.","Direct Mapping; R2RML; Relational database; Relational Database to Resource Description Framework (RDB2RDF); Resource Description Framework (RDF)","Education; Semantic Web; World Wide Web; Automatically generated; Direct mapping; Experimental prototype; R2RML; Relational Database; Relational schemas; Resource description framework; Semantic web applications; Mapping",2-s2.0-84981275461
"Pasupat P., Liang P.","Inferring logical forms from denotations",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012008534&partnerID=40&md5=87485ca88bd4f710b524d519b7bd65d0","A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those that yield the correct denotation-from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WIKITABLEQUESTIONS dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Semantics; Core problems; Large spaces; Learning semantics; Logical forms; Search spaces; Set of rules; Dynamic programming",2-s2.0-85012008534
"Pandey H.M., Chaudhary A., Mehrotra D.","Grammar induction using bit masking oriented genetic algorithm and comparative analysis",2016,"Applied Soft Computing Journal",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945939307&doi=10.1016%2fj.asoc.2015.09.044&partnerID=40&md5=3a2c98df916581157659c8966c558f9c","This paper presents bit masking oriented genetic algorithm (BMOGA) for context free grammar induction. It takes the advantages of crossover and mutation mask-fill operators together with a Boolean based procedure in two phases to guide the search process from ith generation to (i + 1)th generation. Crossover and mutation mask-fill operations are performed to generate the proportionate amount of population in each generation. A parser has been implemented checks the validity of the grammar rules based on the acceptance or rejection of training data on the positive and negative strings of the language. Experiments are conducted on collection of context free and regular languages. Minimum description length principle has been used to generate a corpus of positive and negative samples as appropriate for the experiment. It was observed that the BMOGA produces successive generations of individuals, computes their fitness at each step and chooses the best when reached to threshold (termination) condition. As presented approach was found effective in handling premature convergence therefore results are compared with the approaches used to alleviate premature convergence. The analysis showed that the BMOGA performs better as compared to other algorithms such as: random offspring generation approach, dynamic allocation of reproduction operators, elite mating pool approach and the simple genetic algorithm. The term success ratio is used as a quality measure and its value shows the effectiveness of the BMOGA. Statistical tests indicate superiority of the BMOGA over other existing approaches implemented. © 2015 Published by Elsevier B.V.","Bit-masking oriented data structure; Context free grammar; Genetic algorithm; Grammar induction mask-fill operator; Premature convergence","Algorithms; Computational grammars; Context free grammars; Genetic algorithms; Comparative analysis; Crossover and mutation; Dynamic allocations; Grammar induction; Minimum description length principle; Pre-mature convergences; Reproduction operator; Simple genetic algorithm; Context free languages",2-s2.0-84945939307
"Goodman J., Vlachos A., Naradowsky J.","Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011990207&partnerID=40&md5=afcffd7a471cb71fb56708ef535f716e","Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena, resolve anaphora, and identify word senses to eliminate ambiguous interpretations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Banarescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. However, when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions: noise reduction and targeted exploration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-ofthe art results, and improve upon standard transition-based parsing by 4.7 F1 points. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Learning algorithms; Linguistics; Noise abatement; Semantics; Action spaces; Dependency parsing; Feature representation; Imitation learning; Natural languages; Undesirable state; Word sense; Syntactics",2-s2.0-85011990207
"Bogoychev N., Lopez A.","TV-gram language models for massively parallel devices",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011976011&partnerID=40&md5=4341eb4640b1793efae4052a898334ec","For many applications, the query speed of N-gram language models is a computational bottleneck. Although massively parallel hardware like GPUs offer a potential solution to this bottleneck, exploiting this hardware requires a careful rethinking of basic algorithms and data structures. We present the first language model designed for such hardware, using B-trees to maximize data parallelism and minimize memory footprint and latency. Compared with a single-threaded instance of KenLM (Heafield, 2011), a highly optimized CPU-based language model, our GPU implementation produces identical results with a smaller memory footprint and a sixfold increase in throughput on a batch query task. When we saturate both devices, the GPU delivers nearly twice the throughput per hardware dollar even when the CPU implementation uses faster data structures. © 2016 Association for Computational Linguistics.",,"Data structures; Hardware; Linguistics; Program processors; Trees (mathematics); Algorithms and data structures; Computational bottlenecks; Data parallelism; GPU implementation; Massively parallels; Memory footprint; N-gram language models; Single-threaded; Computational linguistics",2-s2.0-85011976011
"Origlia A., Leone E., Sorgente A., Vanacore P., Parascandolo M., Mele F., Cutugno F.","Designing interactive experiences to explore artwork collections: A multimedia dialogue system supporting visits in museum exhibits",2016,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009391326&partnerID=40&md5=15a4d0c44efaeedb6790b6a9f4844ffc","Speech and natural language processing have a central role in the implementation of systems designed to make the museum more reactive to users' inputs and to improve the overall interaction quality. In this paper, we present the design and implementation of a dialogue system to provide multimedia presentations for museum visits. A corpus of speech recordings in Italian was collected with a mobile application to obtain a reference set of possible ways for the users to express their intentions. On the basis of this corpus, a set of recurring syntactic patterns associated to device requests was extracted to let the dialogue system separate device commands from information queries. Disambiguation strategies depending on the context are also applied in presence of partial syntactic patterns. Information queries are answered by automatically assembling portions of semantically annotated texts and are synchronized with relevant multimedia resources. A case study on the '800 exhibit at the Capodimonte museum in Naples is presented3. © 2016, CEUR-WS. All rights reserved.","Cultural heritage; Dialogue systems","Artificial intelligence; Exhibitions; Linguistics; Multimedia systems; Speech processing; Syntactics; Cultural heritages; Design and implementations; Dialogue systems; Interaction quality; Mobile applications; Multimedia presentation; Multimedia resources; NAtural language processing; Natural language processing systems",2-s2.0-85009391326
"Di Costanzo L., Ghosh S., Zardecki C., Burley S.K.","Using the tools and resources of the RCSB protein data bank",2016,"Current Protocols in Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020910136&doi=10.1002%2fcpbi.13&partnerID=40&md5=bfdfde8c8a5d5c49088ed5606bd06ab2","The Protein Data Bank (PDB) archive is the worldwide repository of experimentally determined three-dimensional structures of large biological molecules found in all three kingdoms of life. Atomic-level structures of these proteins, nucleic acids, and complex assemblies thereof are central to research and education in molecular, cellular, and organismal biology, biochemistry, biophysics, materials science, bioengineering, ecology, and medicine. Several types of information are associated with each PDB archival entry, including atomic coordinates, primary experimental data, polymer sequence(s), and summary metadata. The Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB) serves as the U.S. data center for the PDB, distributing archival data and supporting both simple and complex queries that return results. These data can be freely downloaded, analyzed, and visualized using RCSB PDB tools and resources to gain a deeper understanding of fundamental biological processes, molecular evolution, human health and disease, and drug discovery. © 2016 by John Wiley & Sons, Inc.","3D structure; Drug discovery; Ligand; Macromolecule; Search; Single nucleotide variation","anti human immunodeficiency virus agent; Human immunodeficiency virus proteinase; ligand; protein; amino acid sequence; Article; computer; drug targeting; enzyme structure; Human immunodeficiency virus 1; Internet; online system; priority journal; Protein Data Bank; web browser; biology; chemistry; human; procedures; protein database; Computational Biology; Databases, Protein; Humans; Proteins",2-s2.0-85020910136
"Xiao C., Dymetman M., Gardent C.","Sequence-based structured prediction for semantic parsing",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011866616&partnerID=40&md5=89c07924b6b5186e490db6d61dc52b8b","We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints.",,"Computational linguistics; Linearization; Linguistics; Recurrent neural networks; Canonical realization; Logical forms; Natural language questions; Semantic parsing; Sequence prediction; Structured prediction; Semantics",2-s2.0-85011866616
"Zan T., Liu L., Ko H.-S., Hu Z.","Brul: A putback-based bidirectional transformation library for updatable views",2016,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964237827&partnerID=40&md5=1fb623cf120f84ca3fd275e86f0cd580","In work on relational databases, the view-update problem is about how to translate update operations on the view table to corresponding update operations on the source table properly. It is a problem that the translation policies are not unique in many situations. Relational lenses try to solve this problem by providing a list of combinators that let the user write get functions (queries) with specified updated policies for put functions (updates); however this can only provide limited control of update policies which still may not satisfy the user's real needs. In this paper, we implement a library Brul that provides putback-based basic combinators for the user to write the put function with exible update policies easily; from the put function, a unique get function can be derived automatically. Brul is implemented in terms of BiGUL, a core bidirectional programming language which has been formalized in Agda and implemented as a Haskell library. Copyright © by the paper's authors.",,"Problem oriented languages; Bidirectional programming language; Bidirectional transformation; Combinators; Haskell; Relational Database; View update problems; Problem solving",2-s2.0-84964237827
"Iyer S., Konstas I., Cheung A., Zettlemoyer L.","Summarizing source code using a neural attention model",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012016847&partnerID=40&md5=d517497c88494a531d9a971151d3f050","High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely data-driven approach for generating high level summaries of source code. Our model, CODE-NN, uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin. © 2016 Association for Computational Linguistics.",,"Benchmarking; Computational linguistics; Computer programming languages; Linguistics; Long short-term memory; Attention model; Code retrievals; Data-driven approach; High quality source; Large margins; Online forums; Source codes; State of the art; Codes (symbols)",2-s2.0-85012016847
"Kaushik R., Apoorva Chandra S., Mallya D., Chaitanya J.N.V.K., Sowmya Kamath S.","Sociopedia: An interactive system for event detection and trend analysis for twitter data",2016,"Smart Innovation, Systems and Technologies",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945561142&doi=10.1007%2f978-81-322-2529-4_6&partnerID=40&md5=6441a12c750af07041a6c740fbb43868","The emergence of social media has resulted in the generation of highly versatile and high volume data. Most web search engines return a set of links or web documents as a result of a query, without any interpretation of the results to identify relations in a social sense. In the work presented in this paper, we attempt to create a search engine for social media datastreams, that can interpret inherent relations within tweets, using an ontology built from the tweet dataset itself. The main aim is to analyze evolving social media trends and providing analytics regarding certain real world events, that being new product launches, in our case. Once the tweet dataset is pre-processed to extract relevant entities, Wiki data about these entities is also extracted. It is semantically parsed to retrieve relations between the entities and their properties. Further, we perform various experiments for event detection and trend analysis in terms of representative tweets, key entities and tweet volume, that also provide additional insight into the domain. © Springer India 2016.","Knowledge discovery; NLP; Ontology; Semantics; Social media analysis","Data mining; Human computer interaction; Information science; Ontology; Search engines; Semantics; Social networking (online); Event detection; High volumes; Interactive system; NLP; Product launch; Social media analysis; Trend analysis; Web document; World Wide Web",2-s2.0-84945561142
"Bast H., Buchhold B., Haussmann E.","Semantic search on text and knowledge bases",2016,"Foundations and Trends in Information Retrieval",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976287430&doi=10.1561%2f1500000032&partnerID=40&md5=a59447da2d1f52e73747af50cb721949","This article provides a comprehensive overview of the broad area of semantic search on text and knowledge bases. In a nutshell, semantic search is ""search with meaning"". This ""meaning"" can refer to various parts of the search process: understanding the query (instead of just finding matches of its components in the data), understanding the data (instead of just searching it for such matches), or representing knowledge in a way suitable for meaningful retrieval. Semantic search is studied in a variety of different communities with a variety of different views of the problem. In this survey, we classify this work according to two dimensions: the type of data (text, knowledge bases, combinations of these) and the kind of search (keyword, structured, natural language). We consider all nine combinations. The focus is on fundamental techniques, concrete systems, and benchmarks. The survey also considers advanced issues: ranking, indexing, ontology matching and merging, and inference. It also provides a succinct overview of natural language processing techniques that are useful for semantic search: POS tagging, named-entity recognition and disambiguation, sentence parsing, and word vectors. The survey is as self-contained as possible, and should thus also serve as a good tutorial for newcomers to this fascinating and highly topical field. © 2016 H. Bast, B. Buchhold, E. Haussmann.",,"Computational linguistics; Natural language processing systems; Ontology; Surveys; Syntactics; Concrete system; Knowledge basis; Named entity recognition; NAtural language processing; Natural languages; Ontology matching; Semantic search; Sentence parsing; Semantics",2-s2.0-84976287430
"Čerāns K., Būmans G.","Database to ontology mapping patterns in RDB2OWL lite",2016,"Communications in Computer and Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979010955&doi=10.1007%2f978-3-319-40180-5_3&partnerID=40&md5=a040bdb174df127aa4816d6848d0a447","We describe the RDB2OWL Lite language for relational database to RDF/OWL mapping specification and discuss the architectural and content specification patterns arising in mapping definition. RDB2OWL Lite is a simplification of original RDB2OWL with aggregation possibilities and order-based filters removed, while providing in-mapping SQL view definition possibilities. The mapping constructs and their usage patterns are illustrated on mapping examples from medical domain: medicine registries and hospital information system. The RDB2OWL Lite mapping implementation is offered both via translation into D2RQ and into standard R2RML mapping notations. © Springer International Publishing Switzerland 2016.","Database to ontology mapping; Mapping patterns; Ontologies; RDF","Information systems; Medical information systems; Ontology; Specifications; Content specifications; Hospital information systems; Mapping pattern; Mapping specifications; Medical domains; Ontology mapping; Relational Database; Usage patterns; Mapping",2-s2.0-84979010955
"Chen X., Liu C., Shin R., Song D., Chen M.","Latent attention for if-then program synthesis",2016,"Advances in Neural Information Processing Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018910377&partnerID=40&md5=ec239d09f28a6351d520e66c2efea183","Automatic translation from natural language descriptions into programs is a longstanding challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art[3]. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data. © 2016 NIPS Foundation - All Rights Reserved.",,"Network architecture; Neural networks; Translation (languages); Automatic translation; Natural languages; Novel neural network; One-shot learning; Program synthesis; Textual description; Training procedures; Two-stage process; Program translators",2-s2.0-85018910377
"Sezer T.","Tweets corpus; building a corpus by social media [Tweets derlemi sosyal medyadan derlem oluşturmak]",2016,"Milli Egitim",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015218911&partnerID=40&md5=587f0d9a5c153e65b3aae1b0c2145df9","Social media is an important part of the modern life in our era. Users share their ideas, status, photos, life turning points and opinions using instruments of social media. This study will focus on building and usage features of the TweetS corpus build by using 1 million Tweets, under TS Corpus project. The corpus represents 6 new part-of-speech tags, peculiar to Internet, that were never used before in part-of-speech tagging of Turkish texts before. Also a new tokenizer had prepared in order to process the data. The study states the need for the corpora that use social media as a data source.","Corpus; Part-of-speech tagging; Tokenization; Ts corpus; Twitter",,2-s2.0-85015218911
"Fragkos P., Ioannidis C.","Assessment of lidargrammetry for spatial data extraction",2016,"Proceedings of SPIE - The International Society for Optical Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989856204&doi=10.1117%2f12.2240653&partnerID=40&md5=35abae9547456ad2a00e8ca0b647fff1","Lidargrammetry concerns the production of inferred stereopairs (ISPs) from LiDAR intensity images, intended to stereodigitize spatial data in digital photogrammetric stations. The production of ISPs is based on the principle of stereoorthomates and the extraction of a derivative intensity image, in which an artificial x-parallax is being introduced; other techniques have also developed in order to best utilize the 3D nature of LiDAR data. Lidargrammetry is a relatively new approach, not yet assessed properly, in order to quantify its derivative spatial data quality and the impact of its reduced photointerpretative ability, comparing to typical photogrammetric stereomodels. In this paper, a dense point cloud of 55 points/m2 is used, which is thinned out to 25 points/m2 and 7 points/m2in order to simulate scan missions of lower pulse repetition frequency. ISPs are being produced from each of these point cloud's intensity images using the slope parallel projection method and building footprints are being extracted. Using the denser point cloud's footprints as control data, the relative accuracy of the thinner point cloud's footprints is assessed, in order to evaluate the effect of the decreasing resolution in the digitization process. Estimated footprint's relative accuracy (2σ) is 0.5m and 1m for the 25 points/m2 and the 7 points/m2 clouds respectively. Moreover, a reference footprint dataset was derived, by a stereorestitution procedure, using high resolution optical aerial images. Absolute spatial accuracy ranges around 1.5 m making the Lidargrammetric technique capable for extracting spatial data suitable even for large scale mapping. © 2016 SPIE.","Inferred stereo pairs; Intensity images; LiDAR; Lidargrammetry; Spatial accuracy","Extraction; Geometrical optics; Optical radar; Photogrammetry; Remote sensing; Stereo image processing; Intensity images; Lidar intensity image; Lidargrammetry; Parallel projection; Pulse repetition frequencies; Spatial accuracy; Spatial data quality; Stereo pair; Image processing",2-s2.0-84989856204
"Dong L., Lapata M.","Language to logical form with neural attention",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012010015&partnerID=40&md5=687f87c03c609fbbe39f4b92d18fe4ff","Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domainor representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Encoding (symbols); Semantics; Encoder-decoder; General method; Linguistic features; Natural languages; Output sequences; Semantic parsing; Traditional approaches; Vector representations; Linguistics",2-s2.0-85012010015
"Anjos E., Lee J., Satti S.R.","SJSON: A succinct representation for JavaScript object notation documents",2016,"2016 11th International Conference on Digital Information Management, ICDIM 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014414723&doi=10.1109%2fICDIM.2016.7829787&partnerID=40&md5=a0841d5f94773259a945484e05602aee","The massive amounts of data processed in modern computational systems is becoming a problem of increasing importance. This data is commonly stored directly or indirectly through the use of data exchange languages, such as JavaScript Object Notation (JSON), for human-readable platform agnostic access. This paper focuses on describing and analyzing SJSON, a library that explores succinct representations of JSON documents as a means to achieve reduced memory usage of files in main memory, and to permit the compression of JSON files stored in disk. In SJSON we represent the document structure with succinct trees, as opposed to the usual pointer-based implementation. Furthermore, the remaining raw data are organized in arrays of attributes and values. Attributes are stripped of redundancies and stored in a simple contiguous array, while values are represented through a bit string indexed array. The scheme here proposed is then evaluated with respect to a number of metrics comparing its performance with popular libraries, anssd possible improvements to the representation are then presented. © 2016 IEEE.","balanced parentheses; heterogeneous arrays; JSON data exchange language; succinct data structures","Electronic data interchange; Information management; balanced parentheses; Computational system; Document structure; Heterogeneous array; JavaScript object notations (JSON); JSON data exchange language; Succinct data structure; Succinct representation; High level languages",2-s2.0-85014414723
"Venčkauskas A., Jusas V., Paulikas K., Toldinas J.","A methodology and tool for investigation of artifacts left by the BitTorrent client",2016,"Symmetry",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975677617&doi=10.3390%2fsym8060040&partnerID=40&md5=6da53b53645c14f89f93f1724ccd95f1","The BitTorrent client application is a popular utility for sharing large files over the Internet. Sometimes, this powerful utility is used to commit cybercrimes, like sharing of illegal material or illegal sharing of legal material. In order to help forensics investigators to fight against these cybercrimes, we carried out an investigation of the artifacts left by the BitTorrent client. We proposed a methodology to locate the artifacts that indicate the BitTorrent client activity performed. Additionally, we designed and implemented a tool that searches for the evidence left by the BitTorrent client application in a local computer running Windows. The tool looks for the four files holding the evidence. The files are as follows: *.torrent, dht.dat, resume.dat, and settings.dat. The tool decodes the files, extracts important information for the forensic investigator and converts it into XML format. The results are combined into a single result file. © 2016 by the authors.","BitTorrent protocol; Cybercrime; Forensics investigation; XML format",,2-s2.0-84975677617
"Aghaebrahimian A., Jurčíček F.","Constraint-based open-domain question answering using knowledge graph search",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010190057&doi=10.1007%2f978-3-319-45510-5_4&partnerID=40&md5=ade7ce454944c0b2763d03181022f653","We introduce a highly scalable approach for open-domain question answering with no dependence on any logical form to surface form mapping data set or any linguistic analytic tool such as POS tagger or named entity recognizer. We define our approach under the Constrained Conditional Models framework which lets us scale to a full knowledge graph with no limitation on the size. On a standard benchmark, we obtained competitive results to state-of-the-art in open-domain question answering task. © Springer International Publishing Switzerland 2016.","Constrained conditional models; Knowledge graph; Question answering; Vector representation","Computational linguistics; Conditional models; Constraint-based; Knowledge graphs; Open domain question answering; Question Answering; Scalable approach; State of the art; Vector representations; Natural language processing systems",2-s2.0-85010190057
"Xu K., Reddy S., Feng Y., Huang S., Zhao D.","Question answering on freebase via relation extraction and textual evidence",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012023662&partnerID=40&md5=d9fa9fab185adedacdb6734cab363da8","Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F1 of 53.3%, a substantial improvement over the state-of-the-art. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Linguistics; Semantics; Annotated training data; Multiple constraint; Question Answering; Question answering systems; Relation extraction; Representation method; Semantic parsing; State of the art; Extraction",2-s2.0-85012023662
"He Y., Du Y., Hughes S., Zhai J., Hallstrom J.O., Sridhar N.","DESALβ: A framework for implementing self-stabilizing embedded network applications",2016,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009775121&doi=10.1007%2f978-3-319-47075-7_34&partnerID=40&md5=333dd9f6208008f705c125dc66a4cc83",[No abstract available],,,2-s2.0-85009775121
"Yih W.-T., Richardson M., Meek C., Chang M.-W., Suh J.","The value of semantic parse labeling for knowledge base question answering",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Short Papers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016629133&partnerID=40&md5=dddf487cb85b66c79d7693e35c365f2f","We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Linguistics; User interfaces; High-accuracy; Knowledge base; Labeled dataset; Question Answering; Small scale; Semantics",2-s2.0-85016629133
"Aziz B., Bader M., Hippolyte C.","Search-based SQL injection attacks testing using genetic programming",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962228936&doi=10.1007%2f978-3-319-30668-1_12&partnerID=40&md5=5342712b3e96661ccc3b1be8f05384df","Software testing is a key phase of many development methodologies as it provides a natural opportunity for integrating security early in the software development lifecycle. However despite the known importance of software testing, this phase is often overlooked as it is quite difficult and labour-intensive to obtain test datasets to effectively test an application. This lack of adequate automatic software testing renders software applications vulnerable to malicious attacks after they are deployed as detected software vulnerabilities start having an impact during the production phase. Among such attacks are SQL injection attacks. Exploitation of SQL injection vulnerabilities by malicious programs could result in severe consequences such as breaches of confidentiality and false authentication. We present in this paper a search-based software testing technique to detect SQL injection vulnerabilities in software applications. This approach uses genetic programming as a means of generating our test datasets, which are then used to test applications for SQL injection-based vulnerabilities. © Springer International Publishing Switzerland 2016.","Genetic programming; Search-based testing; SQL injections","Application programs; Computer crime; Genetic algorithms; Genetic programming; Integration testing; Network security; Software design; Development methodology; Search-based software testing; Search-based testing; Software applications; Software development life cycle; Software vulnerabilities; SQL injection; Sql injection attacks; Software testing",2-s2.0-84962228936
"Erekhinskaya T., Balakrishna M., Tatu M., Moldovan D.","Personalized medical reading recommendation: Deep semantic approach",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964063291&doi=10.1007%2f978-3-319-32055-7_8&partnerID=40&md5=8321ecf227b18c01b163185a0cbf9531","Therapists are faced with the overwhelming task of identifying, reading, and incorporating new information from a vast and fast growing volume of publications into their daily clinical decisions. In this paper, we propose a system that will semantically analyze patient records and medical articles, perform medical domain specific inference to extract knowledge profiles, and finally recommend publications that best match with a patient’s health profile. We present specific knowledge extraction and matching details, examples, and results from the mental health domain. © Springer International Publishing Switzerland 2016.","Deep semantic extraction; Diagnostic inference; Medical information retrieval","Database systems; Extraction; Semantics; Clinical decision; Diagnostic inferences; Medical domains; Mental health; Patient record; Semantic approach; Semantic extraction; Specific knowledge; Diagnosis",2-s2.0-84964063291
"Khani F., Rinard M., Liang P.","Unanimous prediction for 100% precision with application to learning semantic mappings",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011876524&partnerID=40&md5=2b68471a103d8904f6a7700c41cc6e77","Can we train a system that, on any new input, either says ""don't know"" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is wellspecified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Mapping; Semantics; Learning semantics; Logical forms; Semantic parsing; Training data; Forecasting",2-s2.0-85011876524
"Sorokin A.A., Baytin A.V., Galinskaya I.E., Rykunova E.D., Shavrina T.O.","SPELLRUEVAL: The first competition on automatic spelling correction for Russian",2016,"Komp'juternaja Lingvistika i Intellektual'nye Tehnologii",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020393525&partnerID=40&md5=ee2de4928be8f4cbdb63de20eb5fe017","This paper reports on the first competition on automatic spelling correction for Russian language - SpellRuEval - held within the framework of ""Dialogue Evaluation"". The competition aims to bring together groups of Russian academic researchers and IT-companies in order to gain and exchange the experience in automatic spelling correction, especially concentrating on social media texts. The data for the competition was taken from Russian segment of Live Journal. 7 teams took part in the competition, the best results were achieved by the model using edit distance and phonetic similarity for candidate search and n-gram language model for their reranking. We discuss in details the algorithms used by the teams, as well as the methodology of evaluation for automatic spelling correction.","Automatic methods for processing Russian; Automatic spelling correction; Language of social media; Spelling correction",,2-s2.0-85020393525
"Jastrząb T., Kwiatkowski G., Sadowski P.","Mapping of selected synsets to semantic features",2016,"Communications in Computer and Information Science",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964739516&doi=10.1007%2f978-3-319-34099-9_28&partnerID=40&md5=9820d889dc1dafc4008d2e15152d5aab","In the paper we devise a novel algorithm related to the area of natural language processing. The algorithm is capable of building a mapping between the sets of semantic features and the words available in semantic dictionaries called wordnets. In our research we consider wordnets as ontologies, paying particular attention to hypernymy relation. The correctness of the proposal is verified experimentally based on a selected set of semantic features. plWordNet semantic dictionary is considered as a reference source, providing required information for the mapping. The algorithm is evaluated on an instance of a decision problem related to data classification. The quality measures of the classification include: false positive rate, false negative rate and accuracy. A measure of a strength of membership (SOM) in a semantic feature class is proposed and its impact on the aforementioned quality measures is evaluated. © Springer International Publishing Switzerland 2016.","Natural language processing; Ontologies; Semantic features; Wordnets","Algorithms; Computational linguistics; Mapping; Natural language processing systems; Ontology; Quality control; Semantics; Data classification; Decision problems; False negative rate; False positive rates; NAtural language processing; Semantic dictionaries; Semantic features; Wordnets; Data mining",2-s2.0-84964739516
"Sorokin A.A., Shavrina T.O.","Automatic spelling correction for Russian social media texts",2016,"Komp'juternaja Lingvistika i Intellektual'nye Tehnologii",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020472992&partnerID=40&md5=fe3c2bd0d7d39aa0eac3f1071c0f3e8f","This paper describes an automatic spelling correction system for Russian. The system utilizes information from different levels, using edit distance for candidate search and a combination of weighted edit distance and language model for candidate hypotheses selection. The hypotheses are then reranked by logistic regression using edit distance score, language model score etc. as features. We also experimented with morphological and semantic features but did not get any advantage. Our system has won the first SpellRuEval competition for Russian spell checkers by all the metrics and achieved F1-Measure of 75%.","Automatic spelling correction; Non-word errors; Real-word errors; Russian spellchecking; Social media language; Spelling correction; Spelling correction for Russian; Spelling correction on corpora",,2-s2.0-85020472992
"Gong C., Zhu W., Ziwei W., Chen J., Qu Y.","Taking up the gaokao challenge: An information retrieval approach",2016,"IJCAI International Joint Conference on Artificial Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006176244&partnerID=40&md5=ca23c3e82f1ce3f8bce160daa0405cad","Answering questions in a university's entrance examination like Gaokao in China challenges AI technology. As a preliminary attempt to take up this challenge, we focus on multiple-choice questions in Gaokao, and propose a three-stage approach that exploits and extends information retrieval techniques. Taking Wikipedia as the source of knowledge, our approach obtains knowledge relevant to a question by retrieving pages from Wikipedia via string matching and context-based disambiguation, and then ranks and filters pages using multiple strategies to draw critical evidence, based on which the truth of each option is assessed via relevancebased entailment. It achieves encouraging results on real-life questions in recent history tests, significantly outperforming baseline approaches.",,"Artificial intelligence; AI Technologies; And filters; Context-based; Entrance examination; Information retrieval approach; Multiple choice questions; Multiple strategy; String matching; Information retrieval",2-s2.0-85006176244
"Stuckenschmidt H., Ponzetto S.P., Meilicke C.","Detecting meaningful compounds in complex class labels",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997327305&doi=10.1007%2f978-3-319-49004-5_40&partnerID=40&md5=463d3af8c64a6b289199e567d80736da","Real-world ontologies such as, for instance, those for the medical domain often represent highly specific, fine-grained concepts using complex labels that consist of a sequence of sublabels. In this paper, we investigate the problem of automatically detecting meaningful compounds in such complex class labels to support methods that require an automatic understanding of their meaning such as, for example, ontology matching, ontology learning and semantic search. We formulate compound identification as a supervised learning task and investigate a variety of heterogeneous features, including statistical (i.e., knowledgelean) as well as knowledge-based, for the task at hand. Our classifiers are trained and evaluated using a manually annotated dataset consisting of about 300 complex labels taken from real-world ontologies, which we designed to provide a benchmarking gold standard for this task. Experimental results show that by using a combination of distributional and knowledge-based features we are able to reach an accuracy of more than 90% for compounds of length one and almost 80% for compounds of length two. Finally, we evaluate our method in an extrinsic experimental setting: this consists of a use case highlighting the benefits of using automatically identified compounds for the high-end semantic task of ontology matching. © Springer International Publishing AG 2016.",,"Classification (of information); Knowledge based systems; Knowledge engineering; Knowledge management; Ontology; Semantics; Automatic understanding; Heterogeneous features; Knowledge based; Medical domains; Ontology learning; Ontology matching; Semantic search; Support method; Gold compounds",2-s2.0-84997327305
"Zhang Y., Wang H., Lepage Y.","HSSA tree structures for BTG-based preordering in machine translation",2016,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation, PACLIC 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015885084&partnerID=40&md5=61efb926951cef1bfacad43fc16ff0b2","The Hierarchical Sub-Sentential Alignment (HSSA) method is a method to obtain aligned binary tree structures for two aligned sentences in translation correspondence. We propose to use the binary aligned tree structures delivered by this method as training data for preordering prior to machine translation. For that, we learn a Bracketing Transduction Grammar (BTG) from these binary aligned tree structures. In two oracle experiments in English to Japanese and Japanese to English translation, we show that it is theoretically possible to outperform a baseline system with a default distortion limit of 6, by about 2.5 and 5 BLEU points and, 7 and 10 RIBES points respectively, when preordering the source sentences using the learnt preordering model and using a distortion limit of 0. An attempt at learning a preordering model and its results are also reported.",,"Aircraft engine manufacture; Binary trees; Bins; Computational linguistics; Computer aided language translation; Translation (languages); Aligned sentences; Baseline systems; Binary tree structure; Btg-based; Machine translations; Pre orderings; Training data; Tree structures; Trees (mathematics)",2-s2.0-85015885084
"Jia R., Liang P.","Data recombination for neural semantic parsing",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012023602&partnerID=40&md5=1841f1fe1a125adfd436565bfb5b4861","Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a highprecision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision. © 2016 Association for Computational Linguistics.",,"Context free grammars; Linguistics; Semantics; Conditional independences; High-precision; Logical regularities; Prior knowledge; Recurrent networks; Semantic parsing; State-of-the-art performance; Synchronous context-free grammars; Formal languages",2-s2.0-85012023602
"Wang L., Grefenstette E., Hermann K.M., Kočiský T., Senior A., Wang F., Blunsom P.","Latent predictor networks for code generation",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011954268&partnerID=40&md5=bf448371aebc848dd567cb3fc196b421","Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks. © 2016 Association for Computational Linguistics.",,"Linguistics; Network architecture; Neural networks; Arbitrary number; Language generation; Natural languages; Novel neural network; Output sequences; Predictor network; Programming codes; Structured specification; Computational linguistics",2-s2.0-85011954268
"Zaytsev V.","Cotransforming grammars with shared packed parse forests",2016,"Electronic Communications of the EASST",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978902298&doi=10.14279%2ftuj.eceasst.73.1032.1020&partnerID=40&md5=d93183a19de924697782760151f99a74","SPPF (shared packed parse forest) is the best known graph representation of a parse forest (family of related parse trees) used in parsing with ambiguous/ conjunctive grammars. Systematic general purpose transformations of SPPFs have never been investigated and are considered to be an open problem in software language engineering. In this paper, we motivate the necessity of having a transformation operator suite for SPPFs and extend the state of the art grammar transformation operator suite to metamodel/model (grammar/graph) cotransformations.","Cotransformation; Generalised parsing; Parse graphs",,2-s2.0-84978902298
"Dai Z., Li L., Xu W.","CFO: Conditional Focused neural question answering with large-scale knowledge bases",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011824612&partnerID=40&md5=616b226c372cf14fb722538e4be2556d","How can we enable computers to automatically answer questions like ""Who created the character Harry Potter""? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions - ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions, and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions - the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Deep neural networks; Knowledge based systems; Linguistics; Natural language processing systems; Embeddings; Factoid questions; Knowledge base; Knowledge basis; Natural languages; Probabilistic framework; Question Answering; State of the art; Recurrent neural networks",2-s2.0-85011824612
"Vidyashree B.N., Kumar S.","An e-learning environment using automated development of domain ontology",2016,"Proceedings of the International Conference on e-Learning, ICEL",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979217727&partnerID=40&md5=5cfd4418744a537602a1bf18e81d7bcf","In this digital era, most people prefer using e-books, e-learning and online courses for their learning over the traditional method of using a hard copy of a book. Learning systems supported by technology i.e., e-learning uses internet to deliver and extend learning. Now-a-days e-learning has become more popular than traditional face-to-face teaching because of its flexibility. It eliminates distance and hence saves time and cost and fits into our busy schedule. But the major problem faced by people using the internet as a learning tool is to choose the relevant learning material from the bulk of resources. They spend too much time in searching learning material which adapts to their background, experience and level of expertise in a particular domain. Often they end up getting high redundancy of learning material or not so relevant resources and the same learning material gets presented to everyone without considering their background and profile. Such e-learning systems are not interactive and effective. It will be of no use if a beginner is recommended advanced topics and an advanced learner the basic ones. Since there is a lot of learning material available on the internet today, it will be difficult for an educator to recommend the right learning material instantly, taking into account student preferences and the student finds it hard to decide the right learning material which meets his need. Our system provides an interactive e-learning environment which provides an interface to understand the student's background and his level of expertise for building a student profile. Based on this, high rated learning materials of suitable difficulty level is chosen for pre-processing. It uses natural language processing techniques and data mining concepts to retrieve the most relevant learning material and suggests the pre-requisites and follow-up topics. In addition to this, our system builds the ontology automatically for the whole course. Since the student quickly grasps the pictorial representation rather than reading the context, ontology is very effective for the purpose of e-learning where there is no face-to-face interaction between the student and educator. Our system helps the educator to improve the quality of learning material by providing the feedback and ratings collected from the student about the relevancy of the recommended learning material.","E-learning; Follow-up; Learning material; Ontology; Pre-requisite; Student profiling; Tf-idf","Computer aided instruction; Data handling; Data mining; Education; Facsimile; Internet; Learning systems; Natural language processing systems; Online systems; Ontology; Students; Teaching; E-learning environment; Face-to-face interaction; Follow up; Learning materials; NAtural language processing; Pictorial representation; Pre-requisites; Tf-idf; E-learning",2-s2.0-84979217727
"Gulwani S.","Programming by examples: Applications, algorithms, and ambiguity resolution",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976614459&doi=10.1007%2f978-3-319-40229-1_2&partnerID=40&md5=cdb4b6460ace33a89696533595840a75","99% of computer end users do not know programming, and struggle with repetitive tasks. Programming by Examples (PBE) can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: (i) Restriction of the search space to an appropriate domainspecific language that offers balanced expressivity and readability (ii) A divide-and-conquer based deductive search paradigm that inductively reduces the problem of synthesizing a program of a certain kind that satisfies a given specification into sub-problems that refer to sub-programs or sub-specifications. Another challenge in PBE is to resolve the ambiguity in the example based specification. We will discuss two complementary approaches: (a) machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and (b) active-learning based user interaction models. The above concepts will be illustrated using FlashFill, FlashExtract, and FlashRelate— PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies. © Springer International Publishing Switzerland 2016.",,"Algorithms; Artificial intelligence; Learning systems; Specifications; Web browsers; Ambiguity resolution; Data manipulations; Divide and conquer; Domain specific languages; Programming by Example; Ranking technique; Technical challenges; User interaction; Computer programming",2-s2.0-84976614459
"Andreas J., Rohrbach M., Darrell T., Klein D.","Learning to compose neural networks for question answering",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993660571&partnerID=40&md5=2bd323204780ad7083555055f6866b45","We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains. ©2016 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Reinforcement learning; Assembly parameters; Benchmark datasets; Composable; Model use; Module networks; Natural languages; Question Answering; Structured knowledge; Natural language processing systems",2-s2.0-84993660571
"Ma Z., Yan L.","A review of RDF storage in NoSQL databases",2016,"Managing Big Data in Cloud Computing Environments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981161425&doi=10.4018%2f978-1-4666-9834-5.ch009&partnerID=40&md5=688756e5dad59b2bac8569b9a8bc34fc","The Resource Description Framework (RDF) is a model for representing information resources on the Web. With the widespread acceptance of RDF as the de-facto standard recommended by W3C (World Wide Web Consortium) for the representation and exchange of information on the Web, a huge amount of RDF data is being proliferated and becoming available. So RDF data management is of increasing importance, and has attracted attentions in the database community as well as the Semantic Web community. Currently much work has been devoted to propose different solutions to store large-scale RDF data efficiently. In order to manage massive RDF data, NoSQL (""not only SQL"") databases have been used for scalable RDF data store. This chapter focuses on using various NoSQL databases to store massive RDF data. An up-to-date overview of the current state of the art in RDF data storage in NoSQL databases is provided. The chapter aims at suggestions for future research. © 2016 by IGI Global. All rights reserved.",,"Database systems; Digital storage; Information management; Database community; De facto standard; Exchange of information; Information resource; Large-scale RDF datum; Resource description framework; State of the art; World wide web consortiums; Semantic Web",2-s2.0-84981161425
"Anderson P., Fernando B., Johnson M., Gould S.","SPICE: Semantic propositional image caption evaluation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990036877&doi=10.1007%2f978-3-319-46454-1_24&partnerID=40&md5=71a2ff2904a546cf354edaab166aaca3","There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?. © Springer International Publishing AG 2016.",,"Computer vision; Semantics; Automatic evaluation; Automatic metrics; Evaluation metrics; Human judgments; Image caption; N-grams; Scene graph; System levels; Circuit simulation",2-s2.0-84990036877
"Wang W.Y., Mehdad Y., Radev D.R., Stent A.","A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994176852&partnerID=40&md5=82b381adca55d910e18f0031ea8cb872","A key challenge for timeline summarization is to generate a concise, yet complete storyline from large collections of news stories. Previous studies in extractive timeline generation are limited in two ways: first, most prior work focuses on fully-observable ranking models or clustering models with hand-designed features that may not generalize well. Second, most summarization corpora are text-only, which means that text is the sole source of information considered in timeline summarization, and thus, the rich visual content from news images is ignored. To solve these issues, we leverage the success of matrix factorization techniques from recommender systems, and cast the problem as a sentence recommendation task, using a representation learning approach. To augment text-only corpora, for each candidate sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the state-of-the-art performance of the proposed text-based and multimodal approaches. ©2016 Association for Computational Linguistics.",,"Computational linguistics; Factorization; Linguistics; Network architecture; Neural networks; Clustering model; Convolutional neural network; Learning approach; Low rank approximations; Matrix factorizations; Multi-modal approach; State-of-the-art performance; Visual content; Approximation theory",2-s2.0-84994176852
"Vani K., Gupta D.","Study on extrinsic text plagiarism detection techniques and tools",2016,"Journal of Engineering Science and Technology Review",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991790927&partnerID=40&md5=61d49ea09d3d76a73bd7630851fd2765","The swift evolution of technology has facilitated the access of information through different means which has opened the doors to plagiarism. In today's world of technological outburst, plagiarism is aggravating and has become a serious concern in academia, research and many other fields. To curb this intellectual theft and to ensure academic integrity, efficient software systems to detect them are in urgent need. In this paper, a study on plagiarism is done with the focus on extrinsic text plagiarism detection, which is a fast emerging research area in this domain. The different extrinsic detection techniques and the methodologies involved are reviewed based on the current state of art. Further an overview of some of the available detection software tools, their features and detection efficiency is discussed with some of the output demos. The paper also throws light on the popular PAN competition, which is conducted yearly since 2009 in plagiarism domain and the major tasks involved in it. Further it attempts to identify the problems existing in available tools and the research gaps where immense explorations can be done. © 2016 Eastern Macedonia and Thrace Institute of Technology. All rights reserved.","Extrinsic plagiarism detection; Obfuscations; PAN systems; Software tools; Text plagiarism","Computer aided software engineering; Computer software; Detection efficiency; Detection software; Evolution of technology; Obfuscations; Pan systems; Plagiarism detection; Techniques and tools; Text plagiarism; Intellectual property",2-s2.0-84991790927
"Tschichold C., Schulze M.","Intelligent CALL and written language",2016,"The Routledge Handbook of Language Learning and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966783897&doi=10.4324%2f9781315657899&partnerID=40&md5=f712e290ef19ecd1167bede5baa92755",[No abstract available],,,2-s2.0-84966783897
"Park H., Gweon G., Heo J.","Affix modification-based bilingual pivoting method for paraphrase extraction in agglutinative languages",2016,"2016 International Conference on Big Data and Smart Computing, BigComp 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964689158&doi=10.1109%2fBIGCOMP.2016.7425914&partnerID=40&md5=cc4d13e7f1c6fea8c5c40ca201334e57","Paraphrase extraction is a task that involves the extraction of pairs of paraphrase expressions from a large-scale corpus. Because existing extraction methods are mostly designed for morphologically poor languages such as English, we present a method suited for agglutinative languages that are morphologically complex by attaching inflectional affixes to word stems. Specifically, we use the Korean language as a case study to address two types of problems that occur because existing methods model each lexical form as a separate word. The first problem is lexical data sparsity, and the second problem is not considering the morphological word structure. To mitigate these problems, we propose a novel phrasal paraphrase extraction method called affix modification-based bilingual pivoting method (AMBPM), which extends the existing bilingual pivoting method (BPM). Our experiments show that our proposed method significantly outperforms two state-of-the-art paraphrase extraction methods, namely the syntactic constraints-based bilingual pivoting method (SCBPM) and the skip-gram word embedding model with respect to meaning preservation and grammaticality of the extracted paraphrase pairs. © 2016 IEEE.","affix modification; agglutinative languages; bilingual pivoting method; paraphrase extraction","Computational linguistics; Extraction; affix modification; Agglutinative language; Data sparsity; Extraction method; Korean language; Pivoting methods; Two-state; Word structures; Big data",2-s2.0-84964689158
"Kaliakatsos-Papakostas M., Makris D., Zacharakis A., Tsougras C., Cambouropoulos E.","Learning and blending harmonies in the context of a melodic harmonisation assistant",2016,"IFIP Advances in Information and Communication Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988527708&doi=10.1007%2f978-3-319-44944-9_46&partnerID=40&md5=e908f98f9c73612412772b62aa9cd0a2","How can harmony in diverse idioms be represented in a machine learning system and how can learned harmonic descriptions of two musical idioms be blended to create new ones? This paper presents a creative melodic harmonisation assistant that employs statistical learning to learn harmonies from human annotated data in practically any style, blends the harmonies of two user-selected idioms and harmonises user-input melodies. To this end, the category theory algorithmic framework for conceptual blending is utilised for blending chord transition of the input idioms, to generate an extended harmonic idiom that incorporates a creative combination of the two input ones with additional harmonic material. The results indicate that by learning from the annotated data, the presented harmoniser is able to express the harmonic character of diverse idioms in a creative manner, while the blended harmonies extrapolate the two input idioms, creating novel harmonic concepts. © IFIP International Federation for Information Processing 2016.",,"Artificial intelligence; Blending; Learning systems; Algorithmic framework; Category theory; Harmonic materials; Harmonisation; Statistical learning; User input; Harmonic analysis",2-s2.0-84988527708
"Miura A., Neubig G., Paul M., Nakamura S.","Selecting syntactic, non-redundant segments in active learning for machine translation",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994128642&partnerID=40&md5=1d5bee71ebe68fa0f8c4b4d56d9b93dd","Active learning is a framework that makes it possible to efficiently train statistical models by selecting informative examples from a pool of unlabeled data. Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences. However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content. In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT. Experiments using both simulation and extensive manual translation by professional translators find the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more confident in their translations1. ©2016 Association for Computational Linguistics.",,"Artificial intelligence; Computational linguistics; Computer aided language translation; Linguistics; Active Learning; Bleu scores; Machine translations; Non-redundant; Translation models; Unlabeled data; Translation (languages)",2-s2.0-84994128642
"Wang B., Song Y., Sun Y., Liu J.","Improvements to online distributed monitoring systems",2016,"Proceedings - 15th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 10th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Symposium on Parallel and Distributed Processing with Applications, IEEE TrustCom/BigDataSE/ISPA 2016",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015232403&doi=10.1109%2fTrustCom.2016.0180&partnerID=40&md5=0820c123f62a3447137cdb63b35e3e01","Online monitoring, providing the real-time status information of servers, is indispensable for the management of distributed systems, e.g. failure detection and resource scheduling. The main design challenges for distributed monitoring systems include scalability, fine granularity, reliability and low overheads. And the challenges are growing with the increase of the scales of the distributed systems. To address the above problems, this paper studies improvements to online distributed monitoring systems (ODMSs) from three aspects: online compression algorithm, online compression reliability, and data representation for information interchanges. We summarize and classify the existing online compression algorithms to identify some research gaps that may represent opportunities for future research. A simple solution is proposed to address the problem that the inaccuracy of compression algorithms may be caused by some failures of distributed systems. A bitmap-like data format is presented to reduce the per-node overheads and the overheads of the management node in ODMSs, and compared with other existing formats used in the monitoring system both in mathematical analysis and practical experiment. The results show that the bitmap-like data format achieves best performance overall. © 2016 IEEE.","Distributed systems; Format; Monitoring; Online compression algorithms","Big data; Data privacy; Distributed computer systems; Information management; Network security; Online systems; Real time systems; Scheduling; Compression algorithms; Data representations; Distributed monitoring systems; Distributed systems; Format; Information interchange; Mathematical analysis; Status informations; Monitoring",2-s2.0-85015232403
"Tsvetkov Y., Dyer C.","Cross-lingual bridges with models of lexical borrowing",2016,"Journal of Artificial Intelligence Research",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958547269&partnerID=40&md5=f94a848176fc9095652cc028e8d3ce70","Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a ""donor"" language to a ""recipient"" language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and-in contrast to cognate relationships-borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili's vocabulary is borrowed from the unrelated language Arabic). In this work, we develop a model of morpho-phonological transformations across languages. Its features are based on universal constraints from Optimality Theory (OT), and we show that compared to several standard-but linguistically more naïve-baselines, our OT-inspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages. We demonstrate applications of the lexical borrowing model in machine translation, using resource-rich donor language to obtain translations of out-of-vocabulary loanwords in a lower resource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines. ©2016 AI Access Foundation.",,"Computational linguistics; Cost effectiveness; Linguistics; Speech; Cost effective strategies; Cross-lingual; Lexical information; Machine translations; Optimality theories; Resource-Rich; Training example; Translation (languages)",2-s2.0-84958547269
"Melero M., Costa-Jussà M.R., Lambert P., Quixal M.","Selection of correction candidates for the normalization of Spanish user-generated content",2016,"Natural Language Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949544116&doi=10.1017%2fS1351324914000011&partnerID=40&md5=faf2c5f207424a202806b4d62556c469","We present research aiming to build tools for the normalization of User-Generated Content (UGC). We argue that processing this type of text requires the revisiting of the initial steps of Natural Language Processing, since UGC (micro-blog, blog, and, generally, Web 2.0 user-generated texts) presents a number of nonstandard communicative and linguistic characteristics - often closer to oral and colloquial language than to edited text. We present a corpus of UGC text in Spanish from three different sources: Twitter, consumer reviews, and blogs, and describe its main characteristics. We motivate the need for UGC text normalization by analyzing the problems found when processing this type of text through a conventional language processing pipeline, particularly in the tasks of lemmatization and morphosyntactic tagging. Our aim with this paper is to seize the power of already existing spell and grammar correction engines and endow them with automatic normalization capabilities in order to pave the way for the application of standard Natural Language Processing tools to typical UGC text. Particularly, we propose a strategy for automatically normalizing UGC by adding a module on top of a pre-existing spell-checker that selects the most plausible correction from an unranked list of candidates provided by the spell-checker. To build this selector module we train four language models, each one containing a different type of linguistic information in a trade-off with its generalization capabilities. Our experiments show that the models trained on truecase and lowercase word forms are more discriminative than the others at selecting the best candidate. We have also experimented with a parametrized combination of the models by both optimizing directly on the selection task and doing a linear interpolation of the models. The resulting parametrized combinations obtain results close to the best performing model but do not improve on those results, as measured on the test set. The precision of the selector module in ranking number one the expected correction proposal on the test corpora reaches 82.5% for Twitter text (baseline 57%) and 88% for non-Twitter text (baseline 64%). Copyright © Cambridge University Press 2014.",,"Computational linguistics; Economic and social effects; Internet; Linguistics; Natural language processing systems; Social networking (online); Application of standards; Automatic normalization; Generalization capability; Morphosyntactic tagging; NAtural language processing; Natural Language Processing Tools; User generated content (UGC); User-generated content; Pipeline processing systems",2-s2.0-84949544116
"Rompf T.","The essence of Multi-stage evaluation in LMS",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973865258&doi=10.1007%2f978-3-319-30936-1_17&partnerID=40&md5=ab062c370d782d04bc82e9e57a66bf28","Embedded domain-specific languages (DSLs) are the subject of wide-spread interest, and a variety of implementation techniques exist. Some of them have been invented, and some of them discovered. Many are based on a form of generative or multi-stage programming, where the host language program builds up DSL terms during its evaluation. In this paper, we examine the execution model of LMS (Lightweight Modular Staging), a framework for embedded DSLs in Scala, and link it to evaluation in a two-stage lambda calculus. This clarifies the semantics of certain ad-hoc implementation choices, and provides guidance for implementing similar multi-stage evaluation facilities in other languages. © Springer International Publishing Switzerland 2016.","Domain-specific languages; LMS (Lightweight Modular Staging); Multi-stage programming; Partial evaluation; Scala","Calculations; Computational linguistics; Computer programming languages; Differentiation (calculus); Problem oriented languages; Semantics; XML; Domain specific languages; LMS (Lightweight Modular Staging); Multi-stage programming; Partial evaluation; Scala; Stages",2-s2.0-84973865258
"Yang S., Gao Q., Liu C., Xiong C., Zhu S.-C., Chai J.Y.","Grounded semantic role labeling",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994091982&partnerID=40&md5=3a604d8baf34380530d424c5a43e81ec","Semantic Role Labeling (SRL) captures semantic roles (or participants) such as agent, patient, and theme associated with verbs from the text. While it provides important intermediate semantic representations for many traditional NLP tasks (such as information extraction and question answering), it does not capture grounded semantics so that an artificial agent can reason, learn, and perform the actions with respect to the physical environment. To address this problem, this paper extends traditional SRL to grounded SRL where arguments of verbs are grounded to participants of actions in the physical world. By integrating language and vision processing through joint inference, our approach not only grounds explicit roles, but also grounds implicit roles that are not explicitly mentioned in language descriptions. This paper describes our empirical results and discusses challenges and future directions. ©2016 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Natural language processing systems; Artificial agents; Language description; Physical environments; Question Answering; Semantic representation; Semantic role labeling; Semantic roles; Vision processing; Semantics",2-s2.0-84994091982
"Sempéré G., Philippe F., Dereeper A., Ruiz M., Sarah G., Larmande P.","Gigwa-Genotype investigator for genomewide analyses",2016,"GigaScience",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991527302&doi=10.1186%2fs13742-016-0131-8&partnerID=40&md5=131422bc5a7b6fce4cf63ebb087f6e2d","Background: Exploring the structure of genomes and analyzing their evolution is essential to understanding the ecological adaptation of organisms. However, with the large amounts of data being produced by next-generation sequencing, computational challenges arise in terms of storage, search, sharing, analysis and visualization. This is particularly true with regards to studies of genomic variation, which are currently lacking scalable and user-friendly data exploration solutions. Description: Here we present Gigwa, a web-based tool that provides an easy and intuitive way to explore large amounts of genotyping data by filtering it not only on the basis of variant features, including functional annotations, but also on genotype patterns. The data storage relies on MongoDB, which offers good scalability properties. Gigwa can handle multiple databases and may be deployed in either single- or multi-user mode. In addition, it provides a wide range of popular export formats. Conclusions: The Gigwa application is suitable for managing large amounts of genomic variation data. Its user-friendly web interface makes such processing widely accessible. It can either be simply deployed on a workstation or be used to provide a shared data portal for a given community of researchers. © 2016 The Author(s).","Genomic variations; HapMap; INDEL; MongoDB; NoSQL; SNP; VCF; Web interface","access to information; computer interface; data processing; evolutionary adaptation; genetic variation; genome; genotype; molecular evolution; next generation sequencing; Note; online system; priority journal; biology; DNA sequence; genetic database; genetic variation; genome-wide association study; genotype; information retrieval; Internet; procedures; software; Computational Biology; Databases, Genetic; Genetic Variation; Genome-Wide Association Study; Genotype; Information Storage and Retrieval; Internet; Sequence Analysis, DNA; Software; User-Computer Interface",2-s2.0-84991527302
"Vani K., Gupta D.","Study on extrinsic text plagiarism detection techniques and tools",2016,"Journal of Engineering Science and Technology Review",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006760736&partnerID=40&md5=c610ec12382cc8ceb54412bab23e2c31","The swift evolution of technology has facilitated the access of information through different means which has opened the doors to plagiarism. In today's world of technological outburst, plagiarism is aggravating and has become a serious concern in academia, research and many other fields. To curb this intellectual theft and to ensure academic integrity, efficient software systems to detect them are in urgent need. In this paper, a study on plagiarism is done with the focus on extrinsic text plagiarism detection, which is a fast emerging research area in this domain. The different extrinsic detection techniques and the methodologies involved are reviewed based on the current state of art. Further an overview of some of the available detection software tools, their features and detection efficiency is discussed with some of the output demos. The paper also throws light on the popular PAN competition, which is conducted yearly since 2009 in plagiarism domain and the major tasks involved in it. Further it attempts to identify the problems existing in available tools and the research gaps where immense explorations can be done. © 2016 Eastern Macedonia and Thrace Institute of Technology.","Extrinsic Plagiarism Detection; Obfuscations; PAN systems; Software Tools; Text Plagiarism","Computer aided software engineering; Computer software; Detection efficiency; Detection software; Evolution of technology; Obfuscations; Pan systems; Plagiarism detection; Techniques and tools; Text Plagiarism; Intellectual property",2-s2.0-85006760736
"Preston Carman E., Jr., Westmann T., Borkar V.R., Carey M.J., Tsotras V.J.","A scalable parallel XQuery processor",2015,"Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963750584&doi=10.1109%2fBigData.2015.7363753&partnerID=40&md5=053216b1c4f569107f1beeea383d370a","The wide use of XML for document management and data exchange has created the need to query large repositories of XML data. To efficiently query such large data and take advantage of parallelism, we have implemented Apache VXQuery, an open-source scalable XQuery processor. The system builds upon two other open-source frameworks: Hyracks, a parallel execution engine, and Algebricks, a language agnostic compiler toolbox. Apache VXQuery extends these frameworks and provides an implementation of the XQuery specifics (data model, data-model dependent functions and optimizations, and a parser). We describe the architecture of Apache VXQuery, its integration with Hyracks and Algebricks, and the XQuery optimization rules applied to the query plan to improve path expression efficiency and to enable query parallelism. An experimental evaluation using a real 500GB dataset with various selection, aggregation and join XML queries shows that Apache VXQuery performs well both in terms of scale-up and speed-up. Our experiments show that it is about 3.5x faster than Saxon (an open-source and commercial XQuery processor) on a 4-core, single node implementation, and around 2.5x faster than Apache MRQL (a MapReduce-based parallel query processor) on an eight (4-core) node cluster. © 2015 IEEE.",,"Electronic data interchange; Information services; Multiprocessing systems; Open systems; Parallel processing systems; XML; Dependent functions; Document management; Experimental evaluation; Open source frameworks; Optimization rules; Parallel executions; Parallel queries; Query parallelism; Big data",2-s2.0-84963750584
"Belyaev K., Ray I.","Towards efficient dissemination and filtering of XML data streams",2015,"Proceedings - 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964221907&doi=10.1109%2fCIT%2fIUCC%2fDASC%2fPICOM.2015.278&partnerID=40&md5=d95cb8deaa54092a487c5dbc00f7bdb0","The vast amounts of data generated in near realtime due to prolific use of sensors, pervasive usage of mobile Internet, and popularity of social media platforms, necessitates the efficient dissemination of the semi-structured streaming data to the consuming applications. Towards this end, we introduce the subscriber-centric XML filtering approach for seamless and efficient XML stream replication/distribution mechanism. The subscriber-centric filtering architecture can be configured to support different topologies in order to support efficient message filtering for a large number of concurrent subscribers. It allows selective filtering on the various nodes that improves efficiency and provides applications with data on a need-to-know basis. Moreover, it supports interoperability and allows semi-structured streams generated from multiple sources to be filtered. Our XML filtering network consists of decoupled data producers, message transformation agents and XML brokers that can be deployed in conventional data centers as well as in the public cloud environment We provide detailed performance results of processing filtering queries in several use case scenarios with varying XML message loads and number of nodes involved in the replication/dissemination process. Our results indicate that the subscriber-centric XML filtering architecture is a viable approach for disseminating semi-structured data streams to the various consuming applications. © 2015 IEEE.",,"Data communication systems; Media streaming; Metadata; Network architecture; XML; Filtering architectures; Message transformation; Multiple source; Selective filtering; Semi structured data; Social media platforms; Use case scenario; Xml data streams; Ubiquitous computing",2-s2.0-84964221907
"Mittal S., Joshi K.P., Pearce C., Joshi A.","Parallelizing natural language techniques for knowledge extraction from cloud service level agreements",2015,"Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963756831&doi=10.1109%2fBigData.2015.7364092&partnerID=40&md5=8535c4e4b6083c5c566899333d3a8793","To efficiently utilize their cloud based services, consumers have to continuously monitor and manage the Service Level Agreements (SLA) that define the service performance measures. Currently this is still a time and labor intensive process since the SLAs are primarily stored as text documents. We have significantly automated the process of extracting, managing and monitoring cloud SLAs using natural language processing techniques and Semantic Web technologies. In this paper we describe our prototype system that uses a Hadoop cluster to extract knowledge from unstructured legal text documents. For this prototype we have considered publicly available SLA/terms of service documents of various cloud providers. We use established natural language processing techniques in parallel to speed up cloud legal knowledge base creation. Our system considerably speeds up knowledge base creation and can also be used in other domains that have unstructured data. © 2015 IEEE.","Data Mining; Distributed Systems; Knowledge Extraction","Computational linguistics; Data mining; Extraction; Knowledge based systems; Laws and legislation; Natural language processing systems; Semantic Web; Distributed systems; Knowledge extraction; Labor intensive process; NAtural language processing; Natural language techniques; Semantic Web technology; Service Level Agreements; Service performance; Big data",2-s2.0-84963756831
"Ghansah B., Wu S., Ghansah N.","Rankboost-based result merging",2015,"Proceedings - 15th IEEE International Conference on Computer and Information Technology, CIT 2015, 14th IEEE International Conference on Ubiquitous Computing and Communications, IUCC 2015, 13th IEEE International Conference on Dependable, Autonomic and Secure Computing, DASC 2015 and 13th IEEE International Conference on Pervasive Intelligence and Computing, PICom 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964211536&doi=10.1109%2fCIT%2fIUCC%2fDASC%2fPICOM.2015.136&partnerID=40&md5=6a5f43b5c9b25402737d012074ff8152","The explosion of searchable text content especially on the web has rendered information to be distributed among many disjoint text information sources (Federated Search). How to merge the results returned by selected sources is a major problem of the Federated Search task. We study the problem of learning to rank a set of objects by combining various sources of ranking. The problem of merging search results arises in several domains, for example combining the results of different verticals and also Meta search applications. This paper presents a supervised learning solution to the result merging problem. Our approach combines multiple sources of evidence to inform the merging decision. We use the Rankboost Method, a boosting approach to machine learning which learns a function that merges results based on information that is readily available: i.e. the ranks, titles, summaries, URLs and click-through data, which are found in the results pages. We combine these evidences by treating result merging as a multiclass machine learning problem. By not downloading additional information such as the full document, we decrease processing cost in terms of bandwidth usage and latency. We compare our results against existing result merging methods which rely on evidence found only in ranked lists; Semi-Supervised Learning (SSL), Sample-Agglomerate Fitting Estimate (SAFE) and CORI. An extensive set of experiments demonstrates that our method is more effective than the baseline result-merging algorithm under a variety of conditions. © 2015 IEEE.","Distributed information retrieval; Information retrieval; Machine learning; Rankboost; Result merging","Adaptive boosting; Artificial intelligence; Information retrieval; Learning algorithms; Learning systems; Supervised learning; Ubiquitous computing; Boosting approach; Clickthrough data; Distributed information retrieval; Machine learning problem; Rankboost; Result merging; Semi-supervised learning (SSL); Text information; Merging",2-s2.0-84964211536
"Choudhary N., Gore S.","Impact of intellisense on the accuracy of Natural Language Interface to Database",2015,"2015 4th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961773675&doi=10.1109%2fICRITO.2015.7359310&partnerID=40&md5=dca9078e205a77a0b10a74475b2631f6","Natural Language Interface to Database (NLIDB) is an interface which allows a naive user to ask query to database in his own language. The interface then translates the natural language query into SQL query and retrieves data from the database. NLIDB is not a recently rising topic. Many different approaches have been proposed in past for the development of an NLIDB but none could be used commercially. The main reasons are that the highest accuracy achieved is 96% and not all the concepts of SQL are mapped to them. Even a 4% error can cause massive damage to a business. Thus this paper proposes the design of GUI, implementation of intellisense and an error handling module for Natural Language Interface to Database to achieve 100% accuracy. We have enhanced our intellisense through machine learning so that it could generate suggestions based on previously typed words to help frame a complete and correct query. Traditional intellisenses show list of words whose initial letters are typed by the user and thus help in typing correct words but not correct query. © 2015 IEEE.","error handling; intellisense; interface design; NLIDB; SQL","Artificial intelligence; Computational linguistics; Database systems; Errors; Learning systems; Natural language processing systems; Error handling; IntelliSense; Interface designs; Massive damage; Natural language interface to database; Natural language queries; NLIDB; SQL query; Query processing",2-s2.0-84961773675
"Yang M.-C., Lee D.-G., Park S.-Y., Rim H.-C.","Knowledge-based question answering using the semantic embedding space",2015,"Expert Systems with Applications",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940641418&doi=10.1016%2fj.eswa.2015.07.009&partnerID=40&md5=89d416891933f27b38b62ee23c90ed0c","Semantic transformation of a natural language question into its corresponding logical form is crucial for knowledge-based question answering systems. Most previous methods have tried to achieve this goal by using syntax-based grammar formalisms and rule-based logical inference. However, these approaches are usually limited in terms of the coverage of the lexical trigger, which performs a mapping task from words to the logical properties of the knowledge base, and thus it is easy to ignore implicit and broken relations between properties by not interpreting the full knowledge base. In this study, our goal is to answer questions in any domains by using the semantic embedding space in which the embeddings encode the semantics of words and logical properties. In the latent space, the semantic associations between existing features can be exploited based on their embeddings without using a manually produced lexicon and rules. This embedding-based inference approach for question answering allows the mapping of factoid questions posed in a natural language onto logical representations of the correct answers guided by the knowledge base. In terms of the overall question answering performance, our experimental results and examples demonstrate that the proposed method outperforms previous knowledge-based question answering baseline methods with a publicly released question answering evaluation dataset: WebQuestions. © 2015 Elsevier Ltd.","Distributional semantics; Embedding model; Knowledge base; Labeled-LDA; Neural networks; Question answering","Artificial intelligence; Computational linguistics; Mapping; Natural language processing systems; Neural networks; Semantics; Distributional semantics; Knowledge base; Labeled-LDA; Natural language questions; Question Answering; Question answering systems; Question-answering evaluation; Semantic transformation; Knowledge based systems",2-s2.0-84940641418
"Azizan A., Bakar Z.A.","Query reformulation for specific domain search: Keywords, ontology, domain name",2015,"International Conference on ICT Convergence 2015: Innovations Toward the IoT, 5G, and Smart Media Era, ICTC 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964873479&doi=10.1109%2fICTC.2015.7354797&partnerID=40&md5=a21e68dfb92145ceb7f39b2226f7de0c","In most cases, users are unable to precisely translate their information needs into a query format for the search system to process. Users often submit queries containing terms or keywords that do not match with their intended information. That is why user normally reformulates queries several times to gain more information and relevant set of results. The aim of this paper is to present evaluation of six techniques employed in query reformulation process. This work proposes query keyword, ontology and domain name, as well as the combination of those elements in reformulating query. The experimental result shows that query keyword element is much more important than ontology terms element. Furthermore, ontology terms and domain name are insufficient to be used alone, as it had incredibly increases the recall and decreases the precision. However combination of ontology terms and domain name with query keywords had produced better results. © 2015 IEEE.","Domain name; Durian; Keyword; Ontology; Query reformulation","Ontology; Domain names; Durian; Keyword; Ontology terms; Query formats; Query reformulation; Search system; Search engines",2-s2.0-84964873479
"Abboud A., Backurs A., Williams V.V.","If the Current Clique Algorithms are Optimal, so is Valiant's Parser",2015,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960341224&doi=10.1109%2fFOCS.2015.16&partnerID=40&md5=e03964ffc6eb79f2e77c8ca2adb9120b","The CFG recognition problem is: given a context-free grammar G and a string w of length n, decide if w can be obtained from G. This is the most basic parsing question and is a core computer science problem. Valiant's parser from 1975 solves the problem in O(nO) time, where ? < 2:373 is the matrix multiplication exponent. Dozens of parsing algorithms have been proposed over the years, yet Valiant's upper bound remains unbeaten. The best combinatorial algorithms have mildly subcubic O(n3= log3 n) complexity. Lee (JACM'01) provided evidence that fast matrix multiplication is needed for CFG parsing, and that very efficient and practical algorithms might be hard or even impossible to obtain. Lee showed that any algorithm for a more general parsing problem with running time O(|G| n3 - e) can be converted into a surprising subcubic algorithm for Boolean Matrix Multiplication. Unfortunately, Lee' s hardness result required that the grammar size be |G| = O(n6). Nothing was known for the more relevant case of constant size grammars. In this work, we prove that any improvement on Valiant' s algorithm, even for constant size grammars, either in terms of runtime or by avoiding the inefficiencies of fast matrix multiplication, would imply a breakthrough algorithm for the k-Clique problem: given a graph on n nodes, decide if there are k that form a clique. Besides classifying the complexity of a fundamental problem, our reduction has led us to similar lower bounds for more modern and well-studied cubic time problems for which faster algorithms are highly desirable in practice: RNA Folding, a central problem in computational biology, and Dyck Language Edit Distance, answering an open question of Saha (FOCS'14). © 2015 IEEE.","CFG recognition; clique; combinatorial algorithms; conditional lower bounds; context free grammars; Dyck Edit Distance; hardness in P; natural language processing; parameterized complexity; parsing; RNA folding; tight hardness; Valiant's algorithm","Algorithms; Combinatorial mathematics; Context free grammars; Context free languages; Formal languages; Hardness; Matrix algebra; Natural language processing systems; RNA; CFG recognition; clique; Combinatorial algorithm; Edit distance; Lower bounds; NAtural language processing; Parameterized complexity; parsing; RNA folding; Bioinformatics",2-s2.0-84960341224
"Saha B.","Language Edit Distance and Maximum Likelihood Parsing of Stochastic Grammars: Faster Algorithms and Connection to Fundamental Graph Problems",2015,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960469130&doi=10.1109%2fFOCS.2015.17&partnerID=40&md5=85916df05d24bf267ba36ff1117f2896","Given a context free language G over alphabet Σ and a string s, the language edit distance problem seeks the minimum number of edits (insertions, deletions and substitutions) required to convert s into a valid member of the language L(G). The well-known dynamic programming algorithm solves this problem in cubic time in string length [Aho, Peterson 1972, Myers 1985]. Despite its numerous applications, to date there exists no algorithm that computes the exact or approximate language edit distance problem in true sub cubic time. In this paper we give the first such truly sub-cubic algorithm that computes language edit distance almost optimally. We further solve the local alignment problem, for all substrings of s, we can estimate their language edit distance near-optimally in same time with high probability. Next, we design the very first sub cubic algorithm that given an arbitrary stochastic context free grammar, and a string returns a nearly-optimal maximum likelihood parsing of that string. Stochastic context free grammars significantly generalize hidden Markov models, they lie at the foundation of statistical natural language processing, and have found widespread applications in many other fields. To complement our upper bound result, we show that exact computation of maximum likelihood parsing of stochastic grammars or language edit distance in true sub cubic time will imply a truly sub cubic algorithm for all-pairs shortest paths, a long-standing open question. This will result in a breakthrough for a large range of problems in graphs and matrices due to sub cubic equivalence. By a known lower bound result [Lee 2002], and a recent development [Abboud et al. 2015] even the much simpler problem of parsing a context free grammar requires fast matrix multiplication time. Therefore any nontrivial multiplicative approximation algorithms for either of the two problems in time less than matrix-multiplication are unlikely to exist. © 2015 IEEE.","approximation algorithms; context free grammar; edit distance; fast algorithm","Algorithms; Approximation algorithms; Context free grammars; Dynamic programming; Formal languages; Hidden Markov models; Markov processes; Matrix algebra; Maximum likelihood; Natural language processing systems; Stochastic models; Stochastic systems; All pairs shortest paths; Dynamic programming algorithm; Edit distance; Fast algorithms; Fast matrix multiplication; Multiplicative approximations; Statistical natural language processing; Stochastic context free grammar; Context free languages",2-s2.0-84960469130
"Banerjee P., Rubino R., Roturier J., van Genabith J.","Quality estimation-guided supplementary data selection for domain adaptation of statistical machine translation",2015,"Machine Translation",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937817632&doi=10.1007%2fs10590-014-9165-9&partnerID=40&md5=c61daeed4b190b11c9b656e095dfb056","The problem of domain adaptation in statistical machine translation systems emanates from the fundamental assumption that test and training data are drawn independently from the same distribution (topic, domain, genre, style etc.). In real-life translation tasks, the sparseness of in-domain parallel training data often leads to poor model estimation, and consequentially poor translation quality. Domain adaptation by supplementary data selection aims at addressing this specific issue by selecting relevant parallel training data from out-of-domain or general-domain bi-text to enhance the quality of a poor baseline system. State-of-the-art research in data selection focuses on the development of novel similarity measures to improve the relevance of selected data. However, in this paper we approach the problem from a different perspective. In contrast to the conventional approach of using the entire available target-domain data as a reference for supplementary data selection, we restrict the reference set to only those sentences that are expected to be poorly translated by the baseline MT system using a Quality Estimation model. Our rationale is to focus help (i.e. supplementary training material) to where it is needed most. Automatic quality estimation techniques are used to identify such poorly translated sentences in the target domain. The experiments reported in this paper show that (i) this technique provides statistically significant improvements over the unadapted baseline translation and (ii) using significantly smaller amounts of supplementary data our approach achieves results comparable to state-of-the-art approaches using conventional reference sets. © 2014, Springer Science+Business Media Dordrecht.","Automatic quality estimation; Data selection; Domain adaptation; Statistical machine translation","Computational linguistics; Computer aided language translation; Conventional approach; Data Selection; Domain adaptation; Quality estimation; State-of-the-art approach; Statistical machine translation; Statistical machine translation system; Translation quality; Data reduction",2-s2.0-84937817632
"Ta C.D.C., Thi T.P.","An approach for searching semantic-based keywords over relational database",2015,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959311131&doi=10.1145%2f2833258.2833263&partnerID=40&md5=1498acf990f041d3a8b113410d5f2814","Ontologies apply to many applications in recent years, especially on the semantic web, information retrieval, information extraction, and question answering. The purpose of domain-specific ontology is to get rid of conceptual and terminological confusion. It accomplishes this by specifying a set of generic concepts that characterizes the domain as well as their definitions and interrelationships. There are some languages in order to represent ontologies, such as RDF, OWL. However, these languages are only suitable with ontologies having a small data. It usually uses a database for representing ontologies having big data. However, most of the databases do not sufficiently support the semantic orientated search by Structured Query Language (SQL). Therefore, this paper introduces an approach for semantic-based keyword search over relational databases. This approach can be applied to any relational database system. © 2015 ACM.","Keyword search; Search over relational database; Semantic search","Big data; Computational linguistics; Information retrieval; Natural language processing systems; Query languages; Query processing; Relational database systems; Search engines; Domain-specific ontologies; Keyword search; Question Answering; Relational Database; Semantic search; Small data; Structured query languages; Semantic Web",2-s2.0-84959311131
"Kurc T., Qi X., Wang D., Wang F., Teodoro G., Cooper L., Nalisnik M., Yang L., Saltz J., Foran D.J.","Scalable analysis of Big pathology image data cohorts using efficient methods and high-performance computing strategies",2015,"BMC Bioinformatics",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949254575&doi=10.1186%2fs12859-015-0831-6&partnerID=40&md5=c47622d281241c2a1954e79afb3ba658","Background: We describe a suite of tools and methods that form a core set of capabilities for researchers and clinical investigators to evaluate multiple analytical pipelines and quantify sensitivity and variability of the results while conducting large-scale studies in investigative pathology and oncology. The overarching objective of the current investigation is to address the challenges of large data sizes and high computational demands. Results: The proposed tools and methods take advantage of state-of-the-art parallel machines and efficient content-based image searching strategies. The content based image retrieval (CBIR) algorithms can quickly detect and retrieve image patches similar to a query patch using a hierarchical analysis approach. The analysis component based on high performance computing can carry out consensus clustering on 500,000 data points using a large shared memory system. Conclusions: Our work demonstrates efficient CBIR algorithms and high performance computing can be leveraged for efficient analysis of large microscopy images to meet the challenges of clinically salient applications in pathology. These technologies enable researchers and clinical investigators to make more effective use of the rich informational content contained within digitized microscopy specimens. Â© 2015 Kurc et al.","Databases; gPUs; High performance computing","Cluster analysis; Content based retrieval; Database systems; Image retrieval; Pathology; Program processors; Query processing; Computational demands; Consensus clustering; Content based images; Content-Based Image Retrieval; GPUs; Hierarchical analysis; High performance computing; Shared memory system; Image analysis; consensus; human; human tissue; image retrieval; machine; memory; microscopy; pathology; scientist; algorithm; automated pattern recognition; cancer grading; cluster analysis; computer assisted diagnosis; devices; diagnostic imaging; information retrieval; male; procedures; prostate tumor; tissue microarray; Algorithms; Cluster Analysis; Diagnostic Imaging; Humans; Image Interpretation, Computer-Assisted; Information Storage and Retrieval; Male; Neoplasm Grading; Pattern Recognition, Automated; Prostatic Neoplasms; Tissue Array Analysis",2-s2.0-84949254575
"Yogeswara Rao K., Nageswara Rao P.V.","Topic ontology assisted multi-document summary generation",2015,"International Journal of Applied Engineering Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951873366&partnerID=40&md5=0afd73766ef9413cd93a7d9578838eaa","An effective Multiple Document Summarization (MDS) system is a sound method to provide concise and comprehensive information in a short-form. The conventional summarization techniques exploit the machine learning techniques based sentence extraction and sentence position hypothesis to summarize the huge document collection under the same topic. However, these techniques may lead the redundant and less informative sentences in the summary due to the lack of semantic analysis. To tackle this constraint, this paper attempts to exploit the ontology and word position hypothesis. This paper proposes multi-document SummarY generatioN with the tOPic ontology asSIStance (SYNOPSIS) approach. The core aim of this approach is to balance the objective function that refers to improve the coherence, and salience of the summary and diminish the redundancy of the sentences. To achieve this aim, the SYNOPSIS model investigates, the two phases, namely optimal sentence ranking and sentence selection in MDS system. Initially, the SYNOPSIS uses Yago ontology to identify the context of the keyword semantically and employs the word position hypothesis to discover the importance of the sentence by ranking the document sentences. To further reduce the document content, the SYNOPSIS approach focuses on shortening the sentence length by applying the structure analysis of the original sentences. Finally, it selects the key sentences based on the most relevant information on sentence rank and constructs the summary based on the satisfaction of the objective function. The experimental results demonstrate that the SYNOPSIS approach achieves better performance than the conventional summarization method. © Research India Publications.","And redundancy; Coherence; Document summarization; Ontology; Salience; Summary; Word position",,2-s2.0-84951873366
"Chen W.-H., Yang Y.-C., Wang Y.-H., Li C.-M., Lin K.-Y., Lou J.-C.","Effect of molecular characteristics on the formation of nitrosamines during chlor(am)ination of phenylurea herbicides",2015,"Environmental Sciences: Processes and Impacts",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948745116&doi=10.1039%2fc5em00308c&partnerID=40&md5=6eb767fadf1bfc1a81e22a8a9dc8d628","The objective of this study was to investigate the formation of different nitrosamines during chlorination or chloramination (chlor(am)ination) of five phenylurea herbicides (fluometuron, diuron, linuron, metobromuron, and propanil), with the effects of disinfection approaches, additional inorganic nitrogen, and reaction pH being studied. By analyzing six nitrosamines, N-nitrosodimethylamine (NDMA) and N-nitrosopyrrolidine (NPYR) formation was observed. The dimethylamine functional group was the key to determining whether a particular phenylurea herbicide was an important nitrosamine precursor, as the NDMA conversion ratio was much higher. Chlorination with ammonium or dichloramination enhanced the NDMA formation. NPYR formation from the herbicides that did not form NDMA was detected and was more vigorous during dichloramination or in the presence of either ammonium or nitrite. The NPYR formation was possibly related to the aniline molecular fragment from the phenylurea herbicides. Both NDMA and NPYR formation were higher at pH 8. Overall, the maximum nitrosamine conversions decreased in the order: fluometuron &gt; diuron &gt; propanil &gt; metobromuron &gt; linuron (up to 0.99%, 0.46%, 0.005%, 0.004%, and 0.003% molar conversion rates, respectively) during chlorination or chloramination and dichloramine &gt; free chlorine &gt; monochloramine (up to 0.99%, 0.41%, and 0.005% molar conversion rates, respectively) for given herbicide, chlorine, and nitrogen doses. Applying the results of this study, phenylurea herbicide concentrations ranging from several to tens of μg L-1 will yield a NDMA concentration in drinking water above the level for a theoretical 10-6 lifetime cancer risk. NPYR formation will increase the risk of these phenylurea herbicide concentrations to downstream water users. The true adverse environmental impacts of these phenylurea herbicides are important to emphasize given their high loadings as non-point source pollutants and their typical environmental scenarios (e.g., at neutral pH or with the co-occurrence of inorganic nitrogen), likely resulting in more efficient nitrosamine formation. © 2015 The Royal Society of Chemistry.",,"ammonia; carbanilamide derivative; chlorine; dimethylamine; dimethylnitrosamine; diuron; drinking water; fluometuron; herbicide; linuron; metobromuron; monochloramine; n nitrosopyrrolidine; nitrogen; nitrosamine; propanil; tosylchloramide sodium; disinfectant agent; herbicide; nitrosamine; water pollutant; aqueous solution; Article; cancer risk; chemical reaction kinetics; chemical structure; chloramination; chlorination; disinfection; environmental impact; priority journal; waste water management; water pollutant; water quality; chemistry; disinfection; procedures; water management; Disinfectants; Disinfection; Herbicides; Nitrosamines; Water Pollutants, Chemical; Water Purification",2-s2.0-84948745116
"Minkov E.","Event extraction using structured learning and rich domain knowledge: Application across domains and data sources",2015,"ACM Transactions on Intelligent Systems and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952946870&doi=10.1145%2f2801131&partnerID=40&md5=67ccc21d3b964b668a46bd1d4715fe3c","We consider the task of record extraction from text documents, where the goal is to automatically populate the fields of target relations, such as scientific seminars or corporate acquisition events. There are various inferences involved in the record-extraction process, including mention detection, unification, and field assignments. We use structured learning to find the appropriate field-value assignments. Unlike previous works, the proposed approach generates feature-rich models that enable the modeling of domain semantics and structural coherence at all levels and across fields. Given labeled examples, such an approach can, for instance, learn likely event durations and the fact that start times should come before end times. While the inference space is large, effective learning is achieved using a perceptron-style method and simple, greedy beam decoding. A main focus of this article is on practical aspects involved in implementing the proposed framework for real-world applications. We argue and demonstrate that this approach is favorable in conditions of data shift, a real-world setting in which models learned using a limited set of labeled examples are applied to examples drawn from a different data distribution. Much of the framework's robustness is attributed to the modeling of domain knowledge. We describe design and implementation details for the case study of seminar event extraction from email announcements, and discuss design adaptations across different domains and text genres. © 2015 ACM.","Beam search; Domain knowledge; Information extraction; Perceptron; Structured learning; Template filling","Information analysis; Information retrieval; Neural networks; Semantics; Beam search; Design adaptations; Design and implementations; Domain knowledge; Effective learning; Extraction process; Structural coherence; Structured learning; Data mining",2-s2.0-84952946870
"Nesi P., Pantaleo G., Sanesi G.","A hadoop based platform for natural language processing of web pages and documents",2015,"Journal of Visual Languages and Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949532156&doi=10.1016%2fj.jvlc.2015.10.017&partnerID=40&md5=77021a30e786f3f0bb9b2f830cd091f7","The rapid and extensive pervasion of information through the web has enhanced the diffusion of a huge amount of unstructured natural language textual resources. A great interest has arisen in the last decade for discovering, accessing and sharing such a vast source of knowledge. For this reason, processing very large data volumes in a reasonable time frame is becoming a major challenge and a crucial requirement for many commercial and research fields. Distributed systems, computer clusters and parallel computing paradigms have been increasingly applied in the recent years, since they introduced significant improvements for computing performance in data-intensive contexts, such as Big Data mining and analysis. Natural Language Processing, and particularly the tasks of text annotation and key feature extraction, is an application area with high computational requirements; therefore, these tasks can significantly benefit of parallel architectures. This paper presents a distributed framework for crawling web documents and running Natural Language Processing tasks in a parallel fashion. The system is based on the Apache Hadoop ecosystem and its parallel programming paradigm, called MapReduce. In the specific, we implemented a MapReduce adaptation of a GATE application and framework (a widely used open source tool for text engineering and NLP). A validation is also offered in using the solution for extracting keywords and keyphrase from web documents in a multi-node Hadoop cluster. Evaluation of performance scalability has been conducted against a real corpus of web pages and documents. © 2015 Elsevier Ltd.","Big Data Mining; Distributed systems; Hadoop; Natural language processing; Parallel computing; Part-of-speech tagging; Text parsing; Web crawling",,2-s2.0-84949532156
"Paustenbach D.J., Winans B., Novick R.M., Green S.M.","The toxicity of crude 4-methylcyclohexanemethanol (MCHM): Review of experimental data and results of predictive models for its constituents and a putative metabolite",2015,"Critical Reviews in Toxicology",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945965909&doi=10.3109%2f10408444.2015.1076376&partnerID=40&md5=ca2eb8a0e8b996a2a73455a7aa2d19db","Crude 4-methylcyclohexanemethanol (MCHM) is an industrial solvent used to clean coal. Approximately 10 000 gallons of a liquid mixture containing crude MCHM were accidently released into the Elk River in West Virginia in January 2014. Because of the proximity to a water treatment facility, the contaminated water was distributed to approximately 300 000 residents. In this review, experimental data and computational predictions for the toxicity for crude MCHM, distilled MCHM, its other components and its putative metabolites are presented. Crude MCHM, its other constituents and its metabolites have low to moderate acute and subchronic oral toxicity. Crude MCHM has been shown not to be a skin sensitizer below certain doses, indicating that at plausible human exposures it does not cause an allergic response. Crude MCHM and its constituents cause slight to moderate skin and eye irritation in rodents at high concentrations. These chemicals are not mutagenic and are not predicted to be carcinogenic. Several of the constituents were predicted through modeling to be possible developmental toxicants; however, 1,4-cyclohexanedimethanol, 1,4-cyclohexanedicarboxylic acid and dimethyl 1,4-cyclohexanedicarboxylate did not demonstrate developmental toxicity in rat studies. Following the spill, the Centers for Disease Control and Prevention recommended a short-term health advisory level of 1 ppm for drinking water that it determined was unlikely to be associated with adverse health effects. Crude MCHM has an odor threshold lower than 10 ppb, indicating that it could be detected at concentrations at least 100-fold less than this risk criterion. Collectively, the findings and predictions indicate that crude MCHM poses no apparent toxicological risk to humans at 1 ppm in household water. © 2015 Cardno ChemRisk.","1,4-cyclohexanedicarboxylic acid; 1,4-cyclohexanedimethanol; 4-(methoxymethyl)cyclohexanemethanol; 4-methylcyclohexanecarboxylate; 4-methylcyclohexanemethanol; chemical spill; dimethyl 1,4-cyclohexanedicarboxylate; drinking water; Elk River incident","1,4 cyclohexanedicarboxylic acid; 1,4 cyclohexanedimethanol; 4 methylcyclohexanemethanol; dimethyl 1,4 cyclohexanedicarboxylate; drinking water; solvent; unclassified drug; 4-methylcyclohexanemethanol; cyclohexane derivative; water pollutant; carcinogenicity; cytotoxicity; developmental toxicity; eye irritation; fetus malformation; larval development; mutagenicity; nematode; nonhuman; odor; physical chemistry; quantitative structure activity relation; Review; skin irritation; skin sensitization; skin toxicity; zebra fish; adverse effects; animal; biotransformation; chemical accident; computer simulation; dose response; environmental exposure; human; pharmacokinetics; risk assessment; risk factor; theoretical model; toxicity; toxicity testing; water pollutant; water pollution; water quality; water supply; Animals; Biotransformation; Chemical Hazard Release; Computer Simulation; Cyclohexanes; Dose-Response Relationship, Drug; Environmental Exposure; Humans; Models, Theoretical; Risk Assessment; Risk Factors; Toxicity Tests; Water Pollutants, Chemical; Water Pollution, Chemical; Water Quality; Water Supply",2-s2.0-84945965909
"Smith G.S., Coetzee M.","AFA-RFID: Physical layer authentication for passive RFID tags",2015,"2015 Information Security for South Africa - Proceedings of the ISSA 2015 Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962045283&doi=10.1109%2fISSA.2015.7335078&partnerID=40&md5=d38592915c63873b9ccc62cc6abd24b5","Radio Frequency IDentification, or RFID, is a ubiquitous technology found across many industries, but which is susceptible to breaches of information security. This research introduces analogue fingerprints as a means to authenticate passive RFID tags. An authentication model implemented at the physical layer of a passive RFID tag, using analogue fingerprints is proposed. The use of analogue computing principles increases the amount of potential authentication data whilst reducing the potential for counterfeiting. © 2015 IEEE.","analogue fingerprint; authentication; passive RFID tag; physical layer security","Authentication; Network layers; Security of data; analogue fingerprint; Authentication data; Authentication models; Passive RFID tags; Physical layer security; Physical layers; Ubiquitous technology; Radio frequency identification (RFID)",2-s2.0-84962045283
"Odijk J.","Linguistic research with PaQu",2015,"Computational Linguistics in the Netherlands Journal",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969753001&partnerID=40&md5=4f75332a0df25ba477e9e52570cc7637","In this paper I illustrate the use of the PaQu (Parse and Query) application for carrying out linguistic research. The major findings of this paper are: (1) PaQu is very useful for aiding researchers in efficient manual verification of hypotheses; (2) PaQu can even be used for automatic verification of hypotheses, provided some care is exercised; (3) the Dutch CHILDES data are too small to address certain research questions; and (4) the data found suggest several new hypotheses on the acquisition of lexical properties that should be further explored. © 2015 Jan Odijk.",,"Linguistics; Automatic verification; Research questions; Computational linguistics",2-s2.0-84969753001
"Muntean P., Rabbi A., Ibing A., Eckert C.","Automated Detection of Information Flow Vulnerabilities in UML State Charts and C Code",2015,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963569175&doi=10.1109%2fQRS-C.2015.30&partnerID=40&md5=5c0884198492a439c0823f1b42e95e4f","Information flow vulnerabilities in UML statecharts and C code are detrimental as they can cause data leakagesor unexpected program behavior. Detecting such vulnerabilitieswith static code analysis techniques is challenging because codeis usually not available during the software design phase andprevious knowledge about what should be annotated and trackedis needed. In this paper we propose textual annotations used tointroduce information flow constraints in UML state charts andcode which are afterwards automatically loaded by informationflow checkers that check if imposed constraints hold or not. Weevaluated our approach on 6 open source test cases availablein the National Institute of Standards and Technology (NIST)Juliet test suite for C/C++. Our results show that our approachis effective and can be further applied to other types of UMLmodels and programming languages as well, in order to detectdifferent types of vulnerabilities. © 2015 IEEE.","Information flow vulnerability; Model-based verification; Static code analysis","Codes (symbols); Computer software selection and evaluation; Open source software; Software design; Software reliability; Automated detection; Information flows; Model based verification; National Institute of Standards and Technology; Program behavior; Static code analysis; Textual annotations; UML statecharts; C (programming language)",2-s2.0-84963569175
"Gao T., Dontcheva M., Adar E., Liu Z., Karahalios K.","Datatone: Managing ambiguity in natural language interfaces for data visualization",2015,"UIST 2015 - Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958249800&doi=10.1145%2f2807442.2807478&partnerID=40&md5=0c05389d043070c3756bfe090425a0c2","Answering questions with data is a difficult and time-consuming process. Visual dashboards and templates make it easy to get started, but asking more sophisticated questions often requires learning a tool designed for expert analysts. Natural language interaction allows users to ask questions directly in complex programs without having to learn how to use an interface. However, natural language is often ambiguous. In this work we propose a mixed-initiative approach to managing ambiguity in natural language interfaces for data visualization. We model ambiguity throughout the process of turning a natural language query into a visualization and use algorithmic disambiguation coupled with interactive ambiguity widgets. These widgets allow the user to resolve ambiguities by surfacing system decisions at the point where the ambiguity matters. Corrections are stored as constraints and influence subsequent queries. We have implemented these ideas in a system, DataTone. In a comparative study, we find that DataTone is easy to learn and lets users ask questions without worrying about syntax and proper question form.","Mixed-initiative interfaces; Natural language interaction; Visualization","Algorithmic languages; Computational linguistics; Flow visualization; Natural language processing systems; User interfaces; Visualization; Comparative studies; Complex programs; Mixed initiative; Natural language interaction; Natural language interfaces; Natural language queries; Natural languages; Surfacing system; Data visualization",2-s2.0-84958249800
"Alsufyani A., Molloy M.S.","Subway derailment",2015,"Ciottone's Disaster Medicine",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981517817&doi=10.1016%2fB978-0-323-28665-7.00182-5&partnerID=40&md5=37e77bc27b5d0cf6b675d28fc4d75bed",[No abstract available],,,2-s2.0-84981517817
"Mayer M., Soares G., Grechkin M., Le V., Marron M., Polozov O., Singh R., Zorn B., Gulwani S.","User interaction models for disambiguation in programming by example",2015,"UIST 2015 - Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959261047&doi=10.1145%2f2807442.2807459&partnerID=40&md5=7a2d630e16e5065149f59d001356f9bf","Programming by Examples (PBE) has the potential to revolutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for automating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user?s intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate actionable information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consistent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users? confidence in the result.",,"Artificial intelligence; Computer programming; Software testing; Active Learning; Completion time; End user programming; Example based; Programming by Example; Repetitive task; Test inputs; User interaction; User interfaces",2-s2.0-84959261047
"Eldawy A., Elganainy M., Bakeer A., Abdelmotaleb A., Mokbel M.","Sphinx: Distributed execution of interactive SQL queries on big spatial data",2015,"GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961226189&doi=10.1145%2f2820783.2820869&partnerID=40&md5=aa29c8fc4d3e290349570868e8d4c5be","This paper presents Sphinx, a full-fledged distributed system which uses a standard SQL interface to process big spatial data. Sphinx adds spatial data types, indexes and query processing, inside the code-base of Cloudera Impala for efficient processing of spatial data. In particular, Sphinx is composed of four main components, namely, query parser, indexer, query planner, and query executor. The query parser injects spatial data types and functions in the SQL interface of Sphinx. The indexer creates spatial indexes in Sphinx by adopting a two-layered index design. The query planner utilizes these indexes to construct efficient query plans for range query and spatial join operations. Finally, the query executor carries out these plans on big spatial datasets in a distributed cluster. A system prototype of Sphinx running on real datasets shows up-to three orders of magnitude performance improvement over traditional Impala. © 2015 ACM.","Impala; Range query; Spatial; Spatial join; Sphinx; SQL","Data handling; Geographic information systems; Information systems; Query languages; Query processing; Impala; Range query; Spatial; Spatial join; Sphinx; Spatial distribution",2-s2.0-84961226189
"Zhao X., Jin P., Yue L.","Discovering topic time from web news",2015,"Information Processing and Management",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941993908&doi=10.1016%2fj.ipm.2015.04.001&partnerID=40&md5=59b69a070b7bfcce2ffdf180f65b17bf","Topic time reflects the temporal feature of topics in Web news pages, which can be used to establish and analyze topic models for many time-sensitive text mining tasks. However, there are two critical challenges in discovering topic time from Web news pages. The first issue is how to normalize different kinds of temporal expressions within a Web news page, e.g., explicit and implicit temporal expressions, into a unified representation framework. The second issue is how to determine the right topic time for topics in Web news. Aiming at solving these two problems, we propose a systematic framework for discovering topic time from Web news. In particular, for the first issue, we propose a new approach that can effectively determine the appropriate referential time for implicit temporal expressions and further present an effective defuzzification algorithm to find the right explanation for a fuzzy temporal expression. For the second issue, we propose a relation model to describe the relationship between news topics and topic time. Based on this model, we design a new algorithm to extract topic time from Web news. We build a prototype system called Topic Time Parser (TTP) and conduct extensive experiments to measure the effectiveness of our proposal. The results suggest that our proposal is effective in both temporal expression normalization and topic time extraction. © 2015 Elsevier Ltd. All rights reserved.","Normalization; Relation model; Temporal expression; Topic time; Web news","Natural language processing systems; Social networking (online); Syntactics; Normalization; Relation models; Temporal expressions; Topic time; Web news; Data mining",2-s2.0-84941993908
"Ferrández A., Peral J., De Gregorio E., Trujillo J., Maté A., Ferrández L.J., Rojas Y.","An authoring tool for decision support systems in context questions of ecological knowledge",2015,"Ecological Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949110398&doi=10.1016%2fj.ecoinf.2015.09.007&partnerID=40&md5=261d0dfd2b20b219b7512668d828f732","Decision support systems (DSS) support business or organizational decision-making activities, which require the access to information that is internally stored in databases or data warehouses, and externally in the Web accessed by Information Retrieval (IR) or Question Answering (QA) systems. Graphical interfaces to query these sources of information ease to constrain dynamically query formulation based on user selections, but they present a lack of flexibility in query formulation, since the expressivity power is reduced to the user interface design. Natural language interfaces (NLI) are expected as the optimal solution. However, especially for non-expert users, a real natural communication is the most difficult to realize effectively.In this paper, we propose an NLI that improves the interaction between the user and the DSS by means of referencing previous questions or their answers (i.e. anaphora such as the pronoun reference in ""What traits are affected by them?""), or by eliding parts of the question (i.e. ellipsis such as ""And to glume colour?"" after the question ""Tell me the QTLs related to awn colour in wheat""). Moreover, in order to overcome one of the main problems of NLIs about the difficulty to adapt an NLI to a new domain, our proposal is based on ontologies that are obtained semi-automatically from a framework that allows the integration of internal and external, structured and unstructured information. Therefore, our proposal can interface with databases, data warehouses, QA and IR systems. Because of the high NL ambiguity of the resolution process, our proposal is presented as an authoring tool that helps the user to query efficiently in natural language. Finally, our proposal is tested on a DSS case scenario about Biotechnology and Agriculture, whose knowledge base is the CEREALAB database as internal structured data, and the Web (e.g. PubMed) as external unstructured information. © 2015 Elsevier B.V.","Anaphora; Context questions; Decision support system; Ellipsis; Natural language interface; Ontologies","database; decision making; decision support system; efficiency measurement; graphical method; interface; questionnaire survey; World Wide Web; Triticum aestivum",2-s2.0-84949110398
"Chen L., Javaid M., Di Eugenio B., Žefran M.","The roles and recognition of Haptic-Ostensive actions in collaborative multimodal human-human dialogues",2015,"Computer Speech and Language",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938061943&doi=10.1016%2fj.csl.2015.03.010&partnerID=40&md5=2db6e3706d82db81ffe2e16e9511e773","The RoboHelper project has the goal of developing assistive robots for the elderly. One crucial component of such a robot is a multimodal dialogue architecture, since collaborative task-oriented human-human dialogue is inherently multimodal. In this paper, we focus on a specific type of interaction, Haptic-Ostensive (H-O) actions, that are pervasive in collaborative dialogue. H-O actions manipulate objects, but they also often perform a referring function. We collected 20 collaborative task-oriented human-human dialogues between a helper and an elderly person in a realistic setting. To collect the haptic signals, we developed an unobtrusive sensory glove with pressure sensors. Multiple annotations were then conducted to build the Find corpus. Supervised machine learning was applied to these annotations in order to develop reference resolution and dialogue act classification modules. Both corpus analysis, and these two modules show that H-O actions play a crucial role in interaction: models that include H-O actions, and other extra-linguistic information such as pointing gestures, perform better. For true human-robot interaction, all communicative intentions must of course be recognized in real time, not on the basis of annotated categories. To demonstrate that our corpus analysis is not an end in itself, but can inform actual human-robot interaction, the last part of our paper presents additional experiments on recognizing H-O actions from the haptic signals measured through the sensory glove. We show that even though pressure sensors are relatively imprecise and the data provided by the glove is noisy, the classification algorithms can successfully identify actions of interest within subjects. © 2015 Elsevier Ltd. All rights reserved.","Dialogue act classification; Haptic-Ostensive actions; Multimodal dialogues; Reference resolution","Artificial intelligence; Classification (of information); Learning systems; Man machine systems; Pressure sensors; Robots; Sensory analysis; Speech processing; Supervised learning; Additional experiments; Classification algorithm; Dialogue acts; Haptic-Ostensive actions; Linguistic information; Multimodal dialogue; Reference resolution; Supervised machine learning; Human robot interaction",2-s2.0-84938061943
"Liu S., Foster I., Savage S., Voelker G.M., Saul L.K.","Who is .com? Learning to parse WHOIS records",2015,"Proceedings of the ACM SIGCOMM Internet Measurement Conference, IMC",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954123051&doi=10.1145%2f2815675.2815693&partnerID=40&md5=23659e7de5a3cd7f42a0cfee43b82048","WHOIS is a long-established protocol for querying information about the 280M+ registered domain names on the Internet. Unfortunately, while such records are accessible in a ""human-readable"" format, they do not follow any consistent schema and thus are challenging to analyze at scale. Existing approaches, which rely on manual crafting of parsing rules and per-registrar templates, are inherently limited in coverage and fragile to ongoing changes in data representations. In this paper, we develop a statistical model for parsing WHOIS records that learns from labeled examples. Our model is a conditional random field (CRF) with a small number of hidden states, a large number of domain-specific features, and parameters that are estimated by efficient dynamic-programming procedures for probabilistic inference. We show that this approach can achieve extremely high accuracy (well over 99%) using modest amounts of labeled training data, that it is robust to minor changes in schema, and that it can adapt to new schema variants by incorporating just a handful of additional examples. Finally, using our parser, we conduct an exhaustive survey of the registration patterns found in 102M com domains. © 2015 ACM.","Information extraction; Machine learning; Named entity recognition; WHOIS","Artificial intelligence; Information retrieval; Internet; Internet protocols; Learning algorithms; Learning systems; Natural language processing systems; Random processes; Conditional random field; Data representations; Labeled training data; Named entity recognition; Probabilistic inference; Registration pattern; Statistical modeling; WHOIS; Dynamic programming",2-s2.0-84954123051
"Khabibullin M., Ivanov A., Grigorev S.","On development of static analysis tools for string-embedded languages",2015,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976319530&doi=10.1145%2f2855667.2855672&partnerID=40&md5=b4a161f5ed14f4c7a521d0f4e56ba3a4","Some programs can produce string expressions with embedded code in other programming languages while running. This embedded code should be syntactically correct as it is typically executed by some subsystem. A program in Java language that builds and sends SQL queries to the database it works with can be considered as an example. In such scenarios, languages like SQL are called string-embedded and ones like Java { host languages. In spite of the fact such an approach of programs building is being replaced by alternative ones, for example by ORM and LINQ, string-embedding is still used in practice. Development and reengineering of the programs with stringembedded languages is complicated because the IDE and similar tools process the code embedded in strings as host language string literals and cannot provide the functionality to work with this code. To facilitate the development process, string-embedded code highlighting, completion, navigation and static errors checking would be useful. For the purposes of reengineering, embedded code metrics computation would be helpful. Currently existing tools to string-embedded languages support only operate with one host language and a fixed set of string-embedded ones. Their functionality is often limited. Moreover, it is almost impossible or requires a substantial amount of work to add a support for both new host and string-embedded language. Attempts to extend their functionality often result in the same problem. In this paper we present the platform which can be used for relatively fast and easy building of endpoint tools that provide a support for difeerent string-embedded languages inside diferent host languages. The tools built for T-SQL and arithmetic expressions language embedding in C# are demonstrated as the examples of how the platform can be used. © 2015 ACM.","Approximation; CFG; Control Flow graph; IDE; Integrated development environment; String-embedded language","Codes (symbols); Computational linguistics; Computer software; Data flow analysis; Flow graphs; Integrodifferential equations; Object oriented programming; Query languages; Reengineering; Software engineering; Static analysis; Web services; Approximation; Arithmetic expression; Control flow graphs; Development process; Embedded Languages; Integrated development environment; Java language; Static error; Java programming language",2-s2.0-84976319530
"Tymoshenko K., Moschitti A.","Assessing the impact of syntactic and semantic structures for answer passages reranking",2015,"International Conference on Information and Knowledge Management, Proceedings",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958254447&doi=10.1145%2f2806416.2806490&partnerID=40&md5=3993edf42e81077a057ce0fb21ff2ace","In this paper, we extensively study the use of syntactic and semantic structures obtained with shallow and deeper syntactic parsers in the answer passage reranking task. We propose several dependency-based structures enriched with Linked Open Data (LD) knowledge for representing pairs of questions and answer passages. We use such tree structures in learning to rank (L2R) algorithms based on tree kernel. The latter can represent questions and passages in a tree fragment space, where each substructure represents a powerful syntactic/semantic feature. Additionally since we define links between structures, tree kernels also generate relational features spanning question and passage structures. We derive very important findings, which can be useful to build state-of-the-art systems: (i) full syntactic dependencies can outperform shallow models also using external knowledge and (ii) the semantic information should be derived by effective and high-coverage resources, e.g., LD, and incorporated in syntactic structures to be effective. We demonstrate our findings by carrying out an extensive comparative experimentation on two different TREC QA corpora and one community question answer dataset, namely Answerbag. Our comparative analysis on well-defined answer selection benchmarks consistently demonstrates that our structural semantic models largely outperform the state of the art in passage reranking.","Kernel methods; Learning to rank; Linked data; Question answering; Structural kernels","Forestry; Knowledge management; Natural language processing systems; Semantics; Trees (mathematics); Kernel methods; Learning to rank; Linked datum; Question Answering; Structural kernels; Syntactics",2-s2.0-84958254447
"Fang L., Nguyen K., Xu G., Demsky B., Lu S.","Interruptible tasks: Treating memory pressure as interrupts for highly scalable data-parallel programs",2015,"SOSP 2015 - Proceedings of the 25th ACM Symposium on Operating Systems Principles",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957942544&doi=10.1145%2f2815400.2815407&partnerID=40&md5=255e5f32dae6a87d06319ed8393e7849","Real-world data-parallel programs commonly suffer from great memory pressure, especially when they are executed to process large datasets. Memory problems lead to excessive GC effort and out-of-memory errors, significantly hurting system performance and scalability. This paper proposes a systematic approach that can help data-parallel tasks survive memory pressure, improving their performance and scalability without needing any manual effort to tune system parameters. Our approach advocates interruptible task (ITask), a new type of data-parallel tasks that can be interrupted upon memory pressure-with part or all of their used memory reclaimed-and resumed when the pressure goes away. To support ITasks, we propose a novel programming model and a runtime system, and have instantiated them on two state-of-the-art platforms Hadoop and Hyracks. A thorough evaluation demonstrates the effectiveness of ITask: it has helped real-world Hadoop programs survive 13 out-of-memory problems reported on StackOverflow; a second set of experiments with 5 already well-tuned programs in Hyracks on datasets of different sizes shows that the ITask-based versions are 1.5-3 × faster and scale to 3-24 × larger datasets than their regular counterparts. © Copyright 2015 ACM.",,"Scalability; Data parallel programs; Different sizes; Memory pressure; Memory problems; Out-of-memory errors; Performance and scalabilities; Programming models; Runtime systems; Embedded systems",2-s2.0-84957942544
"Yeh J.-F., Chen W.-Y., Su M.-C.","Chinese spelling checker based on an inverted index list with a rescoring mechanism",2015,"ACM Transactions on Asian and Low-Resource Language Information Processing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021738470&doi=10.1145%2f2826235&partnerID=40&md5=2d1f1d6c3ce6dd8b244151aee4be2378","An approach is proposed for Chinese spelling error detection and correction, in which an inverted index list with a rescoring mechanism is used. The inverted index list is a structure for mapping from word to desired sentence, and for representing nodes in lattices constructed through character expansion (according to predefined phonologically and visually similar character sets). Pruning based on a contextual dependency confidence measure was used to markedly reduce the search space and computational complexity. Relevant mapping relations between the original input and desired input were obtained using a scoring mechanism composed of class-based language and maximum entropy correction models containing character, word, and contextual features. The proposed method was evaluated using data sets provided by SigHan 7 bakeoff. The experimental results show that the proposed method achieved acceptable performance in terms of recall rate or precision rate in error sentence detection and error location detection, and it outperformed other approaches in error location detection and correction. © 2015 ACM.","Contextual information; Correction model; Inverted index list; Language model; Maximum entropy; Spelling checker","Character sets; Entropy; Errors; Indexing (of information); Mapping; Maximum entropy methods; Contextual information; Correction models; Inverted indices; Language model; Spelling checker; Error detection",2-s2.0-85021738470
"Luo J., Dong X., Yang H.","Learning to reinforce search effectiveness",2015,"ICTIR 2015 - Proceedings of the 2015 ACM SIGIR International Conference on the Theory of Information Retrieval",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964318895&doi=10.1145%2f2808194.2809468&partnerID=40&md5=bf2d3c292553244cba4a1127baaabacf","Session search is an Information Retrieval (IR) task which handles a series of queries issued for a search task. In this paper, we propose a novel reinforcement learning style information retrieval framework and develop a new feedback learning algorithm to model user feedback, including clicks and query reformulations, as reinforcement signals and to generate rewards in the RL framework. From a new perspective, we view session search as a cooperative game played between two agents, the user and the search engine. We study the communications between the two agents; they always exchange opinions on ""whether the current stage of search is relevant"" and ""whether we should explore now."" The algorithm infers user feedback models by an EM algorithm from the query logs. We compare to several state-of-the-art session search algorithms and evaluate our algorithm on the most recent TREC 2012 to 2014 Session Tracks. The experimental results demonstrates that our approach is highly effective for improving session search accuracy. © 2015 ACM.","Dynamic information retrieval modeling; Reinforcement learning; Session search; Stochastic game","Algorithms; Game theory; Information retrieval; Learning algorithms; Search engines; Stochastic systems; Dynamic information retrieval; Feedback learning algorithms; Query reformulation; Reinforcement signal; Search Algorithms; Session search; State of the art; Stochastic game; Reinforcement learning",2-s2.0-84964318895
"Das S., Ascano R., Macarty M.","Distributed big data search for analyst queries and data fusion",2015,"2015 18th International Conference on Information Fusion, Fusion 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960503813&partnerID=40&md5=04ef4df3b26b72aa15fa34d3d716a356","We describe here an agent-based Distributed Analytical Search (DAS) tool to search and query distributed 'big data' sources regardless of data's location, content or format. DAS semantically analyzes natural language queries from a web-based user interface. It automatically translates the query to a set of sub-queries by deploying a combination of planning and traditional database query optimization techniques. It then generates a query plan represented in XML and guide the execution by spawning intelligent agents with various types of wrappers as needed for distributed sites. The answers returned by the agents are merged appropriately and return them to the user. We have demonstrated DAS using a variety of data sources that are distributed and heterogeneous. DAS is the prime target for analysts searching relevant data sources to answer priority intelligence requirements without having them to know the details of available data sources. DAS enables fusion systems to search relevant data sources and extract evidence to propagate into the models of the systems. © 2015 IEEE.","analytics; distributed search; High-level fusion; intelligent agents; natural language query; query optimization; query planning; query translation","Big data; Computational linguistics; Data fusion; High level languages; Information fusion; Intelligent agents; Natural language processing systems; Query languages; Query processing; Translation (languages); User interfaces; analytics; Distributed search; High-level fusions; Natural language queries; Query optimization; Query translations; Search engines",2-s2.0-84960503813
"Mestanogullari A., Hahn S., Arni J.K., Löh A.","Type-level web APIs with servant: An exercise in domain-specific generic programming",2015,"WGP 2015 - Proceedings of the 11th ACM SIGPLAN Workshop on Generic Programming, co-located with ICFP 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009069400&doi=10.1145%2f2808098.2808099&partnerID=40&md5=533f0da64e663c8115e9d3c61a69a06b","We describe the design and motivation for Servant, an extensible, type-level DSL for describing Web APIs. Servant APIs are Haskell types. An API type can be interpreted in several different ways: as a server that processes requests, interprets them and dispatches them to appropriate handlers; as a client that can correctly query the endpoints of the API; as systematic documentation for the API; and more. Servant is fully extensible: the API language can be augmented with new constructs, and new interpretations can be defined. The key Haskell features making all this possible are data kinds, (open) type families and (open) type classes. The techniques we use are reminiscent of general-purpose generic programming. However, where most generic programming libraries are interested in automatically deriving programs for a large class of datatypes from many different domains, we are only interested in a small class of datatypes that is used in the DSL for describing APIs.","Embedded domain-specific languages; Generic programming; Haskell; Type-level programming; Web programming","Computer programming languages; Digital subscriber lines; Problem oriented languages; Embedded domain specific languages; Generic programming; Haskell; Type-level programming; Web programming; Application programming interfaces (API)",2-s2.0-85009069400
"Agarwal B., Poria S., Mittal N., Gelbukh A., Hussain A.","Concept-Level Sentiment Analysis with Dependency-Based Semantic Parsing: A Novel Approach",2015,"Cognitive Computation",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937638464&doi=10.1007%2fs12559-014-9316-6&partnerID=40&md5=b4fbe196be7852a02d01f17099a0d31a","Sentiment analysis from unstructured natural language text has recently received considerable attention from the research community. In the frame of biologically inspired machine learning approaches, finding good feature sets is particularly challenging yet very important. In this paper, we focus on this fundamental issue of the sentiment analysis task. Specifically, we employ concepts as features and present a concept extraction algorithm based on a novel concept parser scheme to extract semantic features that exploit semantic relationships between words in natural language text. Additional conceptual information of a concept is obtained using the ConceptNet ontology: Concepts extracted from text are sent as queries to ConceptNet to extract their semantics. We select important concepts and eliminate redundant concepts using the Minimum Redundancy and Maximum Relevance feature selection technique. All selected concepts are then used to build a machine learning model that classifies a given document as positive or negative. We evaluate our concept extraction approach using a benchmark movie review dataset provided by Cornell University and product review datasets on books, DVDs, and electronics. Comparative experimental results show that our proposed approach to sentiment analysis outperforms existing state-of-the-art methods. © 2015, Springer Science+Business Media New York.","ConceptNet; Dependency rules; Minimum Redundancy and Maximum Relevance feature selection; Semantic parser; Sentiment analysis","Artificial intelligence; Computational linguistics; Data mining; Extraction; Learning systems; Natural language processing systems; Redundancy; Reviews; Semantics; ConceptNet; Dependency rules; Machine learning approaches; Machine learning models; Relevance feature selections; Sentiment analysis; State-of-the-art methods; Unstructured natural language; Feature extraction",2-s2.0-84937638464
"Zhang F., Yuan N.J., Wang Y., Xie X.","Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach",2015,"Knowledge and Information Systems",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937635552&doi=10.1007%2fs10115-014-0763-x&partnerID=40&md5=24b1a970b16c5c4c5c8450d22db2635e","Smart card transactions capture rich information of human mobility and urban dynamics and therefore are of particular interest to urban planners and location-based service providers. However, since most transaction systems are only designated for billing purpose, typically, fine-grained location information, such as the exact boarding and alighting stops of a bus trip, is only partially or not available at all, which blocks deep exploitation of this rich and valuable data at individual level. This paper presents a collaborative space alignment framework to reconstruct individual mobility history from a metropolitan-scale smart card transaction dataset. Specifically, we show that by delicately aligning the monetary space and geospatial space with the temporal space, we are able to extrapolate a series of critical domain-specific constraints. Later, these constraints are naturally incorporated into a semi-supervised conditional random field (CRF) to infer the exact boarding and alighting stops of all transit routes, where the features of the CRF model consist of not only pre-defined indicator features extracted from individual trips but also latent features crafted from different users’ trips using collaborative filtering. Here, we consider two types of collaborative features: (1) the similarity in terms of users’ choices of bus lines and (2) latent temporal patterns of users’ commuting behaviors. Extensive experimental results show that our approach achieves a high accuracy, e.g., given only 10 % trips with known alighting/boarding stops, and we successfully inferred more than 79 % alighting and boarding stops from all unlabeled trips. In particular, we validated that the extracted collaborative features significantly contribute to the accuracy of our model. In addition, we have demonstrated that by applying our approach to enrich the data, the performance of a conventional method for identifying users’ home and work places can be dramatically improved (with 83 % improvement on home detection and 38 % improvement on work place detection). The proposed method offers the possibility to mine individual mobility from common public transit transactions, and showcases how uncertain data can be leveraged with domain knowledge and constraints, to support cross-application data-mining tasks. © 2014, Springer-Verlag London.","Collaborative filtering; Mobility; Smart card; Space alignment",,2-s2.0-84937635552
"Trifan M., Ionescu B., Gadea C., Ionescu D.","A graph digital signal processing method for semantic analysis",2015,"SACI 2015 - 10th Jubilee IEEE International Symposium on Applied Computational Intelligence and Informatics, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959267523&doi=10.1109%2fSACI.2015.7208196&partnerID=40&md5=57a9f9ee489d02a528a537dcf856cdd8","This paper focuses on the problem of devising a computationally tractable procedure for representing the natural language understanding (NLU). It approaches this goal, by using distributional models of meaning through a method from graph-based digital signal processing (DSP) which only recently grabbed the attention of researchers from the field of natural language processing (NLP) related to big data analysis. The novelty of our approach lies in the combination of three domains: advances in deep learning algorithms for word representation, dependency parsing for modeling inter-word relations and convolution using orthogonal Hadamard codes for composing the two previous areas, generating a unique representation for the sentence. Two types of problems are resolved in a new unified way: sentence similarity given by the cos function of the corresponding vectors and question-answering where the query is matched to possible answers. This technique resembles the spread spectrum methods from telecommunication theory where multiple users share a common channel, and are able to communicate without interference. In the content of this paper the case of individual words play the role of users sharing the same sentence. Examples of the method application to a standard set of sentences, used for benchmarking the accuracy and the execution time is also given. © 2015 IEEE.","CDMA; dependency parser; distributional semantic composition; Hadamard matrix; SICK corpus; similarity","Artificial intelligence; Benchmarking; Code division multiple access; Computation theory; Computational linguistics; Digital signal processing; Graphic methods; Hadamard matrices; Information science; Learning algorithms; Natural language processing systems; Processing; Semantics; Signal processing; Syntactics; Dependency parser; Digital signal processing (DSP); Digital signal-processing method; NAtural language processing; Natural language understanding; Semantic composition; SICK corpus; similarity; Big data",2-s2.0-84959267523
"Jamgade A.N., Karale S.J.","Ontology based information retrieval system for Academic Library",2015,"ICIIECS 2015 - 2015 IEEE International Conference on Innovations in Information, Embedded and Communication Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957042189&doi=10.1109%2fICIIECS.2015.7193106&partnerID=40&md5=45c9c602f301f2c7037cdd4c0dee8def","Information retrieval system is taking an important role in current search engine which performs searching operation based on keywords which results in enormous amount of data available to the user, from which user cannot figure out the essential and most important information. This limitation may be overcome by a new web architecture known as semantic web which overcome the limitation of keyword based search technique called conceptual or semantic search technique. Natural language processing technique is mostly implemented in QA system for asking user's question and several steps are also followed for conversion of questions to query form for getting an exact answer. In conceptual search, search engine interprets the meaning of user's query and the relation among the concepts that documents contains with respect to a particular domain that produces specific answers instead of giving list of answers. In this paper, we proposed ontology based semantic information retrieval system and Jena semantic web framework in which, user enters an input query which is parsed by Standford Parser then triplet extraction algorithm is used. To all input query, SPARQL query is formed and then it is fired on the knowledge base (Ontology) that finds appropriate RDF triples in knowledge base and retrieve the relevant information using Jena framework. © 2015 IEEE.","Information retrieval; Jena API; Ontology; Query processing; RDF; Semantic web; SPARQL; WordNet","Embedded systems; Information analysis; Information retrieval; Knowledge based systems; Natural language processing systems; Ontology; Query processing; Semantic Web; Academic libraries; Extraction algorithms; Jena API; Keyword-based search; NAtural language processing; Semantic information retrieval; SPARQL; Wordnet; Search engines",2-s2.0-84957042189
"Xhafa F., Naranjo V., Caballé S., Barolli L.","A Software Chain Approach to Big Data Stream Processing and Analytics",2015,"Proceedings - 2015 9th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2015",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959457063&doi=10.1109%2fCISIS.2015.24&partnerID=40&md5=4c5d7d46be53bf3fc9d6c59c218f3994","Big Data Stream processing is among the most important computing trends nowadays. The growing interest on Big Data Stream processing comes from the need of many Internet-based applications that generate huge data streams, whose processing can serve to extract useful analytics and inform for decision making systems. For instance, an IoT-based monitoring systems for a supply-chain, can provide real time data analytics for the business delivery performance. The challenges of processing Big Data Streams reside on coping with real-time processing of an unbounded stream of data, that is, the computing system should be able to compute at high throughput to accommodate the high data stream rate generation in input. Clearly, the higher the data stream rate, the higher should be the throughput to achieve consistency of the processing results (e.g. Preserving the order of events in the data stream). In this paper we show how to map the data stream processing phases (from data generation to final results) to a software chain architecture, which comprises five main components: sensor, extractor, parser, formatter and out putter. We exemplify the approach using the Yahoo!S4 for processing the Big Data Stream from Flight Radar24 global flight monitoring system. © 2015 IEEE.","Big Data Stream; Flight Radar24; Generic Log Adapter; Global Flight Monitoring System; Massive Data Processing; Real-time Analytics; Software Chain Architecture; Yahoo!S4","Computer architecture; Data communication systems; Data handling; Data mining; Decision making; Internet; Monitoring; Real time systems; Supply chains; Chain architecture; Data stream; Flight Radar24; Generic Log Adapter; Monitoring system; Real-time analytics; Yahoo!S4; Big data",2-s2.0-84959457063
"Zhang W., Ahmed A., Yang J., Josifovski V., Smola A.J.","Annotating needles in the haystack without looking: Product information extraction from emails",2015,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954182219&doi=10.1145%2f2783258.2788580&partnerID=40&md5=71bce567d8d7cb395784ed11e3854632","Business-to-consumer (B2C) emails are usually generated by filling structured user data (e.g. purchase, event) into templates. Extracting structured data from B2C emails allows users to track important information on various devices. However, it also poses several challenges, due to the requirement of short response time for massive data volume, the diversity and complexity of templates, and the privacy and legal constraints. Most notably, email data is legally protected content, which means no one except the receiver can review the messages or derived information. In this paper we first introduce a system which can extract structured information automatically without requiring human review of any personal content. Then we focus on how to annotate product names from the extracted texts, which is one of the most difficult problems in the system. Neither general learning methods, such as binary classifiers, nor more specific structure learning methods, such as Conditional Random Field (CRF), can solve this problem well. To accomplish this task, we propose a hybrid approach, which basically trains a CRF model using the labels predicted by binary classifiers (weak learners). However, the performance of weak learners can be low, therefore we use Expectation Maximization (EM) algorithm on CRF to remove the noise and improve the accuracy, without the need to label and inspect specific emails. In our experiments, the EM-CRF model can significantly improve the product name annotations over the weak learners and plain CRFs.","Business intelligence; Structured information extraction","Algorithms; Bins; Classification (of information); Competitive intelligence; Electronic mail; Image segmentation; Information analysis; Information retrieval; Learning systems; Maximum principle; Random processes; Binary classifiers; Business-to-consumer; Conditional random field; Expectation-maximization algorithms; Personal Content; Short response time; Structure-learning; Structured information; Data mining",2-s2.0-84954182219
"Severyn A., Moschittiy A.","Learning to rank short text pairs with convolutional deep neural networks",2015,"SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",87,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953775876&doi=10.1145%2f2766462.2767738&partnerID=40&md5=d2fcce48b981b5c5d5c995e3ebbefeaf","Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering - question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers. © 2015 ACM.","Convolutional neural networks; Learning to rank; Microblog search; Question answering","Computer vision; Convolution; Information retrieval; Network architecture; Neural networks; Query processing; Semantics; Speech recognition; Syntactics; Convolutional neural network; Feature representation; Learning to rank; Micro-blog; NAtural language processing; Question Answering; State-of-the-art performance; State-of-the-art system; Natural language processing systems",2-s2.0-84953775876
"Lengyel L.","Validating rule-based algorithms",2015,"Acta Polytechnica Hungarica",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937420885&partnerID=40&md5=b0f9bfffcac64051c73d47724651f546","A rule-based system is a series of if-then statements that utilizes a set of assertions, to which rules are created on how to act upon those assertions. Rule-based systems often construct the basis of software artifacts which can provide answers to problems in place of human experts. Such systems are also referred as expert systems. Rule-based solutions are also widely applied in artificial intelligence-based systems, and graph rewriting is one of the most frequently applied implementation techniques for their realization. As the necessity for reliable rule-based systems increases, so emerges the field of research regarding verification and validation of graph rewriting-based approaches. Verification and validation indicate determining the accuracy of a model transformation / rule-based system, and ensure that the processing output satisfies specific conditions. This paper introduces the concept of taming the complexity of these verification/validation solutions by starting with the most general case and moving towards more specific solutions. Furthermore, we provide a dynamic (online) method to support the validation of algorithms designed and executed in rule-based systems. The proposed approach is based on a graph rewriting-based solution. © 2015, Budapest Tech Polytechnical Institution. All rights reserved.","Dynamic verification of model transformations; Graph rewriting-based model transformations; Verification/validation of rule-based systems",,2-s2.0-84937420885
"Asiaee A.H., Minning T., Doshi P., Tarleton R.L.","A framework for ontology-based question answering with application to parasite immunology",2015,"Journal of Biomedical Semantics",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937203625&doi=10.1186%2fs13326-015-0029-x&partnerID=40&md5=0b8c5f8b2562c5804171f0b8c3f9a432","Background: Large quantities of biomedical data are being produced at a rapid pace for a variety of organisms. With ontologies proliferating, data is increasingly being stored using the RDF data model and queried using RDF based querying languages. While existing systems facilitate the querying in various ways, the scientist must map the question in his or her mind to the interface used by the systems. The field of natural language processing has long investigated the challenges of designing natural language based retrieval systems. Recent efforts seek to bring the ability to pose natural language questions to RDF data querying systems while leveraging the associated ontologies. These analyze the input question and extract triples (subject, relationship, object), if possible, mapping them to RDF triples in the data. However, in the biomedical context, relationships between entities are not always explicit in the question and these are often complex involving many intermediate concepts. Results: We present a new framework, OntoNLQA, for querying RDF data annotated using ontologies which allows posing questions in natural language. OntoNLQA offers five steps in order to answer natural language questions. In comparison to previous systems, OntoNLQA differs in how some of the methods are realized. In particular, it introduces a novel approach for discovering the sophisticated semantic associations that may exist between the key terms of a natural language question, in order to build an intuitive query and retrieve precise answers. We apply this framework to the context of parasite immunology data, leading to a system called AskCuebee that allows parasitologists to pose genomic, proteomic and pathway questions in natural language related to the parasite, Trypanosoma cruzi. We separately evaluate the accuracy of each component of OntoNLQA as implemented in AskCuebee and the accuracy of the whole system. AskCuebee answers 68 % of the questions in a corpus of 125 questions, and 60 % of the questions in a new previously unseen corpus. If we allow simple corrections by the scientists, this proportion increases to 92 %. Conclusions: We introduce a novel framework for question answering and apply it to parasite immunology data. Evaluations of translating the questions to RDF triple queries by combining machine learning, lexical similarity matching with ontology classes, properties and instances for specificity, and discovering associations between them demonstrate that the approach performs well and improves on previous systems. Subsequently, OntoNLQA offers a viable framework for building question answering systems in other biomedical domains. © 2015 Asiaee et al.","Chagas; Natural language; Ontology; Parasite data; Question answering",,2-s2.0-84937203625
"Poll E., De Ruiter J., Schubert A.","Protocol state machines and session languages: Specification, implementation, and security flaws",2015,"Proceedings - 2015 IEEE Security and Privacy Workshops, SPW 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945894085&doi=10.1109%2fSPW.2015.32&partnerID=40&md5=35d8e4958c70f1059afc7d098e17ba10","Input languages, which describe the set of valid inputs an application has to handle, play a central role in language-theoretic security, in recognition of the fact that overly complex, sloppily specified, or incorrectly implemented input languages are the root cause of many security vulnerabilities. Often an input language not only involves a language of individual messages, but also some protocol with a notion of a session, i.e. A sequence of messages that makes up a dialogue between two parties. This paper takes a closer look at languages for such sessions, when it comes to specification, implementation, and testing - and as a source of insecurity. We show that these 'session' languages are often poorly specified and that errors in implementing them can cause security problems. As a way to improve this situation, we discuss the possibility to automatically infer formal specifications of such languages, in the form of protocol state machines, from implementations by black box testing. © 2015 IEEE.","Formal specification; Fuzzing; Language-theoretic security; Protocol state machine; Reverse engineering","Black-box testing; Formal specification; Reverse engineering; Specifications; Fuzzing; Language-theoretic security; Protocol state machines; Root cause; Security flaws; Security problems; Security vulnerabilities; Computational linguistics",2-s2.0-84945894085
"Gilpin W.","PyPDB: A Python API for the Protein Data Bank",2015,"Bioinformatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959906715&doi=10.1093%2fbioinformatics%2fbtv543&partnerID=40&md5=7de012dd3ee8401ed67c9d9989534ec6","Summary: We have created a Python programming interface for the RCSB Protein Data Bank (PDB) that allows search and data retrieval for a wide range of result types, including BLAST and sequence motif queries. The API relies on the existing XML-based API and operates by creating custom XML requests from native Python types, allowing extensibility and straightforward modification. The package has the ability to perform many types of advanced search of the PDB that are otherwise only available through the PDB website. Availability and implementation: PyPDB is implemented exclusively in Python 3 using standard libraries for maximal compatibility. The most up-to-date version, including iPython notebooks containing usage tutorials, is available free-of-charge under an open-source MIT license via GitHub at https://github.com/williamgilpin/pypdb, and the full API reference is at http://williamgilpin.github.io/pypdb-docs/html/. The latest stable release is also available on PyPI. © 2015 The Author 2015. Published by Oxford University Press. All rights reserved.",,"computer interface; computer language; computer program; protein database; Databases, Protein; Programming Languages; Software; User-Computer Interface",2-s2.0-84959906715
"Zhang K., Hu J., Hua B.","A holistic approach to build real-time stream processing system with GPU",2015,"Journal of Parallel and Distributed Computing",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931273833&doi=10.1016%2fj.jpdc.2015.05.002&partnerID=40&md5=1fabf4e133e21e0677c270cc73127c8e","Stream processing needs to process huge volume of data with strict deadline requirements. These applications generally consume large amount of network bandwidth and involve compute-intensive operations. Accelerating such operations with general purpose GPU has drawn a lot of attention from both academia and industry. However, GPU has not been applied to real-time stream processing due to its programming paradigm and unpredictable latency. In this paper, we study the problem of applying GPU to real-time processing and propose a holistic approach for building real-time stream processing system with GPU. Based on the proposed techniques, we build a GPU-accelerated SRTP reverse proxy that achieves more than 10Gbps overall throughput and guarantees low latency. Our work demonstrates that using GPU in high-speed real-time stream processing is both feasible and attractive. © 2015 Elsevier Inc. All rights reserved.","GPU; High-speed networking; Real-time; Stream processing","Artificial intelligence; Computer programming; General purpose gpu; GPU; High-speed networking; Network bandwidth; Programming paradigms; Real time; Realtime processing; Stream processing; Program processors",2-s2.0-84931273833
"Leung A., Sarracino J., Lerner S.","Interactive parser synthesis by example",2015,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951863764&doi=10.1145%2f2737924.2738002&partnerID=40&md5=47f615da28d824ff164a4d43c6623eca","Despite decades of research on parsing, the construction of parsers remains a painstaking, manual process prone to subtle bugs and pitfalls. We present a programming-by-example framework called Parsify that is able to synthesize a parser from input/output examples. The user does not write a single line of code. To achieve this, Parsify provides: (a) an iterative algorithm for synthesizing and refining a grammar one example at a time, (b) an interface that provides immediate visual feedback in response to changes in the grammar being refined, and (c) a graphical mechanism for specifying example parse trees using only textual selections. We empirically demonstrate the viability of our approach by using Parsify to construct parsers for source code drawn from Verilog, SQL, Apache, and Tiger. Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Parsing; Program synthesis; Programming by example","Algorithms; Computer programming; Computer programming languages; Formal languages; Iterative methods; Synthesis (chemical); Trees (mathematics); Visual communication; Iterative algorithm; Line of codes; Manual process; Parsing; Program synthesis; Programming by Example; Source codes; Visual feedback; Computational linguistics",2-s2.0-84951863764
"Chaplot D.S., Bhattacharyya P., Paranjape A.","Unsupervised Word Sense Disambiguation using Markov random field and dependency parser",2015,"Proceedings of the National Conference on Artificial Intelligence",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959866353&partnerID=40&md5=14d1b0405b7e7dc30a7abe365270dae1","Word Sense Disambiguation is a difficult problem to solve in the unsupervised setting. This is because in this setting inference becomes more dependent on the interplay between different senses in the context due to unavailability of learning resources. Using two basic ideas, sense dependency and selective dependency, we model the WSD problem as a Maximum A Posteriori (MAP) Inference Query on a Markov Random Field (MRF) built using WordNet and Link Parser or Stanford Parser. To the best of our knowledge this combination of dependency and MRF is novel, and our graph-based unsupervised WSD system beats state-of-the-art system on SensEval-2, SensEval-3 and SemEval-2007 English all-words datasets while being over 35 times faster. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,"Artificial intelligence; Graphic methods; Markov processes; Dependency parser; Graph-based; Learning resource; Markov Random Fields; Maximum a posteriori; Stanford; State-of-the-art system; Word Sense Disambiguation; Natural language processing systems",2-s2.0-84959866353
"Leung A., Sarracino J., Lerner S.","Interactive parser synthesis by example",2015,"ACM SIGPLAN Notices",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951184649&doi=10.1145%2f2737924.2738002&partnerID=40&md5=92c41014aa9456a9d7cc243941bcf644","Despite decades of research on parsing, the construction of parsers remains a painstaking, manual process prone to subtle bugs and pitfalls. We present a programming-by-example framework called Parsify that is able to synthesize a parser from input/output examples. The user does not write a single line of code. To achieve this, Parsify provides: (a) an iterative algorithm for synthesizing and refining a grammar one example at a time, (b) an interface that provides immediate visual feedback in response to changes in the grammar being refined, and (c) a graphical mechanism for specifying example parse trees using only textual selections.We empirically demonstrate the viability of our approach by using Parsify to construct parsers for source code drawn from Verilog, SQL, Apache, and Tiger. © 2015 ACM.","Parsing; Program Synthesis; Programming by Example","Algorithms; Formal languages; Iterative methods; Synthesis (chemical); Trees (mathematics); Visual communication; Iterative algorithm; Line of codes; Manual process; Parsing; Program synthesis; Programming by Example; Source codes; Visual feedback; C (programming language)",2-s2.0-84951184649
"Xu K., Zhang S., Feng Y., Huang S., Zhao D.","What is the longest river in the USA? Semantic parsing for aggregation questions",2015,"Proceedings of the National Conference on Artificial Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961218828&partnerID=40&md5=def11608b0bc77d5187cb5e21d467410","Answering natural language questions against structured knowledge bases (KB) has been attracting increasing attention in both IR and NLP communities. The task involves two main challenges: recognizing the questions' meanings, which are then grounded to a given KB. Targeting simple factoid questions, many existing open domain semantic parsers jointly solve these two subtasks, but are usually expensive in complexity and resources. In this paper, we propose a simple pipeline framework to efficiently answer more complicated questions, especially those implying aggregation operations, e.g., argmax, argmin. We first develop a transitionbased parsing model to recognize the KB-independent meaning representation of the user's intention inherent in the question. Secondly, we apply a probabilistic model to map the meaning representation, including those aggregation functions, to a structured query. The experimental results showe that our method can better understand aggregation questions, outperforming the state-of-the-art methods on the Free917 dataset while still maintaining promising performance on a more challenging dataset, WebQuestions, without extra training. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Semantics; Aggregation functions; Aggregation operation; Natural language questions; Probabilistic modeling; State-of-the-art methods; Structured knowledge; Structured queries; User's intentions; Artificial intelligence",2-s2.0-84961218828
"Odat S., Groza T., Hunter J.","Extracting structured data from publications in the art conservation domain",2015,"Digital Scholarship in the Humanities",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974687843&doi=10.1093%2fllc%2ffqu002&partnerID=40&md5=c2e1f6a667c21ea2514debb0ef5dec80","The most common method of publishing new discoveries about art conservation techniques and research has been through traditional full-text publications. Such corpora typically only support searching via metadata (e.g. title, authors, or keywords) and full-text. In particular, it is difficult to discover valuable information about the chemical processes, experimental results, or preservation treatments associated with the conservation of paintings from a specific genre. This article addresses this problem by focusing on the extraction of structured data (that complies with a pre-defined ontology) from a distributed corpus of publications about painting conservation. Our specific extraction method involves a unique combination of named entity recognition (using gazetteer-based and machine learning-based methods) followed by relationship extraction (using rule-based and machine learning-based methods). The resulting structured data are stored in a resource description framework triple store, and a Webbased graphical user interface enables the SPARQL querying, retrieval, and display of the search results. The results from applying our techniques to a corpus of publications on art conservation indicate that our approach achieves higher quality precision and recall in extracting named entities and relations from publications, relative to alternative existing approaches. © The Author 2014. Published by Oxford University.",,,2-s2.0-84974687843
"Beedkar K., Gemulla R.","LASH: Large-scale sequence mining with hierarchies",2015,"Proceedings of the ACM SIGMOD International Conference on Management of Data",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957556881&doi=10.1145%2f2723372.2723724&partnerID=40&md5=9b3bcf1d5364f006417c6fabd18ce48e","We propose LASH, a scalable, distributed algorithm for mining sequential patterns in the presence of hierarchies. LASH takes as input a collection of sequences, each composed of items from some application-specific vocabulary. In contrast to traditional approaches to sequence mining, the items in the vocabulary are arranged in a hierarchy: both input sequences and sequential patterns may consist of items from different levels of the hierarchy. Such hierarchies naturally occur in a number of applications including mining natural-language text, customer transactions, error logs, or event sequences. LASH is the first parallel algorithm for mining frequent sequences with hierarchies; it is designed to scale to very large datasets. At its heart, LASH partitions the data using a novel, hierarchy-aware variant of item-based partitioning and subsequently mines each partition independently and in parallel using a customized mining algorithm called pivot sequence miner. LASH is amenable to a MapReduce implementation; we propose effective and efficient algorithms for both the construction and the actual mining of partitions. Our experimental study on large real-world datasets suggest good scalability and run-time efficiency.",,"Data mining; Application specific; Large-scale sequences; Mining sequential patterns; Natural language text; Real-world datasets; Run-time efficiency; Sequential patterns; Traditional approaches; Algorithms",2-s2.0-84957556881
"Raghavi K.C., Chinnakotla M., Shrivastava M.","""Answer ka type kya he?"" learning to classify questions in code-mixed language",2015,"WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955306053&doi=10.1145%2f2740908.2743006&partnerID=40&md5=788688d2b016e584dd8f2c8a587f0a35","Code-Mixing (CM) is defined as the embedding of linguistic units such as phrases, words, and morphemes of one language into an utterance of another language. CM is a natural phenomenon observed in many multilingual societies. It helps in speeding-up communication and allows wider variety of expression due to which it has become a popular mode of communication in social media forums like Facebook and Twitter. However, current Question Answering (QA) research and systems only support expressing a question in a single language which is an unrealistic and hard proposition especially for certain domains like health and technology. In this paper, we take the first step towards the development of a full-fledged QA system in CM language which is building a Question Classification (QC) system. The QC system analyzes the user question and infers the expected Answer Type (AType). The AType helps in locating and verifying the answer as it imposes certain type-specific constraints. We learn a basic Support Vector Machine (SVM) based QC system for English-Hindi CM questions. Due to the inherent complexities involved in processing CM language and also the unavailability of language processing resources such POS taggers, Chunkers, Parsers, we design our current system using only word-level resources such as language identification, transliteration and lexical translation. To reduce data sparsity and leverage resources available in a resourcerich language, in stead of extracting features directly from the original CM words, we translate them commonly into English and then perform featurization. We created an evaluation dataset for this task and our system achieves an accuracy of 63% and 45% in coarse-grained and fine-grained.","Classification; Code-Mixing; Machine Learning; Question Answering; Support Vector Machines","Artificial intelligence; Classification (of information); Codes (symbols); Learning systems; Linguistics; Mixing; Natural language processing systems; Social networking (online); Speech recognition; Support vector machines; Translation (languages); World Wide Web; Code-mixing; Extracting features; Inherent complexity; Language identification; Language processing; Natural phenomena; Question Answering; Question classification; Computational linguistics",2-s2.0-84955306053
"Zablith F., Osman I.H.","Linking stanford typed dependencies to support text analytics",2015,"WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968611287&doi=10.1145%2f2740908.2741706&partnerID=40&md5=2e73ff7f4992f66511636d9e0c806099","With the daily increase of the amount of published informa- tion, research in the area of text analytics is gaining more visibility. Text processing for improving analytics is being studied from different angles. In the literature, text depen- dencies have been employed to perform various tasks. This includes for example the identification of semantic relations and sentiment analysis. We observe that while text depen- dencies can boost text analytics, managing and preserving such dependencies in text documents that spread across var- ious corpora and contexts is a challenging task. We present in this paper our work on linking text dependencies using the Resource Description Framework (RDF) specification, following the Stanford typed dependencies representation. We contribute to thefield by providing analysts the means to query, extract, and reuse text dependencies for analytical purposes. We highlight how this additional layer can be used in the context of feedback analysis by applying a selection of queries passed to a triple-store containing the generated text dependencies graphs.","Graph analysis; Knowledge representation; Linked data; Per- formance management; Semantic web; Text analytics","Knowledge representation; Text processing; World Wide Web; Feedback analysis; Graph analysis; Linked datum; Resource description framework; Semantic relations; Sentiment analysis; Text analytics; Text document; Semantic Web",2-s2.0-84968611287
"Rubin V.L., Lukoianova T.","Truth and deception at the rhetorical structure level",2015,"Journal of the Association for Information Science and Technology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948402645&doi=10.1002%2fasi.23216&partnerID=40&md5=31a7edfe0e55c6c87a5fb8467a489642","This paper furthers the development of methods to distinguish truth from deception in textual data. We use rhetorical structure theory (RST) as the analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self-ranked as truthful or deceptive, is manually analyzed by assigning RST discourse relations among each story's constituent parts. A vector space model (VSM) assesses each story's position in multidimensional RST space with respect to its distance from truthful and deceptive centers as measures of the story's level of deception and truthfulness. Ten human judges evaluate independently whether each story is deceptive and assign their confidence levels (360 evaluations total), producing measures of the expected human ability to recognize deception. As a robustness check, a test sample of 18 truthful stories (with 180 additional evaluations) is used to determine the reliability of our RST-VSM method in determining deception. The contribution is in demonstration of the discourse structure analysis as a significant method for automated deception detection and an effective complement to lexicosemantic analysis. The potential is in developing novel discourse-based tools to alert information users to potential deception in computer-mediated texts. © 2014 ASIS&T.","computer mediated communications; discourse analysis; natural language processing","Natural language processing systems; Vector spaces; Automated deception detections; Computer mediated communication; Discourse analysis; Discourse structure analysis; NAtural language processing; Rhetorical structure; Rhetorical structure theory; Vector space models; Text processing; clinical trial; deception; human; model; reliability; structure analysis; theoretical model",2-s2.0-84948402645
"Rompf T., Brown K.J., Lee H., Sujeeth A.K., Jonnalagedda M., Amin N., Ofenbeck G., Stojanov A., Klonatos Y., Dashti M., Koch C., Püschel M., Olukotun K.","Go meta! a case for generative programming and DSLS in performance critical systems",2015,"Leibniz International Proceedings in Informatics, LIPIcs",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959045746&doi=10.4230%2fLIPIcs.SNAPL.2015.238&partnerID=40&md5=820a037b4b587b711085c2c27a873563","Most performance critical software is developed using very low-level techniques. We argue that this needs to change, and that generative programming is an effective avenue to enable the use of high-level languages and programming techniques in many such circumstances.","DSLs; Generative programming; Performance; Staging","Computational linguistics; Computer programming languages; Critical software; Critical systems; DSLs; Generative programming; Performance; Programming technique; Staging; High level languages",2-s2.0-84959045746
"Bačíková M., Porubän J., Chodarev S., Nosál' M.","Bootstrapping DSLs from user interfaces",2015,"Proceedings of the ACM Symposium on Applied Computing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955514391&doi=10.1145%2f2695664.2695994&partnerID=40&md5=b6a7d461749cce31dd5459e02de16358","Implementation of domain-specific languages (DSLs) is supported by powerful tools that can automatically generate various artifacts from the language specification. Domain analysis and design of a language, however, are usually done manually. Automatic extraction of domain information can be used to generate preliminary version of the language speci fication and therefore bootstrap the development process. One of the most suitable sources of domain information are graphical user interfaces (GUIs). In this paper we present the experiment validating this approach. Several existing applications were automatically analyzed to extract their domain models and generate a DSL processor based on them. Copyright 2015 ACM.","Domain analysis; Domain model; Domain-specific language; Graphical user interface","Computational linguistics; Computer programming languages; Problem oriented languages; Query languages; User interfaces; Automatic extraction; Development process; Domain analysis; Domain informations; Domain model; Domain specific languages; Graphical user interface (GUIs); Language specification; Graphical user interfaces",2-s2.0-84955514391
"Shnaiderman L., Shmueli O.","Multi-Core Processing of XML Twig Patterns",2015,"IEEE Transactions on Knowledge and Data Engineering",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925272479&doi=10.1109%2fTKDE.2014.2349907&partnerID=40&md5=3ef9d027579c4a0dc13e621e2eff52aa","XML is based on a tree-structured data model. Naturally, the most popular XML querying language (XPath) uses patterns of selection predicates, on multiple elements related by a tree structure, which often may be abstracted by twig patterns. Finding all occurrences of such a twig pattern in an XML database is a basic operation for XML query processing. We present the parallel path stack algorithm (PPS) and the parallel twig stack algorithm (PTS). PPS and PTS are novel and efficient algorithms for matching XML query twig patterns in a parallel multi-threaded computing platform. PPS and PTS are based on the PathStack and TwigStack algorithms [1]. These algorithms employ a sophisticated search technique for limiting processing to specific subtrees. We conducted extensive experimentation with PPS and PTS. We compared PPS and PTS to the standard (sequential) PathStack and TwigStack algorithms in terms of run time (to completion). We checked their performance for varying numbers of threads. Experimental results indicate that using PPS and PTS significantly reduces the running time of queries in comparison with the PathStack/TwigStack algorithm (up to 44 times faster for DBLP queries and up to 22 times faster for XMark queries). © 2014 IEEE.","Concurrency; Query processing","Algorithms; Forestry; Trees (mathematics); XML; Computing platform; Concurrency; Multi-core processing; Multiple elements; Search technique; Tree-structured data; XML query processing; XML twig pattern; Query processing; Algorithms; Experimentation; Languages",2-s2.0-84925272479
"Scontras G., Badecker W., Shank L., Lim E., Fedorenko E.","Syntactic complexity effects in sentence production",2015,"Cognitive Science",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926964905&doi=10.1111%2fcogs.12168&partnerID=40&md5=8041cadc39162bbd4f22fb4e2ad8cb05","Syntactic complexity effects have been investigated extensively with respect to comprehension (e.g., Demberg & Keller, 2008; Gibson, 1998, 2000; Gordon et al., 2001, 2004; Grodner & Gibson, 2005; King & Just, 1991; Lewis & Vasishth, 2005; Lewis et al., 2006; McElree et al., 2003; Wanner & Maratsos, 1978). According to one prominent class of accounts (experience-based accounts; e.g., Hale, 2001; Levy, 2008; Gennari & MacDonald, 2008, 2009; Wells et al., 2009), certain structures cause comprehension difficulty due to their scarcity in the language. But why are some structures less frequent than others? In two elicited-production experiments we investigated syntactic complexity effects in relative clauses (Experiment 1) and wh-questions (Experiment 2) varying in whether or not they contained non-local dependencies. In both experiments, we found reliable durational differences between subject-extracted structures (which only contain local dependencies) and object-extracted structures (which contain nonlocal dependencies): Participants took longer to begin and produce object-extractions. Furthermore, participants were more likely to be disfluent in the object-extracted constructions. These results suggest that there is a cost associated with planning and uttering the more syntactically complex, object-extracted structures, and that this cost manifests in the form of longer durations and disfluencies. Although the precise nature of this cost remains to be determined, these effects provide one plausible explanation for the relative rarity of object-extractions: They are more costly to produce. © 2014 Cognitive Science Society, Inc.","Relative clauses; Sentence processing; Sentence production; Syntactic complexity; Wh-questions; Working memory","adult; comprehension; female; human; language; male; methodology; procedures; psychology; reaction time; short term memory; task performance; Adult; Cognitive Science; Comprehension; Female; Humans; Language; Male; Memory, Short-Term; Reaction Time; Research Design; Task Performance and Analysis",2-s2.0-84926964905
"Porkoláb Z., Sinkovics Á., Siroki I.","DSL in C++ template metaprogram",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925372900&doi=10.1007%2f978-3-319-15940-9_3&partnerID=40&md5=4fe8b2d0554df766542244e98f3c02dc","Domain specific language integration has to provide the right balance between the expressive power of the DSL and the implementation and maintenance cost of the applied integration techniques. In this paper we discuss a DSL integration technique for the C++ programming language. The solution is based on compile-time parsing of the DSL code using the C++ template metaprogramming library called Metaparse. The parser generator is the C++ template metaprogram reimplementation of a runtime Haskell parser generator library. The full parsing phase is executed when the host program is compiled. The library uses only standard C++ language features, thus our solution is highly portable. As a demonstration of the power of this approach, we present a highly efficient and type-safe version of printf and the way it can be constructed using our library. Despite the well known syntactical difficulties of C++ template metaprograms, building embedded languages using Metaparse leads to self-documenting maintenable C++ source code. © Springer International Publishing Switzerland 2015.",,"Computational linguistics; Computer programming languages; High level languages; Integration; Maintenance; Problem oriented languages; Syntactics; C++ template metaprogramming; Domain specific languages; Embedded Languages; Expressive power; Highly-portable; Integration techniques; Maintenance cost; Parser generators; C++ (programming language)",2-s2.0-84925372900
"Nie L., Zhao Y.-L., Akbari M., Shen J., Chua T.-S.","Bridging the vocabulary gap between health seekers and healthcare knowledge",2015,"IEEE Transactions on Knowledge and Data Engineering",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920134020&doi=10.1109%2fTKDE.2014.2330813&partnerID=40&md5=13fa9b8465d656684c138481150035b6","The vocabulary gap between health seekers and providers has hindered the cross-system operability and the inter-user reusability. To bridge this gap, this paper presents a novel scheme to code the medical records by jointly utilizing local mining and global learning approaches, which are tightly linked and mutually reinforced. Local mining attempts to code the individual medical record by independently extracting the medical concepts from the medical record itself and then mapping them to authenticated terminologies. A corpus-aware terminology vocabulary is naturally constructed as a byproduct, which is used as the terminology space for global learning. Local mining approach, however, may suffer from information loss and lower precision, which are caused by the absence of key medical concepts and the presence of irrelevant medical concepts. Global learning, on the other hand, works towards enhancing the local medical coding via collaboratively discovering missing key terminologies and keeping off the irrelevant terminologies by analyzing the social neighbors. Comprehensive experiments well validate the proposed scheme and each of its component. Practically, this unsupervised scheme holds potential to large-scale data. © 2014 IEEE.","global learning; Healthcare; local mining; medical terminology assignment; question answering","Codes (symbols); Health care; Knowledge based systems; Reusability; Global learning; Information loss; Large scale data; Lower precision; Medical concepts; Medical record; Medical terminologies; Question Answering; Terminology",2-s2.0-84920134020
"Liang P., Potts C.","Bringing Machine Learning and Compositional Semantics Together",2015,"Annual Review of Linguistics",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987707828&doi=10.1146%2fannurev-linguist-030514-125312&partnerID=40&md5=13923611b2a6394d1780830df7b75a32","Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. Wealso consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity. © Copyright 2015 by Annual Reviews. All rights reserved.","compositionality; discriminative learning; distributed representations; logical forms; recursive neural networks; semantic parsing",,2-s2.0-84987707828
"Sakthika R.I., Kogilavani S.V.","An integration of triple parser with predicate based approach for search engine development",2015,"2nd International Conference on Electronics and Communication Systems, ICECS 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942683511&doi=10.1109%2fECS.2015.7124969&partnerID=40&md5=da82c4e566f9121b9765b77ff7d347b4","The rapid development of modern technology has resulted in large amount of electronically available information in articles and patents. The search engines are programs that searches the documents in a database correspond to queries specified by the user. Searching by using keywords for millions of documents will not be precise and thus retrieve incomplete information. This makes it difficult for the user to detect relevant documents manually from large document collection. The keyword based approach uses the vector space model for representing both query and documents as vectors. Terms with partial matching are not associated in this method. Predicate based approach is the method to provide the user with a more precise and complete information with a single query using predicates for both query and document representation. Predicates are also called triples that contain complex data structures and are more structured information. Predicate based search engine is done by combining several methods such as predicate based vector space model, query-document similarity function with adjusted TF-IDF and boost function. The goal is to find feasible methods to develop the predicate parser to generate a well structured text. The boost function is integrated with similarity measure to generate a rich and sophisticated information search. The proper order of relevant documents will considerably improve the performance. © 2015 IEEE.","Keyword Search; Predicate Search; Triples","Query languages; Query processing; Search engines; Complete information; Complex data structures; Document Representation; Incomplete information; Keyword search; Predicate Search; Structured information; Triples; Vector spaces",2-s2.0-84942683511
"Palakurthi A., Ruthu S.M., Akula A.R., Mamidi R.","Classification of attributes in a natural language query into different SQL clauses",2015,"International Conference Recent Advances in Natural Language Processing, RANLP",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949745517&partnerID=40&md5=54a27037092aa7bce662d5413b38d894","Attribute information in a natural language query is one of the key features for converting a natural language query into a Structured Query Language1 (SQL) in Natural Language Interface to Database systems. In this paper, we explore the task of classifying the attributes present in a natural language query into different SQL clauses in a SQL query. In particular, we investigate the effectiveness of various features and Conditional Random Fields for this task. Our system uses a statistical classifier trained on manually prepared data. We report our results on three different domains and also show how our system can be used for generating a complete SQL query.",,"Classification (of information); Computational linguistics; Query languages; Query processing; Search engines; Attribute information; Conditional random field; Different domains; Key feature; Natural language interface to database; Natural language queries; Statistical classifier; Structured Query Language; Natural language processing systems",2-s2.0-84949745517
"Haas C., Riezler S.","Response-based learning for machine translation of open-domain database queries",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960157151&partnerID=40&md5=165b2483ce605d04cd71f8a01d78a760","Response-based learning allows to adapt a statistical machine translation (SMT) system to an extrinsic task by extracting supervision signals from task-specific feedback. In this paper, we elicit response signals for SMT adaptation by executing semantic parses of translated queries against the Freebase database. The challenge of our work lies in scaling semantic parsers to the lexical diversity of opendomain databases. We find that parser performance on incorrect English sentences, which is standardly ignored in parser evaluation, is key in model selection. In our experiments, the biggest improvements in F1-score for returning the correct answer from a semantic parse for a translated query are achieved by selecting a parser that is carefully enhanced by paraphrases and synonyms. © 2015 Association for Computational Linguistics.",,"Computer aided language translation; Database systems; Linguistics; Query languages; Query processing; Semantics; Signal processing; Speech transmission; Syntactics; Translation (languages); Database queries; English sentences; F1 scores; Machine translations; Model Selection; Response signal; Statistical machine translation; Computational linguistics",2-s2.0-84960157151
"Baget J.-F., Leclère M., Mugnier M.-L., Rocher S., Sipieter C.","Graal: A toolkit for query answering with existential rules",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951021129&doi=10.1007%2f978-3-319-21542-6_21&partnerID=40&md5=b61b9789b5590dbd42228024a5a0af58","This paper presents Graal, a java toolkit dedicated to ontological query answering in the framework of existential rules. We consider knowledge bases composed of data and an ontology expressed by existential rules. The main features of Graal are the following: a basic layer that provides generic interfaces to store and query various kinds of data, forward chaining and query rewriting algorithms, structural analysis of decidability properties of a rule set, a textual format and its parser, and import of OWL 2 files. We describe in more detail the query rewriting algorithms, which rely on original techniques, and report some experiments. © Springer International Publishing Switzerland 2015.",,"Computer programming languages; Decidability properties; Generic interfaces; Knowledge basis; Ontological query; Query answering; Query rewritings; Rule set; Textual format; Data flow analysis",2-s2.0-84951021129
"Borodin A., Kiselev Y., Mirvoda S., Porshnev S.","On design of domain-specific query language for the metallurgical industry",2015,"Communications in Computer and Information Science",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928910683&doi=10.1007%2f978-3-319-18422-7_45&partnerID=40&md5=4d7da8ecda446db494ced5290360bac9","Many systems have to choose between user-friendly visual query editor and textual querying language with industrial strength in order to deal with big amount of complex data. Some of them provide both ways of accessing data in a warehouse. In this paper, we present key features of domain specific querying language, which we designed as a part of informational system of a steel production plant. This language aims to give an opportunity of easy data manipulation to those who know what the data actually is and to provide an easy way to discover what the data is for others. We also provide an evaluation of the designed language. The main focus of our estimation was to measure effort required for discovering dataset and deriving simple math expressions. Although the paper overviews a data model for one specific domain, it can be easily applied for different domains. © Springer International Publishing Switzerland 2015.","Data warehouse; Domain-specific language; Metallurgy; Query language; Steel plant","Computational linguistics; Computer programming languages; Data warehouses; Iron and steel plants; Metallurgy; Problem oriented languages; Query languages; Steel metallurgy; Steelmaking; Visual languages; Data manipulations; Different domains; Domain specific languages; Domain-specific query languages; Industrial strength; Informational system; Metallurgical industry; Steel production; Search engines",2-s2.0-84928910683
"Thangarasu J., Geetha P.","Diversity in image retrieval based on inferring user image search goals",2015,"2015 International Conference on Pervasive Computing: Advance Communication Technology and Application for Society, ICPC 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929252404&doi=10.1109%2fPERVASIVE.2015.7087105&partnerID=40&md5=ed603fd19f562b4c23d0f6dd606ae34d","Image retrieval is an extremely tough process to retrieve the user perception based on the customer intervention from the dynamic data set. Various technologies are there to retrieve the images with the similar technique is mining user's search goals. In this paper we summarized all the existing technique and we introduced our improved user image search goal prediction and retrieving. To make this browsing process more efficient, image summarization is often needed to address this problem. In web search applications, users submit queries (i.e., some keywords) to search engines to represent their search goals. However, in many cases, queries may not exactly represent what they want. For this problem we analyzed various existing methodologies like visual, clustering, colors/shape based, image, feature based. Each method contains different output and features. In this paper we compared these methods and we proposed our work based on text and image based retrieval. © 2015 IEEE.","Classification & Clustering; Click Session information; Parser (Extraction & Transformation); Re-ranking; User Query","Classification (of information); Image processing; Information retrieval; Search engines; Ubiquitous computing; World Wide Web; Click Session information; Dynamic data sets; Feature-based; Image summarization; Re-ranking; User perceptions; User query; Various technologies; Image retrieval",2-s2.0-84929252404
"Sakthika R.I., Kogilavani S.V.","An integration of predicate parser with supervised learning algorithm for search engine development",2015,"International Journal of Applied Engineering Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942437226&partnerID=40&md5=34a9b180c857753e962bc5cc885a974f","The rapid development of modern technology has resulted in large amount of electronically available information in articles and patents. The search engines are programs that searches the documents in a database correspond to queries specified by the user. Searching by using keywords for millions of documents will not be precise and thus retrieve incomplete information. This makes it difficult for the user to detect relevant documents manually from large document collection. The keyword based approach uses the vector space model for representing both query and documents as vectors. Terms with partial matching are not associated in this method. Predicate based approach is the method to provide the user with a more precise and complete information with a single query using predicates for both query and document representation. Predicates are also called triples that contain complex data structures and are more structured information. Predicate based search engine is done by combining several methods such as predicate based vector space model, query-document similarity function with adjusted TF-IDF and boost function. The goal is to find feasible methods to develop the predicate parser to generate a well structured text. The boost function is integrated with similarity measure to generate a rich and sophisticated information search. The proper order of relevant documents will considerably improve the performance. © Research india publications.","Keyword search; Predicate search; Triples",,2-s2.0-84942437226
"Wang Y., Berant J., Liang P.","Building a semantic parser overnight",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943776557&partnerID=40&md5=d3d1ebd4b605c55f7bbfdf9d4ec53b26","How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. © 2015 Association for Computational Linguistics.",,"Linguistics; Natural language processing systems; Semantics; Compositionality; Crowdsourcing; Logical forms; Training example; Computational linguistics",2-s2.0-84943776557
"Huber R., Klump J.","Agenames a stratigraphic information harvester and text parser",2015,"Earth Science Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939884629&doi=10.1007%2fs12145-014-0171-5&partnerID=40&md5=1c48e866845e8df7718ef58d0490097c","A common task for earth scientists is the search for stratigraphic background information on a certain rock unit, e.g. its age and properties and its position within the hierarchy of stratigraphic units. Analogously, when geoscientists search for information in databases or within the internet the stratigraphic and geospatial constraints serve as a first orientation within a huge amount of data. However, spatio-temporal information is mostly only implicitly encoded e.g. in the title and abstract given in bibliographic databases and library catalogues. Means to decode spatial information from such texts has become commonly available through gazetteers and geocoding services, but the paleotemporal information remains elusive. Agenames is a stratigraphic information harvester and text parser which offers a web-service to parse geological texts and identify stratigraphic terms. The service has both a web-based GUI and a REST interface. The Agenames ontology records the stratigraphic rank of e.g. a chronostratigraphic or lithostratigraphic unit and the hierarchical relations between terms. Any geologic body has an associated age of origin, assigned by relation to a geochronologic unit from the geologic time scale. In a given text Agenames will identify potential stratigraphic keywords and use these terms to assign a geological age estimate. The web-service can also be used to augment web portals or library catalogues. Agenames can be used to generate indices to infer the relations of library catalogue entries or web pages to the approximate geological epoch that is covered in a text. This allows, for instance, including stratigraphic age into a catalogue search without the need to specify all possibly relevant terms in complex queries. © 2014, Springer-Verlag Berlin Heidelberg.","Knowledge management; Ontology; Stratigraphy; Text parsing",,2-s2.0-84939884629
"Ferreira E., Jabaian B., Lefevre F.","Zero-shot semantic parser for spoken language understanding",2015,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959176550&partnerID=40&md5=35a586b4901c51844ec068203bea3ec1","Machine learning algorithms are now common in the state-ofthe- art spoken language understanding models. But to reach good performance they must be trained on a potentially large amount of data which are not available for a variety of tasks and languages of interest. In this work, we present a novel zero-shot learning method, based on word embeddings, allowing to derive a full semantic parser for spoken language understanding. No annotated in-context data are needed, the ontological description of the target domain and generic word embedding features (learned from freely available general domain data) suffice to derive the model. Two versions are studied with respect to how the model parameters and decoding step are handled, including an extension of the proposed approach in the context of conditional random fields. We show that this model, with very little supervision, can reach instantly performance comparable to those obtained by either state-of-the-art carefully handcrafted rule-based or trained statistical models for extraction of dialog acts on the Dialog State Tracking test datasets (DSTC2 and 3). Copyright © 2015 ISCA.","Out-of-domain training data; Spoken language understanding; Word embedding; Zero-shot learning","Algorithms; Artificial intelligence; Learning algorithms; Learning systems; Semantics; Speech communication; Speech recognition; Conditional random field; Handcrafted rules; Learning methods; Model parameters; Spoken language understanding; Training data; Word embedding; Zero-shot learning; Computational linguistics",2-s2.0-84959176550
"Quirk C., Mooney R., Galley M.","Language to code: Learning semantic parsers for if-This-Then-That recipes",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943812532&partnerID=40&md5=89dcd0f2efcb4f00d256ebada560a787","Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descriptions of simple ""if-Then"" rules to executable code. By training and testing on a large corpus of naturally-occurring programs (called ""recipes"") and their natural language descriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing approaches on the highly noisy training data collected from ordinary users, and find that loosely synchronous systems perform best. © 2015 Association for Computationl Linguisticss.",,"Ability testing; Codes (symbols); Linguistics; Natural language processing systems; Semantics; Executable codes; Learning semantics; Natural languages; Naturally occurring; Semantic parsing; Synchronous system; Training and testing; Training data; Computational linguistics",2-s2.0-84943812532
"Smits G., Pivert O., Thion V.","Connected keywords",2015,"Proceedings - International Conference on Research Challenges in Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937912480&doi=10.1109%2fRCIS.2015.7128869&partnerID=40&md5=279ba6a99ddc4de11a466d9e57d2a649","To improve the expressivity and accuracy of database query interfaces, a keyword-based constrained query language is introduced to let users explicitly express the intent of their search using keywords linked by meaningful grammatical connectives. Individually, keywords and connectives correspond to textual descriptions attached to components of the database graph schema, and as a whole, a so-called connected keywords query corresponds to a textual description of an SQL query. The translation process of such a query into SQL is mainly composed of two steps: first, the syntactic structure of the keyword query is analyzed to exhibit projection and selection statements using predefined graph patterns, then non explicit joins are deduced to obtain a complete translation of the keyword query as a meaningful connected subgraph. Experimentations show the relevance of the approach in terms of expressivity and efficiency. © 2015 IEEE.",,"Information science; Query languages; Syntactics; Connected Subgraph; Database queries; Graph patterns; Keyword queries; Keyword-based; Syntactic structure; Textual description; Translation process; Query processing",2-s2.0-84937912480
"Vivekanandam S., Jaganathan P.","A concept based ontology mapping method for effective retrieval of bio-medical documents",2015,"Journal of Medical Imaging and Health Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938406318&doi=10.1166%2fjmihi.2015.1481&partnerID=40&md5=e84386dc62a762aecdd1f5574af0bf35","Nowadays, document retrieval is one of the major tasks in Biomedical Research. Huge data makes the tasks of searching, accessing, displaying, integrating and preserving the data more difficult. While retrieving the documents, the problem that arises is the retrieval of non-related documents. In order to overcome this, a concept based ontology mapping method is proposed in this paper. Initially, a query document is given to the retrieval system for retrieving the matched documents based on the concepts in the document. A Standford parser is used for making the document into sentences and then into Parts-of-speech tagger and Context-free phrase structure grammar representation. The resultant noun phrases and verb phrases are considered as concepts and linking phrases. From these the concept maps are generated for each document. Then the concepts from the concept map of query document are matched with the concept maps of the target document collection using WordNet Ontology. If the Similarity measure for the matching is less than the threshold value means, then alternate documents are checked for the matching. Otherwise, the relevant documents for the matched query is retrieved based on the concept maps from the document storage. The experimentation was carried out with the help of biomedical documents. The performance of this proposed retrieval system was analyzed by the existing methods for different medical documents. The proposed retrieval system shows that the documents are retrieved effectively based on concepts and the system performs with higher accuracy value. Copyright © 2015 American Scientific Publishers All rights reserved.","Concept Mapping; Context-Free Phrase Structure Grammar Representation; Cosine Similarity Measure; Parts-of-Speech Tagger; Stanford Parser; Word Net Ontology","access to information; algorithm; Article; concept mapping; data base; evaluation study; extraction; information retrieval; language; medical documentation; medical information; ontology; search engine; speech",2-s2.0-84938406318
"Ghani Khan M.U., Amanat S., Nasir A., Iqbal R., Idrees M.","A computational framework for unified biological databank to overcome heterogeneity of biological data format",2015,"Proceedings of the Pakistan Academy of Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930424825&partnerID=40&md5=0dd5b8102b3256016b1c07d3a464b391","Building block of life is DNA which produces RNA and protein sequences; all these molecular blocks lie in the category of biological data. Biological data which comprises various organisms like animals, plants, viruses, bacteria and humans,throughout the globe has heterogeneous nature. This heterogeneity is caused by different parameters such as storage mechanism, information representation and content format. The abundance of heterogeneity creates a havoc towards optimized exploitation, integration, storage and retrieval of existing biological data. This work introduces a computational framework for achieving a unified format for biological data, which can accommodate different formats of storage; and data presentation. An information retrieval system for biological data has been developed which sheds light on different recompenses gained by this unified format of the biological data. © Pakistan Academy of Sciences.","Biological data; Heterogeneous format; Integration; Parser; Unified format",,2-s2.0-84930424825
"Xie H., Wang M., Le J., Sun L.","Calculation results characteristics extract and reuse strategy based on hive",2015,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946434689&doi=10.7544%2fissn1000-1239.2015.20140548&partnerID=40&md5=e0735abf58ca30841e8308f0f7cb706d","Jobs in MapReduce workflow need to materialize intermediate data into HDFS (Hadoop distributed file system), which causes a large amount of I/O overhead and low efficiency. Based on existing representative work Hive, this paper proposes a strategy to match and reuse the MapReduce calculation results by extracting and storing the characteristics of the results. Firstly, we define Join-Graph, Join-Object and other structures according to the query condition, which can be used to find reusable results. Based on the abstract syntax tree generated by HiveQL (Hive query language) parser, an algorithm is proposed to generate Join-Object of the query. Followed by traversing the candidate Join-Object list, an algorithm is provided to generate the best reuse solution including single Join-Object and multiple Join-Objects reuse. In addition, we provide three methods to increase the reuse probability, including multi-key selection, arithmetic delay and semantic understanding. Finally, we conduct the experiments using TPC-H and SSB benchmarks. The results show that the efficiency is improved by 28%-52% when reusing single Join-Object by TPC-H, while it is improved by up to 75% when reusing multiple Join-Objects, and the efficiency of all the 22 queries is improved by 15.7% on average. By SSB, the efficiency is improved by 40% to 76%, 55% on average. ©, 2015, Science Press. All right reserved.","Calculation results reuse; Data management; Hive; Join-Object; MapReduce","Efficiency; File organization; Information management; Query languages; Semantics; Syntactics; Abstract Syntax Trees; Calculation results; Hadoop distributed file systems; Hive; Map-reduce; Query conditions; Reuse strategy; Semantic understanding; Trees (mathematics)",2-s2.0-84946434689
"Yih W.-T., Chang M.-W., He X., Gao J.","Semantic parsing via staged query graph generation: Question answering with knowledge base",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943770995&partnerID=40&md5=6731ff30213675961109a8d930b81053","We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5% on the WEBQUESTIONS dataset. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Linguistics; Neural networks; Semantics; Convolutional neural network; Knowledge base; Question Answering; Search problem; Search spaces; Semantic matching; Semantic parsing; Traditional approaches; Natural language processing systems",2-s2.0-84943770995
"Xu K., Feng Y., Huang S., Zhao D.","Question answering via phrasal semantic parsing",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945938886&doi=10.1007%2f978-3-319-24027-5_43&partnerID=40&md5=a24af07fb7cb33e03d511dcf9e7db65d","Understanding natural language questions and converting them into structured queries have been considered as a crucial way to help users access large scale structured knowledge bases. However, the task usually involves two main challenges: recognizing users’ query intention and mapping the involved semantic items against a given knowledge base (KB). In this paper, we propose an efficient pipeline framework to model a user’s query intention as a phrase level dependency DAG which is then instantiated regarding a specific KB to construct the final structured query. Our model benefits from the efficiency of linear structured prediction models and the separation of KB-independent and KB-related modelings. We evaluate our model on two datasets, and the experimental results showed that our method outperforms the state-of-the-art methods on the Free917 dataset, and, with limited training data from Free917, our model can smoothly adapt to new challenging dataset, WebQuestion, without extra training efforts while maintaining promising performances. © Springer International Publishing Switzerland 2015.",,"Association reactions; Knowledge based systems; Natural language processing systems; Semantics; Limited training data; Natural language questions; Question Answering; Semantic parsing; State-of-the-art methods; Structured knowledge; Structured prediction; Structured queries; Query processing",2-s2.0-84945938886
"Amsterdamer Y., Kukliansky A., Milo T.","A natural language interface for querying general and individual knowledge",2015,"Proceedings of the VLDB Endowment",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953847630&partnerID=40&md5=c9813f70cb1a518a44117e79a65cc4e4","Many real-life scenarios require the joint analysis of general knowledge, which includes facts about the world, with individual knowledge, which relates to the opinions or habits of individuals. Recently developed crowd mining platforms, which were designed for such tasks, are a major step towards the solution. However, these platforms require users to specify their information needs in a formal, declarative language, which may be too complicated for naïve users. To make the joint analysis of general and individual knowledge accessible to the public, it is desirable to provide an interface that translates the user questions, posed in natural language (NL), into the formal query languages that crowd mining platforms support. While the translation of NL questions to queries over conventional databases has been studied in previous work, a setting with mixed individual and general knowledge raises unique challenges. In particular, to support the distinct query constructs associated with these two types of knowledge, the NL question must be partitioned and translated using different means; yet eventually all the translated parts should be seamlessly combined to a well-formed query. To account for these challenges, we design and implement a modular translation framework that employs new solutions along with state-of-the art NL parsing tools. The results of our experimental study, involving real user questions on various topics, demonstrate that our framework provides a high-quality translation for many questions that are not handled by previous translation tools. © 2015 VLDB Endowment 2150-8097/15/08.",,"Computational linguistics; Natural language processing systems; Query languages; Query processing; Conventional database; Declarative Languages; Design and implements; General knowledge; Natural language interfaces; Natural languages; State of the art; Translation tools; Translation (languages)",2-s2.0-84953847630
"Steinhardt J., Liang P.","Learning with relaxed supervision",2015,"Advances in Neural Information Processing Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965124443&partnerID=40&md5=f0991d61171b8a0e8dce97a11be60d59","For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.",,"Information science; Query processing; Asymptotic consistencies; Database queries; Graphics programs; Latent variable; Parameter estimate; Relaxation parameter; Rigorous approach; Equivalence classes",2-s2.0-84965124443
"Karan E., Irizarry J., Haymaker J.","Generating IFC models from heterogeneous data using semantic web",2015,"Construction Innovation",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928802436&doi=10.1108%2fCI-05-2014-0030&partnerID=40&md5=66cb441a41a8a44e1b01da61ec091556","Purpose - This paper aims to develop a framework to represent semantic web query results as Industry Foundation Class (IFC) building models. The subject of interoperability has received considerable attention in the construction literature in recent years. Given the distributed, semantically heterogeneous data sources, the problem is to retrieve information accurately and with minimal human intervention by considering their semantic descriptions. Design/methodology/approach - This paper provides a framework to translate semantic web query results into the XML representations of IFC schema and data. Using the concepts and relationships in an IFC schema, the authors first develop an ontology to specify an equivalent IFC entity in the query results. Then, a mapping structure is defined and used to translate and fill all query results into an ifcXML document. For query processing, the proposed framework implements a set of predefined query mappings between the source schema and a corresponding IFC output schema. The resulting ifcXML document is validated with an XML schema validating parser and then loaded into a building information modeling (BIM) authoring tool. Findings - The research findings indicate that semantic web technology can be used, accurately and with minimal human intervention, to maintain semantic-level information when transforming information between web-based and BIM formats. The developed framework for representing IFC-compatible outputs allows BIM users to query and access building data at any time over the web from data providers. Originality/value - Currently, the results of semantic web queries are not supported by BIM authoring tools. Thus, the proposed framework utilizes the capabilities of semantic web and query technologies to transform the query results to an XML representation of IFC data.","BIM; Construction management; IFC; Interoperability; Query mapping; Semantic web",,2-s2.0-84928802436
"Liu Y., Hao Y., Zhu X., Li J.","A question answering system built on domain knowledge base",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937396081&doi=10.1007%2f978-3-319-21042-1_9&partnerID=40&md5=e59f1c791e7704c981f90f64a66b751e","Interactive Question Answering (QA) system is capable of answering users’ questions with managing/understanding the dialogs between human and computer. With the increasing amount of online information, it is highly needed to answer users’ concerns on a specific domain such as health-related questions. In this paper, we proposed a general framework for domain-specific interactive question answering systems which takes advance of domain knowledge bases. First, a semantic parser is generated to parse users’ questions to the corresponding logical forms on basis of the knowledge base structure. Second, the logical forms are translated into query language to further harvest answers from the knowledge base. Moreover, our framework is featured with automatic dialog strategy development which relies on manual intervention in traditional interactive QA systems. For evaluation purpose, we applied our framework to a Chinese interactive QA system development, and used a health-related knowledge base as domain scenario. It shows promising results in parsing complex questions and holding long history dialog. © Springer International Publishing Switzerland 2015.",,"Artificial intelligence; Information management; Knowledge based systems; Query languages; Semantics; Social networking (online); Complex questions; Domain knowledge; Domain knowledge base; Domain specific; Manual intervention; On-line information; Question answering systems; Strategy development; Natural language processing systems",2-s2.0-84937396081
"Sathick J., Venkat J.","A generic framework for extraction of knowledge from social web sources (social networking websites) for an online recommendation system",2015,"International Review of Research in Open and Distance Learning",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929259738&partnerID=40&md5=c1826896062a81531d600ed8f26368b7","Mining social web data is a challenging task and finding user interest for personalized and non-personalized recommendation systems is another important task. Knowledge sharing among web users has become crucial in determining usage of web data and personalizing content in various social websites as per the user's wish. This paper aims to design a framework for extracting knowledge from web sources for the end users to take a right decision at a crucial juncture. The web data is collected from various web sources and structured appropriately and stored as an ontology based data repository. The proposed framework implements an online recommender application for the learners online who pursue their graduation in an open and distance learning environment. This framework possesses three phases: data repository, knowledge engine, and online recommendation system. The data repository possesses common data which is attained by the process of acquiring data from various web sources. The knowledge engine collects the semantic data from the ontology based data repository and maps it to the user through the query processor component. Establishment of an online recommendation system is used to make recommendations to the user for a decision making process. This research work is implemented with the help of an experimental case study which deals with an online recommendation system for the career guidance of a learner. The online recommendation application is implemented with the help of R-tool, NLP parser and clustering algorithm.This research study will help users to attain semantic knowledge from heterogeneous web sources and to make decisions.","K-means clustering; Knowledge engine; Knowledge extraction; Knowledge management; Online recommender interface; Query mapping; R tool, natural language processing; Social web source; Web mining, decision making system",,2-s2.0-84929259738
"Booth J., Di Eugenio B., Cruz I.F., Wolfson O.","Robust natural language processing for urban trip planning",2015,"Applied Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945577924&doi=10.1080%2f08839514.2015.1082280&partnerID=40&md5=6453b44c3d58831c0bf3b37f7881a697","Navigating large, urban transportation networks is a complicated task. A user needs to negotiate the available modes of transportation, their schedules, and how they are interconnected. In this article we present a Natural Language interface for trip planning in complex, multimodal, urban transportation networks. Our objective is to provide robust understanding of complex requests while giving the user flexibility in their language.We designed TRANQUYL, a transportation query language for trip planning; we developed a user-centric ontology, which defines the concepts covered by the interface and allows for a broad vocabulary. NL2TRANQUYL, the software system built on these foundations, translates English requests into formal TRANQUYL queries. Through a detailed intrinsic evaluation, we show that NL2TRANQUYL is highly accurate and robust with respect to paraphrasing requests, as well as handling fragmented or telegraphic requests. © 2015 Taylor & Francis Group, LLC.",,"Complex networks; Computational linguistics; Natural language processing systems; Query languages; Transportation; Highly accurate; Multi-modal; Natural language interfaces; NAtural language processing; Software systems; Trip planning; Urban transportation networks; User-centric; Urban transportation",2-s2.0-84945577924
"Harwell J., Pentoney C., Leroy G.","Finding and understanding medical information online",2015,"Information Technology for Patient Empowerment in Healthcare",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960297589&doi=10.1515%2f9781614514343-016&partnerID=40&md5=21fcc0e51a13dc4a8f42bed147df3c58","The internet provides a wealth of information brought together by millions of people. Searching for information is one of the most frequent online activities. As a result, search engines have become tremendously efficient in gathering more information and presenting it quickly and efficiently. The information is available at the click of a button and presented in bite-size text snippets. There are two potential problems with today’s use of search engines that have an especially disturbing impact in health care. First of all, not all information is trustworthy, much is incomplete, biased or subjective, and finding a complete answer in response to a query is not facilitated by the search engine interface or the results presentation. Second, searching objectively is difficult. Only a few words are used to search among millions of documents, those are used by the search engine to guess, estimate, or decide what is relevant, and only matching information is returned even if the stated query is erroneous. Users, in turn, are often unaware of missing information. Many, especially younger generations, do not read entire texts but limit themselves to scanning snippets provided in the results list.[1] These limitations are problematic since these search engines inform and influence entire families and communities. This chapter will review query options for searching and presenting results and how these impact searching for and understanding of medical information by patients, caregivers and medical professionals. © 2015 Walter de Gruyter Inc., Berlin/Boston/Munich.",,"Bioinformatics; Query processing; Medical information; Medical professionals; Missing information; Results presentation; Search engine interfaces; Searching for informations; Wealth of information; Younger generations; Search engines",2-s2.0-84960297589
"Tran D.H., Nguyen H.P., Hanh Le D.","Easysearch: Finding relevant functions based on API documentation",2015,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910618814&doi=10.1007%2f978-3-319-11680-8_12&partnerID=40&md5=7914014c128225da8aeb0a0317c49d75","This subject proposes a hybrid approach integrating keyword-based and semantic-based search to find relevant functions. Structure and contents of API documentation (API-Doc) are mined in our approach. The approach uses natural language processing technologies to expand queries, exploits the multi-level categorized structure of API-Doc to find candidates, and applies API-Doc’s short function descriptions to refine solutions. Our case study with eleven Java programmers demonstrates that our approach is better than the other approaches in precision, recall and F-measure criteria. © Springer International Publishing Switzerland 2015.","API documentation; Code search; Keyword weighting; Query expansion; Semantic ranking","Java programming language; Natural language processing systems; Semantics; Systems engineering; Code search; Hybrid approach; Java programmers; Keyword weighting; Keyword-based; NAtural language processing; Query expansion; Semantic rankings; Application programming interfaces (API)",2-s2.0-84910618814
"Ribeiro F., Muhammad H., Maidl A.M., Ierusalimschy R.","Preserving lexical scoping when dynamically embedding languages",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951831790&doi=10.1007%2f978-3-319-24012-1_3&partnerID=40&md5=46fb7af8ef4433254e6af3f7db309172","There are various situations in which one may want to embed source code from one language into another, for example when combining relational query languages with application code or when performing staged meta-programming. Typically, one will want to transfer data between these languages. We propose an approach in which the embedded code shares variables with the host language, preserving lexical scoping rules even after the code is converted into an intermediate representation. We demonstrate this approach through a module for meta-programming using Lua as both embedded and host languages. Our technique supports dynamically generated code, requires no special annotation of functions to be translated and is implemented as a library, requiring no source pre-processing or changes to the host language execution environment. © Springer International Publishing Switzerland 2015.","Domain-specific languages; Embedded languages; Lua; Meta-programming; Multi-stage programming","Codes (symbols); Computer programming languages; Object oriented programming; Problem oriented languages; Query languages; Domain specific languages; Embedded Languages; Lua; Meta Programming; Multi-stage programming; Computational linguistics",2-s2.0-84951831790
"Hixon B., Clark P., Hajishirzi H.","Learning knowledge graphs for question answering through conversational dialog",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959088414&partnerID=40&md5=3aa278bc25edc8bb00e9371c32acfabb","We describe how a question-answering system can learn about its domain from conversational dialogs. Our system learns to relate concepts in science questions to propositions in a fact corpus, stores new concepts and relations in a knowledge graph (KG), and uses the graph to solve questions. We are the first to acquire knowledge for question-answering from open, natural language dialogs without a fixed ontology or domain model that predetermines what users can say. Our relation-based strategies complete more successful dialogs than a query expansion baseline, our taskdriven relations are more effective for solving science questions than relations from general knowledge sources, and our method is practical enough to generalize to other domains. © 2015 Association for Computational Linguistics.",,"Artificial intelligence; Linguistics; Natural language processing systems; Domain model; General knowledge; Knowledge graphs; Natural languages; Query expansion; Question Answering; Question answering systems; Relation-based; Computational linguistics",2-s2.0-84959088414
"Liu X., Gao J., He X., Deng L., Duh K., Wang Y.-Y.","Representation learning using multi-task deep neural networks for semantic classification and information retrieval",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960086318&partnerID=40&md5=47974320519472a5f8235c42c72516ce","Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Information retrieval; Linguistics; Natural language processing systems; Semantics; World Wide Web; Deep neural networks; Domain adaptation; Multiple domains; Multiple tasks; NAtural language processing; Query classification; Semantic classification; Training data; Classification (of information)",2-s2.0-84960086318
"Hughes J.N., Annex A., Eichelberger C.N., Fox A., Hulbert A., Ronquest M.","GeoMesa: A distributed architecture for spatio-temporal fusion",2015,"Proceedings of SPIE - The International Society for Optical Engineering",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948823516&doi=10.1117%2f12.2177233&partnerID=40&md5=0127014e31dfdf9b9f25574c10069e68","Recent advances in distributed databases and computing have transformed the landscape of spatio-temporal machine learning. This paper presents GeoMesa, a distributed spatio-temporal database built on top of Hadoop and column-family databases such as Accumulo and HBase, that includes a suite of tools for indexing, managing and analyzing both vector and raster data. The indexing techniques use space filling curves to map multidimensional data to the single lexicographic list managed by the underlying distributed database. In contrast to traditional non-distributed RDBMS, GeoMesa is capable of scaling horizontally by adding more resources at runtime; the index rebalances across the additional resources. In the raster domain, GeoMesa leverages Accumulo's server-side iterators and aggregators to perform raster interpolation and associative map algebra operations in parallel at query time. The paper concludes with two geo-time data fusion examples: using GeoMesa to aggregate Twitter data by keywords; and georegistration to drape full-motion video (FMV) over terrain. © 2015 SPIE.","Accumulo; Distributed computing; Distributed databases; Geohash; GeoMesa; NoSQL; Space filling curve; Spatio-temporal fusion","Artificial intelligence; Data fusion; Database systems; Distributed computer systems; Indexing (of information); Information science; Learning systems; Query processing; Accumulo; Distributed database; Geohash; GeoMesa; NoSQL; Space-filling curve; Spatio-temporal fusions; Rasterization",2-s2.0-84948823516
"Kuboň V., Lopatková M.","Word-order analysis based upon treebank data",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952035427&doi=10.1007%2f978-3-319-27060-9_4&partnerID=40&md5=6bd01c7937583904b6a2146339da6eb3","The paper describes an experiment consisting in the attempt to quantify word-order properties of three Indo-European languages (Czech, English and Farsi). The investigation is driven by the endeavor to find an objective way how to compare natural languages from the point of view of the degree of their word-order freedom. Unlike similar studies which concentrate either on purely linguistic or purely statistical approach, our experiment tries to combine both – the observations are verified against large samples of sentences from available tree banks, and, at the same time, we exploit the ability of our tools to analyze selected important phenomena (as, e.g., the differences of the word order of a main and a subordinate clause) more deeply. The quantitative results of our research are collected from the syntactically annotated treebanks available for all three languages. Thanks to the HamleDT project, it is possible to search all treebanks in a uniform way by means of a universal query tool PML-TQ. This is also a secondary goal of this paper – to demonstrate the research potential provided by language resources which are to a certain extent unified. © Springer International Publishing Switzerland 2015.",,"Artificial intelligence; Forestry; Soft computing; European languages; Language resources; Natural languages; Quantitative result; Query tools; Research potential; Statistical approach; Subordinate clause; Computational linguistics",2-s2.0-84952035427
"Song M., Eom S., Shin S., Lee K.-H.","Enriching mobile semantic search with web services",2015,"Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing, IEEE ICSC 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925625455&doi=10.1109%2fICOSC.2015.7050850&partnerID=40&md5=68775eff6d2bf15571b1cbfac5478471","With the increasing number of mobile devices, there have been many researches on searching and managing a large volume of mobile data. Most of the mobile platforms today provide users with keyword-based full text search (FTS) in order to search for mobile data. Recently, voice search interfaces have been deployed. These search methods, however, query only the keywords given as an input to local databases in mobile devices. Therefore, it is quite difficult to figure out and to provide what a user really wants. To overcome this limitation, we propose a semantic search method for mobile platforms. The proposed method augments the results of semantic search on local databases with their related useful Web information according to the intention and context information of a user. Although there are various semantic search techniques, it is hard to apply the existing methods to mobile devices due to the characteristics of mobile devices such as isolated database structures and limited computing resources. To enable semantic search on mobile devices, we also propose a lightweight mobile ontology. The proposed mobile ontology is also aligned with related Web information to enrich search results. Experimental results from prototype implementation of the proposed method verify that our approach provides more accurate results than the conventional FTS does. In addition, the proposed method shows an acceptable amount of response time and battery consumption. © 2015 IEEE.","mobile search; semantic web; web services","Database systems; Mobile devices; Mobile phones; Mobile telecommunication systems; Query processing; Semantic Web; Social networking (online); Web services; Websites; Battery consumption; Computing resource; Context information; Database structures; Full-text search; Mobile search; Mobile semantics; Prototype implementations; World Wide Web",2-s2.0-84925625455
"Prakash A., Chinnakotla M.K., Patel D., Garg P.","Did you know? - Mining interesting trivia for entities from wikipedia",2015,"IJCAI International Joint Conference on Artificial Intelligence",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949766952&partnerID=40&md5=86e62e6be16815446cb1a76b3b93e90b","Trivia is any fact about an entity which is interesting due to its unusualness, uniqueness, unexpectedness or weirdness. In this paper, we propose a novel approach for mining entity trivia from their Wikipedia pages. Given an entity, our system extracts relevant sentences from its Wikipedia page and produces a list of sentences ranked based on their interestingness as trivia. At the heart of our system lies an interestingness ranker which learns the notion of interestingness, through a rich set of domain-independent linguistic and entity based features. Our ranking model is trained by leveraging existing user-generated trivia data available on the Web instead of creating new labeled data. We evaluated our system on movies domain and observed that the system performs significantly better than the defined baselines. A thorough qualitative analysis of the results revealed that our rich set of features indeed help in surfacing interesting trivia in the top ranks.",,"Artificial intelligence; Query languages; Domain independents; Interestingness; Labeled data; Qualitative analysis; Ranking model; User-generated; Wikipedia; World Wide Web",2-s2.0-84949766952
"Gao Y., Hong C., Wu X.","Semantic parsing using construction categorization",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945891986&doi=10.1007%2f978-3-319-23862-3_57&partnerID=40&md5=17e952839c4cc8c5efc82c7882f504a8","Semantic parsing which maps a natural language sentence into its meaning representation is considered in this paper. A novel approach which involves construction categorization is proposed. Constructions encode the correspondences of concept sequences and their meaning representations. They are categorized using the syntactic relations included in a predefined hierarchical concept base. The semantic parser construct the meaning representations for the input sentences based on these constructions. Evaluations on a benchmark dataset, Geo-Query, demonstrate that the proposed semantic parser provides favorable accuracy, as well as the generalization performance. © Springer International Publishing Switzerland 2015.","Construction categorization; Hierarchical information; Semantic parsing","Artificial intelligence; Benchmarking; Computational linguistics; Context free grammars; Learning systems; Semantics; Syntactics; Benchmark datasets; Concept-base; Generalization performance; Hierarchical information; Natural languages; Semantic parsing; Big data",2-s2.0-84945891986
"Pust M., Hermjakob U., Knight K., Marcu D., May J.","Parsing english into abstract meaning representation using syntax-based machine translation",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959899142&partnerID=40&md5=462ac19943f12aa818c40af6e83e36e0","We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results. © 2015 Association for Computational Linguistics.",,"Computer aided language translation; Natural language processing systems; Semantics; Syntactics; Translation (languages); Machine translations; Semantic resources; Specific languages; State of the art; Computational linguistics",2-s2.0-84959899142
[No author name available],"4th Joint International Conference on Semantic Technology, JIST 2014",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929165516&partnerID=40&md5=04a91de435f443924397dcc8ada20e3a","The proceedings contain 33 papers. The special focus in this conference is on Ontology and Reasoning, Linked Data, Learning and Discovery, RDF and SPARQL and Ontological Engineering. The topics include: Revisiting default description logics - and their role in aligning ontologies; on desirable properties of the structural subsumption-based similarity measure; a graph-based approach to ontology debugging in DL Lite; reasoning for ALCQ extended with a flexible meta-modelling hierarchy; ontology based inferences engine for veterinary diagnosis; a roadmap for navigating the life sciences linked open data cloud; link prediction in linked data of interspecies interactions using hybrid recommendation approach; linked data exploitation for tourist mobile apps in rural areas; publishing Danish agricultural government data as semantic web data; a lightweight treatment of inexact dates; a multi-strategy learning approach to competitor identification; mining type information from Chinese online encyclopedias; a grouping algorithm for RDF change detection on mapreduce; graph pattern based RDF Data compression; optimizing SPARQL query processing on dynamic and static data based on query time/freshness requirements using materialization; RDF a parser for hybrid resources; ontology construction and schema merging using an application-independent ontology for humanitarian aid in disaster management; development of the belief culture ontology and its application: case study of the greater Mekong subregion and representations of psychological functions of peer support services for diabetic patients.",,,2-s2.0-84929165516
"Gao Y., Hong C., Wu X.","Semantic parsing using hierarchical concept base",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945949205&doi=10.1007%2f978-3-319-23862-3_56&partnerID=40&md5=8b419c92317f55afa8260e2147f44714","Compositional question answering first maps natural language sentences into meaning representations, then a meaning interpreter is used to evaluate the corresponding answers against a database. A novel approach is proposed in this paper which involves a concept base with rich hierarchical information. A new meaning representation form is introduced correspondingly to match the hierarchical concept base. A set of constructions which encode the correspondence of concept sequences and their meaning representations are used for parsing. The experimental results show that the proposed semantic parser performs favorably in terms of both accuracy and generalization performance compared to existing semantic parsers. © Springer International Publishing Switzerland 2015.","Concept base; Hierarchical information; Semantic parsing","Artificial intelligence; Computational linguistics; Learning systems; Natural language processing systems; Semantics; Syntactics; Concept-base; Generalization performance; Hierarchical information; Natural languages; Question Answering; Semantic parsing; Big data",2-s2.0-84945949205
"Dong L., Wei F., Liu S., Zhou M., Xu K.","A statistical parsing framework for sentiment classification",2015,"Computational Linguistics",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931065275&doi=10.1162%2fCOLI_a_00221&partnerID=40&md5=6030cf0b196b5d0ac5dbcf26d684f3b1","We present a statistical parsing framework for sentence-level sentiment classification in this article. Unlike previous works that use syntactic parsing results for sentiment analysis, we develop a statistical parser to directly analyze the sentiment structure of a sentence.We show that complicated phenomena in sentiment analysis (e.g., negation, intensification, and contrast) can be handled the same way as simple and straightforward sentiment expressions in a unified and probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs), and provide a formal description of the sentiment parsing framework. We develop the parsing model to obtain possible sentiment parse trees for a sentence, from which the polarity model is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated to selecting the best sentiment tree. We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, s.parser, from a large amount of review sentences with users’ ratings as rough sentiment polarity labels. Extensive experiments on existing benchmark data sets show significant improvements over baseline sentiment classification approaches. © 2015 Association for Computational Linguistics.",,,2-s2.0-84931065275
"Shi S., Wang Y., Lin C.-Y., Liu X., Rui Y.","Automatically solving number word problems by semantic parsing and reasoning",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905357&partnerID=40&md5=896799c7b238c03803208f60d768d8d8","This paper presents a semantic parsing and reasoning approach to automatically solving math word problems. A new meaning representation language is designed to bridge natural language text and math expressions. A CFG parser is implemented based on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall. © 2015 Association for Computational Linguistics.",,"Bridges; Computational linguistics; Natural language processing systems; Semantics; Grammar rules; Natural language text; Reasoning approach; Representation languages; Semantic parsing; Test sets; Word problem; Problem solving",2-s2.0-84959905357
"Das S., Ascano R., Macarty M.","Agent-based distributed analytical search",2015,"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958542077&doi=10.1007%2f978-3-319-18944-4_4&partnerID=40&md5=d19aeb8b972934a9e3e57c24f6e51520","We describe here an agent-based Distributed Analytical Search (DAS) tool to search and query distributed ""big data"" sources regardless of data's location, content or format. DAS semantically analyzes natural language queries from a web-based user interface. It automatically translates the query to a set of sub-queries by deploying a combination of planning and traditional database query optimization techniques. It then generates a query plan represented in XML and guide the execution by spawning intelligent agents with various types of wrappers as needed for distributed sites. The answers returned by the agents are merged appropriately and return them to the user. We have demonstrated DAS using a variety of data sources that are distributed and heterogeneous. The tool is the prime product of our company with big enterprises as our target market. © 2015 Springer International Publishing Switzerland.",,,2-s2.0-84958542077
"Chen W., Kowatch R., Lin S., Splaingard M., Huang Y.","Interactive cohort identification of sleep disorder patients using natural language processing and i2b2",2015,"Applied Clinical Informatics",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930727297&doi=10.4338%2fACI-2014-11-RA-0106&partnerID=40&md5=1272286d3c7249b21b8c66894b00954c","Nationwide Children’s Hospital established an i2b2 (Informatics for Integrating Biology & the Bedside) application for sleep disorder cohort identification. Discrete data were gleaned from semistructured sleep study reports. The system showed to work more efficiently than the traditional manual chart review method, and it also enabled searching capabilities that were previously not possible. Objective: We report on the development and implementation of the sleep disorder i2b2 cohort identification system using natural language processing of semi-structured documents. Methods: We developed a natural language processing approach to automatically parse concepts and their values from semi-structured sleep study documents. Two parsers were developed: a regular expression parser for extracting numeric concepts and a NLP based tree parser for extracting textual concepts. Concepts were further organized into i2b2 ontologies based on document structures and in-domain knowledge. Results: 26,550 concepts were extracted with 99% being textual concepts. 1.01 million facts were extracted from sleep study documents such as demographic information, sleep study lab results, medications, procedures, diagnoses, among others. The average accuracy of terminology parsing was over 83% when comparing against those by experts. The system is capable of capturing both standard and non-standard terminologies. The time for cohort identification has been reduced significantly from a few weeks to a few seconds. Conclusion: Natural language processing was shown to be powerful for quickly converting large amount of semi-structured or unstructured clinical data into discrete concepts, which in combination of intuitive domain specific ontologies, allows fast and effective interactive cohort identification through the i2b2 platform for research and clinical use. © Schattauer 2015.","Clinical ontology; Cohort identification; I2b2; Natural language processing (NLP); Sleep disorder","biological ontology; cohort analysis; computer interface; data mining; human; medical informatics; natural language processing; nomenclature; procedures; Biological Ontologies; Cohort Studies; Data Mining; Humans; Medical Informatics; Natural Language Processing; Terminology as Topic; User-Computer Interface",2-s2.0-84930727297
"Artzi Y., Lee K., Zettlemoyer L.","Broad-coverage CCG semantic parsing with AMR",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959878086&partnerID=40&md5=e82c2a0d5043c802be68a9ca9fa58eb6","We propose a grammar induction technique for AMR semantic parsing. While previous grammar induction techniques were designed to re-learn a new parser for each target application, the recently annotated AMR Bank provides a unique opportunity to induce a single model for understanding broad-coverage newswire text and support a wide range of applications. We present a new model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, significantly outperforming the previous state of the art. © 2015 Association for Computational Linguistics.",,"Computational grammars; Computational linguistics; Natural language processing systems; Semantics; F1 scores; Factor graphs; Grammar induction; Semantic parsing; Single models; State of the art; Target application; Formal languages",2-s2.0-84959878086
"Liao Z., Zeng Q., Wang Q.","A supervised semantic parsing with lexical extension and syntactic constraint",2015,"International Conference Recent Advances in Natural Language Processing, RANLP",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949770330&partnerID=40&md5=ff68ead42195ca43c40bfb75fea04c2e","Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding meaning representations. In this paper, we present a novel supervised semantic parsing algorithm, which includes the lexicon extension and the syntactic supervision. This algorithm adopts a large-scale knowledge base from the open-domain Freebase to construct efficient, rich Combinatory Categorial Grammar (CCG) lexicon in order to supplement the inadequacy of its manually-annotated training dataset in the small closed-domain while allows for the syntactic supervision from the dependency-parsed sentences to penalize the ungrammatical semantic parses. Evaluations on both benchmark closed-domain datasets demonstrate that this approach learns highly accurate parser, whose parsing performance benefits greatly from the open-domain CCG lexicon and syntactic constraint.",,"Benchmarking; Computational grammars; Computational linguistics; Formal languages; Knowledge based systems; Natural language processing systems; Semantics; Combinatory categorial grammar (CCG); Highly accurate; Knowledge base; Performance benefits; Semantic parsing; Training dataset; Syntactics",2-s2.0-84949770330
"Zhang M., Huang T., Cao Y., Hou L.","Target detection and knowledge learning for domain restricted question answering",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951282160&doi=10.1007%2f978-3-319-25207-0_27&partnerID=40&md5=31f38dec89b7c004e08bf31bf1fd0155","Frequent Asked Questions(FAQ) answering in restricted domain has attracted increasing attentions in various areas. FAQ is a task to automated response user’s typical questions within specific domain. Most researches use NLP parser to analyze user’s intention and employ ontology to enrich the domain knowledge. However, syntax analysis performs poorly on the short and informal FAQ questions, and external ontology knowledge bases in specific domains are usually unavailable and expensive to manually construct. In our research, we propose a semiautomatic domain-restricted FAQ answering framework SDFA, without relying on any external resources. SDFA detects the targets of questions to assist both the fast domain knowledge learning and the answer retrieval. The proposed framework has been successfully applied in real project on bank domain. Extensive experiments on two large datasets demonstrate the effectiveness and efficiency of the approaches. © Springer International Publishing Switzerland 2015.","Domain knowledge learning; Domain restricted; Frequent Asked Questions; Question answering; Target-word","Computational linguistics; Syntactics; Domain knowledge; Domain restricted; Frequent Asked Questions; Question Answering; Target words; Natural language processing systems",2-s2.0-84951282160
"Choi E., Kwiatkowski T., Zettlemoyer L.","Scalable semantic parsing with partial ontologies",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943740568&partnerID=40&md5=fa44cf227348fe00b1223dd63b8c0e90","We consider the problem of building scalable semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Learning algorithms; Linguistics; Semantics; Supervised learning; Entity attribute extractions; New approaches; Noun phrase; Referring expressions; Semantic parsing; Semi- supervised learning; Natural language processing systems",2-s2.0-84943740568
"Kuyten P., Bollegala D., Hollerit B., Prendinger H., Aizawa K.","A discourse search engine based on rhetorical structure theory",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925427613&partnerID=40&md5=0405f326315968f2d6ee9d91c6ccdd97","Representing a document as a bag-of-words and using keywords to retrieve relevant documents have seen a great success in large scale information retrieval systems such as Web search engines. Bag-of-words representation is computationally efficient and with proper term weighting and document ranking methods can perform surprisingly well for a simple document representation method. However, such a representation ignores the rich discourse structure in a document, which could provide useful clues when determining the relevancy of a document to a given user query. We develop the first-ever Discourse Search Engine (DSE) that exploits the discourse structure in documents to overcome the limitations associated with the bag-of-words document representations in information retrieval. We use Rhetorical Structure Theory (RST) to represent a document as a discourse tree connecting numerous elementary discourse units (EDUs) via discourse relations. Given a query, our discourse search engine can retrieve not only relevant documents to the query, but also individual statements from those relevant documents that describe some discourse relations to the query. We propose several ranking scores that consider the discourse structure in the documents to measure the relevance of a pair of EDUs to a query. Moreover, we combine those individual relevance scores using a random decision forest (RDF) model to create a single relevance score. Despite the numerous challenges of constructing a rich document representation using the discourse relations in a document, our experimental results show that it improves the F-score in an information retrieval task. We publicly release our manually annotated test collection to expedite future research in discourse-based information retrieval. © Springer International Publishing Switzerland 2015.",,"Information retrieval; Information retrieval systems; Text processing; World Wide Web; Computationally efficient; Discourse structure; Document ranking method; Document Representation; Large scale information retrieval; Relevant documents; Rhetorical structure theory; Rich document representation; Search engines",2-s2.0-84925427613
"Sarma B.D., Sarma M., Prasanna S.R.M.","Semi-automatic syllable labelling for assamese language using hmm and vowel onset-offset points",2015,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951204529&doi=10.1007%2f978-81-322-2464-8_11&partnerID=40&md5=60533f627df0524efe1a63a938f77d59","Syllables play an important role in speech synthesis and recognition. Prosodic information is embedded into syllable units of speech. Here we present a method for semi-automatic syllable labelling of Assamese speech utterances using Hidden Markov Models (HMMs) and vowel onset-offset points. Semi-automatic syl- lable labelling means syllable labelling of the speech signal when transcription or the text corresponding to the speech file is provided. HMM models for 15 broad classes of phone is built. Time label of the transcription is obtained by the forced alignment procedure using the HMM models. A parser is used to convert the word transcription to syllable transcription using certain syllabification rules. This syllable transcription and the time label of the phones are used to get the time label of the syllables. Now the syllable labelling output is refined using the knowledge of vowel onset point and vowel offset point derived from the speech signal using different signal processing techniques. This refinement gives improvement in terms of both syllable detection as well as average deviation in the syllable onset and offset. © Springer India 2015.","Hidden markov model; Syllabification; Syllable; Vowel offset point; Vowel onset point","Automation; Linguistics; Markov processes; Signal processing; Speech; Speech communication; Speech recognition; Speech synthesis; Telephone sets; Transcription; Trellis codes; Average deviation; Hidden markov models (HMMs); Signal processing technique; Speech utterance; Syllabification; Syllable; Vowel offset point; Vowel onset point; Hidden Markov models",2-s2.0-84951204529
"Hakkani-Tür D., Ju Y.-C., Zweig G., Tur G.","Clustering novel intents in a conversational interaction system with semantic parsing",2015,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959108461&partnerID=40&md5=7184a9fe185071b375fc9f169ae56059","Spoken language understanding (SLU) in today's conversational systems focuses on recognizing a set of domains, intents, and associated arguments, that are determined by application developers. User requests that are not covered by these are usually directed to search engines, and may remain unhandled. We propose a method that aims to find common user intents amongst these uncovered, out-of-domain utterances, with the goal of supporting future phases of dialog system design. Our approach relies on finding common semantic patterns in uncovered user utterances using an abstract Meaning Representation based semantic parser. We represent the corpus as a graph and find subgraphs that represent clusters, by pruning the corpus graph according to frequency and entropy. We employ crowd-workers to select and label the resulting clusters and compare resulting clusters with two baselines. Experimental analyses show that we obtain higher coverage and accuracy with the semantic parsing based clustering method. Furthermore, since the intents and candidate slots are already induced, these utterances can also be used in unsupervised SLU modeling. In intent classification experiments, we show that the statistical model trained using the clusters formed by this approach results in higher classification F-measure (showing about 25% relative improvement) in comparison to the alternatives. Copyright © 2015 ISCA.","Dialog system coverage; Discovering user intents; Semantic clustering; Spoken language understanding","Cluster analysis; Computational linguistics; Search engines; Semantics; Speech communication; Application developers; Conversational interaction; Conversational systems; Dialog systems; Discovering user intents; Experimental analysis; Semantic clustering; Spoken language understanding; Speech recognition",2-s2.0-84959108461
"Yan S., Wan X.","Deep dependency substructure-based learning for multidocument summarization",2015,"ACM Transactions on Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941549686&doi=10.1145%2f2766447&partnerID=40&md5=962277f664dedfa45840e8a641e86ed2","Most extractive style topic-focused multidocument summarization systems generate a summary by ranking textual units in multiple documents and extracting a proper subset of sentences biased to the given topic. Usually, the textual units are simply represented as sentences or n-grams, which do not carry deep syntactic and semantic information. This article presents a novel extractive topic-focused multidocument summarization framework. The framework proposes a new kind of more meaningful and informative units named frequent Deep Dependency Sub-Structure (DDSS) and a topic-sensitive Multi-Task Learning (MTL) model for frequent DDSS ranking. Given a document set, first, we parse all the sentences into deep dependency structures with a Head-driven Phrase Structure Grammar (HPSG) parser and mine the frequent DDSSs after semantic normalization. Then we employ a topic-sensitive MTL model to learn the importance of these frequent DDSSs. Finally, we exploit an Integer Linear Programming (ILP) formulation and use the frequent DDSSs as the essentials for summary extraction. Experimental results on two DUC datasets demonstrate that our proposed approach can achieve state-of-the-art performance. Both the DDSS information and the topic-sensitive MTL model are validated to be very helpful for topic-focused multidocument summarization. © 2015 ACM.","Deep dependency sub-structure; Document summarization; Learning; Multi-task","Computational grammars; Formal languages; Integer programming; Linearization; Natural language processing systems; Semantics; Syntactics; Dependency structures; Document summarization; Head-driven phrase structure grammars; Integer Linear Programming; Learning; Multi-document summarization; State-of-the-art performance; Sub-structures; Data mining",2-s2.0-84941549686
"Florencia-JuÁRez R., GonzÁLez B J.J., Pazos R R.A., MartÍNez F J.A., Morales-RodrÍGuez M.A.L.","Analysis of some database schemas used to evaluate natural language interfaces to databases",2015,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931287918&doi=10.1007%2f978-3-319-17747-2_42&partnerID=40&md5=1fc351cb766cbfdb702ab05a017081aa","Most research work about the development of Natural Language Interface to Databases (NLIDB) has been focused on the study of the interpretation and translation from natural language queries to SQL queries. For this purpose and in order to improve the performance in a NLIDB, researchers have addressed different issues related to natural language processing. In addition to this, we consider that the performance of a NLIDB also depends on its ability to adapt to a database schema. For this reason, we analyzed the Geobase, ATIS and Northwind database schemas, commonly used to evaluate NLIDBs. As a result of this analysis, we present in this paper some issues arising from the three database schemas analyzed, which they should be considered in the implementation of a NLIDB to improve its performance. © Springer International Publishing Switzerland 2015.",,,2-s2.0-84931287918
"Attardi G.","Representation of word sentiment, idioms and senses",2015,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938505767&partnerID=40&md5=fc43a290884134d44e9ec9cf2f938fa9","Distributional Semantic Models (DSM) that represent words as vectors of weights over a high dimensional feature space have proved very ef-fective in representing semantic or syntactic word similarity. For certain tasks however it is important to represent contrasting aspects such as polarity, differ-ent senses or idiomatic use of words. We present two methods for creating em-beddings that take into account such characteristics: a feed-forward neural net-work for learning sentiment specific and a skip-gram model for learning sense specific embeddings. Sense specific embeddings can be used to disambiguate queries and other classification tasks. We present an approach for recognizing idiomatic expressions by means of the embeddings. This can be used to seg-ment queries into meaningful chunks. The implementation is available as a li-brary implemented in Python with core numerical processing written in C++, using a parallel linear algebra library for efficiency and scalability.",,"Information retrieval; Linear algebra; Vector spaces; Classification tasks; Embeddings; Gram models; High-dimensional feature space; Numerical processing; Parallel linear algebras; Semantic Model; Word similarity; Semantics",2-s2.0-84938505767
"Bou S., Amagasa T., Kitagawa H.","Path-based keyword search over XML streams",2015,"International Journal of Web Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938317547&doi=10.1108%2fIJWIS-04-2015-0013&partnerID=40&md5=0dda51b88c03f2a4390127c05755d2ce","Purpose-In purpose of this paper is to propose a novel scheme to process XPath-based keyword search over Extensible Markup Language (XML) streams, where one can specify query keywords and XPath-based filtering conditions at the same time. Experimental results prove that our proposed scheme can efficiently and practically process XPath-based keyword search over XML streams. Design/methodology/approach-To allow XPath-based keyword search overXMLstreams, it was attempted to integrate YFilter (Diao et al., 2003) with CKStream (Hummel et al., 2011). More precisely, the nondeterministic finite automation (NFA) of YFilter is extended so that keyword matching at text nodes is supported. Next, the stack data structure is modified by integrating set of NFA states in YFilter with bitmaps generated from set of keyword queries in CKStream. Findings-Extensive experiments were conducted using both synthetic and real data set to show the effectiveness of the proposed method. The experimental results showed that the accuracy of the proposed method was better than the baseline method (CKStream), while it consumed less memory. Moreover, the proposed scheme showed good scalability with respect to the number of queries. Originality/value-Due to the rapid diffusion of XML streams, the demand for querying such information is also growing. In such a situation, the ability to query by combining XPath and keyword search is important, because it is easy to use, but powerful means to queryXMLstreams. However, none of existing works has addressed this issue. This work is to cope with this problem by combining an existing XPath-based YFilter and a keyword-search-based CKStream for XML streams to enable XPath-based keyword search. © 2015 Emerald Group Publishing Limited.","Applications of web mining and searching; Indexing and retrieval of XML data; Web search and information extraction","Computer hardware description languages; Data mining; Hypertext systems; Information retrieval; Pipeline processing systems; Search engines; Social networking (online); World Wide Web; Baseline methods; Extensible Mark-Up language (XML); Key word matching; Nondeterministic finite automations; Synthetic and real data; Web Mining; Web searches; XML data; XML",2-s2.0-84938317547
"Ali F., Kim E.K., Kim Y.-G.","Type-2 fuzzy ontology-based semantic knowledge for collision avoidance of autonomous underwater vehicles",2015,"Information Sciences",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922132308&doi=10.1016%2fj.ins.2014.10.013&partnerID=40&md5=edf2d28eb47bd5aa820112e43cc26d03","The volume of obstacles encountered in the marine environment is rapidly increasing, which makes the development of collision avoidance systems more challenging. Several fuzzy ontology-based simulators have been proposed to provide a virtual platform for the analysis of maritime missions. However, due to the simulators' limitations, ontology-based knowledge cannot be utilized to evaluate maritime robot algorithms and to avoid collisions. The existing simulators must be equipped with smart semantic domain knowledge to provide an efficient framework for the decision-making system of AUVs. This article presents type-2 fuzzy ontology-based semantic knowledge (T2FOBSK) and a simulator for marine users that will reduce experimental time and the cost of marine robots and will evaluate algorithms intelligently. The system reformulates the user's query to extract the positions of AUVs and obstacles and convert them to a proper format for the simulator. The simulator uses semantic knowledge to calculate the degree of collision risk and to avoid obstacles. The available type-1 fuzzy ontology-based approach cannot extract intensively blurred data from the hazy marine environment to offer actual solutions. Therefore, we propose a type-2 fuzzy ontology to provide accurate information about collision risk and the marine environment during real-time marine operations. Moreover, the type-2 fuzzy ontology is designed using Protégé OWL-2 tools. The DL query and SPARQL query are used to evaluate the ontology. The distance to closest point of approach (DCPA), time to closest point of approach (TCPA) and variation of compass degree (VCD) are used to calculate the degree of collision risk between AUVs and obstacles. The experimental and simulation results show that the proposed architecture is highly efficient and highly productive for marine missions and the real-time decision-making system of AUVs. © 2014 Elsevier Inc.","Autonomous vehicle; Information retrieval; Knowledge representation; Ontology-based type-2 fuzzy inference layer; Type-2 fuzzy ontology","Collision avoidance; Data mining; Decision making; Fuzzy inference; Information retrieval; Knowledge representation; Ontology; Semantics; Simulators; Autonomous Vehicles; Closest point of approach; Collision avoidance systems; Decision-making systems; Of autonomous underwater vehicles; Ontology-based; Real time decision-making; Type-2 fuzzy ontologies; Autonomous underwater vehicles",2-s2.0-84922132308
"Thompson P., Carter J., McNaught J., Ananiadou S.","Semantically enhanced search system for historical medical archives",2015,"2015 Digital Heritage International Congress, Digital Heritage 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965156282&doi=10.1109%2fDigitalHeritage.2015.7419530&partnerID=40&md5=6bfc26dba2a33115c889b21d97e71a93","Large-scale efforts to digitise historical documents are making it increasingly easy for researchers of history to carry out searches over vast amounts of historical data from their computers. Although the constant growth in the volume of digitised historical text is enriching the body of knowledge that scholars of history have at their fingertips, it can often be difficult to explore such data collections efficiently without becoming overwhelmed. Standard keyword-based search systems treat documents as collections of unrelated words, and do not take into account their structure and meaning. Accordingly, keyword searches will often return many irrelevant documents. Equally, shifts in terminology usage over time can make it difficult to formulate queries that will retrieve all relevant documents from long-spanning historical archives. In this paper, we describe a new semantically oriented system for searching archives of historical medical documents covering wide time spans. By applying text mining techniques to the archives, the system allows for efficient searching, firstly by automatically suggesting ways to expand queries with (possibly time-sensitive) related terms, and secondly by allowing search results to be refined/explored using medically and historically relevant semantic information. © 2015 IEEE.","medical history; Semantic search; text mining","Data mining; Safety devices; Semantics; Historical archive; Historical documents; Keyword-based search; Medical history; Semantic information; Semantic search; Text mining; Text mining techniques; Search engines",2-s2.0-84965156282
"Andriescu E., Martinez T., Issarny V.","Composing message translators and inferring their data types using tree automata",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944204462&doi=10.1007%2f978-3-662-46675-9_3&partnerID=40&md5=7775ceaafaabd2a304b86acc1a3f48df","Modern distributed systems and Systems of Systems (SoS) are built as a composition of existing components and services. As a result, systems communicate (either internally, locally or over networks) using protocol stacks of ever-increasing complexity whose messages need to be translated (i.e., interpreted, generated, analyzed and transformed) by third-party systems such as services dedicated to security or interoperability. We observe that current approaches in software engineering are unable to provide an efficient solution towards reusing message translators associated with the message formats composed in protocol stacks. Instead, developers must write ad hoc “glue-code” whenever composing two or more message translators. In addition, the data structures of the output must be integrated/harmonized with the target system. In this paper we propose a solution to the above that enables the composition of message translators according to a high-level user-provided query. While the composition scheme we propose is simple, the inference of the resulting data structures is a problem that has not been solved up to now. This leads us to contribute with a novel data type inference mechanism, which generates a data-schema using tree automata, based on the aforementioned user query. © Springer-Verlag Berlin Heidelberg 2015.",,"Automata theory; Complex networks; Data structures; Forestry; Interoperability; Network protocols; Network security; Systems engineering; Trees (mathematics); Distributed systems; Glue-codes; Message format; Protocol stack; Systems of systems; Target systems; Third party systems; Tree automata; Software engineering",2-s2.0-84944204462
"Özgür A., Hur J., He Y.","Extension of the interaction network ontology for literature mining of gene-gene interaction networks from sentences with multiple interaction keywords",2015,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948668888&partnerID=40&md5=87c6fe46cfb4d1049f6b348d34aa8036","The Interaction Network Ontology (INO) has been demonstrated to be valuable in providing a structured ontological vocabulary for literature mining of gene-gene interactions from biomedical literature. Our analysis of the Learning Logic in Language (LLL) challenge and vaccine datasets showed that many interactions are signaled with 2 or more interaction keywords used in combination. In this paper, we extend the INO by adding combinatory patterns of two or more literature mining keywords to related INO interaction classes. An INO-based literature mining pipeline was further developed based on SPARQL queries and SciMiner, an in-house literature mining program. The majority of the gene interaction sentences from the LLL and vaccine datasets were found to use multiple keywords to represent interaction types. A comprehensive analysis of the LLL dataset identified 27 gene regulation interaction types each associated with multiple keywords. Special patterns were discovered from the hierarchical structure of these 27 INO types.","Gene-gene interaction; Interaction network ontology; Literature mining; SciMiner","Data integration; Data mining; Genes; Semantic Web; Vaccines; Biomedical literature; Comprehensive analysis; Gene-gene interaction; Hierarchical structures; Interaction networks; Literature mining; Multiple interactions; SciMiner; Medical computing",2-s2.0-84948668888
"Karkar A.G., Al Ja'Am J.M.","A native Arabic eLearning mobile application to transpose Arabic text to illustrations",2015,"2015 International Conference on Information and Communication Technology Research, ICTRC 2015",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944111650&doi=10.1109%2fICTRC.2015.7156424&partnerID=40&md5=b45282273cb3e98a6bed11c6b5e9aad8","Mobile eLearning are capable to avail children in improving learning capabilities, self-consciousness living, and profession growth. They can inspire the perfection of eLearning with the diverse subservient smart handled devices. Most available software applications that are attainable by children are done without taking in consideration their needs and specific deficiency. We have investigated Arabic mobile educational system that presents illustrations automatically to characterize content of Arabic stories for children. In order to display these illustrations, different sections have been implemented which comprise: the processing of natural Arabic language, applying semantic queries on expanded multi-domain educational ontology, word-to-word based relationship extraction, and querying different online search-engines. The fundamental destination of our proposed work is to improve children educational skills which include observation, comprehension, realization, and deduction. © 2015 IEEE.","Arabic Natural Language Processing; Engineering Education; Mobile Learning; Multimedia; Ontology","Application programs; Computational linguistics; E-learning; Engineering education; Engineering research; Natural language processing systems; Ontology; Search engines; Semantics; Arabic natural language processing; Educational ontologies; Educational systems; Mobile applications; Mobile Learning; Multimedia; Relationship extraction; Software applications; Education",2-s2.0-84944111650
"Abawajy J., Fernando H.","Policy-based SQLIA detection and prevention approach for RFID systems",2015,"Computer Standards and Interfaces",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913580426&doi=10.1016%2fj.csi.2014.08.005&partnerID=40&md5=7caaede40d75094c4dcb6c266fd5c409","While SQL injection attacks have been plaguing web application systems for years, the possibility of them affecting RFID systems was only identified very recently. However, very little work exists to mitigate this serious security threat to RFID-enabled enterprise systems. In this paper, we propose a policy-based SQLIA detection and prevention method for RFID systems. The proposed technique creates data validation and sanitization policies during content analysis and enforces those policies during runtime monitoring. We tested all possible types of dynamic queries that may be generated in RFID systems with all possible types of attacks that can be mounted on those systems. We present an analysis and evaluation of the proposed approach to demonstrate the effectiveness of the proposed approach in mitigating SQLIA. © 2014 Elsevier B.V. All rights reserved.","Data sanitization; Data validation; Policy; RFID; SQLIA","Network security; Public policy; Analysis and evaluation; Data validation; Prevention methods; Runtime Monitoring; Sanitization; Sql injection attacks; SQLIA; Web application systems; Radio frequency identification (RFID)",2-s2.0-84913580426
"Foret A.","A logical information system proposal for browsing terminological resources",2015,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955241738&partnerID=40&md5=06a0fae5ee29b48c4e59c852ec3c4ff6","This article presents an automated construction of a logical information context from a terminological resource, available in xml ; we apply this to the resource FranceTerme and to Camelis tool and we discuss how the resulting context can be used with such a tool dedicated to logical contexts. The purpose of this development and the choices related to this experiment is twofold: to facilitate the use of a rich linguistic resource available as open-data in xml ; to test and envision a systematic transformation of such xml resources to logical contexts. A logical view of a context allows to explore information in a flexible way, without writing explicit queries, it may also provide insights on the quality of the data. Such a context can be enriched by other information (of diverse natures), it can also be linked with other applications (according to arguments supplied by the context).",,"Artificial intelligence; Metadata; Automated construction; Information contexts; Linguistic resources; Logical contexts; Logical information systems; Open datum; Terminology",2-s2.0-84955241738
"Nuzzolese A.G., Peroni S., Recupero D.R.","MACJa: Metadata and citations jailbreaker",2015,"Communications in Computer and Information Science",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951299530&doi=10.1007%2f978-3-319-25518-7_10&partnerID=40&md5=f77aadf43410ba4caf99abc01fd46dd7","This paper presents the Metadata And Citations Jailbreaker (a.k.a. MACJa – IPA /’matsja/), i.e., a method for processing the research papers available in CEUR-WS.org and stored as PDF files in order to extract relevant semantic data and publish them in a RDF triplestore according to the Semantic Publishing And Referencing (SPAR) Ontologies. In particular, the extraction of all the information needed for addressing the queries of the Semantic Publishing Challenge 2015 (task 2) is guaranteed by MACJa by using techniques based on Natural Language Processing (i.e., Combinatory Categorial Grammar, Discourse Representation Theory, Linguistic Frames), Semantic Web technologies and good Ontology Design practices (i.e., Content Analysis, Ontology Design Patterns, Discourse Referent Extraction and Linking, Topic Extraction). © Springer International Publishing Switzerland 2015.","MACJa; Semantic Publishing; SPAR Ontologies","Computational grammars; Computational linguistics; Data mining; Metadata; Natural language processing systems; Combinatory categorial grammar; Content analysis; Discourse representation theory; MACJa; NAtural language processing; Semantic publishing; Semantic Web technology; SPAR ontologies; Semantic Web",2-s2.0-84951299530
"Ye Z., Jia Z., Yang Y., Huang J., Yin H.","Research on open domain question answering system",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951285886&doi=10.1007%2f978-3-319-25207-0_49&partnerID=40&md5=5a71e2ea5772ff6dc68434270f9a4be6","Aiming at open domain question answering system evaluation task in the fourth CCF Natural Language Processing and Chinese Computing Conference (NLPCC2015), a solution of automatic question answering which can answer natural language questions is proposed. Firstly, SPE (Subject Predicate Extraction) algorithm is presented to find answers from the knowledge base, and then WKE (Web Knowledge Extraction) algorithm is used to extract answers from search engine query result. Experimental data provided in the evaluation task includes the knowledge base and questions in natural language. The evaluation result shows that MRR is 0.5670, accuracy is 0.5700, and average F1 is 0.5240, and indicates the proposed method is feasible in open domain question answering system. © Springer International Publishing Switzerland 2015.","Automatic question answering; Information extraction; Natural language understanding; Open domain","Artificial intelligence; Computational linguistics; Data mining; Information retrieval; Knowledge based systems; Search engines; Speech recognition; Automatic question answering; Evaluation results; Knowledge extraction; NAtural language processing; Natural language questions; Natural language understanding; Open domain; Open domain question answering; Natural language processing systems",2-s2.0-84951285886
"Fogel A., Fung S., Pedrosa L., Walraed-Sullivan M., Govindan R., Mahajan R., Millstein T.","A general approach to network configuration analysis",2015,"Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2015",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964083670&partnerID=40&md5=fd606243a278cf9da725eb750e402ee7","We present an approach to detect network configuration errors, which combines the benefits of two prior approaches. Like prior techniques that analyze configuration files, our approach can find errors proactively, before the configuration is applied, and answer ""what if"" questions. Like prior techniques that analyze data-plane snapshots, our approach can check a broad range of forwarding properties and produce actual packets that violate checked properties. We accomplish this combination by faithfully deriving and then analyzing the data plane that would emerge from the configuration. Our derivation of the data plane is fully declarative, employing a set of logical relations that represent the control plane, the data plane, and their relationship. Operators can query these relations to understand identified errors and their provenance. We use our approach to analyze two large university networks with qualitatively different routing designs and find many misconfigurations in each. Operators have confirmed the majority of these as errors and have fixed their configurations accordingly. © 2015 by The USENIX Association. All Rights Reserved.",,"Systems analysis; Configuration files; Control planes; Data planes; Logical relations; Misconfigurations; Network configuration; Routing designs; Errors",2-s2.0-84964083670
"Suresh kumar G., Zayaraz G.","Concept relation extraction using Naïve Bayes classifier for ontology-based question answering systems",2015,"Journal of King Saud University - Computer and Information Sciences",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929144034&doi=10.1016%2fj.jksuci.2014.03.001&partnerID=40&md5=ed1dd21798c5a0df4bf599a4f301d967","Domain ontology is used as a reliable source of knowledge in information retrieval systems such as question answering systems. Automatic ontology construction is possible by extracting concept relations from unstructured large-scale text. In this paper, we propose a methodology to extract concept relations from unstructured text using a syntactic and semantic probability-based Naïve Bayes classifier. We propose an algorithm to iteratively extract a list of attributes and associations for the given seed concept from which the rough schema is conceptualized. A set of hand-coded dependency parsing pattern rules and a binary decision tree-based rule engine were developed for this purpose. This ontology construction process is initiated through a question answering process. For each new query submitted, the required concept is dynamically constructed, and ontology is updated. The proposed relation extraction method was evaluated using benchmark data sets. The performance of the constructed ontology was evaluated using gold standard evaluation and compared with similar well-performing methods. The experimental results reveal that the proposed approach can be used to effectively construct a generic domain ontology with higher accuracy. Furthermore, the ontology construction method was integrated into the question answering framework, which was evaluated using the entailment method. © 2014 King Saud University.","Dependency parsing; Ontology development; Question answering system; Relation extraction",,2-s2.0-84929144034
"Li G., Ross K.E., Arighi C.N., Peng Y., Wu C.H., Vijay-Shanker K.","miRTex: A Text Mining System for miRNA-Gene Relation Extraction",2015,"PLoS Computational Biology",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943550654&doi=10.1371%2fjournal.pcbi.1004391&partnerID=40&md5=8f82238888ea1fe3e11246f08fca1bb5","MicroRNAs (miRNAs) regulate a wide range of cellular and developmental processes through gene expression suppression or mRNA degradation. Experimentally validated miRNA gene targets are often reported in the literature. In this paper, we describe miRTex, a text mining system that extracts miRNA-target relations, as well as miRNA-gene and gene-miRNA regulation relations. The system achieves good precision and recall when evaluated on a literature corpus of 150 abstracts with F-scores close to 0.90 on the three different types of relations. We conducted full-scale text mining using miRTex to process all the Medline abstracts and all the full-length articles in the PubMed Central Open Access Subset. The results for all the Medline abstracts are stored in a database for interactive query and file download via the website at http://proteininformationresource.org/mirtex. Using miRTex, we identified genes potentially regulated by miRNAs in Triple Negative Breast Cancer, as well as miRNA-gene relations that, in conjunction with kinase-substrate relations, regulate the response to abiotic stress in Arabidopsis thaliana. These two use cases demonstrate the usefulness of miRTex text mining in the analysis of miRNA-regulated biological processes. © 2015 Li et al.",,"microRNA; microRNA 159; microRNA 169; microRNA 398; microRNA 402; unclassified drug; microRNA; abiotic stress; accuracy; Arabidopsis thaliana; Article; data analysis software; data extraction; data mining; down regulation; enzyme substrate complex; false positive result; gene expression regulation; gene identification; genetic association; genetic database; genetic parameters; genetic regulation; human; information processing; information retrieval; microRNA gene relation; molecular interaction; nonhuman; protein protein interaction; publication; recall bias; text mining system; transcription regulation; triple negative breast cancer; biological model; biology; classification; data mining; gene; genetics; procedures; Computational Biology; Data Mining; Databases, Genetic; Genes; Humans; MicroRNAs; Models, Genetic; Periodicals as Topic",2-s2.0-84943550654
"Ananiadou S., Thompson P., Nawaz R., McNaught J., Kell D.B.","Event-based textmining for biology and functional genomics",2015,"Briefings in Functional Genomics",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941652650&doi=10.1093%2fbfgp%2felu015&partnerID=40&md5=4f326fa2d40718166699962c8934fe09","The assessment of genome function requires a mapping between genome-derived entities and biochemical reactions, and the biomedical literature represents a rich source of information about reactions between biological components. However, the increasingly rapid growth in the volume of literature provides both a challenge and an opportunity for researchers to isolate information about reactions of interest in a timely and efficient manner. In response, recent text mining research in the biology domain has been largely focused on the identification and extraction of 'events', i.e. categorised, structured representations of relationships between biochemical entities, from the literature. Functional genomics analyses necessarily encompass events as so defined. Automatic event extraction systems facilitate the development of sophisticated semantic search applications, allowing researchers to formulate structured queries over extracted events, so as to specify the exact types of reactions to be retrieved. This article provides an overview of recent research into event extraction. We cover annotated corpora on which systems are trained, systems that achieve state-of-the-art performance and details of the community shared tasks that have been instrumental in increasing the quality, coverage and scalability of recent systems. Finally, several concrete applications of event extraction are covered, together with emerging directions of research. © The Author 2014. Published by Oxford University Press. All rights reserved.","Event extraction; Semantic annotation; Semantic search; Text mining","Article; data extraction; data mining; evaluation study; functional genomics; information processing; machine learning; semantics; systems biology; task performance; biology; data mining; genomics; human; medical research; phenotype; procedures; Biomedical Research; Computational Biology; Data Mining; Genomics; Humans; Phenotype; Semantics",2-s2.0-84941652650
"Pembe F.C., Güngör T.","A tree-based learning approach for document structure analysis and its application to web search",2015,"Natural Language Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937737921&doi=10.1017%2fS1351324914000023&partnerID=40&md5=613368c4ed517a9481a61b0d17d8e9e9","In this paper, we study the problem of structural analysis of Web documents aiming at extracting the sectional hierarchy of a document. In general, a document can be represented as a hierarchy of sections and subsections with corresponding headings and subheadings. We developed two machine learning models: heading extraction model and hierarchy extraction model. Heading extraction was formulated as a classification problem whereas a tree-based learning approach was employed in hierarchy extraction. For this purpose, we developed an incremental learning algorithm based on support vector machines and perceptrons. The models were evaluated in detail with respect to the performance of the heading and hierarchy extraction tasks. For comparison, a baseline rule-based approach was used that relies on heuristics and HTML document object model tree processing. The machine learning approach, which is a fully automatic approach, outperformed the rule-based approach. We also analyzed the effect of document structuring on automatic summarization in the context of Web search. The results of the task-based evaluation on TREC queries showed that structured summaries are superior to unstructured summaries both in terms of accuracy and user ratings, and enable the users to determine the relevancy of search results more accurately than search engine snippets. Copyright © Cambridge University Press 2014.",,"Artificial intelligence; Extraction; Information retrieval; Knowledge engineering; Learning algorithms; Learning systems; Search engines; Social networking (online); Websites; Automatic approaches; Automatic summarization; Document structure analysis; Extraction model; Incremental learning; Learning approach; Machine learning approaches; Rule-based approach; World Wide Web",2-s2.0-84937737921
"Boguslavsky I., Dikonov V., Iomdin L., Lazursky A., Sizov V., Timoshenko S.","Semantic analysis and question answering: A system under development",2015,"Komp'juternaja Lingvistika i Intellektual'nye Tehnologii",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952783717&partnerID=40&md5=56208617cb52e7406582bfcd3d38d928","The paper presents a system of semantic analysis and a question answering system implemented on its basis for a specific subject domain: (European) football match news. As input, the system obtains a natural language question (in Russian), which it answers with an element (or elements) from the repository of individuals. The core part of the system is the semantic analyzer of natural language texts. For each sentence of the text processed, the special semantic analysis component of ETAP-3 linguistic processor constructs a semantic structure, which consists of a set of triples of the type semantic-relation (individual,individual). Semantic relations and individuals constituting this structure correspond to the elements of the ontology, which can thus be viewed as a functional analogue of a dictionary for the semantic language. Semantic structures of sentences belonging to a particular text are integrated thanks to coreference and anaphora resolution and converted into an OWL-document, which is later used as a database. This database is supplemented by background knowledge from the repository of individuals concerning specific teams, football players, and games. Thanks to this resource, we are able to find an answer to the question using not only the data contained in different sentences of the text but also in the repository of individuals. If the user asks ""What team defeated the champion of Spain?"" while we have a text reporting that ""Slutsky's players outplayed Atletico Madrid"" then the system will establish the correspondence with the question, the text, and the records in the depository of individuals, and will come with the correct answer ""CSKA"". The semantic structure obtained from the natural language question is converted into a SPARQL query addressed to the database. Currently, all parts of the system are operating in the test mode.","Coreference; Deep semantic analysis; Ontology; Question answering; Semantic dictionary",,2-s2.0-84952783717
"Sary I.P., Siswandono, Budiati T.","N-phenylbenzamide synthesis by nucleophilic substitution with 1,3-diphenylthiourea",2015,"International Journal of Pharmacy and Pharmaceutical Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924348476&partnerID=40&md5=85ccb1982c0f01bbb53a6574b9dae948","Objective: N-phenylbenzamides are important and biologically active compounds. N-phenylbenzamides have been synthesized by some routes. An attempt has been made to find out the new route to synthesize N-phenylbenzamides. Methods: The reaction was carried out by reacting substituted benzoyl chlorides with 1,3-diphenylthiourea in the presence of triethylamine in THF at 70 °C. After 4hr the product was purified and identified. Results: An excellent and pure yield of N-phenylbenzamides was obtained by reacting substituted benzoyl chlorides with 1,3-diphenylthiourea. The proposed mechanism follows imino alcohol-amide tautomerism and suggests the involvement of rearrangement intermediate. Conclusion: 1,3-diphenylthiourea is inexpensive commodity chemical and it is found to be the useful reagent for the direct conversion to N-phenylbenzamide. The proposed mechanism follows imino alcohol-amide tautomerism and suggests the involvement of rearrangement intermediate. The synthesis gave pure, high yield, and the one and only isolated product. © 2015, IJPPS. All rights reserved.","1, 3-diphenylthiourea; Imino alcohol-amide tautomerism; N-phenylbenzamide; Rearrangement intermediate","1 benzoyl 1,3 diphenylthiourea; 1,3 diphenylthiourea; benzamide derivative; n phenylbenzamide; nucleophile; thiourea derivative; unclassified drug; Article; carbon nuclear magnetic resonance; chemical analysis; imino alcohol amide tautomerism; infrared radiation; infrared spectroscopy; nucleophilicity; proton nuclear magnetic resonance; reaction analysis; substitution reaction; tautomer",2-s2.0-84924348476
"Rocha R.G.C., Ribeiro R., Cassimiro D., Leandro R., Espinhara D., Tavares E., Oliveira A., Franca G., Rodrigues C., Meira S.","A System Based on Ontology and Case-Based Reasoning to Support Distributed Teams",2015,"Proceedings - 12th International Conference on Information Technology: New Generations, ITNG 2015",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936774804&doi=10.1109%2fITNG.2015.71&partnerID=40&md5=6a098d23ae051e152ccb7010f9fc56b3","The intrinsic nature of distributed software development (DSD) brings new challenges, such as communication issues and sharing information efficiently. Software companies have a tendency to face these challenges using individual and isolated approaches, making difficult to spread good practices for the DSD community. In other contexts, concepts and techniques from Artificial Intelligence (AI) are frequently used in order to improve the functioning of systems and process. This work is based on the following AI concepts: ontologies, case-based reasoning (CBR) and natural language processing (NLP). We propose a system, based on ontology and case-based reasoning, that operates as follows: i) we use a tool for ontology storage, access and processing; and ii) an ontology-based CBR tool which aims to aid software companies by recommending techniques and best practices for minimizing or solving potential challenges that may be faced by DSD processes. The main results from this research are: i) a specific ontology for distributed software development teams; ii) a tool to facilitate the access and manipulation of the proposed ontology; and iii) a case based reasoning system that utilizes natural language processing. Initial results of the performed experiments indicate a success rate of 91.7% in the recommendation of solutions for potential problems coming from DSD processes. © 2015 IEEE.","Case-based Reasoning; component; Distributed Software Development; Ontology","Artificial intelligence; Computational linguistics; Natural language processing systems; Ontology; Software design; Case based reasoning systems; Casebased reasonings (CBR); Distributed software development; Distributed teams; NAtural language processing; Potential problems; Sharing information; Systems and process; Case based reasoning",2-s2.0-84936774804
"Søgaard A., Agic Z., Alonso H.M., Plank B., Bohnet B., Johannsen A.","Inverted indexing for cross-lingual NLP",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943769140&partnerID=40&md5=154116e142c9fa5b04db741f2449c66b","We present a novel, count-based approach to obtaining inter-lingual word representations based on inverted indexing of Wikipedia. We present experiments applying these representations to 17 datasets in document classification, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efficient and almost parameter-free, and, more importantly, it enables multi-source crosslingual learning. In 14/17 cases, we improve over using state-of-The-Art bilingual embeddings. © 2015 Association for Computational Linguistics.",,"Classification (of information); Computational linguistics; Indexing (of information); Information retrieval systems; Linguistics; Computationally efficient; Dependency parsing; Document Classification; Inverted indexing; Multi-Sources; State of the art; Word alignment; Word representations; Natural language processing systems",2-s2.0-84943769140
"Tchon J.L., Barnidge T.J.","Review of the evolution of display technologies for next-generation aircraft",2015,"Proceedings of SPIE - The International Society for Optical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954040221&doi=10.1117%2f12.2181092&partnerID=40&md5=e9d0bb2aff4e2496df4935e9acc4a78d","Advancements in electronic display technologies have provided many benefits for military avionics. The modernization of legacy tanker transport aircraft along with the development of next-generation platforms, such as the KC-46 aerial refueling tanker, offers a timeline of the evolution of avionics display approaches. The adaptation of advanced flight displays from the Boeing 787 for the KC-46 flight deck also provides examples of how avionics display solutions may be leveraged across commercial and military flight decks to realize greater situational awareness and improve overall mission effectiveness. This paper provides a review of the display technology advancements that have led to today's advanced avionics displays for the next-generation KC-46 tanker aircraft. In particular, progress in display operating modes, backlighting, packaging, and ruggedization will be discussed along with display certification considerations across military and civilian platforms. © 2015 COPYRIGHT SPIE.","AMLCD; avionics; CRT; display technology; LCD; LED; liquid-crystal display","Avionics; Cathode ray tubes; Display devices; Helmet mounted displays; Light emitting diodes; Aerial refueling tankers; AMLCD; Display technologies; Electronic display; Military avionics; Next-generation aircraft; Situational awareness; Tanker transport aircraft; Liquid crystal displays",2-s2.0-84954040221
"Bottoni P., Ceriani M.","Towards an ontology-based generic pipeline editor",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924559457&partnerID=40&md5=8db16465e2dc5b570261021df2f9dd18","The pipeline concept is widely used in computer science to represent non-sequential computations, from scientific workflows to streaming transformation languages. While pipelines stand out as a highly visual representation of computation, several pipeline languages lack visual editors of production quality. We propose a method by which a generic pipeline editor can be built, centralizing the features needed to maintain and edit different pipeline languages. To foster adoption, especially in less programming-savvy communities, the proposed visual editor will be web-based. An ontology-based approach is adopted for the description of both the general features of the pipelines and the specific languages to be supported. Concepts, properties and constraints are defined using the Web Ontology Language (OWL), providing grounding in existing standards and extensibility. The work also leverages existing ontologies defined for scientific worlkflows. © Springer International Publishing Switzerland 2015.",,"Computational linguistics; Information systems; Ontology; Pipelines; Social networking (online); Production quality; Scientific workflows; Sequential computations; Specific languages; Transformation languages; Visual editors; Visual representations; Web ontology language; Visual languages",2-s2.0-84924559457
"Rahoman M.-M., Ichise R.","InteSearch: An intelligent linked data information access framework",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928920369&doi=10.1007%2f978-3-319-15615-6_12&partnerID=40&md5=22c08accc93fac8cdd54dc0feac0e78d","Information access over linked data requires to determine subgraph(s), in linked data’s underlying graph, that correspond to the required information need. Usually, an information access framework is able to retrieve richer information by checking of a large number of possible subgraphs. However, on the fly checking of a large number of possible subgraphs increases information access complexity. This makes an information access frameworks less effective. A large number of contemporary linked data information access frameworks reduce the complexity by introducing different heuristics but they suffer on retrieving richer information. Or, some frameworks do not care about the complexity. However, a practically usable framework should retrieve richer information with lower complexity. In linked data information access, we hypothesize that pre-processed data statistics of linked data can be used to efficiently check a large number of possible subgraphs. This will help to retrieve comparatively richer information with lower data access complexity. Preliminary evaluation of our proposed hypothesis shows promising performance. © Springer International Publishing Switzerland 2015.","Data access complexity; Data statistics; Information access; Linked data","Data handling; Semantic Web; Semantics; Data access; Data statistics; Information access; Linked datum; Lower complexity; On the flies; Pre-processed data; Underlying graphs; Information retrieval",2-s2.0-84928920369
"Kravets A.G., Mironenko A.G., Nazarov S.S., Kravets A.D.","Patent application text pre-processing for patent examination procedure",2015,"Communications in Computer and Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951862267&doi=10.1007%2f978-3-319-23766-4_8&partnerID=40&md5=139b053d8913052b3743813a6ada87d1","This paper is devoted to the development of text pre-processing module applied in natural language for statistical and semantic analysis. Due to increasing the number of incoming requests, the term of applications examination is sometimes up to several years. Therefore, there is a need to develop a variety of decision support systems that would accelerate the process of patent examination. We divided patents claims on simple sentences and build on their basis of semantic networks, followed by simplification. Implementation of the system will reduce the timing of the examination on the merits. The paper includes an overview of modern systems and models, description of the proposed natural language text pre-processing model, a description of the process automation and a general description of the developed system. Relevance of the developed system consists in the necessity standardize the patent applications text for more convenient to search for similar patents and analysis. © Springer International Publishing Switzerland 2015.","Big data; Natural language processing; Patent examination; Prior-art patent search","Artificial intelligence; Big data; Computational linguistics; Decision support systems; Patents and inventions; Semantics; General description; NAtural language processing; Natural language text; Patent applications; Patent examination; Patent search; Process automation; Semantic analysis; Natural language processing systems",2-s2.0-84951862267
"Sharp R., Jansen P., Surdeanu M., Clark P.","Spinning straw into gold: Using free text to train monolingual alignment models for non-factoid question answering",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960085764&partnerID=40&md5=7a2b76d5d1fb251581984b98ac90c11a","Monolingual alignment models have been shown to boost the performance of question answering systems by ""bridging the lexical chasm"" between questions and answers. The main limitation of these approaches is that they require semistructured training data in the form of question-answer pairs, which is difficult to obtain in specialized domains or lowresource languages. We propose two inexpensive methods for training alignment models solely using free text, by generating artificial question-answer pairs from discourse structures. Our approach is driven by two representations of discourse: a shallow sequential representation, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We show that these alignment models trained directly from discourse structures imposed on free text improve performance considerably over an information retrieval baseline and a neural network language model trained on the same data. © 2015 Association for Computational Linguistics.",,"Alignment; Artificial intelligence; Computation theory; Linguistics; Text processing; Discourse structure; Factoid questions; Improve performance; Network language; Question answering systems; Question-answer pairs; Rhetorical structure theory; Semi-structured; Computational linguistics",2-s2.0-84960085764
"Duan N.","Overview of the NLPCC 2015 shared task: Open domain QA",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951291301&doi=10.1007%2f978-3-319-25207-0_53&partnerID=40&md5=3b8f4346aab0451363dd464cea3b2603","In this paper, we give the overview of the open domain Question Answering (or open domain QA) shared task in NLPCC 2015. We first review the background of QA, and then describe open domain QA shared task in this year’s NLPCC, including the construction of the benchmark datasets, the auxiliary dataset, and the evaluation metrics. The evaluation results of submissions from participating teams are presented in the experimental part, together with a brief introduction to the techniques used in each participating team’s QA system. © Springer International Publishing Switzerland 2015.","Knowledge base; Question answering","Computational linguistics; Knowledge based systems; Benchmark datasets; Evaluation metrics; Evaluation results; Knowledge base; Open domain question answering; Participating teams; QA system; Question Answering; Natural language processing systems",2-s2.0-84951291301
"Lee D., Lee J., Kim E.-K., Lee J.","Dialog act modeling for virtual personal assistant applications using a small volume of labeled data and domain knowledge",2015,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959111421&partnerID=40&md5=cd3bedcec6a7901e6b1b1c8042724ab0","Recently, virtual personal assistant (VPA) applications have been employed in mobile devices, which provide a natural and convenient interface between human and machines. As the VPA services become popular, consumers demand for a wider service than their scope, so the rapid development becomes more important. This paper introduces a dialog act modeling approach for VPA applications, which is an extension of a Latent Dirichlet Allocation model. This approach enables the rapid and costeffective development by reducing human efforts for manual labeling and the development of a fail-safe product by incorporating domain knowledge such as dictionaries, cross-lingual data, and logic rules. The experimental results showed that a reliable and high-performance dialog act model was built only with a small volume of labeled data and domain knowledge. Copyright © 2015 ISCA.","Dialog act modeling; Spoken language understanding; Virtual personal assistant","Mobile devices; Speech communication; Speech recognition; Statistics; Virtual reality; Cross-lingual; Dialog act models; Domain knowledge; Labeled data; Latent Dirichlet allocation; Manual labeling; Personal assistants; Spoken language understanding; Modeling languages",2-s2.0-84959111421
"Soeken M., Harris C.B., Abdessaied N., Harris I.G., Drechsler R.","Automating the translation of assertions using natural language processing techniques",2015,"Forum on Specification and Design Languages",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940521976&doi=10.1109%2fFDL.2014.7119356&partnerID=40&md5=f3388a07f0db473457eb1d25bb67755d","In order to verify natural language assertions from a specification automatically, they need to be translated into formal representations. This process is error-prone and can lead to a product that does not meet the initial intentions.We automate this process by first partitioning all assertions into subsets based on sentence similarity and then providing a translation template for each subset which must be completed by the designer. Since many assertions are described by similar sentences, the number of manual translation steps can be decreased significantly. We evaluated our approach by translating English constraint sentences from an industrial specification into SystemVerilog assertions. © 2014 ECSI.","Design automation; Natural language processing","Computational linguistics; Computer aided design; Design; Natural language processing systems; Specifications; Design automations; Error prones; Formal representations; NAtural language processing; Natural languages; Sentence similarity; SystemVerilog assertions; Translation templates; Translation (languages)",2-s2.0-84940521976
"Florêncio C.C., Daenen J., Ramon J., Van den Bussche J., Van Dyck D.","Naive infinite enumeration of context-free languages in incremental polynomial time",2015,"Journal of Universal Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938489496&partnerID=40&md5=52121323260f144bc29962a160720dc5","We consider the naive bottom-up concatenation scheme for a context-free language and show that this scheme has the incremental polynomial time property. This means that all members of the language can be enumerated without duplicates so that the time between two consecutive outputs is bounded by a polynomial in the number of strings already generated. © J.UCS.","Context-free grammar; Incremental polynomial time; Polynomial delay; Systematic generation",,2-s2.0-84938489496
"Nowson S., Perez J., Brun C., Mirkin S., Roux C.","XRCE personal language analytics engine for multilingual author profiling",2015,"CEUR Workshop Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982853984&partnerID=40&md5=0d996550b33a8480ca84ea7f833e64d6","This technical notebook describes the methodology used - and results achieved - for the PAN 2015 Author Profiling Challenge by the team from Xerox Research Centre Europe (XRCE). This year, personality traits are introduced alongside age and gender in a corpus of tweets in four languages - English, Spanish, Italian and Dutch. We describe a largely language agnostic methodology for classification which uses language specific linguistic processing to generate features. We also report on experiments in which we use machine translation to accommodate for languages in which there is less training data. Native language results are successful, but socio-demographic signals in language seem to be lost under MT conditions.",,"Classification (of information); Linguistic processing; Machine translations; Native language; Personality traits; Training data; Xerox research centre; Computational linguistics",2-s2.0-84982853984
"Suwarningsih W., Supriana I., Purwarianti A.","Towards a framework for an indonesian medical question generator",2015,"Telkomnika (Telecommunication Computing Electronics and Control)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923122877&doi=10.12928%2fTELKOMNIKA.v13i1.648&partnerID=40&md5=a96beba985ea6d3e2b7d361255db1dd1","Question generating is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation. In this paper, we attempt to describe a general framework that could help develop and characterise efforts to Indonesian medical questions generation. We propose a new style of question generation that actively uses sentences within a document as a source of answers. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g. identification of keywords or key phrases to NER based on PICO frame) to turn a declarative sentence into questions. The final result of this research is a pattern of question and answer pairs, where the test results show the pattern matching algorithm precision value of 0.101 and a recall of 0.712.","Generating question; Indonesian medical text; PICO frame","Pattern matching; Semantics; Generating question; Indonesian medical text; Key-phrase; Pattern matching algorithms; PICO frame; Semantic representation; Syntactic transformations; Algorithms",2-s2.0-84923122877
"Shi T., Steinhardt J., Liang P.","Learning where to sample in structured prediction",2015,"Journal of Machine Learning Research",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954329057&partnerID=40&md5=33f4771030fdc7557592c894d54fb6fa","In structured prediction, most inference al-gorithms allocate a homogeneous amount of computation to all parts of the output, which can be wasteful when different parts vary widely in terms of difficulty. In this paper, we propose a heterogeneous approach that dynamically allocates computation to the dif-ferent parts. Given a pre-trained model, we tune its inference algorithm (a sampler) to increase test-time throughput. The inference algorithm is parametrized by a meta-model and trained via reinforcement learning, where actions correspond to sampling candidate parts of the output, and rewards are log-likelihood improvements. The meta-model is based on a set of domain-general meta-features capturing the progress of the sam-pler. We test our approach on five datasets and show that it attains the same accuracy as Gibbs sampling but is 2 to 5 times faster. Copyright 2015 by the authors.",,"Artificial intelligence; Reinforcement learning; Gibbs sampling; Inference algorithm; Log likelihood; Meta model; Meta-features; Structured prediction; Test time; Inference engines",2-s2.0-84954329057
"Pasupat P., Liang P.","Compositional semantic parsing on semi-structured tables",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943788139&partnerID=40&md5=7c41c4678ea15715e450f1c3e9820d87","Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: Answering complex questions on semi-structured tables using question-Answer pairs as supervision. The central challenge arises from two compounding factors: The broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Semantics; Combinatorial explosion; Complex questions; Compositional semantics; Knowledge sources; Parsing algorithm; Question Answering; Question-answer pairs; Semantic parsing; Natural language processing systems",2-s2.0-84943788139
"Kimelfeld B.","Extending datalog intelligence",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951160939&doi=10.1007%2f978-3-319-22002-4_1&partnerID=40&md5=c9384f629cd7b0640a8530719f6e8f28","Prominent sources of Big Data include technological and social trends, such as mobile computing, blogging, and social networking. The means to analyse such data are becoming more accessible with the development of business models like cloud computing, open-source and crowd sourcing. But that data have characteristics that pose challenges to traditional database systems. Due to the uncontrolled nature by which data is produced, much of it is free text, often in informal natural language, leading to computing environments with high levels of uncertainty and error. In this talk I will offer a vision of a database system that aims to facilitate the development of modern data-centric applications, by naturally unifying key functionalities of databases, text analytics, machine learning and artificial intelligence. I will also describe my past research towards pursuing the vision by extensions of Datalog — a well studied rule-based programming paradigm that features an inherent integration with the database, and has a robust declarative semantics. These extensions allow for incorporating information extraction from text, and for specifying statistical models by probabilistic programming. © Springer International Publishing Switzerland 2015.",,"Artificial intelligence; Computer programming; Database systems; Intelligent databases; Learning systems; Open source software; Open systems; Semantics; Social sciences computing; Business models; Computing environments; Crowd sourcing; Declarative semantics; Natural languages; Open sources; Probabilistic programming; Text analytics; Big data",2-s2.0-84951160939
"Hong K., Marcus M., Nenkova A.","System combination for multi-document summarization",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959925146&partnerID=40&md5=f968a8532f776e467103780b0fc125b4","We present a novel framework of system combination for multi-document summarization. For each input set (input), we generate candidate summaries by combining whole sentences from the summaries generated by different systems. We show that the oracle among these candidates is much better than the summaries that we have combined. We then present a supervised model to select among the candidates. The model relies on a rich set of features that capture content importance from different perspectives. Our model performs better than the systems that we combined based on manual and automatic evaluations. We also achieve very competitive performance on six DUC/TAC datasets, comparable to the state-of-the-art on most datasets. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Automatic evaluation; Competitive performance; Input set; Multi-document summarization; State of the art; System combination; Natural language processing systems",2-s2.0-84959925146
"Al-Kabi M.N., Kazakzeh S.A., Abu Ata B.M., Al-Rababah S.A., Alsmadi I.M.","A novel root based Arabic stemmer",2015,"Journal of King Saud University - Computer and Information Sciences",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959936016&doi=10.1016%2fj.jksuci.2014.04.001&partnerID=40&md5=63f9a70d4b9650e771e81cd2cafd647e","Stemming algorithms are used in information retrieval systems, indexers, text mining, text classifiers etc., to extract stems or roots of different words, so that words derived from the same stem or root are grouped together. Many stemming algorithms were built in different natural languages. Khoja stemmer is one of the known and widely used Arabic stemmers. In this paper, we introduced a new light and heavy Arabic stemmer. This new stemmer is presented in this study and compared with two well-known Arabic stemmers. Results showed that accuracy of our stemmer is slightly better than the accuracy yielded by each one of those two well-known Arabic stemmers used for evaluation and comparison. Evaluation tests on our novel stemmer yield 75.03% accuracy, while the other two Arabic stemmers yield slightly lower accuracy. © 2015 The Authors.","Computational intelligence; Information retrieval; Natural Language Processing (NLP); Stemming",,2-s2.0-84959936016
"Heafield K., Kshirsagar R., Barona S.","Language identification and modeling in specialized hardware",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944030684&partnerID=40&md5=90b561ae1f1bd138ada291fefe84a3b8","We repurpose network security hardware to perform language identification and lan-guage modeling tasks. Lhe hardware is a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at lan-guage identification and 1.8 to 6 times as fast at part-of-speech language modeling. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Hardware; Linguistics; Modeling languages; Network security; Deep neural networks; Language identification; Language model; Modeling task; Part Of Speech; Pushdown; Regular expressions; Security hardware; Specialized hardware; Natural language processing systems",2-s2.0-84944030684
"Tran G., Herder E., Markert K.","Joint graphical models for date selection in timeline summarization",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943751777&partnerID=40&md5=65cb37d51ad33bea8470ccee6c674916","Automatic timeline summarization (TLS) generates precise, dated overviews over (often prolonged) events, such as wars or economic crises. One subtask of TLS selects the most important dates for an event within a certain time frame. Date selection has up to now been handled via supervised machine learning approaches that estimate the importance of each date separately, using features such as the frequency of date mentions in news corpora. This approach neglects interactions between different dates that occur due to connections between subevents. We therefore suggest a joint graphical model for date selection. Even unsupervised versions of this model perform as well as supervised state-of-Theart approaches. With parameter tuning on training data, it outperforms prior supervised models by a considerable margin. © 2015 Association for Computational Linguistics.",,"Artificial intelligence; Computational linguistics; Graphic methods; Learning algorithms; Learning systems; Linguistics; Speech recognition; Supervised learning; Economic crisis; GraphicaL model; News corpora; Parameter-tuning; Sub-events; Supervised machine learning; Time frame; Training data; Natural language processing systems",2-s2.0-84943751777
"Hübschle-Schneider L., Raman R.","Tree compression with top trees revisited",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948954388&doi=10.1007%2f978-3-319-20086-6_2&partnerID=40&md5=d684de020c1506d65bae34b79e65b4b0","We revisit tree compression with top trees (Bille et al. [2]), and present several improvements to the compressor and its analysis. By significantly reducing the amount of information stored and guiding the compression step using a RePair-inspired heuristic, we obtain a fast compressor achieving good compression ratios, addressing an open problem posed by [2]. We show how, with relatively small overhead, the compressed file can be converted into an in-memory representation that supports basic navigation operations in worst-case logarithmic time without decompression. We also show a much improved worst-case bound on the size of the output of top-tree compression (answering an open question posed in a talk on this algorithm by Weimann in 2012). © Springer International Publishing Switzerland 2015.","Grammar compression; Top trees; Tree compression; XML compression","Algorithms; Forestry; Amount of information; Compressed files; Logarithmic time; Tree compression; XML compression; Trees (mathematics)",2-s2.0-84948954388
"Nguyen V.H., Nguyen H.T., Snasel V.","Normalization of Vietnamese tweets on Twitter",2015,"Advances in Intelligent Systems and Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946413654&doi=10.1007%2f978-3-319-21206-7_16&partnerID=40&md5=405a90967afdf836192ae8df929d406f","We study a task of noisy text normalization focusing on Vietnamese tweets. This task aims to improve the performance of applications mining or analyzing semantics of social media contents as well as other social network analysis applications. Since tweets on Twitter are noisy, irregular, short and consist of acronym, spelling errors, processing those tweets is more challenging than that of news or formal texts. In this paper, we proposed a method that aims to normalize Vietnamese tweets by detecting non-standard words as well as spelling errors and correcting them. The method combines a language model with dictionaries and Vietnamese vocabulary structures. We build a dataset including 1,360 Vietnamese tweets to evaluate the proposed method. Experiment results show that our method achieved encouraging performance with 89% F1-Score. © Springer International Publishing Switzerland 2015.","Normalization of noisy texts; Spelling error detection and correction; Twitter","Data handling; Information analysis; Semantics; Social networking (online); F1 scores; Language model; Normalization of noisy texts; Social media; Spelling errors; Text normalizations; Twitter; Vietnamese; Errors",2-s2.0-84946413654
"Sharma S., Srinivas P.Y.K.L., Balabantaray R.C.","Text normalization of code mix and sentiment analysis",2015,"2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946239106&doi=10.1109%2fICACCI.2015.7275819&partnerID=40&md5=612c8681e39cfd6d1cd92106afc5b7d0","The field of getting insights from various text forms such as feedback, opinions, blogs and classifying them based on their polarity as positive or negative is known as sentiment analysis. But from last few years we find huge amount of code - mix (mixture of two languages) text available on social media. This text is available in Romanized English format in Indian social media, which is the transliteration of one language into another, which demands normalization to get further insights into the text. In this paper, we have presented various methods to normalize the text and judged the polarity of the statement as positive or negative using various sentiment resources. © 2015 IEEE.","code mix; normalization; phonetic; roman script; sentiment","Computational linguistics; Data mining; Information science; Social networking (online); normalization; phonetic; roman script; sentiment; Sentiment analysis; Social media; Text normalizations; Codes (symbols)",2-s2.0-84946239106
"Espinosa-Anke L., Ronzano F., Saggion H.","Hypernym extraction: Combining machine-learning and dependency grammar",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942626374&doi=10.1007%2f978-3-319-18111-0_28&partnerID=40&md5=3580aefe32c48d9619cd1a43e3b4a1a6","Hypernym extraction is a crucial task for semantically motivated NLP tasks such as taxonomy and ontology learning, textual entailment or paraphrase identification. In this paper, we describe an approach to hypernym extraction from textual definitions, where machine-learning and post-classification refinement rules are combined. Our best-performing configuration shows competitive results compared to state-of-the-art systems in a well-known benchmarking dataset. The quality of our features is measured by combining them in different feature sets and by ranking them by their Information Gain score. Our experiments confirm that both syntactic and definitional information play a crucial role in the hypernym extraction task. © Springer International Publishing Switzerland 2015.",,"Artificial intelligence; Classification (of information); Computational linguistics; Extraction; Linguistics; Syntactics; Text processing; Definitional information; Dependency grammar; Information gain; Ontology learning; Paraphrase identifications; Post classification; State-of-the-art system; Textual entailment; Learning systems",2-s2.0-84942626374
"Chali Y., Hasan S.A., Mojahid M.","A reinforcement learning formulation to the complex question answering problem",2015,"Information Processing and Management",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923372945&doi=10.1016%2fj.ipm.2015.01.002&partnerID=40&md5=7c634ec5dff718fd7d6f0115890b622c","We use extractive multi-document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem. Given a set of complex questions, a list of relevant documents per question, and the corresponding human generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to previously unseen complex questions. A reward function is used to measure the similarities between the candidate (machine generated) summary sentences and the abstract summaries. In the training stage, the learner iteratively selects the important document sentences to be included in the candidate summary, analyzes the reward function and updates the related feature weights accordingly. The final weights are used to generate summaries as answers to unseen complex questions in the testing stage. Evaluation results show the effectiveness of our system. We also incorporate user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experiments reveal the positive impact of the user interaction component on the reinforcement learning framework. © 2015 Elsevier Ltd. All rights reserved.","Complex question answering; Multi-document summarization; Reinforcement learning; Reward function; User interaction modeling","Abstracting; Iterative methods; Natural language processing systems; Text processing; Automatic Generation; Complex questions; Evaluation results; Multi-document summarization; Relevant documents; Reward function; Sentence selection; User Interaction Modeling; Reinforcement learning",2-s2.0-84923372945
"Wen D., Gao Y., Yang G.","Semantic analysis-enhanced natural language interaction in ubiquitous learning",2015,"Lecture Notes in Educational Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966321698&doi=10.1007%2f978-3-662-44659-1_7&partnerID=40&md5=ece52905e59a8e7f5cfb8c32eb3cf6a7","Natural language interaction (NLI) is vital and ubiquitous by nature in education environments. It will keep playing key roles in ubiquitous learning and even show stronger presence there. NLI may happen ubiquitously, with many varied forms of texts, bigger textual data, and different learning situations on all kinds of devices, to meet new user needs, thus pose challenges on its design and development. This chapter introduces how natural language processing (NLP) technologies can be employed to help build and improve NLI that can support ubiquitous learning. We emphasize semantic analysis such as semantic role labeling and semantic similarity, and develop and use them to enhance question and answer processing, automated question answering, and automatic text summarization that are involved in our educational systems. Our proposed approaches can improve the technology of natural language processing and help develop different NLI systems in the ubiquitous learning environments and eventually benefit learners. © Springer-Verlag Berlin Heidelberg 2015.","Automatic text summarization; Natural language processing; Question answering; Semantic analysis; Topic modeling; Ubiquitous learning",,2-s2.0-84966321698
"Chen Y.-N., Wang W.Y., Rudnicky A.I.","Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959178594&partnerID=40&md5=af5ab3b0b403aa55ed1bc903528674aa","A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and filling of semantics slots in spoken dialogue systems. More specifically, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots. © 2015 Association for Computational Linguistics.",,"Computation theory; Computational linguistics; Inference engines; Linguistics; Random processes; Semantics; Speech processing; Speech recognition; Syntactics; Coherent semantics; Dependency grammar; Inference algorithm; Knowledge graphs; Semantic dependency; Spoken dialogue system; Spoken language understanding; Syntactic dependencies; Modeling languages",2-s2.0-84959178594
"Sun R., Zhang Y., Zhang M., Ji D.","Event-driven headline generation",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943772962&partnerID=40&md5=c2afca49930a2abe9a6a988fd8b87fef","We propose an event-driven model for headline generation. Given an input document, the system identifies a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-The-Art systems. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Event driven model; Event structures; Event-driven; Headline generation; Sentence compression; Standard evaluations; State-of-the-art system; Structural events; Natural language processing systems",2-s2.0-84943772962
"Badal V.D., Kundrotas P.J., Vakser I.A.","Text Mining for Protein Docking",2015,"PLoS Computational Biology",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953222695&doi=10.1371%2fjournal.pcbi.1004630&partnerID=40&md5=ac76206d1ca53c17a4833165103479a3","The rapidly growing amount of publicly available information from biomedical research is readily accessible on the Internet, providing a powerful resource for predictive biomolecular modeling. The accumulated data on experimentally determined structures transformed structure prediction of proteins and protein complexes. Instead of exploring the enormous search space, predictive tools can simply proceed to the solution based on similarity to the existing, previously determined structures. A similar major paradigm shift is emerging due to the rapidly expanding amount of information, other than experimentally determined structures, which still can be used as constraints in biomolecular structure prediction. Automated text mining has been widely used in recreating protein interaction networks, as well as in detecting small ligand binding sites on protein structures. Combining and expanding these two well-developed areas of research, we applied the text mining to structural modeling of protein-protein complexes (protein docking). Protein docking can be significantly improved when constraints on the docking mode are available. We developed a procedure that retrieves published abstracts on a specific protein-protein interaction and extracts information relevant to docking. The procedure was assessed on protein complexes from Dockground (http://dockground.compbio.ku.edu). The results show that correct information on binding residues can be extracted for about half of the complexes. The amount of irrelevant information was reduced by conceptual analysis of a subset of the retrieved abstracts, based on the bag-of-words (features) approach. Support Vector Machine models were trained and validated on the subset. The remaining abstracts were filtered by the best-performing models, which decreased the irrelevant information for ~ 25% complexes in the dataset. The extracted constraints were incorporated in the docking protocol and tested on the Dockground unbound benchmark set, significantly increasing the docking success rate. © 2015 Badal et al.",,,2-s2.0-84953222695
"Cohan A., Goharian N.","Scientific article summarization using citation-context and article's discourse structure",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959880100&partnerID=40&md5=e9d80d38756294575027aaa7843a9d18","We propose a summarization approach for scientific articles which takes advantage of citation-context and the document discourse model. While citations have been previously used in generating scientific summaries, they lack the related context from the referenced article and therefore do not accurately reflect the article's content. Our method overcomes the problem of inconsistency between the citation summary and the article's content by providing context for each citation. We also leverage the inherent scientific article's discourse for producing better summaries. We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of ROUGE scores on TAC2014 scientific summarization dataset. While the dataset we use for evaluation is in the biomedical domain, most of our approaches are general and therefore adaptable to other domains. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Biomedical domain; Citation contexts; Discourse model; Discourse structure; Scientific articles; Natural language processing systems",2-s2.0-84959880100
"Zwicklbauer S., Seifert C., Granitzer M.","From general to specialized domain: Analyzing three crucial problems of biomedical entity disambiguation",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983761058&doi=10.1007%2f978-3-319-22849-5_6&partnerID=40&md5=0d123c25aaafbb8a298a034a81b28666","Entity disambiguation is the task of mapping ambiguous terms in natural-language text to its entities in a knowledge base. Most disambiguation systems focus on general purpose knowledge bases like DBpedia but leave out the question how those results generalize to more specialized domains. This is very important in the context of Linked Open Data, which forms an enormous resource for disambiguation. We implement a ranking-based (Learning To Rank) disambiguation system and provide a systematic evaluation of biomedical entity disambiguation with respect to three crucial and well-known properties of specialized disambiguation systems. These are (i) entity context, i.e. the way entities are described, (ii) user data, i.e. quantity and quality of externally disambiguated entities, and (iii) quantity and heterogeneity of entities to disambiguate, i.e. the number and size of different domains in a knowledge base. Our results show that (i) the choice of entity context that is used to attain the best disambiguation results strongly depends on the amount of available user data, (ii) disambiguation results with large-scale and heterogeneous knowledge bases strongly depend on the entity context, (iii) disambiguation results are robust against a moderate amount of noise in user data and (iv) some results can be significantly improved with a federated disambiguation approach that uses different entity contexts. Our results indicate that disambiguation systems must be carefully adapted when expanding their knowledge bases with special domain entities. © Springer International Publishing Switzerland 2015.","Entity disambiguation; Learning to rank; Linked data; Semantic web","Knowledge based systems; Semantic Web; Different domains; Entity disambiguation; Heterogeneous Knowledge; Learning to rank; Linked datum; Linked open datum; Natural language text; Systematic evaluation; Expert systems",2-s2.0-84983761058
"Guo X., Cheng G., Pan W., Dinhtu T., Liang Y.","A novel search engine-based method for discovering command and control server",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951946133&doi=10.1007%2f978-3-319-27137-8_24&partnerID=40&md5=b914c3d37e1f0f9ccdbf0995332ceb6c","To solve the problem of getting command and control (C&C) server address covertly for malware of Botnet or advanced persistent threats, we propose a novel C&C-server address discovery scheme via search engine. This scheme is com-posed of five modules. The botmaster uses publish module to issue C&C-server IPs in diaries of several free blogs on Internet firstly. Then these diaries could be indexed by search engine (SE). When the infected terminal becomes a bot, it uses keyword production module to produce search keyword and submits some or all these keywords to SEs to obtain the search engine result pages (SERPs). For items in SERPs, the bot uses filtering algorithm to remove noise items and leave valid items whose abstract contain C&C-server IPs. Lastly the bot utilizes extraction and conversion module to extract these C&C-server IPs and translates them into binary format. The experimental results show that our proposed scheme is fully able to discover and obtain C&C-server IPs via various search engines. Furthermore, if we set proper threshold value for SE, it can extract C&C-server IPs accurately and efficiently. © Springer International Publishing Switzerland 2015.","Advanced persistent threat (APT); Botnet; Command and control server; Search engine; Top-K algorithm","Algorithms; Computer architecture; Internet; Malware; Parallel architectures; Advanced persistent threat (APT); Botnet; Command and control; Filtering algorithm; K algorithm; Search engine results; Search engines (se); Threshold-value; Search engines",2-s2.0-84951946133
"McGuffin L.J., Atkins J.D., Salehe B.R., Shuid A.N., Roche D.B.","IntFOLD: An integrated server for modelling protein structures and functions from amino acid sequences",2015,"Nucleic Acids Research",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979859950&doi=10.1093%2fnar%2fgkv236&partnerID=40&md5=d7cdb4fd3f1646b456cfa01f56ca0f49","IntFOLD is an independent web server that integrates our leading methods for structure and function prediction. The server provides a simple unified interface that aims to make complex protein modelling data more accessible to life scientists. The server web interface is designed to be intuitive and integrates a complex set of quantitative data, so that 3D modelling results can be viewed on a single page and interpreted by non-expert modellers at a glance. The only required input to the server is an amino acid sequence for the target protein. Here we describe major performance and user interface updates to the server, which comprises an integrated pipeline of methods for: tertiary structure prediction, global and local 3D model quality assessment, disorder prediction, structural domain prediction, function prediction and modelling of protein-ligand interactions. The server has been independently validated during numerous CASP (Critical Assessment of Techniques for Protein Structure Prediction) experiments, as well as being continuously evaluated by the CAMEO (Continuous Automated Model Evaluation) project. The IntFOLD server is available at: http://www.reading.ac.uk/bioinf/IntFOLD/. © The Author(s) 2015.",,"algorithm; amino acid sequence; Article; computer interface; computer model; computer prediction; Internet; ligand binding; priority journal; protein domain; protein function; protein interaction; protein structure; protein tertiary structure; software; web server; chemical structure; computer program; Internet; physiology; protein conformation; sequence analysis; ligand; protein; Algorithms; Internet; Ligands; Models, Molecular; Protein Conformation; Protein Structure, Tertiary; Proteins; Sequence Analysis, Protein; Software",2-s2.0-84979859950
"Nesi P., Pantaleo G., Sanesi G.","A distributed framework for NLP-based keyword and keyphrase extraction from web pages and documents",2015,"Proceedings - DMS 2015: 21st International Conference on Distributed Multimedia Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969758562&doi=10.18293%2fDMS2015-024&partnerID=40&md5=e9cd8ac5d065844a51b868b188cf935c","The recent growth of the World Wide Web at increasing rate and speed and the number of online available resources populating Internet represent a massive source of knowledge for various research and business interests. Such knowledge is, for the most part, embedded in the textual content of web pages and documents, which is largely represented as unstructured natural language formats. In order to automatically ingest and process such huge amounts of data, single-machine, non-distributed architectures are proving to be inefficient for tasks like Big Data mining and intensive text processing and analysis. Current Natural Language Processing (NLP) systems are growing in complexity, and computational power needs have been significantly increased, requiring solutions such as distributed frameworks and parallel computing programming paradigms. This paper presents a distributed framework for executing NLP related tasks in a parallel environment. This has been achieved by integrating the APIs of the widespread GATE open source NLP platform in a multi-node cluster, built upon the open source Apache Hadoop file system. The proposed framework has been evaluated against a real corpus of web pages and documents.","Distributed systems; Natural Language Processing; Parallel computing; Part-of-speech tagging","Big data; Computational linguistics; Computer software; Data mining; Distributed computer systems; Multimedia systems; Open source software; Open systems; Parallel processing systems; Text processing; Websites; World Wide Web; Distributed architecture; Distributed framework; Distributed systems; NAtural language processing; Parallel environment; Part of speech tagging; Programming paradigms; Unstructured natural language; Natural language processing systems",2-s2.0-84969758562
"Birch G., Fischer B., Poppleton M.R.","Fast model-based fault localisation with test suites",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951119641&doi=10.1007%2f978-3-319-21215-9_3&partnerID=40&md5=3b7a883edc7185124f865aefea3a8d24","Fault localisation, i.e. the identification of program locations that cause errors, takes significant effort and cost. We describe a fast model-based fault localisation algorithm which, given a test suite, uses symbolic execution methods to fully automatically identify a small subset of program locations where genuine program repairs exist. Our algorithm iterates over failing test cases and collects locations where an assignment change can repair exhibited faulty behaviour. Our main contribution is an improved search through the test suite, reducing the effort for the symbolic execution of the models and leading to speed-ups of more than two orders of magnitude over the previously published implementation by Griesmayer et al. We implemented our algorithm for C programs, using the KLEE symbolic execution engine, and demonstrate its effectiveness on the Siemens TCAS variants. Its performance is in line with recent alternative modelbased fault localisation techniques, but narrows the location set further without rejecting any genuine repair locations where faults can be fixed by changing a single assignment. © Springer International Publishing Switzerland 2015.","Automated debugging; Fault localization; Symbolic execution","Algorithms; Application programs; C (programming language); Location; Model checking; Repair; Testing; Automated debugging; C programs; FAST model; Fault localization; Localisation; Orders of magnitude; Symbolic execution; Test case; Software testing",2-s2.0-84951119641
"Sulír M., Nosál M.","Sharing developers' mental models through source code annotations",2015,"Proceedings of the 2015 Federated Conference on Computer Science and Information Systems, FedCSIS 2015",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958063923&doi=10.15439%2f2015F301&partnerID=40&md5=4c007f9b045b2a48fa7d89d70e936536","Context: Developers possess mental models containing information far beyond what is explicitly captured in the source code. Objectives: We investigate the possibility to use source code annotations to capture parts of the developers' mental models and later reuse them by other programmers during program comprehension and maintenance. Method: We performed two studies and a controlled experiment. Results: Developers' mental models overlap and thus can be shared. Possible use cases of shared annotations are hypotheses confirmation, feature location, obtaining new knowledge, finding relationships and maintenance notes. In the experiment, the presence of annotations reduced program comprehension and maintenance time by 34%. Conclusion: Annotations are a viable way to share programmers' thoughts. © 2015, IEEE.",,"Codes (symbols); Cognitive systems; Computer programming languages; Computer software reusability; Information systems; Maintenance; Controlled experiment; Feature location; Mental model; Program comprehension and maintenances; Source codes; Computer programming",2-s2.0-84958063923
"Lieto A., Minieri A., Piana A., Radicioni D.P.","A knowledge-based system for prototypical reasoning",2015,"Connection Science",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929289837&doi=10.1080%2f09540091.2014.956292&partnerID=40&md5=a8452b6c838f95ef37f16b5d3139b46f","In this work we present a knowledge-based system equipped with a hybrid, cognitively inspired architecture for the representation of conceptual information. The proposed system aims at extending the classical representational and reasoning capabilities of the ontology-based frameworks towards the realm of the prototype theory. It is based on a hybrid knowledge base, composed of a classical symbolic component (grounded on a formal ontology) with a typicality based one (grounded on the conceptual spaces framework). The resulting system attempts to reconcile the heterogeneous approach to the concepts in Cognitive Science with the dual process theories of reasoning and rationality. The system has been experimentally assessed in a conceptual categorisation task where common sense linguistic descriptions were given in input, and the corresponding target concepts had to be identified. The results show that the proposed solution substantially extends the representational and reasoning ‘conceptual’ capabilities of standard ontology-based systems. © 2014, © 2014 Taylor & Francis.","common sense reasoning; conceptual spaces; dual process theory; formal ontologies; knowledge representation; prototypical reasoning","Knowledge representation; Ontology; Commonsense reasoning; Conceptual spaces; Dual-process theories; Formal ontology; prototypical reasoning; Knowledge based systems",2-s2.0-84929289837
"Filice S., Da San Martino G., Moschitti A.","Structural representations for learning relations between pairs of texts",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943805083&partnerID=40&md5=4757b7e90f2a69d552c1d45e5f282adf","This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: The second text answers to, or conveys exactly the same information of, or is implied by, the first text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we define syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of stateof-the-Art models for this type of relational learning. Our findings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identification and Textual Entailment Recognition. © 2015 Association for Computationl Linguisticss.",,"Computational linguistics; Linguistics; Semantics; Syntactics; Trees (mathematics); Comparative analysis; Learning relations; Relational learning; Semantic relations; Semantic structures; State of the art; Structural representation; Textual entailment; Natural language processing systems",2-s2.0-84943805083
"Martínez C.P., Morteo G.L., Reyes M.M., Gelbukh A.","Wikipedia-based learning path generation",2015,"Computacion y Sistemas",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943619366&doi=10.13053%2fCyS-19-3-2079&partnerID=40&md5=44670804602a55e21d1ababa026d4bd9","We describe a method for automatic generation of a learning path for education or self-education. As a knowledge base, our method uses the semantic structure view from Wikipedia, leveraging on its broad variety of covered concepts. We evaluate our results by comparing them with the learning paths suggested by a group of teachers. Our algorithm is a useful tool for instructional design process.","Adaptive intelligent web-based educational systems; Educational resources; Learning path; Spanish language; Wikipedia",,2-s2.0-84943619366
"Mirkin S., Nowson S., Brun C., Perez J.","Motivating personality-aware machine translation",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959905628&partnerID=40&md5=496def3105de8c77597d5dbab494de2d","Language use is known to be influenced by personality traits as well as by sociodemographic characteristics such as age or mother tongue. As a result, it is possible to automatically identify these traits of the author from her texts. It has recently been shown that knowledge of such dimensions can improve performance in NLP tasks such as topic and sentiment modeling. We posit that machine translation is another application that should be personalized. In order to motivate this, we explore whether translation preserves demographic and psychometric traits. We show that, largely, both translation of the source training data into the target language, and the target test data into the source language has a detrimental effect on the accuracy of predicting author traits. We argue that this supports the need for personal and personality-aware machine translation models. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Computer aided language translation; Natural language processing systems; Improve performance; Machine translation models; Machine translations; Mother tongues; Personality traits; Socio-demographic characteristics; Source language; Target language; Translation (languages)",2-s2.0-84959905628
"Gaspers J., Cimiano P., Wrede B.","Semantic parsing of speech using grammars learned with weak supervision",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960145799&partnerID=40&md5=7f4ec5622030a28ece157c90ccde0212","Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Semantics; Speech; Language model; Low-loss; N-gram models; Recognition error; Semantic grammars; Semantic parsing; Semantic speech recognition; Speech recognizer; Speech recognition",2-s2.0-84960145799
"Neelakantan A., Chang M.-W.","Inferring missing entity type instances for knowledge base completion: New dataset and methods",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959904798&partnerID=40&md5=5939abd51a315d13f2f02f76afc727d4","Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method. © 2015 Association for Computational Linguistics.",,"Knowledge based systems; Linguistics; Automatic evaluation; Baseline methods; Completion methods; External informations; Global objective; Large-scale dataset; Quality prediction; Relation extraction; Computational linguistics",2-s2.0-84959904798
"Liu W., Li Y., Tao D., Wang Y.","A general framework for co-training and its applications",2015,"Neurocomputing",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952629066&doi=10.1016%2fj.neucom.2015.04.087&partnerID=40&md5=072a6b1c3f7056b558acfdc321fe4e08","Co-training is one of the major semi-supervised learning paradigms in which two classifiers are alternately trained on two distinct views and they teach each other by adding the predictions of unlabeled data to the training set of the other view. Co-training can achieve promising performance, especially when there is only a small number of labeled data. Hence, co-training has received considerable attention, and many variant co-training algorithms have been developed. It is essential and informative to provide a systematic framework for a better understanding of the common properties and differences in these algorithms. In this paper, we propose a general framework for co-training according to the diverse learners constructed in co-training. Specifically, we provide three types of co-training implementations, including co-training on multiple views, co-training on multiple classifiers, and co-training on multiple manifolds. Finally, comprehensive experiments of different methods are conducted on the UCF-iPhone dataset for human action recognition and the USAA dataset for social activity recognition. The experimental results demonstrate the effectiveness of the proposed solutions. © 2015 Elsevier B.V.","Co-training; Human action recognition; Multi-view; Semi-supervised learning; Social activity recognition","Classification (of information); Gesture recognition; Image recognition; Motion estimation; Pattern recognition; Co-training; Human-action recognition; Multi-views; Semi- supervised learning; Social activities; Supervised learning; Article; classification algorithm; co training on multiple classifier; co training on multiple manifold; co training on multiple view; human; human action recognition; human experiment; information processing; learning algorithm; machine learning; prediction; priority journal; smartphone; social activity recognition; support vector machine",2-s2.0-84952629066
"Ardehaly E.M., Culotta A.","Inferring latent attributes of Twitter users with label regularization",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960158336&partnerID=40&md5=018d9419837d7e3684a1b9e14fdb18e6","Inferring latent attributes of online users has many applications in public health, politics, and marketing. Most existing approaches rely on supervised learning algorithms, which require manual data annotation and therefore are costly to develop and adapt over time. In this paper, we propose a lightly supervised approach based on label regularization to infer the age, ethnicity, and political orientation of Twitter users. Our approach learns from a heterogeneous collection of soft constraints derived from Census demographics, trends in baby names, and Twitter accounts that are emblematic of class labels. To counteract the imprecision of such constraints, we compare several constraint selection algorithms that optimize classification accuracy on a tuning set. We find that using no user-annotated data, our approach is within 2% of a fully supervised baseline for three of four tasks. Using a small set of labeled data for tuning further improves accuracy on all tasks. © 2015 Association for Computational Linguistics.",,"Classification (of information); Constraint theory; Linguistics; Social networking (online); Classification accuracy; Data annotation; Heterogeneous collections; Labeled data; Online users; Political orientation; Selection algorithm; Soft constraint; Computational linguistics",2-s2.0-84960158336
"Bhagavatula C.S., Noraset T., Downey D.","TabEL: Entity linking in web tables",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952327246&doi=10.1007%2f978-3-319-25007-6_25&partnerID=40&md5=fc95c54e0d8a759de652ecd3946947b7","Web tables form a valuable source of relational data. The Web contains an estimated 154 million HTML tables of relational data, with Wikipedia alone containing 1.6 million high-quality tables. Extracting the semantics of Web tables to produce machine-understandable knowledge has become an active area of research. A key step in extracting the semantics of Web content is entity linking (EL): the task of mapping a phrase in text to its referent entity in a knowledge base (KB). In this paper we present TabEL, a new EL system for Web tables. TabEL differs from previous work by weakening the assumption that the semantics of a table can be mapped to pre-defined types and relations found in the target KB. Instead, TabEL enforces soft constraints in the form of a graphical model that assigns higher likelihood to sets of entities that tend to co-occur in Wikipedia documents and tables. In experiments, TabEL significantly reduces error when compared to current state-of-the-art table EL systems, including a 75% error reduction on Wikipedia tables and a 60% error reduction on Web tables. We also make our parsed Wikipedia table corpus and test datasets publicly available for future work. © Springer International Publishing Switzerland 2015.","Entity linking; Graphical models; Named entity disambiguation; Web tables","Errors; Graphic methods; Knowledge based systems; Speech recognition; Entity linking; Error reduction; GraphicaL model; Named entity disambiguations; Relational data; Soft constraint; State of the art; Web tables; Semantic Web",2-s2.0-84952327246
"Irfan R., King C.K., Grages D., Ewen S., Khan S.U., Madani S.A., Kolodziej J., Wang L., Chen D., Rayes A., Tziritas N., Xu C.-Z., Zomaya A.Y., Alzahrani A.S., Li H.","A survey on text mining in social networks",2015,"Knowledge Engineering Review",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925387234&doi=10.1017%2fS0269888914000277&partnerID=40&md5=aacf5e854eab1e8fedf7b067474d6a6e","In this survey, we review different text mining techniques to discover various textual patterns from the social networking sites. Social network applications create opportunities to establish interaction among people leading to mutual learning and sharing of valuable knowledge, such as chat, comments, and discussion boards. Data in social networking websites is inherently unstructured and fuzzy in nature. In everyday life conversations, people do not care about the spellings and accurate grammatical construction of a sentence that may lead to different types of ambiguities, such as lexical, syntactic, and semantic. Therefore, analyzing and extracting information patterns from such data sets are more complex. Several surveys have been conducted to analyze different methods for the information extraction. Most of the surveys emphasized on the application of different text mining techniques for unstructured data sets reside in the form of text documents, but do not specifically target the data sets in social networking website. This survey attempts to provide a thorough understanding of different text mining techniques as well as the application of these techniques in the social networking websites. This survey investigates the recent advancement in the field of text analysis and covers two basic approaches of text mining, such as classification and clustering that are widely used for the exploration of the unstructured text available on the Web. © Cambridge University Press, 2015.",,"Complex networks; Semantics; Social networking (online); Surveys; Text processing; Websites; Classification and clustering; Extracting information; Grammatical construction; Network applications; Social networking sites; Text mining techniques; Unstructured data; Unstructured texts; Data mining",2-s2.0-84925387234
"Robertson M., Yeoman I., Smith K.A., Mcmahon-Beattie U.","Technology, society, and visioning the future of music festivals",2015,"Event Management",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960443530&doi=10.3727%2f152599515X14465748774001&partnerID=40&md5=c48947b8b60cc54e45cff66cf989df56","Many music festivals fail because the experiences offered do not ensure relevance and meaning to the attendee. Engagement with new and virtual landscapes and with the enhanced sensory feelings and imaginations that technologies can offer may alleviate this. Utilizing a futures frame, this conceptual article contributes to the pursuit of successful future event design by applying a normative visionary methodology-employing trend analysis, scenarios, and science fiction to create prototypes that may assist in the formation of appropriate experience options and opportunities for music festivals of the future. It is proposed that this technique may aid positive social outcomes. © 2015 Cognizant Comm. Corp.","Future studies; Music festivals; Play; Scenarios; Science fiction; Society",,2-s2.0-84960443530
"Bounhas I., Ayed R., Elayeb B., Bellamine Ben Saoud N.","A hybrid possibilistic approach for Arabic full morphological disambiguation",2015,"Data and Knowledge Engineering",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947869951&doi=10.1016%2fj.datak.2015.06.008&partnerID=40&md5=2b0703a75035b3ed5f16edb457acd5af","Morphological ambiguity is an important phenomenon affecting several tasks in Arabic text analysis, indexing and mining. Nevertheless, it has not been well studied in related works. We investigate, in this paper, new approaches to disambiguate the morphological features of non-vocalized Arabic texts, combining statistical classification and linguistic rules. Indeed, we perform unsupervised training from unlabelled vocalized Arabic corpora. Thus, the training and testing sets contain imperfect instances (i.e. having ambiguous attributes and/or classes). To handle imperfect data, we compare two approaches: i) a possibilistic approach allowing to handle imperfection in a direct manner; and, ii) a data transformation-based approach permitting to convert an imperfect dataset to a perfect one, thus allowing to exploit classical classifiers. We also present an approach dealing with unknown (Out-of-Vocabulary) words. The experiments focus mainly on classical texts, which were not sufficiently studied in related works. We show that the possibilistic approach performs better than the transformation-based one. Besides, we report encouraging results as far as i) the role of linguistic rules in enhancing the disambiguation rates; and, ii) the accuracy of our approach for full morphological disambiguation of unknown words. © 2015 Elsevier B.V. All rights reserved.","Arabic morphological disambiguation; Imperfect data; Linguistic rules; Out-of-Vocabulary word analysis; Possibilistic classification","Computational linguistics; Linguistics; Metadata; Imperfect data; Linguistic rules; Morphological disambiguation; Out of vocabulary words; Possibilistic classifications; Classification (of information)",2-s2.0-84947869951
"Sarker A., Gonzalez G.","Portable automatic text classification for adverse drug reaction detection via multi-corpus training",2015,"Journal of Biomedical Informatics",40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924285421&doi=10.1016%2fj.jbi.2014.11.002&partnerID=40&md5=725dc47813281cb239e2255465b40569","Objective: Automatic detection of adverse drug reaction (ADR) mentions from text has recently received significant interest in pharmacovigilance research. Current research focuses on various sources of text-based information, including social media-where enormous amounts of user posted data is available, which have the potential for use in pharmacovigilance if collected and filtered accurately. The aims of this study are: (i) to explore natural language processing (NLP) approaches for generating useful features from text, and utilizing them in optimized machine learning algorithms for automatic classification of ADR assertive text segments; (ii) to present two data sets that we prepared for the task of ADR detection from user posted internet data; and (iii) to investigate if combining training data from distinct corpora can improve automatic classification accuracies. Methods: One of our three data sets contains annotated sentences from clinical reports, and the two other data sets, built in-house, consist of annotated posts from social media. Our text classification approach relies on generating a large set of features, representing semantic properties (. e.g., sentiment, polarity, and topic), from short text nuggets. Importantly, using our expanded feature sets, we combine training data from different corpora in attempts to boost classification accuracies. Results: Our feature-rich classification approach performs significantly better than previously published approaches with ADR class F-scores of 0.812 (previously reported best: 0.770), 0.538 and 0.678 for the three data sets. Combining training data from multiple compatible corpora further improves the ADR F-scores for the in-house data sets to 0.597 (improvement of 5.9 units) and 0.704 (improvement of 2.6 units) respectively. Conclusions: Our research results indicate that using advanced NLP techniques for generating information rich features from text can significantly improve classification accuracies over existing benchmarks. Our experiments illustrate the benefits of incorporating various semantic features such as topics, concepts, sentiments, and polarities. Finally, we show that integration of information from compatible corpora can significantly improve classification performance. This form of multi-corpus training may be particularly useful in cases where data sets are heavily imbalanced (. e.g., social media data), and may reduce the time and costs associated with the annotation of data in the future. © 2014 The Authors.","Adverse drug reaction; Natural language processing; Pharmacovigilance; Social media monitoring; Text classification","Artificial intelligence; Computational linguistics; Drug products; Information filtering; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Pharmacodynamics; Semantics; Social networking (online); Text processing; Adverse drug reactions; NAtural language processing; Pharmacovigilance; Social media monitoring; Text classification; Classification (of information); accuracy; adverse drug reaction; analytical error; Article; automation; classification; classification algorithm; classifier; drug surveillance program; Internet; learning algorithm; machine learning; natural language processing; portable automatic text classification; priority journal; social media; social network; support vector machine; adverse drug reaction; algorithm; artificial intelligence; data mining; drug surveillance program; factual database; human; information processing; procedures; quality control; reproducibility; Adverse Drug Reaction Reporting Systems; Algorithms; Artificial Intelligence; Automatic Data Processing; Benchmarking; Data Collection; Data Mining; Databases, Factual; Drug-Related Side Effects and Adverse Reactions; Humans; Internet; Natural Language Processing; Pharmacovigilance; Reproducibility of Results; Social Media",2-s2.0-84924285421
"Liu J.-W., Liu Y., Luo X.-L.","Semi-supervised learning methods",2015,"Jisuanji Xuebao/Chinese Journal of Computers",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941965874&doi=10.11897%2fSP.J.1016.2015.01592&partnerID=40&md5=6105771270c3421f6346cf21b75b65da","Semi-supervised learning is used to study how to improve performance in the presence of both examples and instances, and it has become a hot area of machine learning field. In view of the theoretical significance and practical value of semi-supervised learning, semi-supervised learning methods were reviewed in this paper systematically. Firstly, some concepts about semi-supervised learning were summarized, including definition of semi-supervised learning, development of research, assumptions relied on semi-supervised learning methods and classification of semi-supervised learning. Secondly, semi-supervised learning methods were detailed from four aspects, including classification, regression, clustering, and dimension reduction. Thirdly, theoretical analysis on semi-supervised learning was studied, and error bounds and sample complexity were given. Finally, the future research on semi-supervised learning was discussed. ©, 2015, Science Press. All right reserved.","Label; Labeled examples; Pair-wise constraints; Semi-supervised learning; Unlabeled instances","Artificial intelligence; Error analysis; Labeling; Learning algorithms; Learning systems; Dimension reduction; Improve performance; Labeled examples; Pairwise constraints; Sample complexity; Semi- supervised learning; Semi-supervised learning methods; Unlabeled instances; Supervised learning",2-s2.0-84941965874
"Fang A.C., Cao J.","Text genres and registers: The computation of linguistic features",2015,"Text Genres and Registers: The Computation of Linguistic Features",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943171436&doi=10.1007%2f978-3-662-45100-7&partnerID=40&md5=237aa844e2eb7d3da4ba4c374227d531","This book is a description of some of the most recent advances in text classification as part of a concerted effort to achieve computer understanding of human language. In particular, it addresses state-of-the-art developments in the computation of higher-level linguistic features, ranging from etymology to grammar and syntax for the practical task of text classification according to genres, registers and subject domains. Serving as a bridge between computational methods and sophisticated linguistic analysis, this book will be of particular interest to academics and students of computational linguistics as well as professionals in natural language engineering. © Springer-Verlag Berlin Heidelberg 2015.",,,2-s2.0-84943171436
"Deelman E., Vahi K., Juve G., Rynge M., Callaghan S., Maechling P.J., Mayani R., Chen W., Ferreira Da Silva R., Livny M., Wenger K.","Pegasus, a workflow management system for science automation",2015,"Future Generation Computer Systems",107,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923339542&doi=10.1016%2fj.future.2014.10.008&partnerID=40&md5=87e8dcacbee1f9cd2abffc4fb42ed540","Modern science often requires the execution of large-scale, multi-stage simulation and data analysis pipelines to enable the study of complex systems. The amount of computation and data involved in these pipelines requires scalable workflow management systems that are able to reliably and efficiently coordinate and automate data movement and task execution on distributed computational resources: campus clusters, national cyberinfrastructures, and commercial and academic clouds. This paper describes the design, development and evolution of the Pegasus Workflow Management System, which maps abstract workflow descriptions onto distributed computing infrastructures. Pegasus has been used for more than twelve years by scientists in a wide variety of domains, including astronomy, seismology, bioinformatics, physics and others. This paper provides an integrated view of the Pegasus system, showing its capabilities that have been developed over time in response to application needs and to the evolution of the scientific computing platforms. The paper describes how Pegasus achieves reliable, scalable workflow execution across a wide variety of computing infrastructures. © 2014 Elsevier B.V. All rights reserved.","Pegasus; Scientific workflows; Workflow management system","Automation; Bioinformatics; Information management; Pipelines; Work simplification; Abstract workflow; Computing infrastructures; Cyber infrastructures; Distributed computational resources; Pegasus; Scientific workflows; Workflow execution; Workflow management systems; Distributed computer systems",2-s2.0-84923339542
"Liu S., Tang B., Chen Q., Wang X.","Drug name recognition: Approaches and resources",2015,"Information (Switzerland)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952313857&doi=10.3390%2finfo6040790&partnerID=40&md5=ed5a07b88c16cde308721cc17d750178","Drug name recognition (DNR), which seeks to recognize drug mentions in unstructured medical texts and classify them into pre-defined categories, is a fundamental task of medical information extraction, and is a key component of many medical relation extraction systems and applications. A large number of efforts have been devoted to DNR, and great progress has been made in DNR in the last several decades. We present here a comprehensive review of studies on DNR from various aspects such as the challenges of DNR, the existing approaches and resources for DNR, and possible directions. © 2015 by the authors.","Biomedical texts; Drug information extraction; Drug name recognition","Classification (of information); Information analysis; Information retrieval; Biomedical text; Name recognition; Relation extraction; Medical information systems",2-s2.0-84952313857
"Bordawekar R., Blainey B., Puri R.","Analyzing analytics",2015,"Synthesis Lectures on Computer Architecture",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946887435&doi=10.2200%2fS00678ED1V01Y201511CAC035&partnerID=40&md5=5877bd691aee0d2329e8e71b914a0867","This book aims to achieve the following goals: (1) to provide a high-level survey of key analytics models and algorithms without going into mathematical details; (2) to analyze the usage patterns of these models; and (3) to discuss opportunities for accelerating analytics workloads using software, hardware, and system approaches.E book first describes 14 key analytics models (exemplars) that span data mining, machine learning, and data management domains. For each analytics exemplar, we summarize its computational and runtime patterns and apply the information to evaluate parallelization and acceleration alternatives for that exemplar. Using case studies from important application domains such as deep learning, text analytics, and business intelligence (BI), we demonstrate how various software and hardware acceleration strategies are implemented in practice. This book is intended for both experienced professionals and students who are interested in understanding core algorithms behind analytics workloads. It is designed to serve as a guide for addressing various open problems in accelerating analytics workloads, e.g., new architectural features for supporting analytics workloads, impact on programming models and runtime systems, and designing analytics systems.","Analytics; Hardware acceleration; Parallel algorithms","Algorithms; Application programs; Artificial intelligence; Data mining; Hardware; Learning systems; Parallel algorithms; Analytics; Architectural features; Hardware acceleration; Management domains; Mathematical details; Models and algorithms; Programming models; Software and hardwares; Information management",2-s2.0-84946887435
"Bordawekar R., Blainey B., Puri R.","Analyzing Analytics",2015,"Synthesis Lectures on Computer Architecture",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975032480&doi=10.2200%2fS00678ED1V01Y201511CAC035&partnerID=40&md5=b130ffc8cb68ff4b321055ba8427f2ae","This book aims to achieve the following goals: (1) to provide a high-level survey of key analytics models and algorithms without going into mathematical details; (2) to analyze the usage patterns of these models; and (3) to discuss opportunities for accelerating analytics workloads using software, hardware, and system approaches. The book first describes 14 key analytics models (exemplars) that span data mining, machine learning, and data management domains. For each analytics exemplar, we summarize its computational and runtime patterns and apply the information to evaluate parallelization and acceleration alternatives for that exemplar. Using case studies from important application domains such as deep learning, text analytics, and business intelligence (BI), we demonstrate how various software and hardware acceleration strategies are implemented in practice. This book is intended for both experienced professionals and students who are interested in understanding core algorithms behind analytics workloads. It is designed to serve as a guide for addressing various open problems in accelerating analytics workloads, e.g., new architectural features for supporting analytics workloads, impact on programming models and runtime systems, and designing analytics systems. Copyright © 2016 by Morgan & Claypool.","analytics; hardware acceleration; parallel algorithms","Application programs; Artificial intelligence; Data mining; Information management; Learning systems; Parallel algorithms; Reconfigurable hardware; analytics; Architectural features; Hardware acceleration; Management domains; Mathematical details; Models and algorithms; Programming models; Software and hardwares; Hardware",2-s2.0-84975032480
"Qadir J., Hasan O.","Applying Formal Methods to Networking: Theory, Techniques, and Applications",2015,"IEEE Communications Surveys and Tutorials",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925844212&doi=10.1109%2fCOMST.2014.2345792&partnerID=40&md5=2d930d475e70ac4a7b88b32634227ed5","Despite its great importance, modern network infrastructure is remarkable for the lack of rigor in its engineering. The Internet, which began as a research experiment, was never designed to handle the users and applications it hosts today. The lack of formalization of the Internet architecture meant limited abstractions and modularity, particularly for the control and management planes, thus requiring for every new need a new protocol built from scratch. This led to an unwieldy ossified Internet architecture resistant to any attempts at formal verification and to an Internet culture where expediency and pragmatism are favored over formal correctness. Fortunately, recent work in the space of clean slate Internet design - in particular, the software defined networking (SDN) paradigm - offers the Internet community another chance to develop the right kind of architecture and abstractions. This has also led to a great resurgence in interest of applying formal methods to specification, verification, and synthesis of networking protocols and applications. In this paper, we present a self-contained tutorial of the formidable amount of work that has been done in formal methods and present a survey of its applications to networking. © 1998-2012 IEEE.","Computer networks; formal specifications; formal verification","Abstracting; Computer networks; Formal specification; Formal verification; Internet; Internet protocols; Network architecture; Specifications; Control and management; Formal correctness; Internet architecture; Internet communities; ITS applications; Network infrastructure; Networking protocols; Software defined networking (SDN); Formal methods",2-s2.0-84925844212
"Liu B.","Sentiment analysis: Mining opinions, sentiments, and emotions",2015,"Sentiment Analysis: Mining Opinions, Sentiments, and Emotions",80,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953400767&doi=10.1017%2fCBO9781139084789&partnerID=40&md5=f910f7ea614b45664be2c10ab83f34a4","Sentiment analysis is the computational study of people's opinions, sentiments, emotions, and attitudes. This fascinating problem is increasingly important in business and society. It offers numerous research challenges but promises insight useful to anyone interested in opinion analysis and social media analysis. This book gives a comprehensive introduction to the topic from a primarily natural-language-processing point of view to help readers understand the underlying structure of the problem and the language constructs that are commonly used to express opinions and sentiments. It covers all core areas of sentiment analysis, includes many emerging themes, such as debate analysis, intention mining, and fake-opinion detection, and presents computational methods to analyze and summarize opinions. It will be a valuable resource for researchers and practitioners in natural language processing, computer science, management sciences, and the social sciences. © Bing Liu 2015.",,"Behavioral research; Computer resource management; Data mining; Management science; Business and societies; Computational studies; Language constructs; Natural languages; Opinion detections; Research challenges; Sentiment analysis; Social media analysis; Natural language processing systems",2-s2.0-84953400767
"Wassom B.D.","Augmented Reality Law, Privacy, and Ethics: Law, Society, and Emerging AR Technologies",2014,"Augmented Reality Law, Privacy, and Ethics: Law, Society, and Emerging AR Technologies",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943388119&doi=10.1016%2fC2013-0-13413-7&partnerID=40&md5=dc49bfdc4c7854fb2404d3bb6ee13c08","Augmented Reality (AR) is the blending of digital information in a real-world environment. A common example can be seen during any televised football game, in which information about the game is digitally overlaid on the field as the players move and position themselves. Another application is Google Glass, which enables users to see AR graphics and information about their location and surroundings on the lenses of their ""digital eyewear"", changing in real-time as they move about. Augmented Reality Law, Privacy, and Ethics is the first book to examine the social, legal, and ethical issues surrounding AR technology. Digital eyewear products have very recently thrust this rapidly-expanding field into the mainstream, but the technology is so much more than those devices. Industry analysts have dubbed AR the ""eighth mass medium"" of communications. Science fiction movies have shown us the promise of this technology for decades, and now our capabilities are finally catching up to that vision. Augmented Reality will influence society as fundamentally as the Internet itself has done, and such a powerful medium cannot help but radically affect the laws and norms that govern society. No author is as uniquely qualified to provide a big-picture forecast and guidebook for these developments as Brian Wassom. A practicing attorney, he has been writing on AR law since 2007 and has established himself as the worlds foremost thought leader on the intersection of law, ethics, privacy, and AR. Augmented Reality professionals around the world follow his Augmented Legality® blog. This book collects and expands upon the best ideas expressed in that blog, and sets them in the context of a big-picture forecast of how AR is shaping all aspects of society. Augmented reality thought-leader Brian Wassom provides you with insight into how AR is changing our world socially, ethically, and legally. Includes current examples, case studies, and legal cases from the frontiers of AR technology. Learn how AR is changing our world in the areas of civil rights, privacy, litigation, courtroom procedure, addition, pornography, criminal activity, patent, copyright, and free speech. An invaluable reference guide to the impacts of this cutting-edge technology for anyone who is developing apps for it, using it, or affected by it in daily life.",,"Blending; Digital devices; Laws and legislation; Philosophical aspects; Courtroom procedures; Criminal activities; Cutting edge technology; Digital information; Industry analysts; Real world environments; Reference guides; Science fictions; Augmented reality",2-s2.0-84943388119
"Hoque M.M., Quaresma P.","SEMONTOQA - A semantic understanding-based ontological framework for factoid question answering",2014,"ACM International Conference Proceeding Series",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959869260&doi=10.1145%2f2824864.2824886&partnerID=40&md5=4fe6d0bcdd29f5080009b83c7f795520","This paper presents an outline of an Ontological and Se- mantic understanding-based model (SEMONTOQA) for an open-domain factoid Question Answering (QA) system. The outlined model analyses unstructured English natural lan- guage texts to a vast extent and represents the inherent con- tents in an ontological manner. The model locates and ex- tracts useful information from the text for various question types and builds a semantically rich knowledge-base that is capable of answering different categories of factoid ques- tions. The system model converts the unstructured texts into a minimalistic, labelled, directed graph that we call a Syntactic Sentence Graph (SSG). An Automatic Text In- terpreter using a set of pre-learnt Text Interpretation Sub- graphs and patterns tries to understand the contents of the SSG in a semantic way. The system proposes a new fea- ture and action based Cognitive Entity-Relationship Net- work designed to extend the text understanding process to an in-depth level. Application of supervised learning allows the system to gradually grow its capability to understand the text in a more fruitful manner. The system incorpo- rates an effective Text Inference Engine which takes the re- sponsibility of inferring the text contents and isolating enti- ties, their features, actions, objects, associated contexts and other properties, required for answering questions. A similar understanding-based question processing module interprets the user's need in a semantic way. An Ontological Mapping Module, with the help of a set of pre-defined strategies de- signed for different classes of questions, is able to perform a mapping between a question's ontology with the set of ontologies stored in the background knowledge-base. Em- pirical verification is performed to show the usability of the proposed model. The results achieved show that, this model can be used effectively as a semantic understanding based alternative QA system. © 2015 ACM.","Deep question answering system; Natural language processing; Semantic mapping; Syntactic Sentence Graph; Text inference; Text ontology; Text understanding","Artificial intelligence; Directed graphs; Information retrieval; Knowledge based systems; Mapping; Ontology; Semantics; Syntactics; NAtural language processing; Question answering systems; Semantic mapping; Text inference; Text understanding; Natural language processing systems",2-s2.0-84959869260
"Bou S., Amagasa T., Kitagawa H.","Filtering XML streams by XPath and keywords",2014,"ACM International Conference Proceeding Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985916838&doi=10.1145%2f2684200.2684309&partnerID=40&md5=1eaf6f3b4f3d4d794393831f909e3042","With the rise of Web search engines, processing keyword search over XML and XML streams has drawn much attention from many researchers. Compared to conventional query methods, keyword search has several benefits for its simplicity and its user-friendliness in querying XML databases Therefore, a great deal of effort has been put on this search paradigm by trying to improve the quality of search result of pure keyword search, where only keywords are allowed as a query. However, due to the vagueness of keyword search, it is hard to accurately express real search intention with just keyword search. We observe that there are many cases where the combination of path-based query and keyword search is a better choice and can deal with such challenge. To address this problem, we propose a method to integrate XPath and keyword search so that users can accurately express their search demands. The experimental results show that the proposed scheme can process queries over XML streams practically. Copyright 2014 ACM.","Keyword search; XML streams; XPath; XQuery","Information analysis; Information retrieval; Search engines; Web services; Websites; XML; Keyword search; Query methods; Search intentions; User friendliness; XML database; XML stream; XPath; XQuery; World Wide Web",2-s2.0-84985916838
"Torres Moreno J.M.","Automatic Text Summarization",2014,"Automatic Text Summarization",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926405408&doi=10.1002%2f9781119004752&partnerID=40&md5=0a2c3f34a52e78d650a7bfe2f33a380a","Textual information in the form of digital documents quickly accumulates to create huge amounts of data. The majority of these documents are unstructured: it is unrestricted text and has not been organized into traditional databases. Processing documents is therefore a perfunctory task, mostly due to a lack of standards. It has thus become extremely difficult to implement automatic text analysis tasks. Automatic Text Summarization (ATS), by condensing the text while maintaining relevant information, can help to process this ever-increasing, difficult-to-handle, mass of information. This book examines the motivations and different algorithms for ATS. The author presents the recent state of the art before describing the main problems of ATS, as well as the difficulties and solutions provided by the community. The book provides recent advances in ATS, as well as current applications and trends. The approaches are statistical, linguistic and symbolic. Several examples are also included in order to clarify the theoretical concepts. © 2014 John Wiley & Sons, Inc. All rights reserved.",,,2-s2.0-84926405408
"Rahoman M.-M., Ichise R.","Automatic inclusion of semantics over keyword-based linked data retrieval",2014,"IEICE Transactions on Information and Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909592363&doi=10.1587%2ftransinf.2014EDP7073&partnerID=40&md5=0f15290a1a45bc59724e68ebd4a22fae","Keyword-based linked data information retrieval is an easy choice for general-purpose users, but the implementation of such an approach is a challenge because mere keywords do not hold semantic information. Some studies have incorporated templates in an effort to bridge this gap, but most such approaches have proven ineffective because of inefficient template management. Because linked data can be presented in a structured format, we can assume that the data's internal statistics can be used to effectively influence template management. In this work, we explore the use of this influence for template creation, ranking, and scaling. Then, we demonstrate how our proposal for automatic linked data information retrieval can be used alongside familiar keyword-based information retrieval methods, and can also be incorporated alongside other techniques, such as ontology inclusion and sophisticated matching, in order to achieve increased levels of performance. © 2014 The Institute of Electronics, Information and Communication Engineers.","Data statistics; Information access; Keyword; Linked data","Data handling; Information retrieval; Semantics; Data statistics; Information access; Keyword; Keyword-based; Linked datum; Semantic information; Template management; Information management",2-s2.0-84909592363
"Xu Y., Hua J., Ni Z., Chen Q., Fan Y., Ananiadou S., Chang E.I.-C., Tsujii J.","Anatomical entity recognition with a hierarchical framework augmented by external resources",2014,"PLoS ONE",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908428859&doi=10.1371%2fjournal.pone.0108396&partnerID=40&md5=2bc38bb14cbf6315331072ff8731ca09","References to anatomical entities in medical records consist not only of explicit references to anatomical locations, but also other diverse types of expressions, such as specific diseases, clinical tests, clinical treatments, which constitute implicit references to anatomical entities. In order to identify these implicit anatomical entities, we propose a hierarchical framework, in which two layers of named entity recognizers (NERs) work in a cooperative manner. Each of the NERs is implemented using the Conditional Random Fields (CRF) model, which use a range of external resources to generate features. We constructed a dictionary of anatomical entity expressions by exploiting four existing resources, i.e., UMLS, MeSH, RadLex and BodyPart3D, and supplemented information from two external knowledge bases, i.e., Wikipedia and WordNet, to improve inference of anatomical entities from implicit expressions. Experiments conducted on 300 discharge summaries showed a micro-averaged performance of 0.8509 Precision, 0.7796 Recall and 0.8137 F1 for explicit anatomical entity recognition, and 0.8695 Precision, 0.6893 Recall and 0.7690 F1 for implicit anatomical entity recognition. The use of the hierarchical framework, which combines the recognition of named entities of various types (diseases, clinical tests, treatments) with information embedded in external knowledge bases, resulted in a 5.08% increment in F1. The resources constructed for this research will be made publicly available. ©;2014 Xu et al.",,"accuracy; anatomical concepts; Article; book; hierarchical framework; information processing; knowledge base; medical record; medical terminology; practice guideline; algorithm; anatomy; artificial intelligence; automated pattern recognition; controlled vocabulary; human; knowledge base; natural language processing; semantics; Algorithms; Anatomy; Artificial Intelligence; Humans; Knowledge Bases; Natural Language Processing; Pattern Recognition, Automated; Semantics; Vocabulary, Controlled",2-s2.0-84908428859
"Grigorev S., Verbitskaia E., Ivanov A., Polubelova M., Mavchun E.","String-embedded language support in integrated development environment",2014,"ACM International Conference Proceeding Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984994802&doi=10.1145%2f2687233.2687247&partnerID=40&md5=a2f16f7b5fbe36ada70f6ec3b7646489","Most general-purpose programming languages allow to use string literals as source code in other languages (they are named string-embedded languages). Such strings can be executed or interpreted by dedicated runtime component. This way host program can communicate with DBMS or web browser. The most common example of string-embedded language is Dynamic SQL or SQL embedded into C#, C++, Java or other general-purpose programming languages. Standard Integrated Development Environment functionality such as syntax highlighting or static error checking in embedded languages can help developers who use such technique, but it is necessary to process string literals as a code to provide these features. We present a platform allowing to create tools for string-embedded languages processing easily, and compare it with other similar tools like IntelliLang. We also demonstrate a plug-in for ReSharper created by using the platform. The plug-in provides code highlighting and static error checking for string-embedded T-SQL in C#.","Abstract parsing; Dynamic SQL; IDE; Integrated development environment; Lexer generator; Parser generator; String-embedded language","Codes (symbols); Computer programming; Java programming language; Object oriented programming; Software engineering; Syntactics; Web services; Embedded Languages; General-purpose programming language; Integrated development environment; Lexer generator; Parser generators; Runtimes; Source codes; Static error; C++ (programming language)",2-s2.0-84984994802
"Harpaz R., Callahan A., Tamang S., Low Y., Odgers D., Finlayson S., Jung K., LePendu P., Shah N.H.","Text Mining for Adverse Drug Events: the Promise, Challenges, and State of the Art",2014,"Drug Safety",37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925709587&doi=10.1007%2fs40264-014-0218-z&partnerID=40&md5=634e393f60414f5ae71adb1f4cfc7e0c","Text mining is the computational process of extracting meaningful information from large amounts of unstructured text. It is emerging as a tool to leverage underutilized data sources that can improve pharmacovigilance, including the objective of adverse drug event (ADE) detection and assessment. This article provides an overview of recent advances in pharmacovigilance driven by the application of text mining, and discusses several data sources—such as biomedical literature, clinical narratives, product labeling, social media, and Web search logs—that are amenable to text mining for pharmacovigilance. Given the state of the art, it appears text mining can be applied to extract useful ADE-related information from multiple textual sources. Nonetheless, further research is required to address remaining technical challenges associated with the text mining methodologies, and to conclusively determine the relative contribution of each textual source to improving pharmacovigilance. © 2014, Springer International Publishing Switzerland.",,"adverse drug reaction; Article; biomedicine; drug surveillance program; information processing; machine learning; natural language processing; packaging; priority journal; social media; text mining; data mining; drug labeling; drug surveillance program; factual database; human; Internet; procedures; publication; Data Collection; Data Mining; Databases, Factual; Drug Labeling; Humans; Internet; Periodicals as Topic; Pharmacovigilance; Social Media",2-s2.0-84925709587
"Rees T.","Taxamatch, an algorithm for near ('Fuzzy') matching of scientific names in taxonomic databases",2014,"PLoS ONE",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907459114&doi=10.1371%2fjournal.pone.0107510&partnerID=40&md5=bb75f8efe9aa8e736b8779b2c7e4d4e4","Misspellings of organism scientific names create barriers to optimal storage and organization of biological data, reconciliation of data stored under different spelling variants of the same name, and appropriate responses from user queries to taxonomic data systems. This study presents an analysis of the nature of the problem from first principles, reviews some available algorithmic approaches, and describes Taxamatch, an improved name matching solution for this information domain. Taxamatch employs a custom Modified Damerau-Levenshtein Distance algorithm in tandem with a phonetic algorithm, together with a rule-based approach incorporating a suite of heuristic filters, to produce improved levels of recall, precision and execution time over the existing dynamic programming algorithms n-grams (as bigrams and trigrams) and standard edit distance. Although entirely phonetic methods are faster than Taxamatch, they are inferior in the area of recall since many real-world errors are non-phonetic in nature. Excellent performance of Taxamatch (as recall, precision and execution time) is demonstrated against a reference database of over 465,000 genus names and 1.6 million species names, as well as against a range of error types as present at both genus and species levels in three sets of sample data for species and four for genera alone. An ancillary authority matching component is included which can be used both for misspelled names and for otherwise matching names where the associated cited authorities are not identical. © 2014 Tony Rees.",,"accuracy; Article; book; computer language; controlled study; data base; dynamics; fuzzy system; information processing; mathematical analysis; Modified Damerau Levenshtein Distance algorithm; phonetic algorithm; probability; reference database; Taxamatch; taxonomy; algorithm; classification; factual database; procedures; Algorithms; Classification; Databases, Factual",2-s2.0-84907459114
"Shirai M., Yanagisawa T., Miura T.","Context-Based Query Using Dependency Structures Based on Latent Topic Model",2014,"Journal on Data Semantics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975317687&doi=10.1007%2fs13740-013-0031-3&partnerID=40&md5=4ed548b299d246394b0646f8fe08aa9f","To improve and enhance information retrieval on text database, there have been many approaches proposed so far, but few investigation captures context aspects of queries (of languages) directly. Here, we propose a new approach to retrieve contextual dependencies in Japanese based on latent topic model. The key idea comes from dependency structure which captures context in the database and the queries. We examine some experimental results to see the effectiveness. © 2013, Springer-Verlag Berlin Heidelberg.","Dependency structure; Latent Dirichlet Allocation; Query; Topic model",,2-s2.0-84975317687
"Wagner C.","Model-driven software migration: A methodology: Reengineering, recovery and modernization of legacy systems",2014,"Model-Driven Software Migration: A Methodology: Reengineering, Recovery and Modernization of Legacy Systems",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930859272&doi=10.1007%2f978-3-658-05270-6_1&partnerID=40&md5=bf701bbd3f0fc748824b1ca1f5859693","Today, reliable software systems are the basis of any business or company. The continuous further development of those systems is the central component in software evolution. It requires a huge amount of time- man power- as well as financial resources. The challenges are size, seniority and heterogeneity of those software systems. Christian Wagner addresses software evolution: the inherent problems and uncertainties in the process. He presents a model-driven method which leads to a synchronization between source code and design. As a result the model layer will be the central part in further evolution and source code becomes a by-product. For the first time a model-driven procedure for maintenance and migration of software systems is described. The procedure is composed of a model-driven reengineering and a model-driven migration phase. The application and effectiveness of the procedure are confirmed with a reference implementation applied to four exemplary systems. © Springer Fachmedien Wiesbaden 2014. All rights are reserved.",,"Computer software; Reengineering; Central component; Financial resources; Model-driven; Model-driven method; Reference implementation; Software Evolution; Software migration; Software systems; Legacy systems",2-s2.0-84930859272
"Sauthoff G., Giegerich R.","Yield grammar analysis and product optimization in a domain-specific language for dynamic programming",2014,"Science of Computer Programming",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898441479&doi=10.1016%2fj.scico.2013.09.011&partnerID=40&md5=739af4d453b76676c55009ee0b2d04f0","Dynamic programming algorithms are traditionally expressed by a set of matrix recurrences - a low level of abstraction, which renders the design of novel dynamic programming algorithms difficult and makes debugging cumbersome. Bellman's GAP is a declarative, domain-specific language, which supports dynamic programming over sequence data. It implements the algebraic style of dynamic programming and allows one to specify algorithms by combining so-called yield grammars with evaluation algebras. Products on algebras allow to create novel types of analyses from already given ones, without modifying tested components. Bellman's GAP extends the previous concepts of algebraic dynamic programming in several respects, such as an ""interleaved"" product operation and the analysis of multi-track input. Extensive analysis of the yield grammar and the evaluation algebras is required for generating efficient imperative code from the algebraic specification. This article gives an overview of the analyses required and presents several of them in detail. Measurements with ""real-world"" applications demonstrate the quality of the code produced. © 2013 Elsevier B.V.","Algebraic dynamic programming; Compiler optimization; Domain-specific language; Regular tree grammar; RNA structure prediction","Algebra; Algorithms; Problem oriented languages; Algebraic dynamic programming; Compiler optimizations; Domain specific languages; Regular tree grammars; RNA structures; Dynamic programming",2-s2.0-84898441479
"Quinlan K., Shults R.A., Rudd R.A.","Child passenger deaths involving alcohol-impaired drivers",2014,"Pediatrics",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901845850&doi=10.1542%2fpeds.2013-2318&partnerID=40&md5=aec98b50d79cd7dcab487ecd80a0a0c2","BACKGROUND AND OBJECTIVE: Approximately 1 in 5 child passenger deaths in the United States involves an alcohol-impaired driver, most commonly the child's own driver. The objective of this study was to document recent trends and state-specific rates of these deaths. METHODS: A descriptive analysis of 2001-2010 Fatality Analysis Reporting System data for child passengers aged <15 years killed in alcohol-impaired driving crashes. Driver impairment was defined as a blood alcohol concentration of ≥0.08 g/dL. RESULTS: During 2001-2010, 2344 children <15 years were killed in crashes involving at least 1 alcohol-impaired driver. Of these children, 1515 (65%) were riding with an impaired driver. Annual deaths among children riding with an alcohol-impaired driver decreased by 41% over the decade. Among the 37 states included in the state-level analysis, Texas (272) and California (135) had the most children killed while riding with an impaired driver and South Dakota (0.98) and New Mexico (0.86) had the highest annualized child passenger death rates (per 100 000 children). Most (61%) child passengers of impaired drivers were unrestrained at the time of the crash. One-third of the impaired drivers did not have a valid driver's license. CONCLUSIONS: Alcohol-impaired driving remains a substantial threat to the safety of child passengers in the United States, and typically involves children being driven by impaired drivers. This risk varies meaningfully among states. To make further progress, states and communities could consider increased use of effective interventions and efforts aimed specifically at protecting child passengers from impaired drivers. Copyright © 2014 by the American Academy of Pediatrics.","Accident prevention; Child passenger safety; Epidemiology-injury; Injury prevention and control; Motor vehicle accidents","accidental death; adult; alcohol blood level; article; cause of death; child; child death; childhood mortality; controlled study; descriptive research; drunken driving; fatality; female; human; male; middle aged; priority journal; United States; young adult; accident prevention; child passenger safety; epidemiology-injury; injury prevention and control; motor vehicle accidents; Accidents, Traffic; Adolescent; Alcoholic Intoxication; Automobile Driving; Cause of Death; Child; Child, Preschool; Ethanol; Female; Humans; Infant; Male; United States",2-s2.0-84901845850
"Kahlaoui A., Abran A.","Demystifying domain specific languages",2014,"Computational Linguistics: Concepts, Methodologies, Tools, and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946004982&doi=10.4018%2f978-1-4666-6042-7.ch012&partnerID=40&md5=1a92907b86a772b7bb5325b2f42967e2","Domain Specific Languages (DSLs) provide interesting characteristics that align well with the goals and mission of model-driven software engineering. However, there are still some issues that hamper widespread adoption. In this chapter, the authors discuss two of these issues. The first relates to the vagueness of the term DSL, which they address by studying the individual terms: domain, specificity, and language. The second is related to the difficulty of developing DSLs, which they address with a view to making DSL development more accessible via processes, standards, and tools. © 2014 by IGI Global. All rights reserved.",,"Computer programming languages; Software engineering; Domain specific languages; Model driven software engineering; Digital subscriber lines",2-s2.0-84946004982
"Günther S.","Design patterns and design principles for internal domain-specific languages",2014,"Computational Linguistics: Concepts, Methodologies, Tools, and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945984649&doi=10.4018%2f978-1-4666-6042-7.ch017&partnerID=40&md5=49cde649e6a600c64d10dd83b420a89a","Internal DSLs are a special kind of DSLs that use an existing programming language as their host. To build them successfully, knowledge regarding how to modify the host language is essential. In this chapter, the author contributes six DSL design principles and 21 DSL design patterns. DSL Design principles provide guidelines that identify specific design goals to shape the syntax and semantic of a DSL. DSL design patterns express proven knowledge about recurring DSL design challenges, their solution, and their connection to each other - forming a rich vocabulary that developers can use to explain a DSL design and share their knowledge. The chapter presents design patterns grouped into foundation patterns (which provide the skeleton of the DSL consisting of objects and methods), notation patterns (which address syntactic variations of host language expressions), and abstraction patterns (which provide the domain-specific abstractions as extensions or even modifications of the host language semantics). © 2014 by IGI Global. All rights reserved.",,"Abstracting; Problem oriented languages; Semantics; Syntactics; Design challenges; Design goal; Design Patterns; Design Principles; Domain specific; Domain specific languages; Language semantics; Syntactic variations; Digital subscriber lines",2-s2.0-84945984649
"Burtsev A., Mishrikoti N., Eide E., Ricci R.","Weir: A streaming language for performance analysis",2014,"Operating Systems Review (ACM)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955292233&doi=10.1145%2f2525528.2525537&partnerID=40&md5=1098a37186b027ce7dfdab225531446b","For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner. This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performanceanalysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform. Copyright © 2013 ACM.",,"Computational linguistics; Computer software reusability; Hydraulic structures; Pipeline processing systems; Pipelines; Weirs; Analysis algorithms; Data collection mechanism; Multiple contexts; Parallel environment; Performance analysis; Performance measurements; Programming environment; Systems performance analysis; Computer hardware description languages",2-s2.0-84955292233
"Neumann R.S., Kumar S., Haverkamp T.H.A., Shalchian-Tabrizi K.","BLASTGrabber: A bioinformatic tool for visualization, analysis and sequence selection of massive BLAST data",2014,"BMC Bioinformatics",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902536172&doi=10.1186%2f1471-2105-15-128&partnerID=40&md5=ec6f4c470837f3fdc5869aed319f71d3","Background: Advances in sequencing efficiency have vastly increased the sizes of biological sequence databases, including many thousands of genome-sequenced species. The BLAST algorithm remains the main search engine for retrieving sequence information, and must consequently handle data on an unprecedented scale. This has been possible due to high-performance computers and parallel processing. However, the raw BLAST output from contemporary searches involving thousands of queries becomes ill-suited for direct human processing. Few programs attempt to directly visualize and interpret BLAST output; those that do often provide a mere basic structuring of BLAST data.Results: Here we present a bioinformatics application named BLASTGrabber suitable for high-throughput sequencing analysis. BLASTGrabber, being implemented as a Java application, is OS-independent and includes a user friendly graphical user interface. Text or XML-formatted BLAST output files can be directly imported, displayed and categorized based on BLAST statistics. Query names and FASTA headers can be analysed by text-mining. In addition to visualizing sequence alignments, BLAST data can be ordered as an interactive taxonomy tree. All modes of analysis support selection, export and storage of data. A Java interface-based plugin structure facilitates the addition of customized third party functionality.Conclusion: The BLASTGrabber application introduces new ways of visualizing and analysing massive BLAST output data by integrating taxonomy identification, text mining capabilities and generic multi-dimensional rendering of BLAST hits. The program aims at a non-expert audience in terms of computer skills; the combination of new functionalities makes the program flexible and useful for a broad range of operations. © 2014 Neumann et al.; licensee BioMed Central Ltd.","Analysis; BLAST; High-throughput; Taxonomy; Text-mining; Visualization","Application programs; Bioinformatics; Blasting; Data mining; Digital storage; Flow visualization; Graphical user interfaces; Java programming language; Search engines; Taxonomies; Trees (mathematics); Visualization; Bioinformatics; Blasting; Data mining; Data visualization; Digital storage; Flow visualization; Graphical user interfaces; Java programming language; Taxonomies; Throughput; Trees (mathematics); User interfaces; Visualization; Analysis; Bioinformatics applications; High-performance computers; High-throughput; High-throughput sequencing; Sequence informations; Taxonomy identification; Text-mining; High performance computers; High throughput; Text mining; Data visualization; Search engines; algorithm; amino acid sequence; article; biology; computer analysis; computer program; factual database; high throughput sequencing; methodology; molecular genetics; sequence alignment; sequence analysis; Algorithms; Amino Acid Sequence; Computational Biology; Computing Methodologies; Databases, Factual; High-Throughput Nucleotide Sequencing; Molecular Sequence Data; Sequence Alignment; Sequence Analysis, Protein; Software",2-s2.0-84902536172
"Su W., Li Y., Lochovsky F.H.","Query interfaces understanding by statistical parsing",2014,"WWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990967194&doi=10.1145%2f2567948.2579702&partnerID=40&md5=1a42ecfaf911541af075a9bdd1e960ee","Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced-grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers. © Copyright 2014 by the International World Wide Web Conferences Steering Committee.","Maximum entropy; Query interface","Entropy; Formal languages; Maximum entropy methods; Probability; Query languages; World Wide Web; Hierarchical queries; Maximum entropy principle; Online database; Parsing methods; Query capabilities; Query interfaces; Statistical parser; Statistical parsing; Query processing",2-s2.0-84990967194
"Chen Y.-N., Wang W.Y., Rudnicky A.I.","Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems",2014,"2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943797217&doi=10.1109%2fSLT.2014.7078639&partnerID=40&md5=619a497a75a57f99055cf63ffd3e33e8","Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13% relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems. © 2014 IEEE.","Distributional semantics; Frame semantics; Unsupervised slot induction","Computational linguistics; Speech processing; Speech recognition; Vector spaces; Distributional semantics; Frame semantics; Language understanding; Relative average precision; Representative views; Spoken dialogue system; Unsupervised slot induction; Word representations; Semantics",2-s2.0-84943797217
"Athanasopoulou G., Klasinas I., Georgiladakis S., Iosif E., Potamianos A.","Using lexical, syntactic and semantic features for non-terminal grammar rule induction in spoken dialogue systems",2014,"2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946688240&doi=10.1109%2fSLT.2014.7078641&partnerID=40&md5=26d6d5b48706b58af80b6903bba5c59f","In this work, we propose an algorithm for the automatic induction of non-terminal grammar rules for Spoken Dialogue Systems (SDS). Initially, a grammar developer provides the system with a minimal set of rules that serve as seeding examples. Using these seed rules and (optionally) a seed corpus, in-domain data are harvested and filtered from the web. A challenging task is identifying relevant chunks (phrases) in the web-harvested corpus that are good candidates for enhancing the seed grammar. We propose and evaluate rule-based and statistical classification algorithms for this purpose that use lexical, syntactic and semantic features. Induced grammars are evaluated in terms of accuracy of the proposed rules for two spoken dialogue domains. Results show up to four times absolute precision improvement compared to the naive grammar induction approach using semantic phrase similarity. © 2014 IEEE.","Grammar enhancement; Grammar induction; Spoken dialogue systems; Spoken language understanding","Algorithms; Computational grammars; Computational linguistics; Semantics; Social networking (online); Speech recognition; Syntactics; Automatic induction; Grammar induction; Precision improvement; Semantic features; Spoken dialogue; Spoken dialogue system; Spoken language understanding; Statistical classification; Speech processing",2-s2.0-84946688240
"Li X., Tur G., Hakkani-Tur D., Li Q.","Personal knowledge graph population from user utterances in conversational understanding",2014,"2014 IEEE Workshop on Spoken Language Technology, SLT 2014 - Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983195590&doi=10.1109%2fSLT.2014.7078578&partnerID=40&md5=1cdb2929b884ab85366b7f988ad9290f","Knowledge graphs provide a powerful representation of entities and the relationships between them, but automatically constructing such graphs from spoken language utterances presents the novelty and numerous challenges. In this paper, we introduce a statistical language understanding approach to automatically construct personal (user-centric) knowledge graphs in conversational dialogs. Such information has the potential to better understand the users' requests, fulfilling them, and enabling other technologies such as developing better inferences or proactive interactions. Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language utterances. Hence, as a first step, we exploit the personal factual relation triples from Freebase to mine natural language snippets with a search engine, and the resulting snippets containing pairs of related entities to create the training data. This data is then used to build three key language understanding components: (1) Personal Assertion Classification identifies the user utterances that are relevant with personal facts, e.g., 'my mother's name is Rosa'; (2) Relation Detection classifies the personal assertion utterance into one of the predefined relation classes, e.g., 'parents'; and (3) Slot Filling labels the attributes or arguments of relations, e.g., 'name(parents): Rosa'. Our experiments using the Microsoft conversational understanding system demonstrate the performance of this proposed approach on the population of personal knowledge graphs. © 2014 IEEE.","Knowledge graph; Personal assertion; Relation detection; Slot filling; Spoken language understanding","Computational linguistics; Graphic methods; Search engines; Semantics; Knowledge graphs; Language understanding; Natural languages; Personal assertion; Related entities; Semantic parsing; Spoken language understanding; Spoken languages; Speech recognition",2-s2.0-84983195590
"Reinaldha F., Widagdo T.E.","Natural Language Interfaces to Database (NLIDB): Question handling and unit conversion",2014,"Proceedings of 2014 International Conference on Data and Software Engineering, ICODSE 2014",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988231348&doi=10.1109%2fICODSE.2014.7062663&partnerID=40&md5=d9c0fa39104c73527cb785c6de72214c","Natural Language Interfaces to Database is a type of database interface that allows the user to access the data using natural language. Recent development used as the base of mis paper was made by Wibisono in 2013. Wibisono developed a domain-independent NLIDB using Stanford Dependency Parser as a tool in processing user input. This paper presents a solution to two of Wibisono's NLIDB problems, which are inability to process question-type queries and unit conversion. User input will first be identified, whether it is a question-type query or a directive-type query. Methods from Wibisono's NLIDB are then modified in order to be able to process question-type query. Unit conversion is done using a library called JScience in two phases of the process, before the SQL is produced (if necessary), and before the data is shown to the user. The proposed system was tested and was able to process some questions that follow a pattern and also to convert data in measurement units and some currency units. © 2014 IEEE.","natural language interfaces to database; natural language query; question-type query; Stanford Dependency Parser; units","Data handling; Database systems; Natural language processing systems; Query processing; Software engineering; Units of measurement; Dependency parser; Natural language interfaces to database; Natural language queries; Question type; units; Computational linguistics",2-s2.0-84988231348
"Chou C.-P., Jea K.-F.","Unambiguous syntactic eXtensible Markup Language query matching on eXtensible Markup Language streams",2014,"Concurrent Engineering Research and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896858067&doi=10.1177%2f1063293X13515692&partnerID=40&md5=30b1ab8031723b3fbdafa63e8a3cd25d","We propose a method of syntactic pattern recognition for eXtensible Markup Language query matching on eXtensible Markup Language streams. A set of declarative grammar rules is generated, and then its parser, which is produced by full-fledged compiler tools, can be used to match multiple queries concurrently with only one-time scan on eXtensible Markup Language streams. The grammar is proved unambiguous with lemmas and theorems. We also analyze the time complexity and prove the correctness of multiple-query matching. Several experiments were conducted to demonstrate the efficiency and scalability of the proposed method in various aspects. As a result, the proposed method is beneficial for building efficient eXtensible Markup Language-based publish/subscribe applications. © The Author(s) 2013.","continuous query; eXtensible Markup Language streams; syntactic pattern recognition; twig-query matching; unambiguity","Continuous queries; Multiple queries; Publish/subscribe; Query matching; Syntactic pattern recognition; Time complexity; twig-query matching; unambiguity; Pattern recognition; Syntactics; XML",2-s2.0-84896858067
"Di Buccio E., Melucci M., Moro F.","Detecting verbose queries and improving information retrieval",2014,"Information Processing and Management",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894620758&doi=10.1016%2fj.ipm.2013.09.003&partnerID=40&md5=a65cffc74f50510ef27decc4a4a28a10","Although most of the queries submitted to search engines are composed of a few keywords and have a length that ranges from three to six words, more than 15% of the total volume of the queries are verbose, introduce ambiguity and cause topic drifts. We consider verbosity a different property of queries from length since a verbose query is not necessarily long, it might be succinct and a short query might be verbose. This paper proposes a methodology to automatically detect verbose queries and conditionally modify queries. The methodology proposed in this paper exploits state-of-the-art classification algorithms, combines concepts from a large linguistic database and uses a topic gisting algorithm we designed for verbose query modification purposes. Our experimental results have been obtained using the TREC Robust track collection, thirty topics classified by difficulty degree, four queries per topic classified by verbosity and length, and human assessment of query verbosity. Our results suggest that the methodology for query modification conditioned to query verbosity detection and topic gisting is significantly effective and that query modification should be refined when topic difficulty and query verbosity are considered since these two properties interact and query verbosity is not straightforwardly related to query length. © 2013 Elsevier Ltd. All rights reserved.","Information retrieval; Query expansion; Query modification; Verbose queries",,2-s2.0-84894620758
"Maziero E.G., Jorge M.L.D.R.C., Pardo T.A.S.","Revisiting Cross-document Structure Theory for multi-document discourse parsing",2014,"Information Processing and Management",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893049242&doi=10.1016%2fj.ipm.2013.12.003&partnerID=40&md5=71b9861a85638052a98c57035aa4c81e","Multi-document discourse parsing aims to automatically identify the relations among textual spans from different texts on the same topic. Recently, with the growing amount of information and the emergence of new technologies that deal with many sources of information, more precise and efficient parsing techniques are required. The most relevant theory to multi-document relationship, Cross-document Structure Theory (CST), has been used for parsing purposes before, though the results had not been satisfactory. CST has received many critics because of its subjectivity, which may lead to low annotation agreement and, consequently, to poor parsing performance. In this work, we propose a refinement of the original CST, which consists in (i) formalizing the relationship definitions, (ii) pruning and combining some relations based on their meaning, and (iii) organizing the relations in a hierarchical structure. The hypothesis for this refinement is that it will lead to better agreement in the annotation and consequently to better parsing results. For this aim, it was built an annotated corpus according to this refinement and it was observed an improvement in the annotation agreement. Based on this corpus, a parser was developed using machine learning techniques and hand-crafted rules. Specifically, hierarchical techniques were used to capture the hierarchical organization of the relations according to the proposed refinement of CST. These two approaches were used to identify the relations among texts spans and to generate multi-document annotation structure. Results outperformed other CST parsers, showing the adequacy of the proposed refinement in the theory. © 2014 Elsevier Ltd. All rights reserved.","Cross-document Structure Theory; Discourse parsing; Machine learning; Multi-document processing","Cross-document structure theories; Discourse parsing; Hierarchical organizations; Hierarchical structures; Hierarchical techniques; Machine learning techniques; Multi-document; Sources of informations; Data processing; Information management; Learning systems",2-s2.0-84893049242
"Nair G.G., Fernandes A., Nair K.","Landmark pharma patent jurisprudence in India",2014,"Journal of Intellectual Property Rights",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898713536&partnerID=40&md5=0aa6e828e3e368603a9aebe410ef796d","In a short span of less than 10 years after the new WTO/TRIPS based product patent regime came into effect, there have been many landmark judgments from Indian Courts on interpretation of various provisions of the exhaustive (Indian) Patents Act, 1970, as amended. Even though very large number of product patents have been granted during this period, patent challenges and infringement suits are limited to a few blockbuster molecules. Currently, litigations are in progress, including those in appeals as well as on matters remanded back to patent office. The landmark cases, in recent times, are dealt with, herein. Additional areas which may require judicial intervention arising out of the ambiguities in the Act and Rules are also briefly dealt with.","Dasatinib; Date of patent; Erlotinib; Gleevec; Ipab; Section 3(d); Sitagliptin; Sorafenib; Sunitinib; Supreme court order",,2-s2.0-84898713536
"Sethuraman L., Venugopal V., Zavvos A., Mueller M.","Structural integrity of a direct-drive generator for a floating wind turbine",2014,"Renewable Energy",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887002612&doi=10.1016%2fj.renene.2013.10.024&partnerID=40&md5=973070a67135e03b1d257c08591d1cd6","In this work, the suitability of a direct-drive radial flux permanent magnet generator is examined as a probable drive-train candidate for a stepped-spar floating wind turbine system that supports a 2MW downwind turbine. The suitability of the generator is assessed based on the structural integrity of its design (i.e., the stability of the air-gap between the rotor and stator) in response to the nacelle motions and its possible design implications on the overall system. Air gap deflections due to structural deflection and bearing tolerances were examined independently. The nacelle motions are obtained from experimental and numerical investigations on a 1:100 scale model. ANSYS suite is used to estimate the structural deformations of the generator and the changes in the air-gap distribution. Also, a simplified analytical model is used to compute the resulting changes in flux density and force distribution along the rotor periphery. The analytical model is also validated by 2D magneto-static simulations by utilising Finite Element Methods Magnetics software (FEMM).Preliminary results suggest that, if the nacelle accelerations are limited to 0.3 times the acceleration due to gravity (. g) and the motion response cycles are below the fatigue limit, the air-gap stability of the generator is more sensitive to magnetic forces. Contributions to air-gap eccentricity from shaft displacements can be limited if the bearing supports can be designed for high stiffness. This also confirmed the adequacy of the platform design. The results also emphasise the need for air-gap management when designing direct-drive generators for floating wind turbines. Two methods are investigated as potential solutions to limit the maximum air-gap deflection to 10% level. The method of increasing structural stiffness led to a structurally unfavourable design that could potentially affect the stability and resonance properties of the system. The method of increasing the design air-gap led to a structurally more favourable design, although this meant an increase in magnetic material and hence the costs. Thus, implementing direct-drive radial flux permanent magnet generators for floating wind turbines is challenged by the difficulty in achieving optimal weight and costs at acceptable performance without compromising the air-gap tolerances. There is a need for an amendment to design standards to recognise the design challenges of Floating wind turbines. © 2013 Elsevier Ltd.","Air-gap; Direct-drive generators; Floating wind turbine; Radial flux permanent magnet generator; Spar","Air-gaps; Direct drive; Floating wind turbines; Permanent magnet generator; Spar; Air-gaps; Direct drive; Floating wind turbines; Permanent magnet generator; Spar; Analytical models; Design; Magnetic materials; Models; Permanent magnets; Shaft displacement; Stiffness; Structural integrity; System stability; Wind turbines; Analytical models; Computer simulation; Design; Electric generators; Electric machine theory; Finite element method; Magnetic materials; Permanent magnets; Shaft displacement; Stiffness; Structural integrity; Synchronous generators; System stability; Wind turbines; Rotors (windings); Rotors (windings); cost-benefit analysis; deflection; design; finite element method; magnetic field; model validation; numerical model; renewable resource; resonance; shaft; software; wind turbine; analytical method; deformation; radial flow; wind turbine",2-s2.0-84887002612
"Berkelmans R., Doyle J., van Oppen M.J.H., Asbridge E.F., Brown A.R.","Efficacy of long-term coral tissue storage in ethanol for genotyping studies",2014,"Coral Reefs",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893731059&doi=10.1007%2fs00338-013-1106-1&partnerID=40&md5=c28ea9c17c83092aa60b448c28bcd5a2","With climate change threatening the future of coral reefs, there is an urgent need for effective coral tissue preservation and repositories from which DNA can be extracted. Most collections use 95 % ethanol as the storage medium, but its efficacy for long-term storage for short-fragment DNA use remains poorly documented. We conducted an accelerated DNA aging trial on three species of coral to ascertain whether ethanol-stored tissue and skeleton samples could yield fit-for-purpose DNA at time scales of 100+ yrs. We conclude that even using a crude DNA extraction technique, samples kept at 40 °C for 20 months yielded DNA of sufficient quality for Symbiodinium and coral host genotyping. If stored at -20 °C, these samples are likely to still yield useable DNA after 100 yrs. Ethanol-stored samples compared favorably in terms of DNA quality, quantity and sample integrity with those stored in an analogue of the commercial storage buffer RNAlater®. © 2013 Springer-Verlag Berlin Heidelberg.","Coral; DNA; Genotyping; PCR; Preservation; Storage",,2-s2.0-84893731059
"Nio L., Sakti S., Neubig G., Toda T., Nakamura S.","Conversation dialog corpora from television and movie scripts",2014,"Oriental COCOSDA 2014 - 17th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment / CASLRE (Conference on Asian Spoken Language Research and Evaluation)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949923988&doi=10.1109%2fICSDA.2014.7051436&partnerID=40&md5=7d1ca926fb4f6024c8da6658fe6ece84","Example-based dialogue systems often require natural conversation templates as examples for response generation. However, in previous work most conversation corpora have been created by hand and do not well portray actual conversations between two people. One way to overcome this problem is to record and transcribe real human-to-human conversation. However, this work is tedious and time consuming. In this work, we utilize conversation scripts from television and movies. We extract conversations from television and movie scripts from the web and perform various types of filtering. In order to ensure that the conversation is performed by two speakers, we introduce a unit of conversation called a tri-turn (a trigram conversation turn) which allow us to filter conversations with more than two speakers. In the end, our conversation corpora contains 86,719 query-response pairs that represent conversation turns performed by two speakers talking to each other1. © 2014 IEEE.",,"Motion pictures; Speech processing; Dialogue systems; Example based; Human-to-human conversation; Query response; Response generation; Tri grams; Speech recognition",2-s2.0-84949923988
"Pai V.M., Rodgers M., Conroy R., Luo J., Zhou R., Seto B.","Workshop on using natural language processing applications for enhancing clinical decision making: An executive summary",2014,"Journal of the American Medical Informatics Association",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893608628&doi=10.1136%2famiajnl-2013-001896&partnerID=40&md5=9be450261d60c8df18781fe27327bf1f","In April 2012, the National Institutes of Health organized a two-day workshop entitled 'Natural Language Processing: State of the Art, Future Directions and Applications for Enhancing Clinical Decision-Making' (NLP-CDS). This report is a summary of the discussions during the second day of the workshop. Collectively, the workshop presenters and participants emphasized the need for unstructured clinical notes to be included in the decision making workflow and the need for individualized longitudinal data tracking. The workshop also discussed the need to: (1) combine evidence-based literature and patient records with machine-learning and prediction models; (2) provide trusted and reproducible clinical advice; (3) prioritize evidence and test results; and (4) engage healthcare professionals, caregivers, and patients. The overall consensus of the NLP-CDS workshop was that there are promising opportunities for NLP and CDS to deliver cognitive support for healthcare professionals, caregivers, and patients.",,"article; caregiver; clinical decision making; consensus; decision support system; evidence based practice; health care personnel; human; machine learning; medical record; natural language processing; workflow; workshop; clinical decision-making; medical knowledge base; medical reasoning; natural language processing; personalized longitudinal healthcare; unstructured clinical notes; Artificial Intelligence; Decision Support Systems, Clinical; Electronic Health Records; Humans; Natural Language Processing",2-s2.0-84893608628
"Heck L., Huang H.","Deep learning of knowledge graph embeddings for semantic parsing of Twitter dialogs",2014,"2014 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2014",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949927567&doi=10.1109%2fGlobalSIP.2014.7032187&partnerID=40&md5=ec7497faa26a96c58bc2991a22d8fbf0","This paper presents a novel method to learn neural knowledge graph embeddings. The embeddings are used to compute semantic relatedness in a coherence-based semantic parser. The approach learns embeddings directly from structured knowledge representations. A deep neural network approach known as Deep Structured Semantic Modeling (DSSM) is used to scale the approach to learn neural embeddings for all of the concepts (pages) of Wikipedia. Experiments on Twitter dialogs show a 23.6% reduction in semantic parsing errors compared to the state-of-the-art unsupervised approach. © 2014 IEEE.","Deep learning; Dialog; Semantic parsing; Twitter","Information science; Knowledge representation; Social networking (online); Deep learning; Deep neural networks; Dialog; Knowledge graphs; Semantic parsing; Semantic relatedness; Twitter; Unsupervised approaches; Semantics",2-s2.0-84949927567
"Rodríguez-Cerezo D., Sarasa-Cabezuelo A., Gómez-Albarrán M., Sierra J.-L.","Serious games in tertiary education: A case study concerning the comprehension of basic concepts in computer language implementation courses",2014,"Computers in Human Behavior",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892441631&doi=10.1016%2fj.chb.2013.06.009&partnerID=40&md5=91ace34f2029d1319f85b907b714a4e8","This paper describes Evaluators, a system for the development of educational serious games oriented to introductory computer language implementation courses similar to those included in Computer Science tertiary curricula. Evaluators lets instructors generate games from collections of exercises addressing basic concepts about the design and implementation of computer languages (in particular, the processing of artificial languages according to the model of attribute grammars). By playing the generated games, students interactively learn the fundamentals of the semantic evaluation process behind attribute grammars. Indeed, they implicitly find solutions to the exercises presented, and they receive immediate feedback about successful and incorrect actions. In addition, the games log students' actions, which can subsequently be analyzed by the instructors using a specialized analytic tool that is included in Evaluators. Assessment of the system, which was performed according to three different dimensions (the instructors' perspective, the students' perspective and educational effectiveness perspective), (a) indicates that the exercise-driven approach of Evaluators is a cost-effective approach amenable to extrapolation to other areas of Computer Science tertiary education, (b) shows a positive attitude of students toward the serious games built with Evaluators, and (c) evidences a positive effect of the system and its pedagogical strategy on long-term student performance. © 2013 Elsevier Ltd. All rights reserved.","Attribute grammar; Authoring tool; Computer Science education; Learning analytics; Serious game","Attribute grammars; Authoring tool; Computer Science Education; Learning analytics; Serious games; Computer games; Computer science; Context sensitive grammars; Curricula; Education computing; Semantics; Teaching; Students",2-s2.0-84892441631
"Bordes A., Bottou L., Collobert R., Roth D., Weston J., Zettlemoyer L.","Introduction to the special issue on learning semantics",2014,"Machine Learning",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894302977&doi=10.1007%2fs10994-013-5381-4&partnerID=40&md5=28c48495ec3e99d9a387f4463fbfb829","The 2014 Special Issue of Machine Learning discusses several papers on learning semantics. The first paper of the special issue, 'From Machine Learning to Machine Reasoning' by Léon Bottou is an essay which attempts to bridge trainable systems, like neural networks, and sophisticated 'all-purpose' inference mechanisms, such as logical or probabilistic inference. The paper 'Learning Perceptually Grounded Word Meanings from Unaligned Parallel Data' by Stefanie Tellex, Pratiksha Thaker, Joshua Joseph and Nicholas Roy describes an approach to map natural language commands to actions for a forklift control task. The paper 'Interactive Relational Reinforcement Learning of Concept Semantics' by Matthias Nickles and Achim Rettinger presents a Relational Reinforcement Learning (RRL) approach for learning denotational concept semantics using symbolic interaction of artificial agents with human users.",,,2-s2.0-84894302977
"Goldwasser D., Roth D.","Learning from natural instructions",2014,"Machine Learning",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894906310&doi=10.1007%2fs10994-013-5407-y&partnerID=40&md5=6f7ced70a2b35bd0618909c12ee25e30","Machine learning is traditionally formalized and investigated as the study of learning concepts and decision functions from labeled examples, requiring a representation that encodes information about the domain of the decision function to be learned. We are interested in providing a way for a human teacher to interact with an automated learner using natural instructions, thus allowing the teacher to communicate the relevant domain expertise to the learner without necessarily knowing anything about the internal representations used in the learning process. In this paper we suggest to view the process of learning a decision function as a natural language lesson interpretation problem, as opposed to learning from labeled examples. This view of machine learning is motivated by human learning processes, in which the learner is given a lesson describing the target concept directly and a few instances exemplifying it. We introduce a learning algorithm for the lesson interpretation problem that receives feedback from its performance on the final task, while learning jointly (1) how to interpret the lesson and (2) how to use this interpretation to do well on the final task. traditional machine learning by focusing on supplying the learner only with information that can be provided by a task expert. We evaluate our approach by applying it to the rules of the solitaire card game. We show that our learning approach can eventually use natural language instructions to learn the target concept and play the game legally. Furthermore, we show that the learned semantic interpreter also generalizes to previously unseen instructions. © 2013 The Author(s).","Indirect supervision; Semantic interpretation; Structure prediction",,2-s2.0-84894906310
"Rodriguez-Cerezo D., Henriques P.R., Sierra J.-L.","Attribute grammars made easier: EvDebugger a visual debugger for attribute grammars",2014,"2014 International Symposium on Computers in Education, SIIE 2014",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946692216&doi=10.1109%2fSIIE.2014.7017699&partnerID=40&md5=0f79ca34ae703959b1da42ee4c330795","Compiler construction courses are usually considered by the students as a difficult subject of the Computer Science degree. The main problem found by the students is to fully understand the theoretical concepts taught during the course and its practical application to build a compiler. In this paper, we present a platform for the development and debugging of language processors based on attribute grammar-oriented specifications. The main aim of this tool is to help students to design their own language processors, supported by the visual debugger included. The animations provided by EvDebugger show, in an attractive way, how the attribute evaluation process is performed. In this way, students are able to solve design problems, improve the effectiveness and efficiency of their language processors and understand their operation through experimentation and debugging provided with the software tool. Besides, we performed an assessment study with students of a Compiler Construction course whose results are presented and discussed in this paper. © 2014 IEEE.","Attribute Grammars; Compiler Generator; Debugger; Education in Compiler Construction","Computational linguistics; Context sensitive grammars; Education; Program compilers; Program debugging; Visual languages; Attribute evaluation; Attribute grammars; Compiler construction; Compiler generators; Debuggers; Design problems; Effectiveness and efficiencies; Language processors; Students",2-s2.0-84946692216
"Henriksson A., Moen H., Skeppstedt M., Daudaravičius V., Duneld M.","Synonym extraction and abbreviation expansion with ensembles of semantic spaces",2014,"Journal of Biomedical Semantics",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906724043&doi=10.1186%2f2041-1480-5-6&partnerID=40&md5=6372c8200bf228434e032b761c5e7247","Background: Terminologies that account for variation in language use by linking synonyms and abbreviations to their corresponding concept are important enablers of high-quality information extraction from medical texts. Due to the use of specialized sub-languages in the medical domain, manual construction of semantic resources that accurately reflect language use is both costly and challenging, often resulting in low coverage. Although models of distributional semantics applied to large corpora provide a potential means of supporting development of such resources, their ability to isolate synonymy from other semantic relations is limited. Their application in the clinical domain has also only recently begun to be explored. Combining distributional models and applying them to different types of corpora may lead to enhanced performance on the tasks of automatically extracting synonyms and abbreviation-expansion pairs. Results: A combination of two distributional models - Random Indexing and Random Permutation - employed in conjunction with a single corpus outperforms using either of the models in isolation. Furthermore, combining semantic spaces induced from different types of corpora - a corpus of clinical text and a corpus of medical journal articles - further improves results, outperforming a combination of semantic spaces induced from a single source, as well as a single semantic space induced from the conjoint corpus. A combination strategy that simply sums the cosine similarity scores of candidate terms is generally the most profitable out of the ones explored. Finally, applying simple post-processing filtering rules yields substantial performance gains on the tasks of extracting abbreviation-expansion pairs, but not synonyms. The best results, measured as recall in a list of ten candidate terms, for the three tasks are: 0.39 for abbreviations to long forms, 0.33 for long forms to abbreviations, and 0.47 for synonyms. Conclusions: This study demonstrates that ensembles of semantic spaces can yield improved performance on the tasks of automatically extracting synonyms and abbreviation-expansion pairs. This notion, which merits further exploration, allows different distributional models - with different model parameters - and different types of corpora to be combined, potentially allowing enhanced performance to be obtained on a wide range of natural language processing tasks. © 2014 Henriksson et al.; licensee BioMed Central Ltd.",,,2-s2.0-84906724043
"Barenghi A., Crespi Reghizzi S., Mandrioli D., Panella F., Pradella M.","The PAPAGENO parallel-parser generator",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900550089&doi=10.1007%2f978-3-642-54807-9_11&partnerID=40&md5=1d4351e3070d5dbcba39fe5ea6cad574","The increasing use of multicore processors has deeply transformed computing paradigms and applications. The wide availability of multicore systems had an impact also in the field of compiler technology, although the research on deterministic parsing did not prove to be effective in exploiting the architectural advantages, the main impediment being the inherent sequential nature of traditional LL and LR algorithms. We present PAPAGENO, an automated parser generator relying on operator precedence grammars. We complemented the PAPAGENO-generated parallel parsers with parallel lexing techniques, obtaining near-linear speedups on multicore machines, and the same speed as Bison parsers on sequential execution. © 2014 Springer-Verlag.","Operator Precedence Grammars; Parallel Parsing; Parser generation","Artificial intelligence; Computer science; Computers; Compiler technology; Deterministic parsing; Multi-core machines; Multi-core processor; Operator precedence grammars; Parallel Parsing; Parser generation; Sequential execution; Program compilers",2-s2.0-84900550089
"Kumar R., Dua M., Jindal S.","D-HIRD: Domain-independent Hindi language interface to relational database",2014,"2014 International Conference on Computation of Power, Energy, Information and Communication, ICCPEIC 2014",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921475533&doi=10.1109%2fICCPEIC.2014.6915344&partnerID=40&md5=79a40df56402dc1827055e1b54e204ea","Person with no knowledge of database language may find it difficult to access the database. Many systems were developed to access the information from the database using natural language. But these systems are domain-dependent i.e. they were developed for a particular domain. We develop a system called D-HIRD for Hindi language which provides domain-independence. For developing a domain-independent system, we divide our system into two modules: Language processing module and Database module. Language processing module use Hindi Shallow Parser for analyzing and parsing the input query. We introduce a domain-identifier component in database module which identifies the domain by using knowledge base. We use three database domain and 80 queries for each database for testing our system. The system is able to correctly identify the domain from the query and translate it into SQL which is executed on the database and result is provided to the user in Hindi language. © 2014 IEEE.","Domain-Independent system; Hindi shallow parser; Natural language interface to database (NLIDB); Pattern-matching System; Question-answering system; Relational Database","Artificial intelligence; Knowledge based systems; Natural language processing systems; Pattern matching; Query languages; Query processing; Domain independents; Hindi shallow parser; Natural language interface to database; Pattern-matching systems; Question answering systems; Relational Database; Computational linguistics",2-s2.0-84921475533
"Charles Britto S.","Development of multi resource web query engine for public information services",2014,"International Journal of Applied Engineering Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941027421&partnerID=40&md5=cc1fe86abe0186d135a2cc1494b0eb4e","In this paper, we propose an analytical integration of Web services in a relational web based database system. This supports the websites to import data from organized data elements or remote heterogeneous websites systems on demand. We proposed a schema for integrating the website information through a supportive tool component. We implement our conventional methodology of integrating the central government public services,state government public services and non government public services through their link collections organized in a separate web service. This paper allows developers to make service calls from within the web base, which introduces a novel way of accessing and using Web services in a database-driven web application. In near future we will include fuzzy web logics in order to launch a web service that will act as a decision support system. © Research India Publications.","Parser; Webdata; Webmining; Webservice",,2-s2.0-84941027421
"Mousavi H., Kerr D., Iseli M., Zaniolo C.","Mining semantic structures from syntactic structures in free text documents",2014,"Proceedings - 2014 IEEE International Conference on Semantic Computing, ICSC 2014",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906969180&doi=10.1109%2fICSC.2014.31&partnerID=40&md5=ddc548d89622ca6b3515d303e144c349","The Web has made possible many advanced text-mining applications, such as news summarization, essay grading, question answering, and semantic search. For many of such applications, statistical text-mining techniques are ineffective since they do not utilize the morphological structure of the text. Thus, many approaches use NLP-based techniques, that parse the text and use patterns to mine and analyze the parse trees which are often unnecessarily complex. Therefore, we propose a weighted-graph representation of text, called Text Graphs, which captures the grammatical and semantic relations between words and terms in the text. Text Graphs are generated using a new text mining framework which is the main focus of this paper. Our framework, SemScape, uses a statistical parser to generate few of the most probable parse trees for each sentence and employs a novel two-step pattern-based technique to extract from parse trees candidate terms and their grammatical relations. Moreover, SemScape resolves co references by a novel technique, generates domain-specific Text Graphs by consulting ontologies, and provides a SPARQL-like query language and an optimized engine for semantically querying and mining Text Graphs. © 2014 IEEE.",,"Data mining; Forestry; Grading; Graphic methods; Natural language processing systems; Query languages; Semantic Web; Semantics; Grammatical relations; Morphological structures; News summarization; Question Answering; Semantic relations; Semantic structures; Statistical parser; Syntactic structure; Syntactics; Data Processing; Forestry; Programing Languages",2-s2.0-84906969180
"Joshi M., Sawant U., Chakrabarti S.","Knowledge graph and corpus driven segmentation and answer inference for telegraphic entity-seeking queries",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949831806&partnerID=40&md5=8c72e81b0bb3d153a80329d363f528f0","Much recent work focuses on formal interpretation of natural question utterances, with the goal of executing the resulting structured queries on knowledge graphs (KGs) such as Freebase. Here we address two limitations of this approach when applied to open-domain, entity-orientedWeb queries. First,Web queries are rarely wellformed questions. They are ""telegraphic"", with missing verbs, prepositions, clauses, case and phrase clues. Second, the KG is always incomplete, unable to directly answer many queries. We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment: A base entity e1, a relation type r, a target entity type t2, and contextual words s. The query seeks entity e2 ε t2 where r(e1, e2) holds, further evidenced by schema-agnostic words s. Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG. We do not trust the best or any specific query segmentation. Instead, evidence in favor of candidate e2s are aggregated across several segmentations. Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG, using over a thousand telegraphic queries adapted from TREC, INEX, and Web- Questions, show the efficacy of our approach. For one benchmark, MAP improves from 0.2-0.29 (competitive baselines) to 0.42 (our system). NDCG@10 improves from 0.29-0.36 to 0.54. © 2014 Association for Computational Linguistics.",,"Linguistics; Natural language processing systems; Social networking (online); ClueWeb; Coarse-grained; Contextual words; Entity-types; Knowledge graphs; Novel techniques; Structured queries; Query processing",2-s2.0-84949831806
"Ball A., Bourreau P., Kien É., Salvati S.","Building PMCFG parsers as datalog program transformations",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903752424&doi=10.1007%2f978-3-662-43742-1_1&partnerID=40&md5=dd682917d98135d55347267ff14861cd","Kanazawa [6,7] proposes to view recognition algorithms for Multiple Context-Free Grammars (MCFGs) as Datalog program transformations. He illustrates this view by transforming a Datalog program implementing CYK recognizers for MCFGs into programs that implement prefix-correct recognizers for MCFGs. We enhance his results in three different directions: (i) we present an extension of Datalog so as to build parsers instead of recognizers; (ii) we adapt his program transformations to extended Datalog programs that construct prefix-correct parsers for Parallel Multiple Context-Free Grammars (PMCFGs); (iii) we propose another program transformation for extended Datalog programs that produces left-corner parsers for PMCFGs. © 2014 Springer-Verlag.",,"Algorithms; Context free grammars; Datalog; Datalog programs; Multiple context-free grammar; Parallel multiple context-free grammars; Program transformations; Recognition algorithm; Computational linguistics",2-s2.0-84903752424
"Fonou-Dombeu J.V., Phiri N.M., Huisman M.","Persistent storage and query of E-government ontologies in relational databases",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906871549&doi=10.1007%2f978-3-319-10178-1_10&partnerID=40&md5=cc32e0c364e80a902c72a6fed6d9879f","Over the past eight years, building ontologies for semantic-driven e-government applications has been a subject of interest to e-government researchers. However, only a few has focused on the persistent storage and query of ontologies of the e-government domain. In this paper, 3 selected e-government ontologies are persistently stored and queried in relational databases. The OWL and RDF codes of these ontologies generated with Protégé or downloaded from the Internet are parsed with Jena API (Application Programming Interface) and loaded into MySQL RDBMS (Relational Database Management System). Thereafter, SPARQL queries are written to extract information from the created ontology databases. Experiments show that (1) the Jena parser scales well and could successfully parse and store e-government ontologies of different sizes into relational databases, and (2) the response times of SPARQL queries written in Jena to MySQL ontology databases are proportional to the sizes of the ontologies. © 2014 Springer International Publishing.","E-government; Jena API; MySQL; Ontology Query; Ontology Storage; Protégé; Semantic Web; SPARQL","Application programming interfaces (API); Database systems; Information systems; Semantic Web; e-Government; Jena API; MySQL; Ontology query; SPARQL; Government data processing",2-s2.0-84906871549
"Flanigan J., Thomson S., Carbonell J., Dyer C., Smith N.A.","A discriminative graph-based parser for the abstract meaning representation",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906930520&partnerID=40&md5=d7c2fba504d7110fb7913297cfa27680","Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available. We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement. The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr. © 2014 Association for Computational Linguistics.",,"Algorithms; Computational linguistics; Optimization; Semantics; Connected Subgraph; Future improvements; Graph-based; LaGrangian relaxation; Novel algorithm; Open source system; Optimization problems; Structured prediction; Syntactics",2-s2.0-84906930520
"Yoshida A., Hachisu Y.","A pattern search method for unpreprocessed C programs based on tokenized syntax trees",2014,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924861169&doi=10.1109%2fSCAM.2014.16&partnerID=40&md5=609bf8c8a95793cf0b2737ef89daad07","Pattern search of programs is a fundamental function for supporting programming. In this paper, we propose a search method for unpreprocessed programs, which are difficult to parse. Our parser directly parses them by rewriting token sequences, and allows minor errors in syntax trees. The search tool takes queries that are the same as the format of program fragments. By using the same parser for both queries and target programs, programmers have no need to describe the detail structures of syntax trees in queries. To support accurate search, we also show an alignment tool for branch directives, which converts undisciplined directives to discipline ones, and a reverse macro expansion tool, which integrates the use of macro calls. Finally, we present some experiments in which we have applied the tools to an open source application, and discuss how to improve our tools. © 2014 IEEE.","parser; pattern search; rewrite rule; unpreprocessed C program","Computer programming; Open source software; Syntactics; Trees (mathematics); C programs; Open source application; parser; Pattern search; Pattern search method; Program fragments; Rewrite rules; Token sequences; C (programming language)",2-s2.0-84924861169
"Jurčíček F., Dušek O., Plátek O.","A factored discriminative spoken language understanding for spoken dialogue systems",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906987316&doi=10.1007%2f978-3-319-10816-2_70&partnerID=40&md5=e402cdd1573a94fa368d745b219dff54","This paper describes a factored discriminative spoken language understanding method suitable for real-time parsing of recognised speech. It is based on a set of logistic regression classifiers, which are used to map input utterances into dialogue acts. The proposed method is evaluated on a corpus of spoken utterances from the Public Transport Information (PTI) domain. In PTI, users can interact with a dialogue system on the phone to find intra- and inter-city public transport connections and ask for weather forecast in a desired city. The results show that in adverse speech recognition conditions, the statistical parser yields significantly better results compared to the baseline well-tuned handcrafted parser. © 2014 Springer International Publishing.","dialogue systems; meaning representation; spoken language understanding","Speech processing; Speech recognition; Weather forecasting; Dialogue acts; Dialogue systems; Logistic regression classifier; meaning representation; Public transport; Spoken dialogue system; Spoken language understanding; Statistical parser; Computational linguistics",2-s2.0-84906987316
"Van Nguyen B., Nguyen D.T.","Towards a semantic linked data retrieval model",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909989270&partnerID=40&md5=1df2c2e208adef1033ecab6583a4b99b","This research aims to introduce our Semantic Linked Data Retrieval Model (SLDRM) which is conceived to perform the traditional Ad-hoc Retrieval task of INEX 2013 Linked Data Track. SLDRM was built based on combining some novel semantic representation models for processing English retrieval queries, and proposed techniques for creating and optimizing queries on Indri search engine to effectively retrieve structured text data. Based on SLDRM, a Semantic Linked Data Retrieval System (SLDRS) was built to assess the performance of SLDRM. The evaluation methods of the INEX 2013 Linked Data Track, based on using standard Ad-hoc search task’s testing topics and two standard scores MRR and TREC MAiP, were used to evaluate the accuracy of SLDRS. The experiments show that MAiP score of our SLDRS is the highest in comparision with all of submitted runs of INEX 2013 Linked Data Track. © Springer International Publishing Switzerland 2014.","Linked data; Query language; Search engine; Semantic model; Structured text retrieval","Data handling; Information retrieval; Query languages; Security of data; Security systems; Semantics; Ad-hoc retrieval tasks; Evaluation methods; Linked datum; Retrieval query; Semantic Model; Semantic representation; Standard scores; Structured text; Search engines",2-s2.0-84909989270
"Stephen W.","The CLAS system at the MediaEval 2014 c@merata Task",2014,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909957389&partnerID=40&md5=470776fa70d0f3a64e1dcdcda958fb94","This paper describes the CLAS system which accepts natural language queries in the domain of music theory to perform passage retrieval from a musical score. This system was produced for participation in the C@merata MediaEval 2014 shared task. The system uses a domain-specific parser to interpret the query and answer generation methods based on feature unification. Performance on this task was encouraging with 0.76 precision and 0.96 recall.",,"Query processing; Domain specific; Feature unification; Generation method; Music theory; Musical score; Natural language queries; Passage retrieval; System use; Natural language processing systems",2-s2.0-84909957389
"Chatterji S., Sreedhara G.S., Desarkar M.S.","An efficient tool for syntactic processing of english query text",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915771315&partnerID=40&md5=0e12605f3b77f70a8a67303ea43ffa20","A large amount of work has been done on syntactic analysis of English texts. But, for analyzing the short phrases without any structured contexts like capitalization, subject-object-verb order, etc. these techniques are not yet proved to be appropriate. In this paper we have attempted the syntactic analysis of the phrases where contextual information is not available. We have developed stemmer, POS tagger, chunker and Named Entity tagger for English short phrases like chats, messages, and queries, using root dictionary and language specific rules. We have evaluated the technique on English queries and observed that our system outperforms some commonly used NLP tools. © Springer International Publishing Switzerland 2014.","Chunk; Named Entity; Parts-of-Speech; Short text analysis; Stemming; Trie","Computational linguistics; Natural language processing systems; Chunk; Named entities; Short texts; Stemming; Trie; Syntactics",2-s2.0-84915771315
"Kozanitis C., Heiberg A., Varghese G., Bafna V.","Using genome query language to uncover genetic variation",2014,"Bioinformatics",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891361949&doi=10.1093%2fbioinformatics%2fbtt250&partnerID=40&md5=c0ff497343735d60adf844f63500b44e","With high-throughput DNA sequencing costs dropping 1000 for human genomes, data storage, retrieval and analysis are the major bottlenecks in biological studies. To address the large-data challenges, we advocate a clean separation between the evidence collection and the inference in variant calling. We define and implement a Genome Query Language (GQL) that allows for the rapid collection of evidence needed for calling variants. Results: We provide a number of cases to showcase the use of GQL for complex evidence collection, such as the evidence for large structural variations. Specifically, typical GQL queries can be written in 510 lines of high-level code and search large datasets (100 GB) in minutes. We also demonstrate its complementarity with other variant calling tools. Popular variant calling tools can achieve one order of magnitude speed-up by using GQL to retrieve evidence. Finally, we show how GQL can be used to query and compare multiple datasets. By separating the evidence and inference for variant calling, it frees all variant detection tools from the data intensive evidence collection and focuses on statistical inference.",,"algorithm; DNA sequence; genetic variation; high throughput sequencing; human; human genome; software; article; computer program; genetic variability; Algorithms; Genetic Variation; Genome, Human; High-Throughput Nucleotide Sequencing; Humans; Sequence Analysis, DNA; Software; Algorithms; Genetic Variation; Genome, Human; High-Throughput Nucleotide Sequencing; Humans; Sequence Analysis, DNA; Software",2-s2.0-84891361949
"Li Y., Yan Y.","Robust algorithms for semantic class labeling in Chinese query understanding",2014,"Journal of Computational Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907769732&doi=10.12733%2fjcis11201&partnerID=40&md5=8347116747c0b6f17e425673c3e38088","In this paper we propose an approach to solve the words' variation induced by automatic speech recognition (ASR) and errors by keyboard in human-computer interaction system. Considering the characteristics of Chinese, fuzzy matching based on Chinese pinyin is utilized to correct the semantic concepts in a natural language query. The approach is in two stages: first, conditional random field (CRF) model is trained for building probabilistic models to segment and label entity names from an input sentence. Second, similarity measure is conducted through Chinese pinyin of both the named entities labeled by a CRF model and lexicon of a dictionary. The experiments compare the performances in output of ASR and keyboard. Results report that the approach based on Chinese pinyin can improve robustness better than the one based on Chinese characters in actual human-computer interaction systems. © 2014 by Binary Information Press","Conditional random field (CRF); Fuzzy matching; Named entity recognition (NER); Similarity function; Spoken language understanding (SLU)","Conditional random field; Fuzzy matching; Named entity recognition; Similarity functions; Spoken language understanding",2-s2.0-84907769732
"Xu K., Feng Y., Zhao D.","Xser@QALD-4: Answering natural language questions via phrasal semantic parsing",2014,"CEUR Workshop Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981234303&partnerID=40&md5=ffe859d77ece8524f717de1474bd8b2f","We present a question answering system (Xser) over Linked Data(DBpedia), converting users' natural language questions into structured queries. There are two challenges involved: recognizing users' query intention and mapping the involved semantic items against a given knowledge base (KB), which will be in turn assembled into a structured query. In this paper, we propose an efficient pipeline framework to model a user's query intention as a phrase level dependency DAG which is then instantiated according to a given KB to construct the final structured query. We evaluate our approach on the QALD-4 test dataset and achieve an F-measure score of 0.72, an average precision of 0.72 and an average recall of 0.71 over 50 questions.","Knowledge base; Phrasal semantic parsing; Question answering","Artificial intelligence; Computational linguistics; Knowledge based systems; Natural language processing systems; Semantics; Statistical tests; F-measure scores; Knowledge base; Linked datum; Natural language questions; Question Answering; Question answering systems; Semantic parsing; Structured queries; Query processing",2-s2.0-84981234303
"Xu K., Zhang S., Feng Y., Zhao D.","Answering natural language questions via phrasal semantic parsing",2014,"Communications in Computer and Information Science",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916208022&partnerID=40&md5=d152675a8cda9785cb05be4f4a8d4e3c","Understanding natural language questions and converting them into structured queries have been considered as a crucial way to help users access large scale structured knowledge bases. However, the task usually involves two main challenges: recognizing users’ query intention and mapping the involved semantic items against a given knowledge base (KB). In this paper, we propose an efficient pipeline framework to model a user’s query intention as a phrase level dependency DAG which is then instantiated regarding a specific KB to construct the final structured query. Our model benefits from the efficiency of linear structured prediction models and the separation of KB-independent and KB-related modelings. We evaluate our model on two datasets, and the experimental results showed that our method outperforms the state-of-the-art methods on the Free917 dataset, and, with limited training data from Free917, our model can smoothly adapt to new challenging dataset, WebQuestion, without extra training efforts while maintaining promising performances. © Springer-Verlag Berlin Heidelberg 2014.",,"Knowledge based systems; Query processing; Semantics; Knowledge base; Limited training data; Natural language questions; Semantic parsing; State-of-the-art methods; Structured knowledge; Structured prediction; Structured queries; Natural language processing systems",2-s2.0-84916208022
"Gerasimov N., Mozgovoy M., Lagunov A.","Semantic sentence structure search engine",2014,"2014 Federated Conference on Computer Science and Information Systems, FedCSIS 2014",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912059631&doi=10.15439%2f2014F343&partnerID=40&md5=8e9b9b2a27b2245f4c79139dcc4f1fff","Many of current web search engines rely on inverted index-based data structures as document information store. Since and inverted index is a map from individual document words to their respective locations, such data structure destructs semantic links between the words, and thus does not support structural user queries. In other words, such systems can only find the documents that contain user-specified words. In this paper we propose to create semantic links between the terms contained in inverted index, and in such way create a semantic network. This network will preserve the internal structure of the stored documents, and will enable the users to perform structural queries. Both structural-saving indexation and structural user search query allow to save semantic speech meaning of the text while search process. © 2014 Polish Information Processing Society.",,"Data structures; Indexing (of information); Semantics; World Wide Web; Internal structure; Inverted indices; Search process; Search queries; Semantic link; Semantic network; Sentence structures; Structural query; Search engines",2-s2.0-84912059631
"Kimelfeld B., Ré C.","Transducing Markov sequences",2014,"Journal of the ACM",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907025261&doi=10.1145%2f2630065&partnerID=40&md5=fc064d72c4127ad3fea02d7102da1170","A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radiofrequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-κ answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any subexponentialapproximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable. © 2014 ACM.","Enumeration; Hidden Markov models; Markov sequences; Probabilistic databases; Ranked query evaluation; Transducers","Transducers; Enumeration; Markov sequence; Probabilistic database; Ranked query evaluations; Transducing; Hidden Markov models",2-s2.0-84907025261
"Cánovas Izquierdo J.L., García Molina J.","Extracting models from source code in software modernization",2014,"Software and Systems Modeling",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899945387&doi=10.1007%2fs10270-012-0270-z&partnerID=40&md5=426e9a5d25ce8a4c5a21750c6d327201","Model-driven software modernization is a discipline in which model-driven development (MDD) techniques are used in the modernization of legacy systems. When existing software artifacts are evolved, they must be transformed into models to apply MDD techniques such as model transformations. Since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (GPL), the extraction of models from GPL code is an essential task in a model-based modernization process. This activity could be performed by tools to bridge grammarware and MDD technical spaces, which is normally carried out by dedicated parsers. Grammar-to-Model Transformation Language (Gra2MoL) is a domain-specific language (DSL) tailored to the extraction of models from GPL code. This DSL is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. Gra2MoL aims to reduce the effort needed to implement grammarware-MDD bridges, since building dedicated parsers is a complex and time-consuming task. Like ATL and RubyTL languages, Gra2MoL incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. The language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. Moreover, it incorporates extensibility and grammar reuse mechanisms. This paper describes Gra2MoL in detail and includes a case study based on the application of the language in the extraction of models from Delphi code. © 2012 Springer-Verlag.","Domain-specific languages; Model-driven engineering; Model-driven software development; Model-driven software modernization; Software modernization","Extraction; Graphical user interfaces; Legacy systems; Mathematical models; Problem oriented languages; Query languages; Software design; Trees (mathematics); Application migrations; Domain specific languages; General-purpose programming language; Model-driven development; Model-driven Engineering; Model-Driven Software Development; Software modernization; Transformation languages; Modernization",2-s2.0-84899945387
"Li S., He Z., Wu J.","An ontology semantic tree based natural language interface",2014,"Proceedings of the 9th International Symposium on Chinese Spoken Language Processing, ISCSLP 2014",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912063110&doi=10.1109%2fISCSLP.2014.6936647&partnerID=40&md5=f8a994afbb48e81b88aeb5081591f83e","As more and more ontology knowledge bases have been published, every user may have access to a wealth of knowledge. However, to acquire the information in ontologies, users have to be familiar with ontologies and its formal query language. Therefore, natural language interfaces (NLI) have been proposed in recent years to bridge the gap between ontologies and non-expert users. Traditional approaches have pretty broad coverage of natural language (NL) and good performance on wellorganised NL queries. But they are subject to the word order, due to the lack of original semantic information of queries. This paper proposes a NLI which accepts NL as input and generates SPARQL (SPARQL Protocol and RDF Query Language) queries as output. To analyze the NL queries, the ontology semantic tree has been used to represent the semantic conceptual structure of NL queries with the support of ontology. Our results show that the proposed system can make use of semantic structure effectively and has a better performance than the baseline system on the queries with flexible word order. © 2014 IEEE.","natural language interface; ontology semantic tree; SPARQL","Query languages; Semantic Web; Semantics; Better performance; Conceptual structures; Natural language interfaces; Ontology semantics; Semantic information; Semantic structures; SPARQL; Traditional approaches; Natural language processing systems",2-s2.0-84912063110
"Ofek N., Rokach L., Mitra P.","Methodology for connecting nouns to their modifying adjectives",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958531562&doi=10.1007%2f978-3-642-54906-9_22&partnerID=40&md5=75c9ef16b0c2362d47a98a0c4361efb0","Adjectives are words that describe or modify other elements in a sentence. As such, they are frequently used to convey facts and opinions about the nouns they modify. Connecting nouns to the corresponding adjectives becomes vital for intelligent tasks such as aspect-level sentiment analysis or interpretation of complex queries (e.g., ""small hotel with large rooms"") for fine-grained information retrieval. To respond to the need, we propose a methodology that identifies dependencies of nouns and adjectives by looking at syntactic clues related to part-of-speech sequences that help recognize such relationships. These sequences are generalized into patterns that are used to train a binary classifier using machine learning methods. The capabilities of the new method are demonstrated in two, syntactically different languages: English, the leading language of international discourse, and Hebrew, whose rich morphology poses additional challenges for parsing. In each language we compare our method with a designated, state-of-the-art parser and show that it performs similarly in terms of accuracy while: (a) our method uses a simple and relatively small training set; (b) it does not require a language specific adaptation, and (c) it is robust across a variety of writing styles. © 2014 Springer-Verlag Berlin Heidelberg.","Information Retrieval; Parsing; Relation Extraction","Binary sequences; C (programming language); Computational linguistics; Information retrieval; Learning systems; Text processing; Binary classifiers; Complex queries; Machine learning methods; Parsing; Part Of Speech; Relation extraction; Sentiment analysis; Specific adaptations; Natural language processing systems",2-s2.0-84958531562
"Peng Z., Cherniack M., Papaemmanouil O.","Devel-op: An optimizer development environment",2014,"Proceedings - International Conference on Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901799230&doi=10.1109%2fICDE.2014.6816760&partnerID=40&md5=02c977178f39627ca76472050d3cea56","Recent advances in the underlying architectures of database management systems (DBMS) have motivated the redesign of key DBMS components such as the query optimizer. Optimizers are inherently difficult to build and maintain, and yet there exists no software engineering tools to facilitate their development. In this paper, we introduce a [Devel]opment Environment for Query [Op]timizers (Devel-Op) designed to facilitate the rapid prototyping, profiling and benchmarking of optimizers. Our current version of the tool permits declarative specification and generation of two key optimizer components (the logical plan enumerator and physical plan generator) as well as debugging and visualization tools for profiling generated components. © 2014 IEEE.",,"Software engineering; Development environment; Optimizers; Query optimizer; Software engineering tools; Visualization tools; Database systems",2-s2.0-84901799230
"Santurkar S., Arora A., Chandrasekaran K.","Stormgen - A Domain specific Language to create ad-hoc storm topologies",2014,"2014 Federated Conference on Computer Science and Information Systems, FedCSIS 2014",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941570893&doi=10.15439%2f2014F278&partnerID=40&md5=f10f7fc1e531e9f4e8970e5342b0bd54","Large-scale distributed data processing has gained significant momentum in research in the past decade. With the introduction of MapReduce, many frameworks have been developed that either implement MapReduce or provide additional functionalities useful in a larger domain. While the framework introduced in the MapReduce paper performs batch-processing of data, Apache Storm performs real-time computation on data. Storm does this with the help of Topologies, and the constituents of the Topology are developed using General-purpose Programming Languages (GPL). A Domain-specific Language (DSL) can provide a higher level of abstraction over GPLs and model the specialized features of a particular domain in a better way. In this paper, we propose the development of Storm Topology generator (Stormgen), a DSL for Storm Topology development, and show how the specifications of this DSL can be utilized during the code generation of exact Storm Topology components in Java. The parser and code generator for Stormgen's syntax are developed using the Eclipse Modelling Framework. The practical use of Stormgen is illustrated with a case study which considers the modelling of a Topology for the Word Count application. © 2014, IEEE.","Apache Storm; Code generation; Domain-specific modelling; Eclipse Modelling Framework; Languages","Automatic programming; Batch data processing; Codes (symbols); Computational linguistics; Computer programming languages; Data handling; Java programming language; Modeling languages; Network components; Problem oriented languages; Program compilers; Query languages; Syntactics; Topology; Code Generation; Distributed data processing; Domain specific languages; Domain-specific modelling; Eclipse modelling frameworks; General-purpose programming language; Level of abstraction; Real-time computations; Storms",2-s2.0-84941570893
"Riezler S., Simianer P., Haas C.","Response-based learning for grounded machine translation",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906922708&partnerID=40&md5=4b6d2a2299ec5e56e0e1003393927bde","We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database. Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for responsebased learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Semantics; Signal processing; Speech transmission; 6 points; F1 scores; Learning approach; Machine translations; Reachability problem; Statistical machine translation; Structured learning; Query processing",2-s2.0-84906922708
"Pu X., Wang J., Song Z., Luo P., Wang M.","Efficient incremental update and querying in AWETO RDF storage system",2014,"Data and Knowledge Engineering",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893755702&doi=10.1016%2fj.datak.2013.11.003&partnerID=40&md5=5a987cd21e1fdc018c35a68f7296c483","With the fast growth of the knowledge bases built over the Internet, storing and querying millions or billions of RDF triples in a knowledge base have attracted increasing research interests. Although the latest RDF storage systems achieve good querying performance, few of them pay much attention to the characteristic of dynamic growth of the knowledge base. Since the building of the knowledge base is usually a continuous process, incremental update over the RDF storage system is in great need. In this paper, to consider the efficiency of both querying and incremental update in RDF data, we propose a hAsh-based tWo-tiEr rdf sTOrage system (abbr. to AWETO) with new index architecture and query execution engine. The performance of our system is systematically measured over two large-scale datasets. Compared with the other three state-of-the-art open source RDF storage systems, our system achieves the best incremental update efficiency meanwhile, the query efficiency is competitive. © 2013 Elsevier B.V.","AWETO; Incremental update; Indexing methods; Query; RDF","AWETO; Incremental updates; Indexing methods; Query; RDF; Efficiency; Knowledge based systems; Open systems; Semantic Web",2-s2.0-84893755702
"Pasupat P., Liang P.","Zero-shot entity extraction from web pages",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906921910&partnerID=40&md5=2ca938ed8aa0104e900d68f6ee24f8d2","In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction systems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach defines a log-linear model over latent extraction predicates, which select lists of entities from the web page. The main challenge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Regression analysis; Entity extractions; Information extraction systems; Learning tasks; Loglinear model; Natural language queries; Semi structured data; Websites",2-s2.0-84906921910
"Wolf S.J., Henrich A., Blank D.","Characterization of toponym usages in texts",2014,"Proceedings of the 8th Workshop on Geographic Information Retrieval, GIR 2014",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942422825&doi=10.1145%2f2675354.2675703&partnerID=40&md5=8c09c5eeb73f5fc50ce0bce09d164292","Toponyms in texts and search queries are often used figuratively and do not directly refer to the locations they reference in their literal sense. Different usage kinds and stylistic devices characterize toponym usages in texts. It is thus crucial for a Geographic Information Retrieval (GIR) system to precisely distinguish these different toponym usages at indexing and at query time in order to best address a given information need and the geospatial footprint of a document. For that purpose, we analyze which of the classic stylistic devices such as allegories, metaphors, or metonymies are used together with toponyms. We use these categories as a foundation for a systematic approach towards the characterization of toponym usages in texts which we believe is necessary to further boost retrieval effectiveness of future GIR systems. A prototype implements this characterization exemplary for texts written in German. We evaluate the effectiveness of our approach against a reference corpus to show the general feasibility. Our approach provides a basis for a wide range of more sophisticated applications such as for example text genre detection. Copyright 2014 ACM.","Computational linguistics; Geographical information retrieval; Geospatial grounding; Linguistic devices","Characterization; Computational linguistics; Linguistics; Search engines; Geo-spatial; Geographic information retrievals (GIR); Geographical information retrievals; Linguistic devices; Retrieval effectiveness; Search queries; Stylistic devices; Text genre; Information retrieval",2-s2.0-84942422825
"Chen D.","Learning shuffle ideals under restricted distributions",2014,"Advances in Neural Information Processing Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937905732&partnerID=40&md5=f814fcc7f2e61f8f20425636b4f4a088","The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set U is the collection of all strings containing some string u ∈ U as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learn-ability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.",,"Heuristic algorithms; Information science; Learning problem; Markovian distribution; Product distributions; Statistical query models; Learning algorithms",2-s2.0-84937905732
"Xu F., Uszkoreit H., Li H., Adolphs P., Cheng X.","Domain-Adaptive Relation Extraction for the Semantic Web",2014,"Cognitive Technologies",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907317911&doi=10.1007%2f978-3-319-06755-1_22&partnerID=40&md5=af474b63f8b02a2ffe93be26dd476db9","In the THESEUS Alexandria use case, information extraction (IE) has been intensively applied to extract facts automatically from unstructured documents, such as Wikipedia and online news, in order to construct ontology-based knowledge databases for advanced information access. In addition, IE is also utilized for analyzing natural language queries for the Alexandria question answering system. The DARE system, a minimally supervised machine learning system for relation extraction, developed at the DFKI LT-Lab, has been adapted and extended to the IE tasks for Alexandria. DARE is domain-adaptive and has been used to learn relation extraction rules automatically for the Alexandria-relevant relations and events. Furthermore, DARE is also applied to the Alexandria opinion mining task for detecting opinion sources, targets and polarities in online news. The DARE system and its learned rules have been integrated into the Alexandria IE pipeline. © Springer International Publishing Switzerland 2014.",,"Information retrieval; Natural language processing systems; Advanced informations; Knowledge database; Natural language queries; Ontology-based; Question answering systems; Relation extraction; Supervised machine learning; Unstructured documents; Data mining",2-s2.0-84907317911
"Unger C., Freitas A., Cimiano P.","An introduction to question answering over linked data",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921736482&partnerID=40&md5=c8d155bb422c69f860ca0d47b0e7a655","While the amount of knowledge available as linked data grows, so does the need for providing end users with access to this knowledge. Especially question answering systems are receiving much interest, as they provide intuitive access to data via natural language and shield end users from technical aspects related to data modelling, vocabularies and query languages. This tutorial gives an introduction to the rapidly developing field of question answering over linked data. It gives an overview of the main challenges involved in the interpretation of a user’s information need expressed in natural language with respect to the data that is queried. The paper summarizes the main existing approaches and systems including available tools and resources, benchmarks and evaluation campaigns. Finally, it lists the open topics that will keep question answering over linked data an exciting area of research in the years to come. © Springer International Publishing Switzerland 2014.",,"Artificial intelligence; Computational linguistics; Data handling; Modeling languages; Natural language processing systems; Query languages; Big data; End users; Linked datum; Natural languages; Question Answering; Question answering systems; Technical aspects; Search engines",2-s2.0-84921736482
"Nio L., Sakti S., Neubig G., Toda T., Nakamura S.","Utilizing human-to-human conversation examples for a multi domain chat-oriented dialog system",2014,"IEICE Transactions on Information and Systems",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901809785&doi=10.1587%2ftransinf.E97.D.1497&partnerID=40&md5=8c9d2b1f9a0d1d16fa17d4cc15076744","This paper describes the design and evaluation of a method for developing a chat-oriented dialog system by utilizing real human-to-human conversation examples from movie scripts and Twitter conversations. The aim of the proposed method is to build a conversational agent that can interact with users in as natural a fashion as possible, while reducing the time requirement for database design and collection. A number of the challenging design issues we faced are described, including (1) constructing an appropriate dialog corpora from raw movie scripts and Twitter data, and (2) developing an multi domain chat-oriented dialog management system which can retrieve a proper system response based on the current user query. To build a dialog corpus, we propose a unit of conversation called a tri-turn (a trigram conversation turn), as well as extraction and semantic similarity analysis techniques to help ensure that the content extracted from raw movie/drama script files forms appropriate dialog-pair (query-response) examples. The constructed dialog corpora are then utilized in a data-driven dialog management system. Here, various approaches are investigated including example-based (EBDM) and response generation using phrase-based statistical machine translation (SMT). In particular, we use two EBDM: syntactic-semantic similarity retrieval and TF-IDF based cosine similarity retrieval. Experiments are conducted to compare and contrast EBDM and SMT approaches in building a chat-oriented dialog system, and we investigate a combined method that addresses the advantages and disadvantages of both approaches. System performance was evaluated based on objective metrics (semantic similarity and cosine similarity) and human subjective evaluation from a small user study. Experimental results show that the proposed filtering approach effectively improve the performance. Furthermore, the results also show that by combing both EBDM and SMT approaches, we could overcome the shortcomings of each. Copyright © 2014 The Institute of Electronics, Information and Communication Engineers.","Cosine similarity; Dialog corpora; Example-based dialog modeling; Machine translation; Response generation; Semantic similarity","Design; Information management; Query processing; Social networking (online); Speech recognition; Speech transmission; Cosine similarity; Dialog corpora; Dialog modeling; Machine translations; Response generation; Semantic similarity; Search engines",2-s2.0-84901809785
"Anand A., Rahli V.","A generic approach to proofs about substitution",2014,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907069772&doi=10.1145%2f2631172.2631177&partnerID=40&md5=07d934593842dbf31352f9f33b54126d","It is well known that reasoning about substitution is a huge ""distraction"" that inevitably gets in the way of formalizing interesting properties of languages with variable bindings. Most formalizations have their own separate definitions of terms and substitution, and properties about it. However there is a great deal of uniformity in the way substitution works and the reasons why its properties hold. We expose this uniformity by defining terms, substitution and α-equality generically in Coq by parametrizing them over a Context Free Grammar annotated with Variable binding information (CFGV). We also provide proofs of many properties about the above definitions (enough to formalize the PER semantics of Nuprl in Coq). Unlike many other tools which generate a custom definition of substitution for each input, all instantiations of our term model share the same substitution function. The proofs about this function have been accepted by Coq's typechecker once and for all. © 2014 ACM.","alpha equality; context free grammars; D.3.1 [Programming Languages]: Formal De and Theory - substitution; Languages; variable bindings; Verication","Context free grammars; Object oriented programming; Query languages; Semantics; alpha equality; Generic approach; PER semantics; Variable binding; Verication; Context free languages",2-s2.0-84907069772
"Krishnamurthy J., Mitchell T.M.","Joint syntactic and semantic parsing with combinatory categorial grammar",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906925083&partnerID=40&md5=fb774a2f71e9c359246cbb010451130d","We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser's predicate vocabulary. We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology. A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syntactic evaluation on CCGbank demonstrates that the parser's dependency Fscore is within 2.5% of state-of-the-art. © 2014 Association for Computational Linguistics.",,"Computational grammars; Knowledge based systems; Semantics; Combinatory categorial grammar; F-score; Knowledge base; Logical forms; Semantic evaluations; Semantic parsing; Semantic representation; Syntactics",2-s2.0-84906925083
"Glass J., Yates A.","Empirically-motivated generalizations of CCG semantic parsing learning algorithms",2014,"14th Conference of the European Chapter of the Association for Computational Linguistics 2014, EACL 2014",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905687100&partnerID=40&md5=0a38b7178a7adeb4fbbbff3d21ba8a9d","Learning algorithms for semantic parsing have improved drastically over the past decade, as steady improvements on benchmark datasets have shown. In this paper we investigate whether they can generalize to a novel biomedical dataset that differs in important respects from the traditional geography and air travel benchmark datasets. Empirical results for two state-of-the-Art PCCG semantic parsers indicates that learning algorithms are sensitive to the kinds of semantic and syntactic constructions used in a domain. In response, we develop a novel learning algorithm that can produce an effective semantic parser for geography, as well as a much better semantic parser for the biomedical dataset. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Semantics; Syntactics; Air travels; Benchmark datasets; Semantic parsing; Learning algorithms",2-s2.0-84905687100
"Fragkoulis M., Spinellis D., Louridas P., Bilas A.","Relational access to unix kernel data structures",2014,"Proceedings of the 9th European Conference on Computer Systems, EuroSys 2014",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900406010&doi=10.1145%2f2592798.2592802&partnerID=40&md5=8a766df72de9b1072c44aeb0cbd68b86","State of the art kernel diagnostic tools like DTrace and Systemtap provide a procedural interface for expressing analysis tasks. We argue that a relational interface to kernel data structures can offer complementary benefits for kernel diagnostics. This work contributes a method and an implementation for mapping a kernel's data structures to a relational interface. The Pico COllections Query Library (PiCO QL) Linux kernel module uses a domain specific language to define a relational representation of accessible Linux kernel data structures, a parser to analyze the definitions, and a compiler to implement an SQL interface to the data structures. It then evaluates queries written in SQL against the kernel's data structures. PiCO QL queries are interactive and type safe. Unlike SystemTap and DTrace, PiCO QL is less intrusive because it does not require kernel instrumentation; instead it hooks to existing kernel data structures through the module's source code. PiCO QL imposes no overhead when idle and needs only access to the kernel data structures that contain relevant information for answering the input queries. We demonstrate PiCO QL's usefulness by presenting Linux kernel queries that provide meaningful custom views of system resources and pinpoint issues, such as security vulnerabilities and performance problems. Copyright © 2007 by the Association for Computing Machinery, Inc.","Diagnostics; Kernel; SQL; Unix","Computer programming languages; Network security; Plasma diagnostics; UNIX; Domain specific languages; Kernel; Kernel instrumentation; Performance problems; Relational representations; Security vulnerabilities; SQL; State of the art; Data structures",2-s2.0-84900406010
"Kawamura T., Matsuzaki K.","Dividing huge XML trees using the m-bridge technique over one-to-one corresponding binary trees",2014,"IPSJ Online Transactions",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958181558&doi=10.2197%2fipsjtrans.7.111&partnerID=40&md5=40849340c3658558765519db489991db","Tree data such as XML trees have recently been getting larger and larger. Parallel and distributed processing is a promising way of dealing with big data, but we need to divide the data in the first step. Since computation over trees often requires relationships between parents and children and/or among siblings, we should pay attention to such relationships. There is a technique called the ""m-bridge"" for dividing trees. We can easily compute m-bridges for trees of any shape. However, division with the m-bridge technique is sometimes unsatisfactory for shallow XML trees. We propose a method of tree division for XML trees in this study, in which we apply the m-bridge technique to a one-to-one corresponding binary tree. We implement the tree division algorithm using the Simple API for XML (SAX) Parser. An important feature of our algorithm is that we transform and divide XML trees in the order that the SAX parser reads the trees. We carried out experiments and discuss the properties of the tree division algorithm we propose. In addition, we discuss how we can use the divided trees with query examples. © 2014 Information Processing Society of Japan.","Binary-tree representation; Data division; Distributed computing; SAX; XML","Big data; Bins; Distributed computer systems; Trees (mathematics); XML; Data division; Division algorithms; Important features; Parallel and distributed processing; Tree data; Tree representation; XML trees; Binary trees",2-s2.0-84958181558
"McFate C., Forbus K.D., Hinrichs T.R.","Using narrative function to extract qualitative information from natural language texts",2014,"Proceedings of the National Conference on Artificial Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908154747&partnerID=40&md5=1f2ceea2f04dae7d39332a476dd4d336","The naturalness of qualitative reasoning suggests that qualitative representations might be an important component of the semantics of natural language. Prior work showed that frame-based representations of qualitative process theory constructs could indeed be extracted from natural language texts. That technique relied on the parser recognizing specific syntactic constructions, which had limited coverage. This paper describes a new approach, using narrative function to represent the higher-order relationships between the constituents of a sentence and between sentences in a discourse. We outline how narrative function combined with query-driven abduction enables the same kinds of information to be extracted from natural language texts. Moreover, we also show how the same technique can be used to extract type-level qualitative representations from text, and used to improve performance in playing a strategy game. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,"Natural language text; Qualitative information",2-s2.0-84908154747
"Griffon N., Schuers M., Soualmia L.F., Grosjean J., Kerdelhué G., Kergourlay I., Dahamna B., Darmoni S.J.","A search engine to access PubMed monolingual subsets: Proof of concept and evaluation in French",2014,"Journal of Medical Internet Research",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918534808&doi=10.2196%2fjmir.3836&partnerID=40&md5=6873c3ade9effce3d16c0d907fe51b75","Background: PubMed contains numerous articles in languages other than English. However, existing solutions to access these articles in the language in which they were written remain unconvincing.Objective: The aim of this study was to propose a practical search engine, called Multilingual PubMed, which will permit access to a PubMed subset in 1 language and to evaluate the precision and coverage for the French version (Multilingual PubMed-French).Methods: To create this tool, translations of MeSH were enriched (eg, adding synonyms and translations in French) and integrated into a terminology portal. PubMed subsets in several European languages were also added to our database using a dedicated parser. The response time for the generic semantic search engine was evaluated for simple queries. BabelMeSH, Multilingual PubMed-French, and 3 different PubMed strategies were compared by searching for literature in French. Precision and coverage were measured for 20 randomly selected queries. The results were evaluated as relevant to title and abstract, the evaluator being blind to search strategy.Results: More than 650,000 PubMed citations in French were integrated into the Multilingual PubMed-French information system. The response times were all below the threshold defined for usability (2 seconds). Two search strategies (Multilingual PubMed-French and 1 PubMed strategy) showed high precision (0.93 and 0.97, respectively), but coverage was 4 times higher for Multilingual PubMed-French.Conclusions: It is now possible to freely access biomedical literature using a practical search tool in French. This tool will be of particular interest for health professionals and other end users who do not read or query sufficiently in English. The information system is theoretically well suited to expand the approach to other European languages, such as German, Spanish, Norwegian, and Portuguese.","Databases, bibliographic; French language; Information storage and retrieval; PubMed; Search engine; User-computer interface","France; human; information retrieval; language; Medical Subject Headings; Medline; procedures; search engine; utilization; France; Humans; Information Storage and Retrieval; Language; Medical Subject Headings; PubMed; Search Engine",2-s2.0-84918534808
"Bao J., Duan N., Zhou M., Zhao T.","Knowledge-based question answering as machine translation",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906923905&partnerID=40&md5=db475a251baa3f6f92e3abbdb3e3baa8","A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Semantics; Knowledge basis (KBs); Machine translations; Minimum error rate trainings; Natural language questions; Question Answering; Question-answer pairs; Relation expression; Translation method; Natural language processing systems",2-s2.0-84906923905
"Saleh I., Joty S., Màrquez L., Moschitti A., Nakov P., Cyphers S., Glass J.","Study of using syntactic and semantic structures for concept segmentation and labeling",2014,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926040770&partnerID=40&md5=2dda9670bb53b030367d53f714758cba","This paper presents an empirical study on using syntactic and semantic information for Concept Segmentation and Labeling (CSL), a well-known component in spoken language understanding. Our approach is based on reranking N-best outputs from a state-of-the-art CSL parser. We perform extensive experimentation by comparing different tree-based kernels with a variety of representations of the available linguistic information, including semantic concepts, words, POS tags, shallow and full syntax, and discourse trees. The results show that the structured representation with the semantic concepts yields significant improvement over the base CSL parser, much larger compared to learning with an explicit feature vector representation. We also show that shallow syntax helps improve the results and that discourse relations can be partially beneficial.",,"Linguistics; Semantics; Speech recognition; Syntactics; Empirical studies; Feature vectors; Linguistic information; Semantic concept; Semantic information; Semantic structures; Spoken language understanding; State of the art; Computational linguistics",2-s2.0-84926040770
"Lee K., Artzi Y., Dodge J., Zettlemoyer L.","Context-dependent semantic parsing for time expressions",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906925645&partnerID=40&md5=d153329579d7c9ea2557a21385bc1f3e","We present an approach for learning context-dependent semantic parsers to identify and interpret time expressions. We use a Combinatory Categorial Grammar to construct compositional meaning representations, while considering contextual cues, such as the document creation time and the tense of the governing verb, to compute the final time values. Experiments on benchmark datasets show that our approach outperforms previous stateof- the-art systems, with error reductions of 13% to 21% in end-to-end performance. © 2014 Association for Computational Linguistics.",,"Benchmarking; Computational grammars; Benchmark datasets; Combinatory categorial grammar; Context dependent; Contextual cue; End-to-end performance; Error reduction; Semantic parsing; Time values; Semantics",2-s2.0-84906925645
"Cantiello P., Martino B.D.","Software porting support with component-based and language neutral source code analysis",2014,"International Journal of Computational Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899521420&doi=10.1504%2fIJCSE.2014.060678&partnerID=40&md5=c31198f04bbbd20627530c4426f8e230","During their lifetimes, programs are subject to modifications, not only to implement new features or to correct bugs, but also to adapt them to new hardware architectures. Parallel environments as multi/many-cores CPUs or GPUs require source code transformations to extract parallelisable regions of code and to allocate workloads on devices. This process requires not only skilled developers, but mainly a deep comprehension of the semantics of the code. In this work is presented a technique and its prototype implementation for component based and language neutral static program analysis that can assist developers in the analysis, comprehension and transformation of code. Source code is statically analysed and modelled with a language neutral representation conceived to permit interoperability between tool components and plugins. This model is designed as a class hierarchy, and implemented in a library to be used by userdeveloped add-ons. Parsers for two languages, control flow, data flow, data dependence analysis modules have been implemented or integrated. An algorithmic recogniser has also been integrated: basic algorithmic concepts are extracted in order to feed a knowledge base, upon which an external reasoner can do queries to find instances of known algorithms. © 2014 Inderscience Enterprises Ltd.","Program comprehension; Software representation; Source code; Static analysis","Algorithms; Computer programming languages; Cosine transforms; Data flow analysis; Interoperability; Knowledge based systems; Program debugging; Program processors; Semantics; Data dependence analysis; Hardware architecture; Neutral representations; Program comprehension; Prototype implementations; Source code transformation; Source codes; Static program analysis; Static analysis",2-s2.0-84899521420
"Cicek A.E., Qi X., Cakmak A., Johnson S.R., Han X., Alshalwi S., Ozsoyoglu Z.M., Ozsoyoglu G.","An online system for metabolic network analysis",2014,"Database",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925547238&doi=10.1093%2fdatabase%2fbau091&partnerID=40&md5=36639bf5f476bc4f0e99c39927e60adf","Metabolic networks have become one of the centers of attention in life sciences research with the advancements in the metabolomics field. A vast array of studies analyzes metabolites and their interrelations to seek explanations for various biological questions, and numerous genome-scale metabolic networks have been assembled to serve for this purpose. The increasing focus on this topic comes with the need for software systems that store, query, browse, analyze and visualize metabolic networks. PathCase Metabolomics Analysis Workbench (PathCaseMAW) is built, released and runs on a manually created generic mammalian metabolic network. The PathCaseMAW system provides a database-enabled framework and Web-based computational tools for browsing, querying, analyzing and visualizing stored metabolic networks. PathCaseMAW editor, with its user-friendly interface, can be used to create a new metabolic network and/or update an existing metabolic network. The network can also be created from an existing genome-scale reconstructed network using the PathCaseMAW SBML parser. The metabolic network can be accessed through a Web interface or an iPad application. For metabolomics analysis, steady-state metabolic network dynamics analysis (SMDA) algorithm is implemented and integrated with the system. SMDA tool is accessible through both the Web-based interface and the iPad application for metabolomics analysis based on a metabolic profile. PathCaseMAW is a comprehensive system with various data input and data access subsystems. It is easy to work with by design, and is a promising tool for metabolomics research and for educational purposes. © The Author(s) 2014. Published by Oxford University Press.",,"computer interface; computer program; genetic database; Internet; metabolism; metabolomics; procedures; Databases, Genetic; Internet; Metabolic Networks and Pathways; Metabolomics; Software; User-Computer Interface",2-s2.0-84925547238
"Sutoyo R., Quix C., Kastrati F.","FactRunner: A new system for NLP-based information extraction from wikipedia",2014,"Lecture Notes in Business Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927654709&doi=10.1007%2f978-3-662-44300-2_14&partnerID=40&md5=2e196847e604ea910a917f4cdb0687cc","Wikipedia is playing an increasing role as a source of humanreadable knowledge, because it contains an enormous amount of high quality information written by human authors. Finding a relevant piece of information in this huge collection of natural language text is often a time-consuming process, as a keyword-based search interface is the main method for querying. Therefore, an iterative process to explore the document collection to find the information of interest is required. In this paper, we present an approach to extract structured information from unstructured documents to enable structured queries. Information Extraction (IE) systems have been proposed for this tasks, but due to the complexity of natural language, they often produce unsatisfying results. As Wikipedia contains, in addition to the plain natural language text, links between documents and other metadata, we propose an approach which exploits this information to extract more accurate structured information. Our proposed system FactRunner focusses on extracting structured information from sentences containing such links, because the links may indicate more accurate information than other sentences. We evaluated our system with a subset of documents from Wikipedia and compared the results with another existing system. The results show that a natural language parser combined with Wikipedia markup can be exploited for extracting facts in form of triple statements with a high accuracy. © Springer-Verlag Berlin Heidelberg 2014.","Information extraction; Semantic search","Information analysis; Information retrieval; Information systems; Iterative methods; Semantics; Social networking (online); Syntactics; World Wide Web; Document collection; High quality information; Information extraction systems; Keyword-based search; Natural language text; Semantic search; Structured information; Unstructured documents; Computational linguistics",2-s2.0-84927654709
"Artzi Y., Das D., Petrov S.","Learning compact lexicons for CCG semantic parsing",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926058883&partnerID=40&md5=297ae6f918fdd69e4becf9585001fb20","We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser. Existing methods incrementally expand the lexicon by greedily adding entries, considering a single training datapoint at a time. We propose using corpus-level statistics for lexicon learning decisions. We introduce voting to globally consider adding entries to the lexicon, and pruning to remove entries no longer required to explain the training data. Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions, achieving up to 25% error reduction, with lexicons that are up to 70% smaller and are qualitatively less noisy. © 2014 Association for Computational Linguistics.",,"Computational grammars; Computational linguistics; Semantics; Combinatory categorial grammar; Error reduction; Executing sequence; Level statistics; Natural languages; Semantic parsing; State-of-the-art performance; Training data; Natural language processing systems",2-s2.0-84926058883
"Carmel D., Mejer A., Pinter Y., Szpektor I.","Improving term weighting for community question answering search using syntactic analysis",2014,"CIKM 2014 - Proceedings of the 2014 ACM International Conference on Information and Knowledge Management",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937598800&doi=10.1145%2f2661829.2661901&partnerID=40&md5=0654bdb358e2636f1633188256d74def","Query term weighting is a fundamental task in information retrieval and most popular term weighting schemes are primarily based on statistical analysis of term occurrences within the document collection. In this work we study how term weighting may benefit from syntactic analysis of the corpus. Focusing on Community-based Question Answering (CQA) sites, we take into account the syntactic function of the terms within CQA texts as an important factor affecting their relative importance for retrieval. We analyze a large log of web queries that landed on Yahoo Answers site, showing a strong deviation between the tendencies of different document words to appear in a landing (click-through) query given their syntactic function. To this end, we propose a novel term weighting method that makes use of the syntactic information available for each query term occurrence in the document, on top of term occurrence statistics. The relative importance of each feature is learned via a learning to rank algorithm that utilizes a click-through query log. We examine the new weighting scheme using manual evaluation based on editorial data and using automatic evaluation over the query log. Our experimental results show consistent improvement in retrieval when syntactic information is taken into account. Copyright 2014 ACM.","Community question answering; Dependency parsing; Learning to rank; Part-of-speech tagging; Term weighting","Computational linguistics; Function evaluation; Information retrieval; Knowledge management; Natural language processing systems; Community question answering; Dependency parsing; Learning to rank; Part of speech tagging; Term weighting; Syntactics",2-s2.0-84937598800
"Roorda D., Kalkman G., Naaijer M., Cranenburgh A.V.","LAF-Fabric: A data analysis tool for linguistic annotation framework with an application to the Hebrew Bible",2014,"Computational Linguistics in the Netherlands Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921916822&partnerID=40&md5=ee05fa3828cb210e6cdcd0ae224fb6f6","The Linguistic Annotation Framework (LAF) provides a general, extensible stand-off markup system for corpora. This paper discusses LAF-Fabric, a new tool to analyse LAF resources in general with an extension to process the Hebrew Bible in particular. We first walk through the history of the Hebrew Bible as text database in decennium-wide steps. Then we describe how LAF-Fabric may serve as an analysis tool for this corpus. Finally, we describe three analytic projects/workows that benefit from the new LAF representation: 1) the study of linguistic variation: extract cooccurrence data of common nouns between the books of the Bible (Martijn Naaijer); 2) the study of the grammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman); 3) construction of a parser of classical Hebrew by Data Oriented Parsing: generate tree structures from the database (Andreas van Cranenburgh). © 2014 Kalkman, Naaijer and van Cranenburgh.",,"Data mining; Data mining; Data mining; Formal languages; Formal languages; Formal languages; Linguistics; Linguistics; Linguistics; Trees (mathematics); Trees (mathematics); Trees (mathematics); Analysis tools; Analysis tools; Analysis tools; Co-occurrence; Co-occurrence; Co-occurrence; Data analysis tool; Data analysis tool; Data analysis tool; Data-oriented parsing; Data-oriented parsing; Data-oriented parsing; Linguistic annotations; Linguistic annotations; Linguistic annotations; Stand-off; Stand-off; Stand-off; Text database; Text database; Text database; Tree structures; Tree structures; Tree structures; Computational linguistics; Computational linguistics; Computational linguistics",2-s2.0-84921916822
"Neumann R.S., Kumar S., Shalchian-Tabrizi K.","BLAST output visualization in the new sequencing era",2014,"Briefings in Bioinformatics",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904730058&doi=10.1093%2fbib%2fbbt009&partnerID=40&md5=cc7c19ce73b15b635a66a96f6da2538d","The Basic Local Alignment Search Tool (BLAST) algorithm remains one of the most widely used bioinformatic programs. For many projects, new sequencing technologies and increased database sizes will increase the BLAST output significantly. Frequently, this output is so large that it is no longer able to be processed manually. As BLAST users are increasingly recruited from mainstream biology without any bioinformatic background, user-friendly programs capable of BLASToutput visualization, analysis and post-processing are in demand. In this review, freely available BLAST output processing programs are categorized as BLAST output interpreters, BLAST environments, BLASToutput parsers or specialized tools. They are evaluated according to their user-friendliness, analysis features and high-throughput data processing capabilities. © The Author 2013. Published by Oxford University Press.","BLAST; High-throughput; Post-processing; Sequencing; User-friendliness; Visualization","algorithm; biology; procedures; sequence alignment; sequence analysis; Algorithms; Computational Biology; Sequence Alignment; Sequence Analysis",2-s2.0-84904730058
"Berant J., Liang P.","Semantic parsing via paraphrasing",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",75,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906924851&partnerID=40&md5=b7f57349fb0921031bb79a4214d690c2","A central challenge in semantic parsing is handling the myriad ways in which knowledge base predicates can be expressed. Traditionally, semantic parsers are trained primarily from text paired with knowledge base information. Our goal is to exploit the much larger amounts of raw text not tied to any knowledge base. In this paper, we turn semantic parsing on its head. Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each. Then, we use a paraphrase model to choose the realization that best paraphrases the input, and output the corresponding logical form. We present two simple paraphrase models, an association model and a vector space model, and train them jointly from question-answer pairs. Our system PARASEMPRE improves stateof- the-art accuracies on two recently released question-answering datasets. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Vector spaces; Association models; Canonical realization; Knowledge base; Natural languages; Question Answering; Question-answer pairs; Semantic parsing; Vector space models; Semantics",2-s2.0-84906924851
"Zhang Q., Chen H., Huang X.","Chinese-English mixed text normalization",2014,"WSDM 2014 - Proceedings of the 7th ACM International Conference on Web Search and Data Mining",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906858902&doi=10.1145%2f2556195.2556228&partnerID=40&md5=f23723f9427b6ee010153dd7936b0420","Along with the expansion of globalization, multilingualism has become a popular social phenomenon. More than one language may occur in the context of a single conversation. This phenomenon is also prevalent in China. A huge variety of informal Chinese texts contain English words, especially in emails, social media, and other user generated informal contents. Since most of the existing natural language processing algorithms were designed for processing monolingual information, mixed multilingual texts cannot be well analyzed by them. Hence, it is of critical importance to preprocess the mixed texts before applying other tasks. In this paper, we firstly analyze the phenomena of mixed usage of Chinese and English in Chinese microblogs. Then, we detail the proposed two-stage method for normalizing mixed texts. We propose to use a noisy channel approach to translate in-vocabulary words into Chinese. For better incorporating the historical information of users, we introduce a novel user aware neural network language model. For the out-of-vocabulary words (such as pronunciations, informal expressions and et al.), we propose to use a graph-based unsupervised method to categorize them. Experimental results on a manually annotated microblog dataset demonstrate the effectiveness of the proposed method. We also evaluate three natural language parsers with and without using the proposed method as the preprocessing step. From the results, we can see that the proposed method can significantly benefit other NLP tasks in processing mixed text. © 2014 ACM.","chinese-english mixed text; user aware neural network language model; words normalization","Computational linguistics; Data mining; Information retrieval; Learning algorithms; Websites; chinese-english mixed text; Historical information; Informal expressions; Monolingual information; NAtural language processing; Network language; Out-of-vocabulary words; words normalization; Natural language processing systems",2-s2.0-84906858902
"Prud'hommeaux E., Gayo J.E.L., Solbrig H.","Shape expressions: An RDF validation and transformation language",2014,"ACM International Conference Proceeding Series",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937698926&doi=10.1145%2f2660517.2660523&partnerID=40&md5=d35e966af594b5073bbc52acd34960e4","RDF is a graph based data model which is widely used for semantic web and linked data applications. In this paper we describe a Shape Expression definition language which enables RDF validation through the declaration of constraints on the RDF model. Shape Expressions can be used to validate RDF data, communicate expected graph patterns for interfaces and generate user interface forms. In this paper we describe the syntax and the formal semantics of Shape Expressions using inference rules. Shape Expressions can be seen as domain specific language to define Shapes of RDF graphs based on regular expressions. Attached to Shape Expressions are semantic actions which provide an extension point for validation or for arbitrary code execution such as those in parser generators. Using semantic actions, it is possible to augment the validation expressiveness of Shape Expressions and to transform RDF graphs in a easy way. We have implemented several validation tools that check if an RDF graph matches against a Shape Expressions schema and infer the corresponding Shapes. We have also implemented two extensions, called GenX and GenJ that leverage the predictability of the graph traversal and create ordered, closed content, XML/Json documents, providing a simple, declarative mapping from RDF data to XML and Json documents. Copyright © 2014 by the Association for Computing Machinery, Inc.","Graphs; RDF; Transformation; Validation","Computational linguistics; Computer programming languages; Formal methods; Graphic methods; Problem oriented languages; Semantics; Syntactics; User interfaces; XML; Domain specific languages; Graphs; Linked data applications; RDF; Regular expressions; Transformation; Transformation languages; Validation; Semantic Web",2-s2.0-84937698926
"Zha Z.-J., Yu J., Tang J., Wang M., Chua T.-S.","Product aspect ranking and its applications",2014,"IEEE Transactions on Knowledge and Data Engineering",35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901027043&doi=10.1109%2fTKDE.2013.136&partnerID=40&md5=60d0c8c4151940e4df0ebbe36bed37b7","Numerous consumer reviews of products are now available on the Internet. Consumer reviews contain rich and valuable knowledge for both firms and users. However, the reviews are often disorganized, leading to difficulties in information navigation and knowledge acquisition. This article proposes a product aspect ranking framework, which automatically identifies the important aspects of products from online consumer reviews, aiming at improving the usability of the numerous reviews. The important product aspects are identified based on two observations: 1) the important aspects are usually commented on by a large number of consumers and 2) consumer opinions on the important aspects greatly influence their overall opinions on the product. In particular, given the consumer reviews of a product, we first identify product aspects by a shallow dependency parser and determine consumer opinions on these aspects via a sentiment classifier. We then develop a probabilistic aspect ranking algorithm to infer the importance of aspects by simultaneously considering aspect frequency and the influence of consumer opinions given to each aspect over their overall opinions. The experimental results on a review corpus of 21 popular products in eight domains demonstrate the effectiveness of the proposed approach. Moreover, we apply product aspect ranking to two real-world applications, i.e., document-level sentiment classification and extractive review summarization, and achieve significant performance improvements, which demonstrate the capacity of product aspect ranking in facilitating real-world applications. © 1989-2012 IEEE.","aspect identification; aspect ranking; consumer review; extractive review summarization; Product aspects; sentiment classification","Information systems; Aspect identifications; aspect ranking; Consumer reviews; extractive review summarization; Product aspects; Sentiment classification; Computational methods",2-s2.0-84901027043
"Thesprasith O., Jaruskulchai C.","CSKU GPRF-QE for medical topic web retrieval",2014,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980723983&partnerID=40&md5=cf130a7242b41dcd9c0558614b0ad8e8","Patients and their relatives have more chances to access their healthinformation in a form of discharge summary. Most of them do not totally understand contents in the discharge summary. The ShARe/CLEF eHealth Evaluation Lab organized a shared task for improving retrieval medical information from the web. Queries of this task are formulated based on information in discharge summaries. This paper investigates efficiency of query expansion using external collection. Co-occur terms in pseudo-relevance feedback of Genomics collection are selected and re-weighted based on Rocchio's formula with dynamic tunable parameters of pseudo-relevance part. LUCENE, vector space model, is baseline retrieval tool. The proposed expansion method improves from baseline in all level cut of nDCG and best perform in P@10 of 3 topics. Using biomedical related collection such as Genomics is useful for medical topics retrieval.","Genomics track 2004; Medical terminology retrieval; Pseudo-relevance feedback; Re-weighting scheme","Feedback; Social networking (online); Terminology; Vector spaces; Expansion methods; Genomics; Medical information; Medical terminologies; Pseudo relevance feedback; Re-weighting; Tunable parameter; Vector space models; Information retrieval",2-s2.0-84980723983
"Mantadelis T., Rocha R.","A portable prolog predicate for printing rational terms",2014,"Proceedings of the International Joint Workshop on Implementation of Constraint and Logic Programming Systems and Logic-Based Methods in Programming Environments 2014, CICLOPS-WLPE 2014",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941349297&partnerID=40&md5=6c86df5424ad642b2feeb388fa91302a","Rational terms or rational trees are terms with one or more infinite sub-terms but with a finite representation. Rational terms appeared as a side effect of omitting the occurs check in the unification of terms, but their support across Prolog systems varies and often fails to provide the expected functionality. A common problem is the lack of support for printing query bindings with rational terms. In this paper, we present a survey discussing the support of rational terms among different Prolog systems and we propose the integration of a Prolog predicate, that works in several existing Prolog systems, in order to overcome the technical problem of printing rational terms. Our rational term printing predicate could be easily adapted to work for the top query printouts, for user printing and for debugging purposes.","Implementation; Portability; Rational terms","Computer programming; Computer programming languages; Computer software portability; Logic programming; A-prolog; Check-in; Implementation; Prolog systems; Rational terms; Rational trees; Side effect; Printing; Printing; Problem Solving",2-s2.0-84941349297
"Waltinger U., Tecuci D., Olteanu M., Mocanu V., Sullivan S.","Natural language access to enterprise data",2014,"AI Magazine",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898655974&partnerID=40&md5=604833eddd22dfa7d173e64e08dbe2af","This article describes USI Answers - a natural language question-answering system for enterprise data. We report on the progress toward the goal of offering easy access to enterprise data to a large number of business users, most of whom are not familiar with the specific syntax or semantics of the underlying data sources. Additional complications come from the nature of the data, which comes both as structured and unstructured. The proposed solution allows users to express questions in natural language, makes apparent the system's interpretation of the query, and allows easy query adjustment and reformulation. The application is in use by more than 1500 users from Siemens Energy. We evaluate our approach on a data set consisting of fleet data. Copyright © 2014, Association for the Advancement of Artificial Intelligence.",,"Artificial intelligence; Business-users; Data set; Data-sources; Enterprise data; Fleet data; Natural languages; Question answering systems; Siemens; Semantics",2-s2.0-84898655974
"Panem S., Gupta M., Varma V.","Structured information extraction from natural disaster events on twitter",2014,"International Conference on Information and Knowledge Management, Proceedings",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937781602&doi=10.1145%2f2663792.2663794&partnerID=40&md5=758c494868c345a1e1dd03adf371b0df","As soon as natural disaster events happen, users are eager to know more about them. However, search engines currently provide a ten blue links interface for queries related to such events. Relevance of results for such queries can be significantly improved if users are shown a structured summary of the fresh events related to such queries. This would not just reduce the number of user clicks to get the relevant information but would also help users get updated with more fine grained attribute-level information. Twitter is a great source that can be exploited for obtaining such fine-grained structured information for fresh natural disaster events. Such events are often reported on Twitter much earlier than on other news media. However, extracting such structured information from tweets is challenging because: 1. tweets are noisy and ambiguous; 2. there is no well defined schema for various types of natural disaster events; 3. it is not trivial to extract attribute-value pairs and facts from unstructured text; and 4. it is difficult to find good mappings between extracted attributes and attributes in the event schema. We propose algorithms to extract attribute-value pairs, and also devise novel mechanisms to map such pairs to manually generated schemas for natural disaster events. Besides the tweet text, we also leverage text from URL links in the tweets to fill such schemas. Our schemas are temporal in nature and the values are updated whenever fresh information flows in from human sensors on Twitter. Evaluation on 58000 tweets for 20 events shows that our system can fill such event schemas with an F1 of 0.6. Copyright © 2014 ACM.","Attribute-Value Extraction; Event Infoboxes; Fact Triplet Extraction; Natural Calamities; Natural Disaster Events; Structured Event Mining; Twitter","Information retrieval; Knowledge management; Knowledge representation; Search engines; Social networking (online); Attribute values; Event Infoboxes; Event mining; Natural Calamities; Natural disasters; Twitter; Disasters",2-s2.0-84937781602
"Kaliszyk C., Urban J.","HOL(y)Hammer: Online ATP Service for HOL Light",2014,"Mathematics in Computer Science",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904196697&doi=10.1007%2fs11786-014-0182-0&partnerID=40&md5=40baf179f54618ff202eb54684ee8a96","HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable) mathematics encoded in the HOL Light system. The service allows its users to upload and automatically process an arbitrary formal development (project) based on HOL Light, and to attack arbitrary conjectures that use the concepts defined in some of the uploaded projects. For that, the service uses several automated reasoning systems combined with several premise selection methods trained on all the project proofs. The projects that are readily available on the server for such query answering include the recent versions of the Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP combinations and 4 decision procedures that contribute to its overall performance. The system is also available for local installation by interested users, who can customize it for their own proof development. An Emacs interface allowing parallel asynchronous queries to the service is also provided. The overall structure of the service is outlined, problems that arise and their solutions are discussed, and an initial account of using the system is given. © 2014, The Author(s).","68T05; 68T15; 68T20; 68T35",,2-s2.0-84904196697
"Bordes A., Weston J., Usunier N.","Open question answering with weakly supervised embedding models",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907010943&doi=10.1007%2f978-3-662-44848-9_11&partnerID=40&md5=149d2ce89b73c3a93618d70e8b37decb","Building computers able to answer questions on any subject is a long standing goal of artificial intelligence. Promising progress has recently been achieved by methods that learn to map questions to logical forms or database queries. Such approaches can be effective but at the cost of either large amounts of human-labeled data or by defining lexicons and grammars tailored by practitioners. In this paper, we instead take the radical approach of learning to map questions to vectorial feature representations. By mapping answers into the same space one can query any knowledge base independent of its schema, without requiring any grammar or lexicon. Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine-tuning step using the weak supervision provided by blending automatically and collaboratively generated resources. We empirically demonstrate that our model can capture meaningful signals from its noisy supervision leading to major improvements over paralex, the only existing method able to be trained on similar weakly labeled data. © 2014 Springer-Verlag.","embedding models; natural language processing; question answering; weak supervision",,2-s2.0-84907010943
"González J.J.B., Florencia-Juárez R., Pazos Rangel R.A., Martínez J.A.F., Morales-Rodríguez M.L.","Using semantic representations to facilitate the domain-knowledge portability of a natural language interface to databases",2014,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927138055&doi=10.1007%2f978-3-319-05170-3_48&partnerID=40&md5=0df1bf49ba3f35389b8c6b0cfd84ace3","Our research is focused on the implementation of a Natural Language Interface to Database. We propose the use of ontologies to model the knowledge required by the interface with the aim of correctly answering natural language queries and facilitate its configuration on other databases. The knowledge of our interface is composed by modeling information about the database schema, its relationship to natural language and some linguistic functions. The design of this modeling allows users to configure the interface without performing complex and tedious tasks, facilitating its portability to other databases. To evaluate the knowledge-domain portability, we configured our interface and the commercial interface ELF in the Northwind database. The results obtained of the experimentation show that the knowledge modeled in our interface allowed it to achieve a good performance. © Springer International Publishing Switzerland 2014.",,,2-s2.0-84927138055
"Antonova A., Misyurev A.","Automatic creation of human-oriented translation dictionaries",2014,"Komp'juternaja Lingvistika i Intellektual'nye Tehnologii",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904798655&partnerID=40&md5=24f548bb5278e01e6192af8098a308bd","This paper addresses the issue of automatic acquisition of a human-oriented translation dictionary from a large parallel corpus. Automatically generated dictionary entries can enrich the output of a statistical machine translation system. We describe an automatic approach to the extraction of translation equivalents, and dictionary entry construction: grouping of synonymic translations, selection of illustrative context examples. The extraction of possible translations is based on statistical machine translation methods. The selection of lemmatized and linguistically motivated phrases is done with the help of morpho-syntactic analysis. In contrast to humanbuilt dictionaries, an automatic dictionary usually contains a certain amount of noisy translations, as a consequence of systematic alignment mistakes and corpus imperfections. A noise reduction approach is proposed. We also provide the result of an evaluation experiment and the comparison of frequency distribution of words in the queries to the dictionary and the frequency distribution of words in plain text.","Bilingual dictionary extraction; Parallel texts",,2-s2.0-84904798655
"Fader A., Zettlemoyer L., Etzioni O.","Open question answering over curated and extracted knowledge bases",2014,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",60,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907031424&doi=10.1145%2f2623330.2623677&partnerID=40&md5=abff169ed96f12e6f0056d5fb7d33cf7","We consider the problem of open-domain question answering (Open QA) over massive knowledge bases (KBs). Existing approaches use either manually curated KBs like Freebase or KBs automatically extracted from unstructured text. In this paper, we present OQA, the first approach to leverage both curated and extracted KBs. A key technical challenge is designing systems that are robust to the high variability in both natural language questions and massive KBs. OQA achieves robustness by decomposing the full Open QA problem into smaller sub-problems including question paraphrasing and query reformulation. OQA solves these sub-problems by mining millions of rules from an unlabeled question corpus and across multiple KBs. OQA then learns to integrate these rules by performing discriminative training on question-answer pairs using a latent-variable structured perceptron algorithm. We evaluate OQA on three benchmark question sets and demonstrate that it achieves up to twice the precision and recall of a state-of-the-art Open QA system. © 2014 ACM.","algorithms; experimentation",,2-s2.0-84907031424
"Pazos Rangel R.A., Aguirre M.A., González J.J., Carpio J.M.","Features and pitfalls that users should seek in natural language interfaces to databases",2014,"Studies in Computational Intelligence",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927145451&doi=10.1007%2f978-3-319-05170-3_44&partnerID=40&md5=6575b83794056cc56e3fbf8588fd1309","Natural Language Interfaces to Databases (NLIDBs) are tools that can be useful in making decisions, allowing different types of users to get information they need using natural language communication. Despite their important features and that for more than 50 years NLIDBs have been developed, their acceptance by end users is very low due to extremely complex problems inherent to natural language, their customization and internal operation, which has produced poor performance regarding queries correctly translated. This chapter presents a study on the main desirable features that NLIDBs should have as well as their pitfalls, describing some study cases that occur in some interfaces to illustrate the flaws of their approach. © Springer International Publishing Switzerland 2014.",,,2-s2.0-84927145451
"Saha B.","The dyck language edit distance problem in near-linear time",2014,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920000534&doi=10.1109%2fFOCS.2014.71&partnerID=40&md5=84e3eb6ccc481464bc63e2aeda4ba950","Given a string σ over alphabet ς and a grammar G defined over the same alphabet, how many minimum number of repairs (insertions, deletions and substitutions) are required to map ς into a valid member of G? The seminal work of Aho and Peterson in 1972 initiated the study of this language edit distance problem providing a dynamic programming algorithm for context free languages that runs in O(|G|2n3) time, where n is the string length and |G| is the grammar size. While later improvements reduced the running time to O(|G|n3), the cubic time complexity on the input length held a major bottleneck for applying these algorithms to their multitude of applications. In this paper, we study the language edit distance problem for a fundamental context free language, DYCK(s) representing the language of well-balanced parentheses of s different types, that has been pivotal in the development of formal language theory. We provide the very first it near-linear time} algorithm to tightly approximate the DYCK(s) language edit distance problem for any arbitrary s. DYCK(s) language edit distance significantly generalizes the well-studied string edit distance problem, and appears in most applications of language edit distance ranging from data quality in databases, generating automated error-correcting parsers in compiler optimization to structure prediction problems in biological sequences. Its nondeterministic counterpart is known as the hardest context free language. Our main result is an algorithm for edit distance computation to DYCK(s) for any positive integer s that runs in O(n1+ ε polylog(n)) time and achieves an approximation factor of O(1/εβ(n)log|OPT|), for any ε &gt; 0. Here OPT is the optimal edit distance to DYCK(s) and β(n) is the best approximation factor known for the simpler problem of string edit distance running in analogous time. If we allow O(n1+ε+|OPT|2nε) time, then the approximation factor can be reduced to O(1/ε log|OPT|). Since the best known near-linear time algorithm for the string edit distance problem has β(n) = polylog(n), under near-linear time computation model both DYCK(s) language and string edit distance problems have polylog(n) approximation factors. This comes as a surprise since the former is a significant generalization of the latter and their exact computations via dynamic programming show a stark difference in time complexity. Rather less surprisingly, we show that the framework for efficiently approximating edit distance to DYCK(s) can be utilized for many other languages. We illustrate this by considering various memory checking languages (studied extensively under distributed verification) such as stack, queue, PQ and DEQUE which comprise of valid transcripts of stacks, queues, priority queues and double-ended queues respectively. Therefore, any language that can be recognized by these data structures, can also be repaired efficiently by our algorithm. © 2014 IEEE.","approximation algorithms; edit distance; formal language; linear time algorithm design",,2-s2.0-84920000534
"Hsiao W.-F., Chang T.-M., Thomas E.","Extracting bibliographical data for PDF documents with HMM and external resources",2014,"Program",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927523493&doi=10.1108%2fPROG-12-2011-0059&partnerID=40&md5=383b3b768e0a7b7770a0ddf948c73a74","Purpose - The purpose of this paper is to propose an automatic metadata extraction and retrieval system to extract bibliographical information from digital academic documents in portable document formats (PDFs). Design/methodology/approach - The authors use PDFBox to extract text and font size information, a rule-based method to identify titles, and an Hidden Markov Model (HMM) to extract the titles and authors. Finally, the extracted titles and authors (possibly incorrect or incomplete) are sent as query strings to digital libraries (e.g. ACM, IEEE, CiteSeerX, SDOS, and Google Scholar) to retrieve the rest of metadata. Findings - Four experiments are conducted to examine the feasibility of the proposed system. The first experiment compares two different HMM models: multi-state model and one state model (the proposed model). The result shows that one state model can have a comparable performance with multi-state model, but is more suitable to deal with real-world unknown states. The second experiment shows that our proposed model (without the aid of online query) can achieve as good performance as other researcher's model on Cora paper header dataset. In the third experiment the paper examines the performance of our system on a small dataset of 43 real PDF research papers. The result shows that our proposed system (with online query) can perform pretty well on bibliographical data extraction and even outperform the free citation management tool Zotero 3.0. Finally, the paper conducts the fourth experiment with a larger dataset of 103 papers to compare our system with Zotero 4.0. The result shows that our system significantly outperforms Zotero 4.0. The feasibility of the proposed model is thus justified. Research limitations/implications - For academic implication, the system is unique in two folds: first, the system only uses Cora header set for HMM training, without using other tagged datasets or gazetteers resources, which means the system is light and scalable. Second, the system is workable and can be applied to extracting metadata of real-world PDF files. The extracted bibliographical data can then be imported into citation software such as endnote or refworks to increase researchers' productivity. Practical implications - For practical implication, the system can outperform the existing tool, Zotero v4.0. This provides practitioners good chances to develop similar products in real applications; though it might require some knowledge about HMM implementation. Originality/value - The HMM implementation is not novel. What is innovative is that it actually combines two HMM models. The main model is adapted from Freitag and Mccallum (1999) and the authors add word features of the Nymble HMM (Bikel et al, 1997) to it. The system is workable even without manually tagging the datasets before training the model (the authors just use cora dataset to train and test on real-world PDF papers), as this is significantly different from what other works have done so far. The experimental results have shown sufficient evidence about the feasibility of our proposed method in this aspect. © 2014, Emerald Group Publishing Limited.","Bibliographical information; Hidden Markov Model; Information extraction; PDF documents",,2-s2.0-84927523493
"Walter S., Unger C., Cimiano P.","M-ATOLL: A framework for the lexicalization of ontologies in multiple languages",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908680785&doi=10.1007%2f978-3-319-11964-9&partnerID=40&md5=1d17595700b13bf9721407e16d1a9839","Many tasks in which a system needs to mediate between natural language expressions and elements of a vocabulary in an ontology or dataset require knowledge about how the elements of the vocabulary (i.e. classes, properties, and individuals) are expressed in natural language. In a multilingual setting, such knowledge is needed for each of the supported languages. In this paper we present M-ATOLL, a framework for automatically inducing ontology lexica in multiple languages on the basis of a multilingual corpus. The framework exploits a set of language-specific dependency patterns which are formalized as SPARQL queries and run over a parsed corpus. We have instantiated the system for two languages: German and English. We evaluate it in terms of precision, recall and F-measure for English and German by comparing an automatically induced lexicon to manually constructed ontology lexica for DBpedia. In particular, we investigate the contribution of each single dependency pattern and perform an analysis of the impact of different parameters. © Springer International Publishing Switzerland 2014.",,,2-s2.0-84908680785
"Verberne S., D'Hondt E., Van Den Bosch A., Marx M.","Automatic thematic classification of election manifestos",2014,"Information Processing and Management",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898648412&doi=10.1016%2fj.ipm.2014.02.006&partnerID=40&md5=d173d5866104935bfd641a9ed5585dec","We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits. We used these data to train a classifier that can automatically label new, unseen election manifestos with themes. Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis. The data that we created will be disclosed to the public through a search interface. This means that it will be possible to query the data and filter them on themes and parties. We optimized the Lipschits classifier on the task of classifying election manifestos using models trained on earlier years. We built a classifier that is suited for classifying election manifestos from 2002 onwards using the data from the 1980s and 1990s. We evaluated the results by having a domain expert manually assess a sample of the classified data. We found that our automatic classifier obtains the same precision as a human classifier on unseen data. Its recall could be improved by extending the set of themes with newly emerged themes. Thus when using old political texts to classify new texts, work is needed to link and expand the set of themes to newer topics. © 2014 Elsevier Ltd. All rights reserved.","Expert evaluation; Political data; Text classification","Classification (of information); Automatic classifiers; Comparative data; Domain experts; Expert evaluation; Political data; Search interfaces; Text classification; Thematic classification; Text processing",2-s2.0-84898648412
"Thomas S.W., Snodgrass R.T., Zhang R.","Benchmark frameworks and τbench",2014,"Software - Practice and Experience",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905573769&doi=10.1002%2fspe.2189&partnerID=40&md5=7df10744239d32ca0210c674cb989d1b","Software engineering frameworks tame the complexity of large collections of classes by identifying structural invariants, regularizing interfaces, and increasing sharing across the collection. We wish to appropriate these benefits for families of closely related benchmarks, say for evaluating query engine implementation strategies. We introduce the notion of a benchmark framework, an ecosystem of benchmarks that are related in semantically rich ways and enabled by organizing principles. A benchmark framework is realized by iteratively changing one individual benchmark into another, say by modifying the data format, adding schema constraints, or instantiating a different workload. Paramount to our notion of benchmark frameworks are the ease of describing the differences between individual benchmarks and the utility of methods to validate the correctness of each benchmark component by exploiting the overarching ecosystem. As a detailed case study, we introduce τBench, a benchmark framework consisting of ten individual benchmarks, spanning XML, XQuery, XML Schema, and PSM, along with temporal extensions to each. The second case study examines the Mining Unstructured Data benchmark framework, and the third examines the potential benefits of rendering the TPC family as a benchmark framework. Copyright © 2013 John Wiley & Sons, Ltd.","benchmarks; temporal databases; XML","Benchmarking; Ecosystems; Iterative methods; Software engineering; Benchmark components; Engineering frameworks; Implementation strategies; Potential benefits; Schema constraints; Structural invariants; Temporal Database; Temporal extensions; XML",2-s2.0-84905573769
"Pasche E., Gobeill J., Kreim O., Oezdemir-Zaech F., Vachon T., Lovis C., Ruch P.","Development and tuning of an original search engine for patent libraries in medicinal chemistry",2014,"BMC Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925881338&doi=10.1186%2f1471-2105-15-S1-S15&partnerID=40&md5=6b2472e5cda8569fdd13c00c2e889dc9","Background: The large increase in the size of patent collections has led to the need of efficient search strategies. But the development of advanced text-mining applications dedicated to patents of the biomedical field remains rare, in particular to address the needs of the pharmaceutical & biotech industry, which intensively uses patent libraries for competitive intelligence and drug development. Methods: We describe here the development of an advanced retrieval engine to search information in patent collections in the field of medicinal chemistry. We investigate and combine different strategies and evaluate their respective impact on the performance of the search engine applied to various search tasks, which covers the putatively most frequent search behaviours of intellectual property officers in medical chemistry: 1) a prior art search task; 2) a technical survey task; and 3) a variant of the technical survey task, sometimes called known-item search task, where a single patent is targeted. Results: The optimal tuning of our engine resulted in a top-precision of 6.76% for the prior art search task, 23.28% for the technical survey task and 46.02% for the variant of the technical survey task. We observed that co-citation boosting was an appropriate strategy to improve prior art search tasks, while IPC classification of queries was improving retrieval effectiveness for technical survey tasks. Surprisingly, the use of the full body of the patent was always detrimental for search effectiveness. It was also observed that normalizing biomedical entities using curated dictionaries had simply no impact on the search tasks we evaluate. The search engine was finally implemented as a web-application within Novartis Pharma. The application is briefly described in the report. Conclusions: We have presented the development of a search engine dedicated to patent search, based on state of the art methods applied to patent corpora. We have shown that a proper tuning of the system to adapt to the various search tasks clearly increases the effectiveness of the system. We conclude that different search tasks demand different information retrieval engines' settings in order to yield optimal end-user retrieval. © 2014 Pasche et al.",,"Competition; Competitive intelligence; Data mining; Libraries; Patents and inventions; Surveys; World Wide Web; Known item searches; Medical chemistry; Medicinal chemistry; Retrieval effectiveness; Retrieval engines; Search behaviours; Search information; Search strategies; Search engines",2-s2.0-84925881338
"Mazumder S., Patel D., Mehta S.","ActMiner: Discovering location-specific activities from community-authored reviews",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906875882&doi=10.1007%2f978-3-319-10160-6_30&partnerID=40&md5=3a3ba3cc1bf1dd2c4b2140c3d440b220","Location-specific community authored reviews are useful resource for discovering location-specific activities and developing various location-aware activity recommendation applications. Existing works on activity discovery have mostly utilized body-worn sensors, images or human GPS traces and discovered generalized activities that do not convey any location-specific knowledge. Moreover, many of the discovered activities are irrelevant and redundant and hence, significantly affect the performance of a location-aware activity recommender system. In this paper, we propose a three-phase Discover-Filer-Merge solution, namely ActMiner, to infer the location-specific relevant and non-redundant activities from community-authored reviews. The proposed solution uses Dependency-aware, Category-aware and Sense-aware approaches in three sequential phases to accomplish its objective. Experimental results on two real-world data sets show that the accuracy and correctness of ActMiner are better than the existing approaches. © 2014 Springer International Publishing.","Activity Discovery and Recommendation; Review Mining","Virtual reality; Activity discoveries; Body-worn sensors; GPS traces; Location-aware; Non-redundant; Real-world; Data warehouses",2-s2.0-84906875882
"Liu D., Peng Z., Liu B., Chen X., Guo Y.","Technology effect phrase extraction in chinese patent abstracts",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958538287&doi=10.1007%2f978-3-319-11116-2_13&partnerID=40&md5=9451af82035ecc866e666d3c4920d912","Patents are the greatest source of technical information in the world. High-efficient patent mining technologies are of great help to technical innovations and protection of intellectual property right. The extraction of technology effect clauses or phrases is an important research area in patent mining. Due to the specialty and uniqueness of patent data, traditional keyword extraction algorithms cannot properly apply to the extraction of technology effect phrases, leaving it dependent on high-cost manual processing. We propose a semi-automatic method based on partitioning corpus to extract technology effect phrases in Chinese patent abstracts. Experiments show that this method achieves satisfying precision and recall while involving little human labor. © 2014 Springer International Publishing Switzerland.","information retrieval; partitioning corpus; Patent mining; technology effect clause; technology effect phrase","Abstracting; Extraction; Information retrieval; Keyword extraction; partitioning corpus; Patent mining; Precision and recall; Protection of intellectual properties; Semiautomatic methods; Technical information; Technical innovation; Patents and inventions",2-s2.0-84958538287
"Zhou D., He Y.","Semi-supervised learning of statistical models for natural language understanding",2014,"Scientific World Journal",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925884774&doi=10.1155%2f2014%2f121650&partnerID=40&md5=5639dcd33d557e83a6c3ec54b257d7e6","Natural language understanding is to specify a computational model that maps sentences to their semantic mean representation. In this paper, we propose a novel framework to train the statistical models without using expensive fully annotated data. In particular, the input of our framework is a set of sentences labeled with abstract semantic annotations. These annotations encode the underlying embedded semantic structural relations without explicit word/semantic tag alignment. The proposed framework can automatically induce derivation rules that map sentences to their semantic meaning representations. The learning framework is applied on two statistical models, the conditional random fields (CRFs) and the hidden Markov support vector machines (HM-SVMs). Our experimental results on the DARPA communicator data show that both CRFs and HM-SVMs outperform the baseline approach, previously proposed hidden vector state (HVS) model which is also trained on abstract semantic annotations. In addition, the proposed framework shows superior performance than two other baseline approaches, a hybrid framework combining HVS and HM-SVMs and discriminative training of HVS, with a relative error reduction rate of about 25% and 15% being achieved in F -measure. © 2014 Deyu Zhou and Yulan He.",,"accuracy; algorithm; Article; comprehension; conditional random field; discriminant analysis; discrimination learning; expectation maximization algorithm; hidden Markov support vector machine; language; recall; semantics; statistical model; natural language processing; Algorithms; Models, Statistical; Natural Language Processing",2-s2.0-84925884774
"Wu W., Yu P., Hou Y.","New design, new process of harmonic drive with short flexspline and its experiment",2014,"Harbin Gongye Daxue Xuebao/Journal of Harbin Institute of Technology",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894795046&partnerID=40&md5=536e1ea41374efc61f316420398aac0b","In order to improve the transmission stiffness of harmonic gear drive with short flexspline, a design of harmonic drive with new teeth profile is proposed. The teeth profiles of flexspline and circular spline are designed as double arc profile and its conjugate profile with certain axial inclination angle, respectively, so that the contact area between meshing teeth is improved. The WEDM-LS processes for teeth of short flexspline and circular spline are presented, and 50 prototype harmonic gear drives with tilt tooth circular spline and 1/4 and 1/2 length-diameter ratio are developed, respectively. The results of transmission stiffness experiments show that the transmission stiffness of the new designed harmonic gear driver is increased by 39% compared to that of the previous designed harmonic gear driver without tilt tooth.","Circular spline; Double arc tooth profile; Harmonic gear drive; Short flexspline; Tilt tooth; Transmission stiffness; WEDM-LS",,2-s2.0-84894795046
"Li B., Wang Y.G., Gao Y.T.","The research of oil fields heterogeneous data management technology",2014,"Advanced Materials Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901253994&doi=10.4028%2fwww.scientific.net%2fAMR.915-916.1397&partnerID=40&md5=0d023e8cb9e2c7241fbe13f4e4e5ac06","With the continuous development of petroleum exploration technology and exploitation business, the data types involved in the petroleum field are getting more complicated and richer, so a lot of heterogeneous data types emerged, data interchange can not be achieved directly among them. In this paper, the integration middle framework of heterogeneous data types is built based on XML, to realize the transition and integration of relational data and XML data, and to provide support for the data sharing and application among data model. © (2014) Trans Tech Publications, Switzerland.","Heterogeneous data; System integration; XML","Information management; Oil fields; Continuous development; Data interchange; Heterogeneous data; Management technologies; Petroleum exploration; Petroleum fields; Relational data; System integration; XML",2-s2.0-84901253994
"Davidson D.B., Esposito A., Tarricone L., Zappatore M., Fiori G.D.","EM programmer's notebook: Effective search and exploitation of electromagnetic knowledge in the Web",2014,"IEEE Antennas and Propagation Magazine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902012831&doi=10.1109%2fMAP.2014.6821774&partnerID=40&md5=c858582f1f65a13829ea3c8c94b1e6fa","We all are aware of the huge amount of electromagnetic information and knowledge available in the Web, both in the form of PDF scientific papers, and in other forms (e.g., software, datasets). Similarly, we all have experienced the frustration of searching the Web for papers and other information, and getting useless or unsatisfactory results. All these limitations clearly demonstrate that information technologies (IT) need further progress so as to improve the efficiency of the mentioned processes. Indeed, there is fervid activity around this goal, and some interesting achievements have begun to appear in terms of search effectiveness and navigation satisfaction. Novel formats for representing scientific-paper contents are proposed, enhancing machine processing capability. However, embracing the new information technology proposals requires a cultural change from authors and from editors, and we cannot take it for granted that it will happen in the near future. Moreover, already-published papers should not be left out from approaching Web innovations. In this paper, we therefore focus both on the emerging technologies for generating more searchable files, and on those technologies allowing the transformation of existing PDF files into more-effective formats. These technologies need to be customized to each specific domain, thus rendering the codification of specific electromagnetic (EM) concepts of paramount importance. A use case is proposed on a specific EM example, so as to demonstrate the viability and effectiveness of the proposed approach. © 1990-2011 IEEE.","annotation; knowledge discovery; natural language processing; ontology; search methods; Semantic Web","Data mining; Electromagnetism; Natural language processing systems; Ontology; Semantic Web; annotation; Electromagnetic information; Emerging technologies; Machine processing; NAtural language processing; Scientific papers; Search method; Searching the Web; Information technology",2-s2.0-84902012831
"Álvarez A., Arzelus H., Etchegoyhen T.","Towards customized automatic segmentation of subtitles",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921523920&partnerID=40&md5=e41541eaff252c29f0231a0d2347f791","Automatic subtitling through speech recognition technology has become an important topic in recent years, where the effort has mostly centered on improving core speech technology to obtain better recognition results. However, subtitling quality also depends on other parameters aimed at favoring the readability and quick understanding of subtitles, like correct subtitle line segmentation. In this work, we present an approach to automate the segmentation of subtitles through machine learning techniques, allowing the creation of customized models adapted to the specific segmentation rules of subtitling companies. Support Vector Machines and Logistic Regression classifiers were trained over a reference corpus of subtitles manually created by professionals and used to segment the output of speech recognition engines. We describe the performance of both classifiers and discuss the merits of the approach for the automatic segmentation of subtitles. © Springer International Publishing Switzerland 2014,","Automatic subtitling; Machine learning; Subtitle segmentation","Artificial intelligence; Computational linguistics; Learning systems; Speech; Automatic segmentations; Automatic subtitling; Line segmentation; Logistic regression classifier; Machine learning techniques; Speech recognition engine; Speech recognition technology; Speech technology; Speech recognition",2-s2.0-84921523920
"Wang A., Kwiatkowski T., Zettlemoyer L.","Morpho-syntactic lexical generalization for CCG semantic parsing",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961287843&partnerID=40&md5=2fd2d7ef892205b196a94600c176061f","In this paper, we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme. We present a new morpho-syntactic factored lexicon that models systematic variations in morphology, syntax, and semantics across word classes. The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered, thereby enabling effective learning from less data. Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data, including up to 45% relative test-error reduction. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Formal languages; Natural language processing systems; Semantics; Benchmark domains; Domain independents; Effective learning; English languages; Grammar induction; Relative test error; State of the art; Systematic variation; Syntactics",2-s2.0-84961287843
"Gao Q., Ni H., Li L., Chen F., Xiao A., Cai H.","Characterization of extracellular enzymes from Aspergillus niger for debittering citrus juice",2014,"Journal of Chinese Institute of Food Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902372541&partnerID=40&md5=eec4663fbe5f081079cbefd88445f4ee","To characterize mechanisms of the debittering citrus juice of extracellular enzymes from Aspergillus niger and to investigate fermentation dynamics of the enzyme activities in relation to the debittering, both solid-state fermentation and submerged fermentation were used to prepare the extracellular enzyme of A. niger, the resulting enzymes were submitted to treat Guanxi pomelo juice, followed by sensory evaluation of the bitter taste and high performance liquid chromatography (HPLC) analyses of the naringin and limonin. The results showed the enzymatic treatment significantly decreased the bitter taste and the content of naringin and limonin. During the solid-state fermentation, the cultures of 12 days showed the best debittering activities, which removed 92.5% of the naringin and 27.7% of the limonin in the juice; whereas the submerged fermentation showed the best debittering effect after 10 days, which generated an enzyme liquid that removed 95.5% of the naringin and 30.8% of the limonin. The results indicated A. niger was able to produce extracellular enzymes that significantly decrease the concentration of naringin and limonin, which resulted in the elimination of the bitter taste of citrus juice, indicating the existence of naringinase and limonin debittering enzyme in the extracellular enzymes. Thus, this study provides a new approach to debitter citrus juice.","Aspergillus niger; Bitterness; Extracellular enzyme; Limonin; Naringin","Aspergillus; Fermentation; Flavonoids; Fruit juices; High performance liquid chromatography; Aspergillus niger; Bitterness; Extracellular enzymes; Limonin; Naringin; Enzymes",2-s2.0-84902372541
"Wysocki B.T., McDonald N.R., Thiem C.D.","Hardware-based artificial neural networks for size, weight, and power constrained platforms",2014,"Proceedings of SPIE - The International Society for Optical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906346527&doi=10.1117%2f12.2052440&partnerID=40&md5=e8f7ffa4f65ffae0d7493eeafa58b523","A fully parallel, silicon-based artificial neural network (CM1K) built on zero instruction set computer (ZISC) technology was used for change detection and object identification in video data. Fundamental pattern recognition capabilities were demonstrated with reduced neuron numbers utilizing only a few, or in some cases one, neuron per category. This simplified approach was used to validate the utility of few neuron networks for use in applications that necessitate severe size, weight, and power (SWaP) restrictions. The limited resource requirements and massively parallel nature of hardware-based artificial neural networks (ANNs) make them superior to many software approaches in resource limited systems, such as micro-UAVs, mobile sensor platforms, and pocket-sized robots.","Artificial neural networks; Pattern recognition; Radial basis function (RBF); Size; Weight and Power (SWaP); Zero instruction set computing (ZISC)","Computer hardware; Hardware; Neural networks; Neurons; Pattern recognition; Radial basis function networks; Fundamental patterns; Mobile sensor platform; Object identification; Radial Basis Function(RBF); Resource requirements; Size; Weight and Power (SWaP); Zero instruction set computing; Computation theory",2-s2.0-84906346527
"Acharya S., Rajasekar A., Shender B.S., Hrebien L., Kam M.","Pulse oximeter signal modeling and fusion for hypoxia monitoring",2014,"FUSION 2014 - 17th International Conference on Information Fusion",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910678534&partnerID=40&md5=5959fe4dc876518e0f46895474b6c3f3","We develop models and fusion rules for oximeters that detect the onset of hypoxia. Hypoxia is a medical condition affecting portions of the body that are deprived of oxygen supply. Prolonged exposure to cerebral oxygen deficiency can lead to unconsciousness or even death. The onset of hypoxia in humans is of concern for those operating in high altitudes, and in military flights characterized by high-acceleration maneuvers. Using oximeters for measuring blood oxygen saturation levels is a common means to detect hypoxia in real time. Many types of oximeters can be used for this task but all are prone to complicated noise characteristics and bias inaccuracies. It may therefore be advisable to collect and combine data streams from multiple oximeters for more reliable Hypoxia/No Hypoxia decisions (compared to decisions made by a single oximeter). Here we develop statistical noise models for three popular types of oximeters (Respironics Novametrix 515B, Nonin forehead pulse oximeter 9847, and Masimo Rad-87). We also combine data streams from these oximeters using a Kalman filter. The result is a smooth and reliable estimate of blood oxygen saturation level which can be used to detect the onset of Hypoxia. © 2014 International Society of Information Fusion.","Colored Noise; Hypoxia Monitoring; Kalman Filter; Pulse Oximeters; Sensor Fusion","Blood; Data communication systems; Kalman filters; Noninvasive medical procedures; Oxygen; Oxygen supply; Blood oxygen saturation; Colored noise; Medical conditions; Noise characteristic; Pulse oximeters; Reliable estimates; Sensor fusion; Statistical noise; Oximeters",2-s2.0-84910678534
"Mousavi H., Kerr D., Iseli M., Zaniolo C.","Harvesting domain specific ontologies from text",2014,"Proceedings - 2014 IEEE International Conference on Semantic Computing, ICSC 2014",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906969596&doi=10.1109%2fICSC.2014.12&partnerID=40&md5=abac1e9f0b32ff5a6e2f110058bd9d55","Ontologies are a vital component of most knowledge-based applications, including semantic web search, intelligent information integration, and natural language processing. In particular, we need effective tools for generating in-depth ontologies that achieve comprehensive converge of specific application domains of interest, while minimizing the time and cost of this process. Therefore we cannot rely on the manual or highly supervised approaches often used in the past, since they do not scale well. We instead propose a new approach that automatically generates domain-specific ontologies from a small corpus of documents using deep NLP-based text-mining. Starting from an initial small seed of domain concepts, our Onto Harvester system iteratively extracts ontological relations connecting existing concepts to other terms in the text, and adds strongly connected terms to the current ontology. As a result, Onto Harvester (i) remains focused on the application domain, (ii) is resistant to noise, and (iii) generates very comprehensive ontologies from modest-size document corpora. In fact, starting from a small seed, Onto Harvester produces ontologies that outperform both manually generated ontologies and ontologies generated by current techniques, even those that require very large well-focused data sets. © 2014 IEEE.",,"Information retrieval; Iterative methods; Linguistics; Natural language processing systems; Domain concepts; Domain-specific ontologies; Effective tool; Intelligent information; Knowledge-based applications; NAtural language processing; Semantic web search; Strongly connected; Harvesters",2-s2.0-84906969596
"Mutiara A.B., Putri T., Silfianti W., Muslim A., Oswari T.","Semantic-web-based searching application for doctors schedule and facilities in hospital",2014,"Journal of Theoretical and Applied Information Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891897175&partnerID=40&md5=eeae42f4ea036f94845d61da6e2d5006","A Hospital as a health supporting media has main information such as existing doctors praxis schedule and availability of facilities. Each hospital has its own format in delivering its information. With the diversity of available information, it is needed a technology that could be able to combine and uniform almost the same information and then present it to the user in the form of mutual relevant to their intended context. The technology used is an ontological semantic-web based search engine. The semantic web method with ontology approach is not only capable of understanding the meaning of a word and a concept, but also the logical relationships between them. In this paper it will be explained the development of search engine with two kinds of data, which are grabed directly (live data) by using the concept of Ontology Web Language (OWL) and manually entered (dummy data) by using the concept of Resource Description Framework (RDF). © 2005 - 2014 JATIT & LLS. All rights reserved.","Doctor; Ontology; OWL; RDF; Semantic web",,2-s2.0-84891897175
"Yang M.-C., Duan N., Zhou M., Rim H.-C.","Joint relational embeddings for knowledge-based question answering",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926018829&partnerID=40&md5=c732706eef6308283196f1b97f5c81e1","Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task. Unlike most previous methods that achieve this goal based on mappings between lexicalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KBQA by leveraging semantic associations between lexical representations and KBproperties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA baseline methods on two publicly released QA data sets. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Semantics; Baseline methods; Embeddings; Knowledge based; Logical forms; Logical predicate; Natural languages; Question Answering; Semantic associations; Natural language processing systems",2-s2.0-84926018829
"Kushman N., Artzi Y., Zettlemoyer L., Barzilay R.","Learning to automatically solve algebra word problems",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906930657&partnerID=40&md5=39d36705728a08544ea223afd32a6e90","We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task. © 2014 Association for Computational Linguistics.",,"Algebra; Algorithms; Computational linguistics; Sentence boundaries; System of linear equations; Word problem; Problem solving",2-s2.0-84906930657
"Jianfang C., Lichao C., Huijun W.","Recognition of Chinese entertainment news words using SVM-based active learning strategy",2014,"Open Cybernetics and Systemics Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929669009&partnerID=40&md5=cf8bdfc4aac42e3099b1bf1dc84d3aef","The recognition of Chinese entertainment news words is an important task for Chinese information processing. In order to solve the problem of recognizing the Chinese entertainment news words, SVM-based method has been introduced in this paper, which uses active leaning strategy. It selects new instances, incrementally, to be labeled and included in its training set to form an incremental course of learning. The results of the test show that the method is efficient and the precision and recall of entertainment news words recognition achieved are 78.92% and 86.42%, respectively. The method in this paper has gained good effect. © Jianfang et al.; Licensee Bentham Open.","Classification; Entertainment news words; Machine learning","Artificial intelligence; Classification (of information); Text processing; Active learning strategies; Chinese information processing; Entertainment news words; Precision and recall; SVM-based methods; Training sets; SVM algorithm; Text classification; Learning systems",2-s2.0-84929669009
"Amaechi B.T., Mathews S.M., Mensinkai P.K.","Effect of theobromine-containing toothpaste on dentin tubule occlusion in situ",2014,"Clinical Oral Investigations",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896562122&doi=10.1007%2fs00784-014-1226-1&partnerID=40&md5=8ceedd1b3de12736e76995d66b94fe8b","Objectives: Dentin hypersensitivity (DH) is treated by either occlusion of dentin tubules or nerve desensitization. This in situ study compared dentin tubules occlusion by theobromine-containing dentifrices with (Theodent-classic-F®, TCF) and without (Theodent-classic®, TC) fluoride with 1,500 ppm fluoride toothpaste, Colgate®-Regular (Fluoride) and Novamin®-containing toothpaste, Sensodyne®-5000-Nupro (Novamin®).Methods: Each subject wore four intraoral appliances bearing dentin blocks while using one of four test dentifrices (n = 20/dentifrice) twice daily for 7 days. The four appliances were removed successively after 1, 2, 3, and 7 days. Treated blocks and their control (untreated) blocks were examined with scanning electron microscopy (SEM). Effects were compared statistically (ANOVA/Tukey’s) based on percentage of surface area covered by deposited precipitate layer (%DPL) and percentage of fully open (%FOT), partially occluded (%POT), and completely occluded (%COT) tubules in each block calculated relative to the number of tubules in their control blocks.Results: SEM observation indicated an increased %COT and %DPL over time. After 1 and 2 days, %COT was comparable with TC and TCF, and significantly (p < 0.05) higher compared with Novamin® and Fluoride. Following 3 and 7 days, %COT was comparable among TC, TCF, and Novamin®, but remained significantly lower in Fluoride. At any time, %DPL was significantly (p < 0.05) higher in TC, TCF, and Novamin® compared with Fluoride.Conclusions: Theobromine-containing toothpastes with and without fluoride have equal potential in occluding dentin tubules within a shorter time period than Novamin®-containing toothpaste; however, the three demonstrated equal potential after 1 week, but not the fluoride toothpaste.Clinical relevance: Theobromine-containing toothpaste promoted dentin tubule occlusion thus shows potential to relief DH. © 2014, Springer-Verlag Berlin Heidelberg.","Dentifrice; Dentin hypersensitivity; Novamin®; Theobromine; Theodent; Tubule occlusion",,2-s2.0-84896562122
"Sabry F., Erradi A., Nassar M., Malluhi Q.M.","Automatic generation of optimized workflow for distributed computations on large-scale matrices",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910666626&partnerID=40&md5=22d15415fad98eeb60292e4de084a68a","Efficient evaluation of distributed computation on large-scale data is prominent in modern scientific computation; especially analysis of big data, image processing and data mining applications. This problem is particularly challenging in distributed environments such as campus clusters, grids or clouds on which the basic computation routines are offered as web/cloud services. In this paper, we propose a locality-aware workflow-based solution for evaluation of large-scale matrix expressions in a distributed environment. Our solution is based on automatic generation of BPEL workflows in order to coordinate long running, asynchronous and parallel invocation of services. We optimize the input expression in order to maximize parallel execution of independent operations while reducing the matrix transfer cost to a minimum. Our approach frees the end-user of the system from the burden of writing and debugging lengthy BPEL workflows. We evaluated our solution on realistic mathematical expressions executed on large-scale matrices distributed on multiple clouds. © Springer-Verlag Berlin Heidelberg 2014.","BPEL workflows; Distributed computations; Large-scale matrices; Location-aware optimization","Big data; Data handling; Distributed computer systems; Image processing; Program debugging; Automatic Generation; Data mining applications; Distributed computations; Distributed environments; Location-aware; Mathematical expressions; Scientific computation; Work-flows; Matrix algebra",2-s2.0-84910666626
"Jovanovic P., Simitsis A., Wilkinson K.","Engine independence for logical analytic flows",2014,"Proceedings - International Conference on Data Engineering",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901755385&doi=10.1109%2fICDE.2014.6816723&partnerID=40&md5=3e501bf6f464311ec314de3e86a8e1bb","A complex analytic flow in a modern enterprise may perform multiple, logically independent, tasks where each task uses a different processing engine. We term these multi-engine flows hybrid flows. Using multiple processing engines has advantages such as rapid deployment, better performance, lower cost, and so on. However, as the number and variety of these engines grows, developing and maintaining hybrid flows is a significant challenge because they are specified at a physical level and, so are hard to design and may break as the infrastructure evolves. We address this problem by enabling flow design at a logical level and automatic translation to physical flows. There are three main challenges. First, we describe how flows can be represented at a logical level, abstracting away details of any underlying processing engine. Second, we show how a physical flow, expressed in a programming language or some design GUI, can be imported and converted to a logical flow. In particular, we show how a hybrid flow comprising subflows in different languages can be imported and composed as a single, logical flow for subsequent manipulation. Third, we describe how a logical flow is translated into one or more physical flows for execution by the processing engines. The paper concludes with experimental results and example transformations that demonstrate the correctness and utility of our system. © 2014 IEEE.",,"Design; Liquid metal cooled reactors; Automatic translation; Better performance; Logical levels; Multiple processing; Physical flow; Physical level; Processing engine; Rapid deployments; Engines",2-s2.0-84901755385
"Le Nguyen M., Shimazu A.","A semi supervised learning model for mapping sentences to logical forms with ambiguous supervision",2014,"Data and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897982015&doi=10.1016%2fj.datak.2013.12.001&partnerID=40&md5=696e4f4e83712ac44326edb6eb6e9ccc","Semantic parsing is the task of mapping a sentence in natural language to a meaning representation. The limitation of previous work on supervised semantic parsing is that it is very difficult to obtain annotated training data in which a sentence is paired with a semantic representation. To deal with this problem, we introduce a semi supervised learning model for semantic parsing with ambiguous supervision. The main idea of our method is to utilize a large amount of data, to enrich feature space with the maximum entropy model using our semantic learner. We evaluate the proposed models on standard corpora to demonstrate that our methods are suitable for semantic parsing. Experimental results show that the proposed methods work efficiently and well on ambiguous data and it is comparable to the state of the art methods. © 2014 Elsevier B.V.","Ambiguous supervision; Semantic parsing; Semi-supervised learning","Maximum entropy methods; Semantics; Supervised learning; Ambiguous supervision; Annotated training data; Maximum entropy modeling; Natural languages; Semantic parsing; Semantic representation; Semi-supervised learning; State-of-the-art methods; Context free grammars",2-s2.0-84897982015
"Chen D., Huang R., Qu B., Jiang S.","Improving static analysis performance using rule-filtering technique",2014,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938304893&partnerID=40&md5=c8f5fe6e649182618fef328a2a4fef45","Static analysis is an efficient approach for software assurance. It is indicated that the most effective usage of it is to perform analysis in an interactive way through software development process, which has a high performance requirement. This paper concentrates on rule-based static analysis tools and proposes an optimized rule-checking algorithm to improve their performance. Our technique filters rules according to their characteristic objects before checking the rules against a specific source file. It is based on an observation that a source file always contains vulnerabilities of a small part of rules rather than all. To investigate our technique's feasibility and effectiveness, we implemented it in an open source static analysis tool called PMD and used it to perform an evaluation. The evaluation results show that our approach can get an average performance promotion of 28.7% compared with original PMD. Additionally, our technique incurs trivial runtime overhead. Copyright © 2014 by Knowledge Systems Institute Graduate School.","Performance improvement; Rule-based static analysis; Software quality; Software validation",,2-s2.0-84938304893
"Hacke M., Willey J., Mitchell G., Rushworth I.D., Higgitt C., Gibson L.T.","Investigation of long term storage solutions for rubber garments",2014,"Journal of the Institute of Conservation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907812071&doi=10.1080%2f19455224.2014.931872&partnerID=40&md5=1a19ef1d7307577c5753dca143fb91f4","A collection of 1980s Mexican rubberised cotton garments was investigated in order to assess conservation treatment options, to improve their long term storage conditions and to investigate the origin of the strong odour associated with the garments. Chemical and mechanical deterioration of the rubber was confirmed by Fourier transform infrared (FTIR) spectroscopy and scanning electron microscopy (SEM). Recommendations for remedial conservation were made after re-shaping tests using the application of gentle heat. Passive diffusion tube sampling was carried out to assess the levels of acids, aldehydes and VOCs emitted fromthe garments. Very high concentrations of acetic acid off-gassing were confirmed and deemed a risk to the cotton fabric. Test set-ups in sealed ambient and anoxic conditions, with and without the inclusion of MicroChamber® paper, showed the effectiveness of this pollutant sorbent in reducing acetic acid levels as well as general odour emitted by the garments. © 2014 Icon, The Institute of Conservation.","Acetic acid; Anoxia; Cotton; MicroChamber®; Rubber; Volatile organic compounds (VOCs)","Cotton; Rubber; Volatile organic compounds; Anoxia; Long-term storage; Micro-chambers; Acetic acid",2-s2.0-84907812071
"Wysocki B., Mc Donald N., Thiem C., Rose G., Gomez M., II","Hardware-based computational intelligence for size, weight, and power constrained environments",2014,"Advances in Information Security",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927629950&doi=10.1007%2f978-1-4614-7597-2_9&partnerID=40&md5=b1921a22adc95d248d23404bb808a00d",[No abstract available],,,2-s2.0-84927629950
"Álvarez-García S., Baeza-Yates R., Brisaboa N.R., Larriba-Pey J.-L., Pedreira O.","Automatic multi-partite graph generation from arbitrary data",2014,"Journal of Systems and Software",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902537939&doi=10.1016%2fj.jss.2014.03.022&partnerID=40&md5=59de37bfebc6c2886249f509a0629118","In this paper we present a generic model for automatic generation of basic multi-partite graphs obtained from collections of arbitrary input data following user indications. The paper also presents GraphGen, a tool that implements this model. The input data is a collection of complex objects composed by a set or list of heterogeneous elements. Our tool provides a simple interface for the user to specify the types of nodes that are relevant for the application domain in each case. The nodes and the relationships between them are derived from the input data through the application of a set of derivation rules specified by the user. The resulting graph can be exported in the standard GraphML format so that it can be further processed with other graph management and mining systems. We end by giving some examples in real scenarios that show the usefulness of this model. © 2014 Elsevier Inc.","Automatic graph generation; Graph processing and analysis","Computer programming languages; Arbitrary inputs; Automatic Generation; Complex objects; Derivation rules; Generic modeling; Graph generation; Graph processing; Mining systems; Input output programs",2-s2.0-84902537939
"Qian X., Liu Y.","Polynomial time joint structural inference for sentence compression",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906928197&partnerID=40&md5=c5c16224e581385dcbc8588d908cecfa","We propose two polynomial time inference algorithms to compress sentences under bigram and dependency-factored objectives. The first algorithm is exact and requires O(n6) running time. It extends Eisner's cubic time parsing algorithm by using virtual dependency arcs to link deleted words. Two signatures are added to each span, indicating the number of deleted words and the rightmost kept word within the span. The second algorithm is a fast approximation of the first one. It relaxes the compression ratio constraint using Lagrangian relaxation, and thereby requires O(n4) running time. Experimental results on the popular sentence compression corpus demonstrate the effectiveness and efficiency of our proposed approach. © 2014 Association for Computational Linguistics.",,"Approximation algorithms; Compression ratio (machinery); Computational linguistics; Polynomial approximation; Syntactics; Effectiveness and efficiencies; Fast approximation; Inference algorithm; LaGrangian relaxation; Parsing algorithm; Polynomial-time; Running time; Sentence compression; Inference engines",2-s2.0-84906928197
"Jurčíček F., Dušek O., Plátek O., Žilka L.","Alex: A statistical dialogue systems framework",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906971903&doi=10.1007%2f978-3-319-10816-2_71&partnerID=40&md5=cd7c7110732f0a32e24039c5d6eea2f5","This paper describes the Alex Dialogue Systems Framework (ADSF). The ADSF currently includes mature components for public telephone network connectivity, voice activity detection, automatic speech recognition, statistical spoken language understanding, and probabilistic belief tracking. The ADSF is used in a real-world deployment within the Public Transport Information (PTI) domain. In PTI, users can interact with a dialogue system on the phone to find intra- and inter-city public transport connections and ask for weather forecast in a desired city. Based on user responses, vast majority of the system users are satisfied with the system performance. © 2014 Springer International Publishing.","automatic speech recognition; dialogue state tracking; dialogue systems; spoken language understanding","Speech recognition; Systems engineering; Weather forecasting; Automatic speech recognition; Dialogue systems; Probabilistic belief; Public telephone; Public transport; Spoken language understanding; State tracking; Voice activity detection; Speech processing",2-s2.0-84906971903
"Kraas A.","Towards an extensible modeling and validation framework for SDL-UML",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949146806&partnerID=40&md5=a70afc62708dda256896127e36a46956","The Specification and Description Language (SDL) has been a domain specific language that is well-established in the telecommunication sector for many years, but only a small set of SDL tools is available.In contrast, a wide range of different kinds of tools can be used for various purposes, such as model transformation, for the Unified Modeling Language (UML). The UML profile for SDL (SDL-UML) makes it possible to specify SDL compliant models in terms of a UML model. In this paper, the extensible SDL-UML Modeling and Validation (SU-MoVal)framework, which supports the specification and validation of models that are compliant to Z.109, is presented. As an additional feature, the SU-MoVal framework also provides an editor for the specification of a textual notation that is mapped to corresponding SDL-UML elements. © Springer International Publishing Switzerland 2014.","Framework; Profile; SDL-UML; Specification; Validation","Computational linguistics; Computer programming languages; Modeling languages; Problem oriented languages; Reusability; Specifications; Domain specific languages; Framework; Modeling and validation; Profile; SDL; Specification and description languages; Telecommunication sector; Validation; Unified Modeling Language",2-s2.0-84949146806
"Wu C.-H., Liu C.-H., Su P.-H.","Sentence extraction with topic modeling for question–answer pair generation",2014,"Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921698960&doi=10.1007%2fs00500-014-1386-6&partnerID=40&md5=5cf27e4f0af00d673001d5ab2282da09","Recently, automatic QA pair generation has been an essential technique to reduce human involvement in the construction of QA systems. In a big data era, huge information is produced every day. Therefore, it is an important issue for QA systems to be able to respond to users with up-to-date information, e.g., to answer questions regarding recent posts on blogs. The major problem in building such systems is the efficiency to capture relevant text sources for specific QA domains. In this study, topic modeling is used as a means to help determine efficiently if an article is of the same topic as a specific domain of interest, e.g., health domain as exemplified in this paper. QA pairs are then generated from these selected articles using the proposed sentence extraction method. Experimental results show that, using the proposed method with topic modeling, a 7.3 % acceptance rate improvement on the generated questions was achieved. © 2014, Springer-Verlag Berlin Heidelberg.","Holism-based detection; QA pair generation; Sentence extraction; Topic modeling","Big data; Extraction; Acceptance rate; In-buildings; QA pair generation; QA system; Sentence extraction; Text sources; Topic Modeling; Data mining",2-s2.0-84921698960
"Dammak S.M., Jedidi A., Bouaziz R.","Automation of the semantic annotation of web resources",2014,"International Journal of Internet Technology and Secured Transactions",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906891482&doi=10.1504%2fIJITST.2014.064513&partnerID=40&md5=a4ee5f613270463900f2a51d39c2f505","The annotation of a web page allows us to associate a semantic to the content of this page. But with the great mass of pages managed through the world, and especially with the advent of the web, their manual annotation is not feasible. In this paper, we will focus on the semiautomatic annotation of the web pages. We will propose an approach and a component (entitled 'Querying Web') for semantic annotation of web pages. The proposal of this component aims at improving the interrogation process in the semantic web environment. Our solution is an enhancement of the first result of annotation done by the 'Semantic Radar' plug-in on the web resources, by new annotations using an enriched domain ontology. Finally, we will present in this paper an evaluation of the automation made by our component. Copyright © 2014 Inderscience Enterprises Ltd.","Domain ontologies; Querying web; Semantic annotation; Semantic radar; Semantic web; Web resources",,2-s2.0-84906891482
[No author name available],"Preface",2014,"Semantic Structures: Advances in Natural Language Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925832844&doi=10.4324%2f9781315857367&partnerID=40&md5=7ab70ae47ddd07e864783ed5692a96f0",[No abstract available],,,2-s2.0-84925832844
"Hall D., Berg-Kirkpatrick T., Canny J., Klein D.","Sparser, better, faster GPU parsing",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906924684&partnerID=40&md5=f74a6faa84c3b2fe2aa35c7883edd72a","Due to their origin in computer graphics, graphics processing units (GPUs) are highly optimized for dense problems, where the exact same operation is applied repeatedly to all data points. Natural language processing algorithms, on the other hand, are traditionally constructed in ways that exploit structural sparsity. Recently, Canny et al. (2013) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power, obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU. In this work, we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing over 404 Viterbi parses per second - more than a 2x speedup - on the same hardware. Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference, improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning - nearly a 6x speedup. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Computer graphics; Context free grammars; Formal languages; Inference engines; Natural language processing systems; Program processors; Coarse-to-fine; Computational power; Data points; Dense problems; Graphics processing units; High quality; Minimum bayes risk; NAtural language processing; Syntactics",2-s2.0-84906924684
"Yih W.-T., He X., Meek C.","Semantic parsing for single-relation question answering",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",73,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906930320&partnerID=40&md5=1bd7883c4687ddde040bbda049d6b4e6","We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA). We focus on single-relation questions and decompose each question into an entity mention and a relation pattern. Using convolutional neural network models, we measure the similarity of entity mentions with entities in the knowledge base (KB) and the similarity of relation patterns and relations in the KB. We score relational triples in the KB using these measures and select the top scoring relational triple to answer the question. When evaluated on an open-domain QA task, our method achieves higher precision across different recall points compared to the previous approach, and can improve F1 by 7 points. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Natural language processing systems; Neural networks; Convolutional neural network; Knowledge base; Open domain question answering; Question Answering; Semantic parsing; Semantic similarity; Semantics",2-s2.0-84906930320
"Liu H., Kešelj V., Blouin C.","Exploring a subgraph matching approach for extracting biological events from literature",2014,"Computational Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905924043&doi=10.1111%2fcoin.12009&partnerID=40&md5=4886b748d2ba250662136b008f63992f","An important task in biological information extraction is to identify descriptions of biological relations and events involving genes or proteins. In this work, we propose a graph-based approach to automatically learn rules for detecting biological events in the life science literature. The event rules are learned by identifying the key contextual dependencies from full parsing of annotated text. The detection is performed by searching for isomorphism between event rules and the dependency graphs of complete sentences. When applying our approach to the data sets of the Task1 of the BioNLP-ST 2009, we achieved a 40.71% F-score in detecting biological events across nine event types. Our 56.32% precision is comparable with the state-of-the-art systems. The approach may also be generalized to extract events from other domains where training data are available because it requires neither manual intervention nor external domain-specific resources. The subgraph matching algorithm we developed is released under the new BSD license and can be downloaded from http://esmalgorithm.sourceforge.net. © 2013 Wiley Periodicals, Inc.","biological event extraction; biological information extraction; subgraph isomorphism; subgraph matching","Information retrieval; Set theory; Syntactics; Biological information; Dependency graphs; Domain specific; Event extraction; Manual intervention; State-of-the-art system; Subgraph isomorphism; Subgraph matching; Data mining",2-s2.0-84905924043
"Broneske D., Dorok S., Köppen V., Meister A.","Software design approaches for mastering variability in database systems",2014,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920020024&partnerID=40&md5=8645da8bf2f1b230310332f833809b36","For decades, database vendors have developed traditional database systems for different application domains with highly differing requirements. These systems are extended with additional functionalities to make them applicable for yet another data-driven domain. The database community observed that these ""one size fits all"" systems provide poor performance for special domains; systems that are tailored for a single domain usually perform better, have smaller memory footprint, and less energy consumption. These advantages do not only originate from different requirements, but also from differences within individual domains, such as using a certain storage device. However, implementing specialized systems means to reimplement large parts of a database system again and again, which is neither feasible for many customers nor efficient in terms of costs and time. To overcome these limitations, we envision applying techniques known from software product lines to database systems in order to provide tailor-made and robust database systems for nearly every application scenario with reasonable effort in cost and time. General Terms Database, Software Engineering.","Database system; Software product line; Variability","Application programs; Energy utilization; Software design; Virtual storage; Application scenario; Database community; Database vendors; Design approaches; Memory footprint; Software Product Line; Specialized systems; Variability; Database systems",2-s2.0-84920020024
"Gardent C.","Syntax and data-to-text generation",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921443460&doi=10.1007%2f978-3-319-11397-5_1&partnerID=40&md5=70cb3bb02e98709a6cd6890a57c3d583","With the development of the web of data, recent statistical, data-to-text generation approaches have focused on mapping data (e.g., database records or knowledge-base (KB) triples) to natural language. In contrast to previous grammar-based approaches, this more recent work systematically eschews syntax and learns a direct mapping between meaning representations and natural language. By contrast, I argue that an explicit model of syntax can help support NLG in several ways. Based on case studies drawn from KB-to-text generation, I show that syntax can be used to support supervised training with little training data; to ensure domain portability; and to improve statistical hypertagging. © Springer International Publishing Switzerland 2014.","Computational grammars; Hybrid symbolic/statistical approaches; Natural language generation; Statistical natural language processing","Computational grammars; Knowledge based systems; Mapping; Syntactics; Database records; Explicit modeling; Grammar based approach; Hybrid symbolic/statistical approaches; Natural language generation; Natural languages; Statistical natural language processing; Supervised trainings; Natural language processing systems",2-s2.0-84921443460
"Nečaský M., Klímek J., Mynarz J., Knap T., Svátek V., Stárka J.","Linked data support for filing public contracts",2014,"Computers in Industry",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901187944&doi=10.1016%2fj.compind.2013.12.006&partnerID=40&md5=79e6ab3c28c9bf51b8e8f579926e29e0","Management of the tendering phase of the public contract lifecycle is a demanding activity with often irrevocable impact on the subsequent realization phase. We investigate the impact of the linked data technology on this process. The public contract information itself can be published as linked data. A specialized vocabulary, the Public Contracts Ontology, was designed for this purpose. Extractors and transformers for public contract datasets in various formats (HTML, CSV, XML) were developed to enable conversion into RDF format corresponding to the vocabulary. Moreover, an application for filing public contracts was implemented. It enables a contracting authority to manage RDF data about itself and its contracts, suppliers to the contracts, to-be-contracted products and services, and actual tenders proposed by bidders. It also provides matchmaking services for finding similar contracts and suitable suppliers for a given call for tenders based on their history, which is a useful feature for contracting authorities. © 2013 Elsevier B.V.","Lifecycle; Linked data; Ontology; Public procurement","Data handling; Life cycle; Ontology; Semantic Web; Supply chains; Contracting authorities; Linked datum; Matchmaking services; Products and services; Public contracts; Public procurement; RDF data; Contracts",2-s2.0-84901187944
"Yıldız T., Yıldırım S., Diri B.","An integrated approach to automatic synonym detection in Turkish corpus",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921633560&partnerID=40&md5=9fb9e24ac0b01bcfb511cd0ec4e550cd","In this study, we designed a model to determine synonymy. Our main assumption is that synonym pairs show similar semantic and dependency relation by the definition. They share same meronym/holonym and hypernym/hyponym relations. Contrary to synonymy, hypernymy and meronymy relations can probably be acquired by applying lexico-syntactic patterns to a big corpus. Such acquisition might be utilized and ease detection of synonymy. Likewise, we utilized some particular dependency relations such as object/subject of a verb, etc. Machine learning algorithms were applied on all these acquired features. The first aim is to find out which dependency and semantic features are the most informative and contribute most to the model. Performance of each feature is individually evaluated with cross validation. The model that combines all features shows promising results and successfully detects synonymy relation. The main contribution of the study is to integrate both semantic and dependency relation within distributional aspect. Second contribution is considered as being first major attempt for Turkish synonym identification based on corpus-driven approach. © Springer International Publishing Switzerland 2014.","Dependency relations; Near-synonym; Pattern-based; Synonym","Artificial intelligence; Learning algorithms; Learning systems; Linguistics; Natural language processing systems; Cross validation; Dependency relation; Integrated approach; Lexico-syntactic patterns; Near-synonym; Pattern-based; Semantic features; Synonym; Semantics",2-s2.0-84921633560
"Adesina A.O., Agbele K.K., Abidoye A.P., Nyongesa H.O.","Text messaging and retrieval techniques for a mobile health information system",2014,"Journal of Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916600402&doi=10.1177%2f0165551514540400&partnerID=40&md5=e965510ed095af7190a6956f76a75d30","Mobile phones have been identified as one of the technologies that can be used to overcome the challenges of information dissemination regarding serious diseases. Short message services, a much used function of cell phones, for example, can be turned into a major tool for accessing databases. This paper focuses on the design and development of a short message services-based information access algorithm to carefully screen information on human immunodeficiency virus/acquired immune deficiency syndrome within the context of a frequently asked questions system. However, automating the short message services-based information search and retrieval poses significant challenges because of the inherent noise in its communications. The developed algorithm was used to retrieve the best-ranked question-answer pair. Results were evaluated using three metrics: average precision, recall and computational time. The retrieval efficacy was measured and it was confirmed that there was a significant improvement in the results of the proposed algorithm when compared with similar retrieval algorithms. © The Author(s) 2014.","Frequently asked question (FAQ); Information retrieval (IR); Mobile health (mHealth); Mobile Health Information system; Question and answer (Q&A) system; Short message service (SMS)/text message","Algorithms; Cellular telephone systems; Cellular telephones; Health; Information dissemination; Information retrieval; Information systems; Message passing; Mobile devices; Mobile phones; Telemedicine; Telephone sets; Text messaging; Viruses; Design and Development; Frequently asked questions; Human immunodeficiency virus; Information search and retrieval; Mobile health; Mobile Health (M-Health); Question-answer pairs; Short message services; Search engines",2-s2.0-84916600402
"Gimpel K., Smith N.A.","Phrase Dependency Machine Translation with Quasi-Synchronous Tree-to-Tree Features",2014,"Computational Linguistics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903479879&doi=10.1162%2fCOLI_a_00175&partnerID=40&md5=41ae8201b790e04186b67dd8979e6fc8","Recent research has shown clear improvement in translation quality by exploiting linguistic syntax for either the source or target language. However, when using syntax for both languages (""tree-to-tree"" translation), there is evidence that syntactic divergence can hamper the extraction of useful rules (Ding and Palmer 2005). Smith and Eisner (2006) introduced quasi-synchronous grammar, a formalism that treats non-isomorphic structure softly using features rather than hard constraints. Although a natural fit for translation modeling, its flexibility has proved challenging for building real-world systems. In this article, we present a tree-to-tree machine translation system inspired by quasi-synchronous grammar. The core of our approach is a new model that combines phrases and dependency syntax, integrating the advantages of phrase-based and syntax-based translation. We report statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. © 2014 Association for Computational Linguistics.",,,2-s2.0-84903479879
"Shrestha N., Ijaz S., Kukkonen-Harjula K.T., Kumar S., Nwankwo C.P.","Workplace interventions for reducing sitting at work",2014,"Cochrane Database of Systematic Reviews",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012058155&doi=10.1002%2f14651858.CD010912&partnerID=40&md5=8bbfc5c4e43d238cbb346dd6f4ed27ce","To evaluate the effects of workplace interventions to reduce sitting at work compared to no intervention or alternative interventions. © 2014 The Cochrane Collaboration.",,"Article; balloon; comparative effectiveness; dancing; human; priority journal; seat; sitting; standing; treadmill; walking; workplace",2-s2.0-85012058155
"Park H., Gweon G., Choi H.-J., Heo J., Ryu P.-M.","Sentential paraphrase generation for agglutinative languages using SVM with a string kernel",2014,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation, PACLIC 2014",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994065790&partnerID=40&md5=2e926e90dc72f5ecd684d09e442fa104","Paraphrase generation is widely used for various natural language processing (NLP) applications such as question answering, multi-document summarization, and machine translation. In this study, we identify the problems occurring in the process of applying existing probabilistic model-based methods to agglutinative languages, and provide solutions by reflecting the inherent characteristics of agglutinative languages. More specifically, we propose and evaluate a sentential paraphrase generation (SPG) method for the Korean language using Support Vector Machines (SVM) with a string kernel. The quality of generated paraphrases is evaluated using three criteria: (1) meaning preservation, (2) grammaticality, and (3) equivalence. Our experiment shows that the proposed method outperformed a probabilistic model-based method by 12%, 16%, and 17%, respectively, with respect to the three criteria. Copyright 2014 by Hancheol Park, Gahgene Gweon, Ho-Jin Choi, Jeong Heo, and Pum-Mo Ryu.",,"Natural language processing systems; Support vector machines; Agglutinative language; Inherent characteristics; Korean language; Machine translations; Multi-document summarization; NAtural language processing; Probabilistic modeling; Question Answering; Quality control",2-s2.0-84994065790
"Tierney M., Subramanian L.","Realizing privacy by definition in social networks",2014,"Proceedings of 5th Asia-Pacific Workshop on Systems, APSYS 2014",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906872127&doi=10.1145%2f2637166.2637232&partnerID=40&md5=ace39c74bbedb419be39e13f44c6f875","Privacy violations in online social networks (OSNs) have become more the norm than the exception. Conventional models of privacy in OSNs offer a limited set of privacy guarantees for information posted and reshared by users in OSNs. In this paper, we propose a completely new model of private information sharing using a refined abstraction of contexts that embodies the philosophy of contextual integrity (CI), which we believe better captures users privacy expectations in OSNs. We present the design of Compass, an online social network inspired by CI, in which three properties hold: (a) users are associated with roles in specific contexts; (b) every piece of information posted by a user is associated with a specific context; (c) norms defined on roles and attributes of posts in a context govern how information is shared across users within that context. © 2014 ACM.",,"Online systems; Contextual integrities; Conventional models; On-line social networks; Online social networks (OSNs); Privacy violation; Private information; Social networking (online)",2-s2.0-84906872127
"Lingren T., Deleger L., Molnar K., Zhai H., Meinzen-Derr J., Kaiser M., Stoutenborough L., Li Q., Solti I.","Evaluating the impact of pre-annotation on annotation speed and potential bias: Natural language processing gold standard development for clinical named entity recognition in clinical trial announcements",2014,"Journal of the American Medical Informatics Association",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901847800&doi=10.1136%2famiajnl-2013-001837&partnerID=40&md5=3b541f46bbe5d2f6cd0da8b34f55b6ed","Objective: To present a series of experiments: (1) to evaluate the impact of pre-annotation on the speed of manual annotation of clinical trial announcements; and (2) to test for potential bias, if pre-annotation is utilized. Methods: To build the gold standard, 1400 clinical trial announcements from the clinicaltrials.gov website were randomly selected and double annotated for diagnoses, signs, symptoms, Unified Medical Language System (UMLS) Concept Unique Identifiers, and SNOMED CT codes. We used two dictionary-based methods to preannotate the text. We evaluated the annotation time and potential bias through F-measures and ANOVA tests and implemented Bonferroni correction. Results: Time savings ranged from 13.85% to 21.5% per entity. Inter-annotator agreement (IAA) ranged from 93.4% to 95.5%. There was no statistically significant difference for IAA and annotator performance in preannotations. Conclusions: On every experiment pair, the annotator with the pre-annotated text needed less time to annotate than the annotator with non-labeled text. The time savings were statistically significant. Moreover, the pre-annotation did not reduce the IAA or annotator performance. Dictionary-based pre-annotation is a feasible and practical method to reduce the cost of annotation of clinical named entity recognition in the eligibility sections of clinical trial announcements without introducing bias in the annotation process.",,"annotation; article; book; clinical trial (topic); gold standard; human; information processing; learning algorithm; machine learning; natural language processing; preannotation; statistical significance; analysis of variance; clinical trial announcements; evaluation study; Information Extraction; information retrieval; methodology; named entity recognition; Pre-annotation; task performance; Unified Medical Language System; clinical trial announcements; Information Extraction; named entity recognition; Natural Language Processing; Pre-annotation; umls; Analysis of Variance; Clinical Trials as Topic; Humans; Information Storage and Retrieval; Natural Language Processing; Time and Motion Studies",2-s2.0-84901847800
"Squire M.","Forge++: The changing landscape of FLOSS development",2014,"Proceedings of the Annual Hawaii International Conference on System Sciences",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902248359&doi=10.1109%2fHICSS.2014.405&partnerID=40&md5=6a37e6a4d4597c09e09442b20e101648","Software forges are centralized online systems that provide useful tools to help distributed development teams work together, especially in free, libre, and open source software (FLOSS). Forge-provided tools may include web space, version control systems, mailing lists and communication forums, bug tracking systems, file downloads, wikis, and the like. Empirical software engineering researchers can mine the artifacts from these tools to better understand how FLOSS is made. As the landscape of distributed software development has grown and changed, the tools needed to make FLOSS have changed as well. There are three newer tools at the center of FLOSS development today: distributed version control based forges (like Github), programmer question-and-answer communities (like Stack Overflow), and pastebin tools (like Gist or Pastebin.com). These tools are extending and changing the toolset used for FLOSS development, and redefining what a software forge looks like. The main contributions of this paper are to describe each of these tools, to identify the data and artifacts available for mining from these tools, and to outline some of the ways researchers can use these artifacts to continue to understand how FLOSS is made. © 2014 IEEE.",,"Information management; Open source software; Systems science; Bug tracking system; Communication forums; Distributed development teams; Distributed software development; Distributed version controls; Empirical Software Engineering; Stack overflow; Version control system; Software engineering",2-s2.0-84902248359
"Jia Z., Zhao H.","A joint graph model for pinyin-to-Chinese conversion with typo correction",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906922603&partnerID=40&md5=84b6953183738c190c888a8a611d7861","It is very import for Chinese language processing with the aid of an efficient input method engine (IME), of which pinyinto- Chinese (PTC) conversion is the core part. Meanwhile, though typos are inevitable during user pinyin inputting, existing IMEs paid little attention to such big inconvenience. In this paper, motivated by a key equivalence of two decoding algorithms, we propose a joint graph model to globally optimize PTC and typo correction for IME. The evaluation results show that the proposed method outperforms both existing academic and commercial IMEs. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Chinese language processing; Core part; Decoding algorithm; Evaluation results; Graph model; Input methods; Graph theory",2-s2.0-84906922603
"Kimelfeld B.","Database principles in Information Extraction",2014,"Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904320637&doi=10.1145%2f2594538.2594563&partnerID=40&md5=58f6a709de68e3b741d23ed8060fcc78","Information Extraction commonly refers to the task of populating a relational schema, having predefined underlying semantics, from textual content. This task is pervasive in contemporary computational challenges associated with Big Data. This tutorial gives an overview of the algorithmic concepts and techniques used for performing Information Extraction tasks, and describes some of the declarative frameworks that provide abstractions and infrastructure for programming extractors. In addition, the tutorial highlights opportunities for research impact through principles of data management, illustrates these opportunities through recent work, and proposes directions for future research. Copyright 2014 ACM.","Database inconsistency; Database repairs; Document spanners; Finite-state transducers; Information extraction; Prioritized repairs; Regular expressions","Information management; Information retrieval; Pattern matching; Semantics; Computational challenges; Database repairs; Document spanners; Finite state transducers; Regular expressions; Relational schemas; Research impacts; Textual content; Database systems",2-s2.0-84904320637
"Keskes I., Zitoune F.B., Belguith L.H.","Splitting Arabic texts into elementary discourse units",2014,"ACM Transactions on Asian Language Information Processing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903266739&doi=10.1145%2f2601401&partnerID=40&md5=782c4d691c5a28a55808a2fc69f3952f","In this article, we propose the first work that investigates the feasibility of Arabic discourse segmentation into elementary discourse units within the segmented discourse representation theory framework. We first describe our annotation scheme that defines a set of principles to guide the segmentation process. Two corpora have been annotated according to this scheme: elementary school textbooks and newspaper documents extracted from the syntactically annotated Arabic Treebank. Then, we propose a multiclass supervised learning approach that predicts nested units. Our approach uses a combination of punctuation, morphological, lexical, and shallow syntactic features. We investigate how each feature contributes to the learning process.We show that an extensive morphological analysis is crucial to achieve good results in both corpora. In addition, we show that adding chunks does not boost the performance of our system. © 2014 ACM.","Arabic language; Discourse segmentation; Elementary discourse units","Linguistics; Arabic languages; Discourse representation theory; Discourse segmentation; Elementary discourse units; Morphological analysis; Segmentation process; Supervised learning approaches; Syntactic features; Computer science",2-s2.0-84903266739
"Van Der Storm T., Cook W.R., Loh A.","The design and implementation of Object Grammars",2014,"Science of Computer Programming",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908315996&doi=10.1016%2fj.scico.2014.02.023&partnerID=40&md5=56abe35990dd4d622ccde3a4597b7f87","An Object Grammar is a variation on traditional BNF grammars, where the notation is extended to support declarative bidirectional mappings between text and object graphs. The two directions for interpreting Object Grammars are parsing and formatting. Parsing transforms text into an object graph by recognizing syntactic features and creating the corresponding object structure. In the reverse direction, formatting recognizes object graph features and generates an appropriate textual presentation. The key to Object Grammars is the expressive power of the mapping, which decouples the syntactic structure from the graph structure. To handle graphs, Object Grammars support declarative annotations for resolving textual names that refer to arbitrary objects in the graph structure. Predicates on the semantic structure provide additional control over the mapping. Furthermore, Object Grammars are compositional so that languages may be defined in a modular fashion. We have implemented our approach to Object Grammars as one of the foundations of the Enso¯ system and illustrate the utility of our approach by showing how it enables definition and composition of domain-specific languages (DSLs). © 2014 Elsevier B.V.","Domain-specific languages; Language composition; Model-driven development; Syntax definition","Design and implementations; Domain specific languages; Language compositions; Model driven development; Syntax definition",2-s2.0-84908315996
"Saluja A., Zhang Y.","Online discriminative learning for machine translation with binary-valued feedback",2014,"Machine Translation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911957864&doi=10.1007%2fs10590-014-9154-z&partnerID=40&md5=e9352acd6946c3112a0b044707a2a45b","Viewing machine translation (MT) as a structured classification problem has provided a gateway for a host of structured prediction techniques to enter the field. In particular, large-margin methods for discriminative training of feature weights, such as the structured perceptron or MIRA, have started to match or exceed the performance of existing methods such as MERT. One issue with these problems in general is the difficulty in obtaining fully structured labels, e.g. in MT, obtaining reference translations or parallel sentence corpora for arbitrary language pairs. Another issue, more specific to the translation domain, is the difficulty in online training and updating of MT systems, since existing methods often require bilingual knowledge to correct translation outputs online. The problem is an important one, especially with the usage of MT in the mobile domain: in the process of translating user inputs, these systems can also receive feedback from the user on the quality of the translations produced. We propose a solution to these two problems, by demonstrating a principled way to incorporate binary-labeled feedback (i.e. feedback on whether a translation hypothesis is a “good” or understandable one or not), a form of supervision that can be easily integrated in an online and monolingual manner, into an MT framework. Experimental results on Chinese–English and Arabic–English corpora for both sparse and dense feature sets show marked improvements by incorporating binary feedback on unseen test data, with gains in some cases exceeding 5.5 BLEU points. Experiments with human evaluators providing feedback present reasonable correspondence with the larger-scale, synthetic experiments and underline the relative ease by which binary feedback for translation hypotheses can be collected, in comparison to parallel data. © 2014, Springer Science+Business Media Dordrecht.","Binary feedback; Machine translation; Online learning; Weak learning","Computational linguistics; Computer aided language translation; Experiments; Online systems; Social networking (online); Binary feedback; Discriminative training; Machine translations; Online discriminative learning; Online learning; Structured prediction; Synthetic experiments; Weak learning; E-learning",2-s2.0-84911957864
"Chen Y.-H., Lu E.J.-L., Tsai M.F.","Finding keywords in blogs: Efficient keyword extraction in blog mining via user behaviors",2014,"Expert Systems with Applications",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885952490&doi=10.1016%2fj.eswa.2013.07.091&partnerID=40&md5=abaa202044b742c7583fd1fd8d00adb2","Readers are becoming accustomed to obtaining useful and reliable information from bloggers. To make access to the vastly increasing resource of blogs more effective, clustering is useful. Results of the literature review suggest that using linking information, keywords, or tags/categories to calculate similarity is critical for clustering. Keywords are commonly retrieved from the full text, which can be a time-consuming task if multiple articles must be processed. For tags/categories, there is also a problem of ambiguity; that is, different bloggers may define tags/categories of identical content differently. Keywords are important not only to reflect the theme of an article through blog readers' perspectives but also to accurately match users' intentions. In this paper, a tracing code is embedded in Blog Connect, a newly developed platform, to collect the keywords queried by readers and then select candidate keywords as co-keywords. The experiments show positive data to confirm that co-keywords can act as a quick path to an article. In addition, co-keyword generation can reduce the complexity and redundancy of full-text keyword retrieval procedures and satisfy blog readers' intentions. © 2013 Elsevier Ltd. All rights reserved.","Blog Connect; Blog mining; Co-keyword; Full-text keyword retrieval procedure; User intention","Blog Connect; Blog mining; Co-keyword; Keyword retrieval; User intention; Behavioral research; Information retrieval; Blogs",2-s2.0-84885952490
"Angeli G., Manning C.D.","NaturalLI: Natural logic inference for common sense reasoning",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926031348&partnerID=40&md5=3d1575164f6b1f9c4afae0765692375c","Common-sense reasoning is important for AI applications, both in NLP and many vision and robotics tasks. We propose NaturalLI: A Natural Logic inference system for inferring common sense facts - for instance, that cats have tails or tomatoes are round - from a very large database of known facts. In addition to being able to provide strictly valid derivations, the system is also able to produce derivations which are only likely valid, accompanied by an associated confidence. We both show that our system is able to capture strict Natural Logic inferences on the Fra- CaS test suite, and demonstrate its ability to predict common sense facts with 49% recall and 91% precision. © 2014 Association for Computational Linguistics.",,"Computer vision; AI applications; Common sense; Commonsense reasoning; Natural logic; Very large database; Natural language processing systems",2-s2.0-84926031348
"Gordon M., Harel D.","Steps towards scenario-based programming with a natural language interface",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904817273&doi=10.1007%2f978-3-642-54848-2_9&partnerID=40&md5=b7b3e02c4a6ffa59d4ddaea1f13243b7","Programming, i.e., the act of creating a runnable artifact applicable to multiple inputs/tasks, is an art that requires substantial knowledge of programming languages and development techniques. As the use of software is becoming far more prevalent in all aspects of life, programming has changed and the need to program has become relevant to a much broader community. In the interest of broadening the pool of potential programmers, we believe that a natural language interface to an intuitive programming language may have a major role to play. In this paper, we discuss recent work on carrying out scenario-based programming directly in a controlled natural language, and sketch possible future directions. © 2014 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Computer science; Computers; Controlled natural language; Development technique; Intuitive programming; Multiple inputs; Natural language interfaces; Possible futures; Scenario-based programming; Natural language processing systems",2-s2.0-84904817273
"Stevenson A., Cordy J.R.","A survey of grammatical inference in software engineering",2014,"Science of Computer Programming",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908351504&doi=10.1016%2fj.scico.2014.05.008&partnerID=40&md5=f29dd29376c07187ba6dcab732ad3210","Grammatical inference - used successfully in a variety of fields such as pattern recognition, computational biology and natural language processing - is the process of automatically inferring a grammar by examining the sentences of an unknown language. Software engineering can also benefit from grammatical inference. Unlike these other fields, which use grammars as a convenient tool to model naturally occurring patterns, software engineering treats grammars as first-class objects typically created and maintained for a specific purpose by human designers. We introduce the theory of grammatical inference and review the state of the art as it relates to software engineering. © 2014 Elsevier B.V.","Grammar induction; Grammatical inference; Software engineering","Grammar induction; Grammatical inferences; Software engineering",2-s2.0-84908351504
"Teixeira A., Ferreira L., Rodrigues M.","Online health information semantic search and exploration: Reporting on two prototypes for performing information extraction on both a hospital intranet and the world wide web",2014,"Text Mining of Web-Based Medical Content",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978452962&partnerID=40&md5=afd7d9154389bcb50291ca0e66a52d96","In this chapter, we apply ontology-based information extraction to unstructured natural language sources to help enable semantic search of health information. We propose a general architecture capable of handling both private and public data. Two of our novel systems that are based on this architecture are presented here. The first system, MedInX, is a Medical Information eXtraction system which processes textual clinical discharge records, performing automatic and accurate mapping of free text reports onto a structured representation. MedInX is designed to be used by health professionals, and by hospital administrators and managers, allowing its users to search the contents of such automatically populated ontologies. The second system, SPHInX, attempts to perform semantic search on health information publicly available on the web in Portuguese. The potential of the proposed approach is clearly shown with usage examples and evaluation results. © 2014 Walter de Gruyter Inc., Boston/Berlin. All rights reserved.",,,2-s2.0-84978452962
"Justo R., Corcoran T., Lukin S.M., Walker M., Torres M.I.","Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web",2014,"Knowledge-Based Systems",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924613190&doi=10.1016%2fj.knosys.2014.05.021&partnerID=40&md5=e23b994d75be9b678978ca15086dec65","Automatic detection of emotions like sarcasm or nastiness in online written conversation is a difficult task. It requires a system that can manage some kind of knowledge to interpret that emotional language is being used. In this work, we try to provide this knowledge to the system by considering alternative sets of features obtained according to different criteria. We test a range of different feature sets using two different classifiers. Our results show that the sarcasm detection task benefits from the inclusion of linguistic and semantic information sources, while nasty language is more easily detected using only a set of surface patterns or indicators. © 2014 Elsevier B.V. All rights reserved.","Emotional language; Feature extraction; Nastiness; Sarcasm; Social web","Feature extraction; Semantics; Social networking (online); Automatic detection of emotion; Detection tasks; Emotional language; Nastiness; Sarcasm; Semantic information; Sets of features; Social webs; Computational linguistics",2-s2.0-84924613190
"Gobbel Dr G.T., Garvin J., Reeves R., Cronin R.M., Heavirland J., Williams J., Weaver A., Jayaramaraja S., Giuse D., Speroff T., Brown S.H., Xu H., Matheny M.E.","Assisted annotation of medical free text using RapTAT",2014,"Journal of the American Medical Informatics Association",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906322293&doi=10.1136%2famiajnl-2013-002255&partnerID=40&md5=5ab927f86f266f11231e8ff41fff45e2","Objective: To determine whether assisted annotation using interactive training can reduce the time required to annotate a clinical document corpus without introducing bias. Materials and methods: A tool, RapTAT, was designed to assist annotation by iteratively pre-annotating probable phrases of interest within a document, presenting the annotations to a reviewer for correction, and then using the corrected annotations for further machine learning-based training before preannotating subsequent documents. Annotators reviewed 404 clinical notes either manually or using RapTAT assistance for concepts related to quality of care during heart failure treatment. Notes were divided into 20 batches of 19-21 documents for iterative annotation and training. Results: The number of correct RapTAT pre-annotations increased significantly and annotation time per batch decreased by ~50% over the course of annotation. Annotation rate increased from batch to batch for assisted but not manual reviewers. Pre-annotation F-measure increased from 0.5 to 0.6 to >0.80 (relative to both assisted reviewer and reference annotations) over the first three batches and more slowly thereafter. Overall inter-annotator agreement was significantly higher between RapTAT-assisted reviewers (0.89) than between manual reviewers (0.85). Discussion: The tool reduced workload by decreasing the number of annotations needing to be added and helping reviewers to annotate at an increased rate. Agreement between the pre-annotations and reference standard, and agreement between the pre-annotations and assisted annotations, were similar throughout the annotation process, which suggests that pre-annotation did not introduce bias. Conclusions: Pre-annotations generated by a tool capable of interactive training can reduce the time required to create an annotated document corpus by up to 50%.",,"article; automation; health care quality; heart failure; human; information processing; machine learning; natural language processing; rapid text annotation tool; Clinical Informatics; Guideline Adherence; Heart Failure; Medical Informatics Computing; Natural Language Processing; Artificial Intelligence; Electronic Health Records; Heart Failure; Humans; Natural Language Processing",2-s2.0-84906322293
"Gatto L., Christoforou A.","Using R and bioconductor for proteomics data analysis",2014,"Biochimica et Biophysica Acta - Proteins and Proteomics",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890569154&doi=10.1016%2fj.bbapap.2013.04.032&partnerID=40&md5=411f6c44a546d9f339f992f9738189f4","This review presents how R, the popular statistical environment and programming language, can be used in the frame of proteomics data analysis. A short introduction to R is given, with special emphasis on some of the features that make R and its add-on packages premium software for sound and reproducible data analysis. The reader is also advised on how to find relevant R software for proteomics. Several use cases are then presented, illustrating data input/output, quality control, quantitative proteomics and data analysis. Detailed code and additional links to extensive documentation are available in the freely available companion package RforProteomics. This article is part of a Special Issue entitled: Computational Proteomics in the Post-Identification Era. Guest Editors: Martin Eisenacher and Christian Stephan. © 2013 Elsevier B.V.","Data analysis statistics; Mass spectrometry; Quality control; Quantitative proteomics; Software","computer language; computer program; data analysis; data processing; human; mass spectrometry; priority journal; proteomics; quality control; quantitative analysis; review; search engine; tandem mass spectrometry; ultra performance liquid chromatography; Data analysis statistics; Mass spectrometry; Quality control; Quantitative proteomics; Software; Amino Acid Sequence; Mass Spectrometry; Molecular Sequence Data; Phosphopyruvate Hydratase; Programming Languages; Proteomics; Quality Control",2-s2.0-84890569154
"Vulić I., Moens M.-F.","Probabilistic models of cross-lingual semantic similarity in context based on latent cross-lingual concepts induced from comparable data",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961290701&partnerID=40&md5=fbcbb19be63c46318fc777da0502c234","We propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts (e.g., crosslingual topics obtained by a multilingual topic model). These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources. Word meaning is represented as a probability distribution over the latent concepts, and a change in meaning is represented as a change in the distribution over these latent concepts. We present new models that modulate the isolated out-ofcontext word representations with contextual knowledge. Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of crosslingual semantic similarity. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Probability distributions; Semantics; Comparable corpora; Contextual knowledge; Lexical resources; Probabilistic approaches; Probabilistic models; Semantic similarity; Word representations; Word translation; Natural language processing systems",2-s2.0-84961290701
"Crossley S.A., Allen L.K., Kyle K., McNamara D.S.","Analyzing Discourse Processing Using a Simple Natural Language Processing Tool",2014,"Discourse Processes",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903648580&doi=10.1080%2f0163853X.2014.910723&partnerID=40&md5=b3e693458e2f7c35515d6d281d1e50b3","Natural language processing (NLP) provides a powerful approach for discourse processing researchers. However, there remains a notable degree of hesitation by some researchers to consider using NLP, at least on their own. The purpose of this article is to introduce and make available a simple NLP (SiNLP) tool. The overarching goal of the article is to proliferate the use of NLP in discourse processing research. The article also provides an instantiation and empirical evaluation of the linguistic features measured by SiNLP to demonstrate their strength in investigating constructs of interest to the discourse processing community. Although relatively simple, the results of this analysis reveal that the tool is quite powerful, performing on par with a sophisticated text analysis tool, Coh-Metrix, on a common discourse processing task (i.e., predicting essay scores). Such a tool could prove useful to researchers interested in investigating features of language that affect discourse production and comprehension. © 2014 © Taylor & Francis Group, LLC.",,,2-s2.0-84903648580
"Reniers D., Voinea L., Ersoy O., Telea A.","The Solid* toolset for software visual analytics of program structure and metrics comprehension: From research prototype to product",2014,"Science of Computer Programming",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885632447&doi=10.1016%2fj.scico.2012.05.002&partnerID=40&md5=eb26c001ce39c3f602eb4f9d6616afba","Software visual analytics (SVA) tools combine static program analysis and fact extraction with information visualization to support program comprehension. However, building efficient and effective SVA tools is highly challenging, as it involves extensive software development in program analysis, graphics, information visualization, and interaction. We present a SVA toolset for software maintenance, and detail two of its components which target software structure, metrics and code duplication. We illustrate the toolset's usage for constructing software visualizations with examples in education, research, and industrial contexts. We discuss the design evolution from research prototypes to integrated, scalable, and easy-to-use products, and present several guidelines for the development of efficient and effective SVA solutions. © 2011 Elsevier B.V. All rights reserved.","Software visualization; Static analysis; Visual tool design","Easy-to-use products; Information visualization; Program structures; Research prototype; Software structures; Software visualization; Static program analysis; Visual tools; Industrial research; Information analysis; Information systems; Software engineering; Tools; Visualization; Static analysis",2-s2.0-84885632447
"Sampathkumar H., Chen X.-W., Luo B.","Mining Adverse Drug Reactions from online healthcare forums using Hidden Markov Model",2014,"BMC Medical Informatics and Decision Making",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928798747&doi=10.1186%2f1472-6947-14-91&partnerID=40&md5=e812e477f90943ba50f9156e87a76710","Background: Adverse Drug Reactions are one of the leading causes of injury or death among patients undergoing medical treatments. Not all Adverse Drug Reactions are identified before a drug is made available in the market. Current post-marketing drug surveillance methods, which are based purely on voluntary spontaneous reports, are unable to provide the early indications necessary to prevent the occurrence of such injuries or fatalities. The objective of this research is to extract reports of adverse drug side-effects from messages in online healthcare forums and use them as early indicators to assist in post-marketing drug surveillance. Methods: We treat the task of extracting adverse side-effects of drugs from healthcare forum messages as a sequence labeling problem and present a Hidden Markov Model(HMM) based Text Mining system that can be used to classify a message as containing drug side-effect information and then extract the adverse side-effect mentions from it. A manually annotated dataset from www.medications.com is used in the training and validation of the HMM based Text Mining system. Results: A 10-fold cross-validation on the manually annotated dataset yielded on average an F-Score of 0.76 from the HMM Classifier, in comparison to 0.575 from the Baseline classifier. Without the Plain Text Filter component as a part of the Text Processing module, the F-Score of the HMM Classifier was reduced to 0.378 on average, while absence of the HTML Filter component was found to have no impact. Reducing the Drug names dictionary size by half, on average reduced the F-Score of the HMM Classifier to 0.359, while a similar reduction to the side-effects dictionary yielded an F-Score of 0.651 on average. Adverse side-effects mined from www.medications.com and www.steadyhealth.com were found to match the Adverse Drug Reactions on the Drug Package Labels of several drugs. In addition, some novel adverse side-effects, which can be potential Adverse Drug Reactions, were also identified. Conclusions: The results from the HMM based Text Miner are encouraging to pursue further enhancements to this approach. The mined novel side-effects can act as early indicators for health authorities to help focus their efforts in post-marketing drug surveillance. © 2014 Sampathkumar et al.; licensee BioMed Central Ltd.","Adverse Drug Reaction; Hidden Markov Model; Machine learning; Online healthcare forums; Pharmacovigilance; Text Mining",,2-s2.0-84928798747
"Peled A.","Traversing digital babel: Information, E-government, and exchange",2014,"Traversing Digital Babel: Information, E-Government, and Exchange",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944902315&partnerID=40&md5=8652744bf72c3c9b947c55f6463da15b","The computer systems of government agencies are notoriously complex. New technologies are piled on older technologies, creating layers that call to mind an archaeological dig. Obsolete programming languages and closed mainframe designs offer barriers to integration with other agency systems. Worldwide, these unwieldy systems waste billions of dollars, keep citizens from receiving services, and even -- as seen in interoperability failures on 9/11 and during Hurricane Katrina -- cost lives. In this book, Alon Peled offers a groundbreaking approach for enabling information sharing among public sector agencies: Using selective incentives to “nudge” agencies to exchange information assets. Peled proposes the establishment of a Public Sector Information Exchange (PSIE), through which agencies would trade information. After describing public sector information sharing failures and the advantages of incentivized sharing, Peled examines the U.S. Open Data program, and the gap between its rhetoric and results. He offers examples of creative public sector information sharing in the United States, Australia, Brazil, the Netherlands, and Iceland. Peled argues that information is a contested commodity, and draws lessons from the trade histories of other contested commodities -- including cadavers for anatomical dissection in nineteenth-century Britain. He explains how agencies can exchange information as a contested commodity through a PSIE program tailored to an individual country’s needs, and he describes the legal, economic, and technical foundations of such a program. Touching on issues from data ownership to freedom of information, Peled offers pragmatic advice to politicians, bureaucrats, technologists, and citizens for revitalizing critical information flows. © 2014 Massachusetts Institute of Technology. All rights reserved.",,,2-s2.0-84944902315
"Mayer G., Jones A.R., Binz P.-A., Deutsch E.W., Orchard S., Montecchi-Palazzi L., Vizcaíno J.A., Hermjakob H., Oveillero D., Julian R., Stephan C., Meyer H.E., Eisenacher M.","Controlled vocabularies and ontologies in proteomics: Overview, principles and practice",2014,"Biochimica et Biophysica Acta - Proteins and Proteomics",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890429234&doi=10.1016%2fj.bbapap.2013.02.017&partnerID=40&md5=9b70f69567c2e274f82d01675bb2c552","This paper focuses on the use of controlled vocabularies (CVs) and ontologies especially in the area of proteomics, primarily related to the work of the Proteomics Standards Initiative (PSI). It describes the relevant proteomics standard formats and the ontologies used within them. Software and tools for working with these ontology files are also discussed. The article also examines the ""mapping files"" used to ensure correct controlled vocabulary terms that are placed within PSI standards and the fulfillment of the MIAPE (Minimum Information about a Proteomics Experiment) requirements. This article is part of a Special Issue entitled: Computational Proteomics in the Post-Identification Era. Guest Editors: Martin Eisenacher and Christian Stephan. © 2013 Elsevier B.V.","Controlled vocabularies; Ontologies in proteomics; Ontology editors and software; Ontology formats; Ontology maintenance; Proteomics data standards","computer program; controlled study; controlled vocabulary; information processing; mass spectrometry; priority journal; proteomics; proteomics standard initiative; review; American Society for Testing and Materials; American Standard Code for Information Interchange; Analytical Data Interchange format for Mass Spectrometry; Analytical Information Markup Language; ANDI-MS; AniML; API; Application Programming Interface; ASCII; ASTM; BRENDA (BRaunschweig ENzyme DAtabase) Tissue Ontology; BTO; ChEBI; Chemical Entities of Biological Interest; Controlled vocabularies; Controlled Vocabulary; CV; Description Logic; DL; EBI; European Bioinformatics Institute; eXtensible Stylesheet Language Transformation; HDF5; Hierarchical Data Format, version 5; Human Proteome Organisation-Proteomics Standards Initiative; HUPO-PSI; ICD; International Classification of Diseases; International Union for Pure and Applied Chemistry; IUPAC; JCAMP-DX; Joint Committee on Atomic and Molecular Physical data-Data eXchange format; MALDI; Mass Spectrometry; Matrix Assisted Laser Desorption Ionization; Medical Subject Headings; MeSH; MI; MIAPE; MIBBI; Minimal Information for Biological and Biomedical Investigations; Minimum Information About a Proteomics Experiment; MITAB; Molecular Interaction; Molecular Interactions TABular format; MS; National Center for Biomedical Ontology; National Center for Biotechnology Information; NCBI; NCBO; netCDF; Network Common Data Format; OBI; OBO; OLS; Ontologies in proteomics; Ontology editors and software; Ontology for Biomedical Investigations; Ontology formats; Ontology Lookup Service; Ontology maintenance; Open Biological and Biomedical Ontologies; OWL; PAR; PATO; Phenotype Attribute Trait Ontology; PRIDE; Protein Affinity Reagents; Proteomics data standards; PRoteomics IDEntifications database; RDF(S); Resource Description Framework (Schema); Selected Reaction Monitoring; SRM; TPP; Trans-Proteomic Pipeline; Uniform Resource Identifier; URI; Web Ontology Language; XSLT; YAFMS; Yet Another Format for Mass Spectrometry; Programming Languages; Proteomics; Software; Vocabulary, Controlled",2-s2.0-84890429234
"Yang J.-F., Yu Q.-B., Guan Y., Jiang Z.-P.","An overview of research on electronic medical record oriented named entity recognition and entity relation extraction",2014,"Zidonghua Xuebao/Acta Automatica Sinica",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907092930&doi=10.3724%2fSP.J.1004.2014.01537&partnerID=40&md5=b2b98e1c69e7993446773ccd7d54eca2","Electronic medical records (EMRs) are generated in the process of clinical treatments. Named entities and entity relations in EMRs reflect patients' health conditions and represent patients' personalized medical knowledge. Consequently, named entity recognition and entity relation extraction on EMR are important expansion of information extraction in the medical domain. In this paper, the language characteristic and structure features of EMR narratives are firstly discussed, and then general methods for named entity recognition and relation extraction are sketched out. Furthermore, this paper introduces and analyzes the tasks and corresponding methods for named entity recognition, entity assertion recognition and relation extraction of EMR in detail. Related shared evaluation tasks and annotated corpora as well as several important dictionaries and knowledge bases are also introduced. Finally, problems to be handled and future research directions are proposed. © 2014 Acta Automatica Sinica. All rights reserved.","Electronic medical record (EMR); Entity relation extraction; Named entity recognition; Shared task","Electronic medical record; Entity relation extractions; Named entity recognition; Shared task",2-s2.0-84907092930
"Richter K.-F., Winter S.","Landmarks: Giscience for intelligent services",2014,"Landmarks: Giscience for Intelligent Services",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954623107&doi=10.1007%2f978-3-319-05732-3&partnerID=40&md5=ab37cbe86b0ff4c5c47c3ae90f59637e","This book covers the latest research on landmarks in GIS, including practical applications. It addresses perceptual and cognitive aspects of natural and artificial cognitive systems, computational aspects with respect to identifying or selecting landmarks for various purposes, and communication aspects of human-computer interaction for spatial information provision. Concise and organized, the book equips readers to handle complex conceptual aspects of trying to define and formally model these situations. The book provides a thorough review of the cognitive, conceptual, computational and communication aspects of GIS landmarks. This review is unique for comparing concepts across a spectrum of sub-disciplines in the field. Portions of the ideas discussed led to the world’s first commercial navigation service using landmarks selected with cognitive principles. Landmarks: GI Science for Intelligent Services targets practitioners and researchers working in geographic information science, computer science, information science, cognitive science, geography and psychology. Advanced-level students in computer science, geography and psychology will also find this book valuable as a secondary textbook or reference. © Springer International Publishing Switzerland 2014.",,"Geographic information systems; Human computer interaction; Reviews; Cognitive aspects; Cognitive principles; Cognitive science; Computational aspects; Geographic information science; Intelligent Services; Navigation service; Spatial informations; Cognitive systems",2-s2.0-84954623107
"Shortliffe E.H., Cimino J.J.","Biomedical informatics: Computer applications in health care and biomedicine: Fourth edition",2014,"Biomedical Informatics: Computer Applications in Health Care and Biomedicine: Fourth Edition",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028500234&doi=10.1007%2f978-1-4471-4474-8&partnerID=40&md5=430ff219efd776c9e545c4b001bfe398","The practice of modern medicine and biomedical research requires sophisticated information technologies with which to manage patient information, plan diagnostic procedures, interpret laboratory results, and carry out investigations. Biomedical Informatics provides both a conceptual framework and a practical inspiration for this swiftly emerging scientific discipline at the intersection of computer science, decision science, information science, cognitive science, and biomedicine. Now revised and in its third edition, this text meets the growing demand by practitioners, researchers, and students for a comprehensive introduction to key topics in the field. Authored by leaders in medical informatics and extensively tested in their courses, the chapters in this volume constitute an effective textbook for students of medical informatics and its areas of application. The book is also a useful reference work for individual readers needing to understand the role that computers can play in the provision of clinical services and the pursuit of biological questions. The volume is organized so as first to explain basic concepts and then to illustrate them with specific systems and technologies. © Springer-Verlag London 2014. All rights reserved.",,,2-s2.0-85028500234
"Vanderbauwhede W., Frolov A., Azzopardi L., Chalamalasetti S.R., Margala M.","High throughput filtering using FPGA-acceleration",2013,"International Conference on Information and Knowledge Management, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889560303&doi=10.1145%2f2505515.2507866&partnerID=40&md5=8e167b94ceb117bcd18e0de59d92577c","With the rise in the amount information of being streamed across networks, there is a growing demand to vet the quality, type and content itself for various purposes such as spam, security and search. In this paper, we develop an energy-efficient high performance information filtering system that is capable of classifying a stream of incoming document at high speed. The prototype parses a stream of documents using a multicore CPU and then performs classification using Field-Programmable Gate Arrays (FPGAs). On a large TREC data collection, we implemented a Naive Bayes classifier on our prototype and compared it to an optimized CPU based-baseline. Our empirical findings show that we can classify documents at 10Gb/s which is up to 94 times faster than the CPU baseline (and up to 5 times faster than previous FPGA based implementations). In future work, we aim to increase the throughput by another order of magnitude by implementing both the parser and filter on the FPGA. Copyright 2013 ACM.","Classification; Efficiency; Filtering; FPGA; Parsing","Data collection; Empirical findings; Energy efficient; FPGA-based implementation; High throughput; Information filtering system; Naive Bayes classifiers; Parsing; Classification (of information); Efficiency; Field programmable gate arrays (FPGA); Filtration; Knowledge management; Information retrieval systems",2-s2.0-84889560303
"Garcia T., Wang T.","Analysis of big data technologies and method - Query large web public RDF datasets on amazon cloud using hadoop and open source parsers",2013,"Proceedings - 2013 IEEE 7th International Conference on Semantic Computing, ICSC 2013",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893969815&doi=10.1109%2fICSC.2013.49&partnerID=40&md5=72e0572179023c447e69e2eff81eac63","Extremely large datasets found in Big Data projects are difficult to work with using conventional databases, statistical software, and visualization tools. Massively parallel software, such as Hadoop, running on tens, hundreds, or even thousands of servers is more suitable for Big Data challenges. Additionally, in order to achieve the highest performance when querying large datasets, it is necessary to work these datasets at rest without preprocessing or moving them into a repository. Therefore, this work will analyze tools and techniques to overcome working with large datasets at rest. Parsing and querying will be done on the raw dataset - the untouched Web Data Commons RDF files. Web Data Commons comprises five billion pages of web pages crawled from the Internet. This work will analyze available tools and appropriate methods to assist the Big Data developer in working with these extremely large, semantic RDF datasets. Hadoop, open source parsers, and Amazon Cloud services will be used to data mine these files. In order to assist in further discovery, recommendations for future research will be included. © 2013 IEEE.","Amazon cloud computing; Any23; Big data; Hadoo; Jena; Map/Reduc; NXParser; Open source software; RDF; Semantic Web",,2-s2.0-84893969815
"Liu J., Pasupat P., Wang Y., Cyphers S., Glass J.","Query understanding enhanced by hierarchical parsing structures",2013,"2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013 - Proceedings",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893706915&doi=10.1109%2fASRU.2013.6707708&partnerID=40&md5=7bce951490f3c0716736e3661c17a463","Query understanding has been well studied in the areas of information retrieval and spoken language understanding (SLU). There are generally three layers of query understanding: domain classification, user intent detection, and semantic tagging. Classifiers can be applied to domain and intent detection in real systems, and semantic tagging (or slot filling) is commonly defined as a sequence-labeling task - mapping a sequence of words to a sequence of labels. Various statistical features (e.g., n-grams) can be extracted from annotated queries for learning label prediction models; however, linguistic characteristics of queries, such as hierarchical structures and semantic relationships, are usually neglected in the feature extraction process. In this work, we propose an approach that leverages linguistic knowledge encoded in hierarchical parse trees for query understanding. Specifically, for natural language queries, we extract a set of syntactic structural features and semantic dependency features from query parse trees to enhance inference model learning. Experiments on real natural language queries show that augmenting sequence labeling models with linguistic knowledge can improve query understanding performance in various domains. © 2013 IEEE.","linguistic parsing; query understanding; semantic tagging","Hierarchical structures; Linguistic parsing; Natural language queries; query understanding; Semantic relationships; Semantic tagging; Spoken language understanding; Statistical features; Feature extraction; Forestry; Linguistics; Natural language processing systems; Semantics; Classification (of information); Classification; Data Processing; Forestry; Languages",2-s2.0-84893706915
"Yu M., Zhao T., Bai Y.","Learning domain differences automatically for dependency parsing adaptation",2013,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896061068&partnerID=40&md5=1540435e5e05b8b9429a8b7466ab0a8f","In this paper, we address the relation between domain differences and domain adaptation for dependency parsing. Our quantitative analyses showed that it is the inconsistent behavior of same features cross-domain, rather than word or feature coverage, that is the major cause of performances decrease of out-domain model. We further studied those ambiguous features in depth and found that the set of ambiguous features is small and has concentric distributions. Based on the analyses, we proposed a DA method. The DA method can automatically learn which features are ambiguous cross domain according to errors made by out-domain model on in-domain training data. Our method is also extended to utilize multiple out-domain models. The results of dependency parser adaptation from WSJ to Genia and Question bank showed that our method achieved significant improvements on small in-domain datasets where DA is mostly in need. Additionally, we achieved improvement on the published best results of CoNLL07 shared task on domain adaptation, which confirms the significance of our analyses and our method.",,"Cross-domain; Dependency parser; Dependency parsing; Domain adaptation; Domain differences; Question banks; Training data; Artificial intelligence; Query languages",2-s2.0-84896061068
"Aït-Kaci H.","An abstract, reusable, and extensible programming language design architecture",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892759133&doi=10.1007%2f978-3-642-41660-6-6&partnerID=40&md5=5bb77b7027e139584354ba0390d47692","There are a few basic computational concepts that are at the core of all programming languages. The exact elements making out such a set of concepts determine (1) the specific nature of the computational services such a language is designed for, (2) for what users it is intended, and (3) on what devices and in what environment it is to be used. It is therefore possible to propose a set of basic building blocks and operations thereon as combination procedures to enable programming software by specifying desired tasks using a tool-box of generic constructs and meta-operations. Syntax specified through LALR(k) grammar technology can be enhanced with greater recognizing power thanks to a simple augmentation of yacc technology. Upon this basis, a set of implementable formal operational semantics constructs may be simply designed and generated (syntax and semantics) à la carte, by simple combination of its desired features. The work presented here, and the tools derived from it, may be viewed as a tool box for generating language implementations with a desired set of features. It eases the automatic practical generation of programming language pioneered by Peter Landin's SECD Machine. What is overviewed constitutes a practical computational algebra extending the polymorphically typed λ-Calculus with object/classes and monoid comprehensions. This paper describes a few of the most salient parts of such a system, stressing most specifically any innovative features - formal syntax and semantics. It may be viewed as a high-level tour of a few reusable programming language design techniques prototyped in the form of a set of composable abstract machine constructs and operations. © 2013 Springer-Verlag Berlin Heidelberg.","λ-Calculus; Abstract Machines; Bottom-up Parsing; Declarative Collections; Denotational Semantics; Intermediate Language; LALR Parser Generation; Monoid Comprehensions; Object-Oriented Programming; Operational Semantics; Polymorphic Types; Programming Language Design; Static/Dynamic Type Checking/Inference","Abstract machines; Bottom-up Parsing; Declarative Collections; Denotational semantics; Intermediate languages; Monoid Comprehensions; Operational semantics; Parser generation; Polymorphic types; Programming language design; Typechecking; Algebra; Calculations; Functional programming; Object oriented programming; Semantics; Syntactics; Tools; Computer programming languages",2-s2.0-84892759133
"Buccoli M., Zanoni M., Sarti A., Tubaro S.","A music search engine based on semantic text-based query",2013,"2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892538770&doi=10.1109%2fMMSP.2013.6659297&partnerID=40&md5=08915ab1153a3b1a3b4c64082a76efb8","Search and retrieval of songs from a large music repository usually relies on added meta-information (e.g., title, artist or musical genre); or on specific descriptors (e.g. mood); or on categorical music descriptors; none of which can specify the desired intensity. In this work, we propose an early example of semantic text-based music search engine. The semantic description takes into account emotional and non-emotional musical aspects. The method also includes a query-by-similarity search approach performed using semantic cues. We model both concepts and musical content in dimensional spaces that are suitable for carrying intensity information on the descriptors. We process the semantic query with a Natural Language parser to capture only the relevant words and qualifiers. We rely on Bayesian Decision theory to model concepts and songs as probability distributions. The resulted ranked list of songs are produced through a posterior probability model. A prototype of the system has been proposed to 53 subjects for evaluation, with good ratings on performance, usefulness and potential. © 2013 IEEE.",,"Bayesian decision theory; Dimensional spaces; Intensity information; Modeling concepts; Posterior probability; Search and retrieval; Semantic descriptions; Text-based queries; Multimedia signal processing; Probability distributions; Search engines; Semantics",2-s2.0-84892538770
"Mazzei A., Lesmo L., Battaglino C., Vendrame M., Bucciarelli M.","Deep natural Language processing for Italian Sign Language translation",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892700752&doi=10.1007%2f978-3-319-03524-6_17&partnerID=40&md5=b9c053b67ee961a7cb75a613f70a8b5e","This paper presents the architecture of a translator from written Italian into Italian Sign Language. We describe the main features of the four modules of this architecture, i.e. a dependency parser for Italian, an ontology based semantic interpreter, a generator based on expert-systems and combinatory categorial grammars, a planner to position signs in space. The result of this translation chain is signed by a virtual character. Finally, we report the results of a first ""intrinsic"" experiment for the evaluation of translation quality. © Springer International Publishing Switzerland 2013.",,"Combinatory categorial grammar; Dependency parser; NAtural language processing; Ontology-based; Sign language; Translation quality; Virtual character; Artificial intelligence; Natural language processing systems; Semantics; Translation (languages)",2-s2.0-84892700752
"Dror G., Maarek Y., Mejer A., Szpektor I.","From query to question in one click: Suggesting synthetic questions to searchers",2013,"WWW 2013 - Proceedings of the 22nd International Conference on World Wide Web",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893069140&partnerID=40&md5=4e179b0770515ffdc90bddbcbc53abd5","In Web search, users may remain unsatisfied for several reasons: the search engine may not be effective enough or the query might not reflect their intent. Years of research focused on providing the best user experience for the data available to the search engine. However, little has been done to address the cases in which relevant content for the specific user need has not been posted on the Web yet. One obvious solution is to directly ask other users to generate the missing content using Community Question Answering services such as Yahoo! Answers or Baidu Zhidao. However, formulating a full-fledged question after having issued a query requires some effort. Some previous work proposed to automatically generate natural language questions from a given query, but not for scenarios in which a searcher is presented with a list of questions to choose from. We propose here to generate synthetic questions that can actually be clicked by the searcher so as to be directly posted as questions on a Community Question Answering service. This imposes new constraints, as questions will be actually shown to searchers, who will not appreciate an awkward style or redundancy. To this end, we introduce a learning-based approach that improves not only the relevance of the suggested questions to the original query, but also their grammatical correctness. In addition, since queries are often underspecified and ambiguous, we put a special emphasis on increasing the diversity of suggestions via a novel diversification mechanism. We conducted several experiments to evaluate our approach by comparing it to prior work. The experiments show that our algorithm improves question quality by 14% over prior work and that adding diversification reduced redundancy by 55%.Copyright is held by the International World Wide Web Conference Committee (IW3C2).","Community-based question answering; Question generation","Community question answering; Community-based question answering; Learning-based approach; Natural language questions; Question generation; Reduced redundancy; User experience; Web searches; Experiments; Natural language processing systems; Search engines; Turbulent flow; World Wide Web",2-s2.0-84893069140
"Bnaya Z., Puzis R., Stern R., Felner A.","Bandit algorithms for social network queries",2013,"Proceedings - SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893563972&doi=10.1109%2fSocialCom.2013.29&partnerID=40&md5=5441e878ab8207755d306483674c9378","In many cases the best way to find a profile or a set of profiles matching some criteria in a social network is via targeted crawling. An important challenge in targeted crawling is to choose the next profile to explore. Existing heuristics for targeted crawling are usually tailored for specific search criterion and could lead to short-sighted crawling decisions. In this paper we propose and evaluate a generic approach for guiding a social network crawler that aims to provide a proper balance between exploration and exploitation based on the recently introduced variant of the Multi-Armed Bandit problem with volatile arms (VMAB). Our approach is general-purpose. In addition, it provides provable performance guarantees. Experimental results indicate that our approach compares favorably with the best existing heuristics on two different domains. © 2013 IEEE.",,"Different domains; Exploration and exploitation; Multi-armed bandit problem; Network crawlers; Performance guarantees; Profiles matching; Search criterion; Targeted Crawling; Social networking (online)",2-s2.0-84893563972
"Al-Aqrabi H., Liu L., Hill R., Cui L.","Faceted search in business intelligence on the cloud",2013,"Proceedings - 2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing, GreenCom-iThings-CPSCom 2013",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893445299&doi=10.1109%2fGreenCom-iThings-CPSCom.2013.148&partnerID=40&md5=ccf1b3ed8df190588516425f2f49344a","Faceted search is a new concept in search engines for implementing a feature of direct search queries using guided navigation among the search results. In this concept, the search results are grouped and ranked under facets that guide a user on the dimensions through which the search results can be viewed. It has been implemented on web based query systems by integrating with semantic search engines (like Google, Yahoo and Bing). However, there is a significant opportunity of implementing faceted search in business intelligence (BI) frameworks to include direct searching features through BI dashboards and custom reporting interfaces. In this paper, a technology positioning map for implementing faceted search in BI framework on cloud computing has been presented. BI on the cloud is based on massively parallel processing of hardware and database resources and an XML based service oriented architecture in which the data warehouses and OLAP cubes are formed using XML data files. The architecture has been expanded to include a DOM parser and a DTD mapping system that will parse the 2D XML views (pulled from cubes formed by many-to-many XML files) and extract the database fields to be stored in a facet repository as per pre-established metadata rules. Whenever an OLAP query is invoked by a user (using a decision map), a query coordinator will fetch the relevant 2D OLAP views and group them under facets fetched from the facet repository taking the services of a metadata coordinator. The user can make use of the facets to create direct queries, generate the targeted dashboards, and hence reduce searching time. © 2013 IEEE.","Business intelligence; Cloud computing; Data files; Faceted search; OLAP; OLAP cubes; XML; XML data warehouse; XML DOM parsing","Data files; Faceted search; OLAP; Olap cubes; Xml data warehouse; Cloud computing; Competitive intelligence; Data warehouses; Geometry; Information services; Internet; Metadata; Query processing; Search engines; Semantics; Service oriented architecture (SOA); XML",2-s2.0-84893445299
"Vera R., Perez-Riverol Y., Perez S., Ligeti B., Kertész-Farkas A., Pongor S.","JBioWH: An open-source Java framework for bioinformatics data integration",2013,"Database",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892744431&doi=10.1093%2fdatabase%2fbat051&partnerID=40&md5=82dcd51b0073ed176145e5abb8c1fc39","The Java BioWareHouse (JBioWH) project is an open-source platform-independent programming framework that allows a user to build his/her own integrated database from the most popular data sources. JBioWH can be used for intensive querying of multiple data sources and the creation of streamlined task-specific data sets on local PCs. JBioWH is based on a MySQL relational database scheme and includes JAVA API parser functions for retrieving data from 20 public databases (e.g. NCBI, KEGG, etc.). It also includes a client desktop application for (non-programmer) users to query data. In addition, JBioWH can be tailored for use in specific circumstances, including the handling of massive queries for high-throughput analyses or CPU intensive calculations. The framework is provided with complete documentation and application examples and it can be downloaded from the Project Web site at http://code.google.com/p/ jbiowh. A MySQL server is available for demonstration purposes at hydrax.icgeb.trieste.it:3307. © The Author(s) 2013. Published by Oxford University Press.",,,2-s2.0-84892744431
"Darmoni S.J., Soualmia L.F., Griffon N., Grosjean J., Kerdelhué G., Kergourlay I., Dahamna B.","Multi-lingual search engine to access pubmed monolingual subsets: A feasibility study",2013,"Studies in Health Technology and Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894353570&doi=10.3233%2f978-1-61499-289-9-966&partnerID=40&md5=8bc9e52e395484321125438b6991f7f6","PubMed contains many articles in languages other than English but it is difficult to find them using the English version of the Medical Subject Headings (MeSH) Thesaurus. The aim of this work is to propose a tool allowing access to a PubMed subset in one language, and to evaluate its performance. Translations of MeSH were enriched and gathered in the information system. PubMed subsets in main European languages were also added in our database, using a dedicated parser. The CISMeF generic semantic search engine was evaluated on the response time for simple queries. MeSH descriptors are currently available in 11 languages in the information system. All the 654,000 PubMed citations in French were integrated into CISMeF database. None of the response times exceed the threshold defined for usability (2 seconds). It is now possible to freely access biomedical literature in French using a tool in French; health professionals and lay people with a low English language may find it useful. It will be expended to several European languages: German, Spanish, Norwegian and Portuguese. © 2013 IMIA and IOS Press.","bibliographic; Databases; French language; Information storage and retrieval; PubMed; User-Computer interface","classification; computer interface; computer program; controlled vocabulary; data base; data mining; feasibility study; information retrieval; Medline; multilingualism; natural language processing; procedures; search engine; translating (language); Data Mining; Database Management Systems; Feasibility Studies; Information Storage and Retrieval; Multilingualism; Natural Language Processing; PubMed; Search Engine; Software; Translating; User-Computer Interface; Vocabulary, Controlled",2-s2.0-84894353570
"Saeidi A., Hage J., Khadka R., Jansen S.","Gelato: GEneric language tools for model-driven analysis of legacy software systems",2013,"Proceedings - Working Conference on Reverse Engineering, WCRE",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893353802&doi=10.1109%2fWCRE.2013.6671328&partnerID=40&md5=346a6c8910f900c485bd837c809afe8f","We present an integrated set of language-independent (generic) tools for analyzing legacy software systems: Gelato. Like any analysis tool, Gelato consists of a set of parsers, tree walkers, transformers, visualizers and pretty printers for different programming languages. Gelato is divided into a set of components, comprising of a set of language-specific bundles and a generic core. By providing a generic core, Gelato enables building tools for analyzing legacy systems independent of the languages they are implemented in. To achieve this, Gelato consists of a generic extensible imperative language called Kernel which provides a separation between syntactic and semantic analysis. We have adopted model-driven techniques to develop the Gelato tool set which is integrated into the Eclipse environment. © 2013 IEEE.",,"Analysis tools; Eclipse environment; Imperative languages; Language tools; Legacy software; Model-driven techniques; Pretty printers; Semantic analysis; Computer software; Legacy systems; Reverse engineering; Semantics; Tools",2-s2.0-84893353802
"Li P., Liu Y., Sun M.","An extended GHKM algorithm for inducing λ-SCFG",2013,"Proceedings of the 27th AAAI Conference on Artificial Intelligence, AAAI 2013",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893428206&partnerID=40&md5=8f1cc79e4ad5ac407270a8c091ff57f3","Semantic parsing, which aims at mapping a natural language (NL) sentence into its formal meaning representation (e.g., logical form), has received increasing attention in recent years. While synchronous context-free grammar (SCFG) augmented with lambda calculus (λ- SCFG) provides an effective mechanism for semantic parsing, how to learn such λ- SCFG rules still remains a challenge because of the difficulty in determining the correspondence between NL sentences and logical forms. To alleviate this structural divergence problem, we extend the GHKM algorithm, which is a state-ofthe- art algorithm for learning synchronous grammars in statistical machine translation, to induce λ- SCFG from pairs of NL sentences and logical forms. By treating logical forms as trees, we reformulate the theory behind GHKM that gives formal semantics to the alignment between NL words and logical form tokens. Experiments on the GEOQUERY dataset show that our semantic parser achieves an F-measure of 90:2%, the best result published to date. Copyright © 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Effective mechanisms; Formal Semantics; Natural languages; Semantic parsing; Statistical machine translation; Structural divergence; Synchronous context-free grammars; Synchronous grammars; Algorithms; Artificial intelligence; Differentiation (calculus); Semantics; Context free grammars",2-s2.0-84893428206
"Sidhu R.","High throughput, tree automata based XML processing using FPGAs",2013,"FPT 2013 - Proceedings of the 2013 International Conference on Field Programmable Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894199461&doi=10.1109%2fFPT.2013.6718333&partnerID=40&md5=a0ec0d2524305725a71a24bcceae1c3c","A novel and efficient approach to XML processing using FPGAs, based upon the sound theoretical formalism of tree automata, is presented. The approach enables the key tasks of schema validation and query to be performed in a unified manner. A remarkably simple implementation of a tree automaton in hardware, as a pair of interacting automata with the states of one forming the input to the other, is described. The implementation can process one XML token in at most two clock cycles. Also, the throughput is achieved for any schema grammar or query (that can be accommodated in the state tables) independent of its complexity. Further, use of tree automata offers greater expressive power for specifying schemas as well as queries than in previous hardware based approaches. Detailed performance evaluation demonstrates the significant throughput improvements of the proposed tree automata based approach compared with software as well as earlier FPGA based approaches. The implementation of XML schema validation on a mid-range FPGA provides sustained throughput from 1.7 to 3.1 Gbps, yielding a five to ten times speedup over an efficient software approach. Due to the very compact implementation, multiple instances can be utilized to further make significant improvements in throughput. © 2013 IEEE.",,,2-s2.0-84894199461
"Alhelbawy A., Gaizauskas R.","Named entity disambiguation using HMMs",2013,"Proceedings - 2013 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Workshops, WI-IATW 2013",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893240708&doi=10.1109%2fWI-IAT.2013.173&partnerID=40&md5=9dc955902cd77352923a0b9baec472e1","In this paper we present a novel approach to disambiguate textual mentions of named entities against the Wikipedia knowledge base. The conditional dependencies between different named entities across Wikipedia are represented as a Markov network. In our approach, named entities are treated as hidden variables and textual mentions as observations. The number of states and observations is huge and naively using the Viterbi algorithm to find the hidden state sequence that emits the query observation sequence is computationally infeasible, given a state space of this size. Based on an observation that is specific to the disambiguation problem, we propose an approach that uses a tailored approximation to reduce the size of the state space, making the Viterbi algorithm feasible. Results show good improvement in disambiguation accuracy relative to the baseline approach and to some state-of-the-art approaches. Also, our approach shows how, with suitable approximations, HMMs can be used in such large-scale state space problems. © 2013 IEEE.",,"Hidden variable; Markov networks; Named entities; Named entity disambiguations; Number of state; Space problem; State-of-the-art approach; Wikipedia knowledge; Intelligent agents; Knowledge based systems; Natural language processing systems",2-s2.0-84893240708
"Li C., Jimeno-Yepes A., Arregui M., Kirsch H., Rebholz-Schuhmann D.","PCorral - Interactive mining of protein interactions from MEDLINE",2013,"Database",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885940486&doi=10.1093%2fdatabase%2fbat030&partnerID=40&md5=f70632581cc0c9b2334cd3b6ff98722a","The extraction of information from the scientific literature is a complex task - for researchers doing manual curation and for automatic text processing solutions. The identification of protein-protein interactions (PPIs) requires the extraction of protein named entities and their relations. Semi-automatic interactive support is one approach to combine both solutions for efficient working processes to generate reliable database content. In principle, the extraction of PPIs can be achieved with different methods that can be combined to deliver high precision and/or high recall results in different combinations at the same time. Interactive use can be achieved, if the analytical methods are fast enough to process the retrieved documents. PCorral provides interactive mining of PPIs from the scientific literature allowing curators to skim MEDLINE for PPIs at low overheads. The keyword query to PCorral steers the selection of documents, and the subsequent text analysis generates high recall and high precision results for the curator. The underlying components of PCorral process the documents on-the-fly and are available, as well, as web service from the Whatizit infrastructure. The human interface summarizes the identified PPI results, and the involved entities are linked to relevant resources and databases. Altogether, PCorral serves curator at both the beginning and the end of the curation workflow for information retrieval and information extraction. © The Author(s) 2013. Published by Oxford University Press.",,"protein; article; computer program; data mining; human; linguistics; Medline; metabolism; methodology; protein protein interaction; search engine; workflow; Data Mining; Humans; MEDLINE; Protein Interaction Maps; Proteins; Search Engine; Software; Vocabulary; Workflow",2-s2.0-84885940486
"Grigorev S., Kirilenko I.","GLR-based abstract parsing",2013,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894600163&doi=10.1145%2f2556610.2556616&partnerID=40&md5=ed0b1bd2038eb516d1de8e500abcdfd3","Abstract parsing is an important step of the processing of dynamically constructed statements or string-embedded languages (such as embedded or dynamic SQL). Existing LALR-based algorithms have performance issues. To increase performance we propose to use a GLR-algorithm as a base for abstract parsing and to reuse graph-structured stack and shared packed parse forest. RNGLR-algorithm modification for abstract parsing is presented. © 2013 ACM.","abstract parsing; abstract translation; dynamic SQL; embedded languages; GLR; injected languages; parsing; RNGLR; string-embedded languages; two-staged languages",,2-s2.0-84894600163
"Li L., Wang Z., Cai T., Chou W.","Compose presence for composite web services by timed AND-OR tree",2013,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896872526&doi=10.1145%2f2539150.2539219&partnerID=40&md5=7859e06fcab6e69b5f2228a5f41a2768","Both presence and web service composition are very important to web based real-time communication systems. Although there is some work to combine presence with individual web services, there still lacks an efficient presence composition method to determine presence for composite web services, which is a collection of web services arranged by a process. This paper proposes a distributed computation model, called TAO-Tree (Timed AND-OR tree) to map discrete presence states to continuous time domain such that presence compositions can be carried recursively by arithmetic operations. Furthermore, the paper describes a method to translate the presence delays back to discrete presence states with minimal errors. We implement the TAO-Tree module in a prototype system and the experiments show the approach is feasible and satisfactory. © 2013 ACM.","AND-OR tree; Presence composition; presence delay; web service composition; WebRTC","And-or tree; Arithmetic operations; Composite Web services; Distributed computations; presence delay; Real-time communication system; Web service composition; WebRTC; Forestry; Information retrieval; Quality of service; Websites; Web services; Forestry; Information Retrieval",2-s2.0-84896872526
"Hassanzadeh K., Reformat M., Pedrycz W., Jamal I., Berezowski J.","T2R: System for converting textual documents into RDF triples",2013,"Proceedings - 2013 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Workshops, WI-IATW 2013",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893225770&doi=10.1109%2fWI-IAT.2013.187&partnerID=40&md5=4ce7da2cfb6af23423e228a329ef1da1","An important contribution of the Semantic Web is a new format of data representation called Resource Description Framework (RDF). In RDF, every piece of information is represented as a triple: subject-property-object. In general, subjects and objects can be shared between multiple RDF triples, and all triples can constitute a densely interlinked network. RDF becomes a very popular format of representing data on the web. As of September 2012, the last available data, more than 31 billions of triples exist on the web. In the paper, we propose a system - called T2R - for automatic acquisition of syntactic and semantic relations among terms from a plain text. These relations are expressed in the form of RDF triples. The proposed method is independent of any prior knowledge and domain specific patterns, and is applicable to any textual resources. The system implementing the approach is capable of identifying grammatical structure of an input sentence and analysing its semantics to generate meaningful RDF triples. We evaluate this approach by proving the quality of our results through case studies. © 2013 IEEE.",,"Automatic acquisition; Data representations; Domain specific; Grammatical structure; Prior knowledge; Resource description framework; Semantic relations; Textual documents; Intelligent agents; Semantic Web",2-s2.0-84893225770
"Boggiano-Castillo M.B., Toledo A.P., Pérez-Alonso A., González-González L.-M., Pérez-Vázquez R.","Automatic insertion of business rules in databases [Inserción automática de reglas de negocio en bases de datos]",2013,"Revista Tecnica de la Facultad de Ingenieria Universidad del Zulia",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894197897&partnerID=40&md5=e4e66d55ab04b3a11aea2791d780c828","One of the actual trends in the development of the information systems is the leading through the focus of the business rules. The business rules dealt with this work are the ones known as constraint rules and let to express constraints of integrity about data in a database. All these can involve several charts of a database and in order to create a rule, the more important item is to automatically find out which chart must be applied and which resources must be used to. In this article a method sort is proposed to sort out the problem related with the Constraint Rules when the database has been created. The examples used refer to just a part of an application to the renal transplantation.","Automatic implementation; Business database; Restriction business rules",,2-s2.0-84894197897
"Yuan N.J., Wang Y., Zhang F., Xie X., Sun G.","Reconstructing individual mobility from smart card transactions: A space alignment approach",2013,"Proceedings - IEEE International Conference on Data Mining, ICDM",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894655991&doi=10.1109%2fICDM.2013.37&partnerID=40&md5=b23b6a20e83553f3290b4ec18da65672","Smart card transactions capture rich information of human mobility and urban dynamics, therefore are of particular interest to urban planners and location-based service providers. However, since most transaction systems are only designated for billing purpose, typically, fine-grained location information, such as the exact boarding and alighting stops of a bus trip, is only partially or not available at all, which blocks deep exploitation of this rich and valuable data at individual level. This paper presents a ""space alignment"" framework to reconstruct individual mobility history from a large-scale smart card transaction dataset pertaining to a metropolitan city. Specifically, we show that by delicately aligning the monetary space and geospatial space with the temporal space, we are able to extrapolate a series of critical domain specific constraints. Later, these constraints are naturally incorporated into a semi-supervised conditional random field to infer the exact boarding and alighting stops of all transit routes with a surprisingly high accuracy, e.g., given only 10% trips with known alighting/boarding stops, we successfully inferred more than 78% alighting and boarding stops from all unlabeled trips. In addition, we demonstrated that the smart card data enriched by the proposed approach dramatically improved the performance of a conventional method for identifying users' home and work places (with 88% improvement on home detection and 35% improvement on work place detection). The proposed method offers the possibility to mine individual mobility from common public transit transactions, and showcases how uncertain data can be leveraged with domain knowledge and constraints, to support cross-application data mining tasks. © 2013 IEEE.",,,2-s2.0-84894655991
"Wason R., Ahmed P., Qasim Rafiq M.","Automata-based software reliability model: The key to reliable software",2013,"International Journal of Software Engineering and its Applications",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891814865&doi=10.14257%2fijseia.2013.7.6.10&partnerID=40&md5=c532cebdb39b5607d78dc80a12ace488","In this paper, we critically analyze the success of the traditional reliability models built to measure and estimate software reliability. We further propose that a Finite State Automata (FSA) based reliability model can serve as a befitting solution to all existing software reliability challenges. The proposed model estimates actual system reliability at runtime. The main advantage of this model is that it allows authentic or real-time reliability estimation, prediction and can also be trained towards dynamic learning of the evolving behavior of software, and fault tolerance. © 2013 SERSC.","Automata-Based Software Reliability Model; Finite State Automata; Finite State Machines (FSMs); Software Reliability",,2-s2.0-84891814865
"Batory D., Gonçalves R., Marker B., Siegmund J.","Dark knowledge and graph grammars in automated software design",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891317118&doi=10.1007%2f978-3-319-02654-1_1&partnerID=40&md5=81bf9b8f92f85c80c3d399f53489b7be","Mechanizing the development of hard-to-write and costly-to-maintain software is the core problem of automated software design. Encoding expert knowledge (a.k.a. dark knowledge) about a software domain is central to its solution. We assert that a solution can be cast in terms of the ideas of language design and engineering. Graph grammars can be a foundation for modern automated software development. The sentences of a grammar are designs of complex dataflow systems. We explain how graph grammars provide a framework to encode expert knowledge, produce correct-by-construction derivations of dataflow applications, enable the generation of high-performance code, and improve how software design of dataflow applications can be taught to undergraduates. © 2013 Springer International Publishing.",,"Core problems; Correct-by-construction; Dataflow; Expert knowledge; Graph grammar; Language design; Software domains; Automation; Computer programming languages; Context sensitive grammars; Encoding (symbols); Formal languages; Graph theory; Software design",2-s2.0-84891317118
"Cicek A.E., Qi X., Cakmak A., Johnson S.R., Han X., Alshalwi S., Ozsoyoglu G.","PathCase-MAW: An online metabolic network analysis workbench",2013,"2013 ACM Conference on Bioinformatics, Computational Biology and Biomedical Informatics, ACM-BCB 2013",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888143522&doi=10.1145%2f2506583.2506621&partnerID=40&md5=e2708db51bc8bb67f6e73a74221ddad4","Metabolic networks have become one of the centers of attention in life sciences research with the advancements in the metabolomics field. A vast array of studies analyzes metabolites and their interrelations to seek explanations for various biological questions, and numerous genome-scale metabolic networks have been assembled to serve for this purpose. The increasing focus on this topic comes with the need for software systems that store, query, browse, analyze, and visualize metabolic networks. PathCase Metabolomics Analysis Workbench (PathCaseMAW) is built, released, and running on a manually created generic mammalian metabolic network. The PathCaseMAW system provides a database-enabled framework and web-based computational tools for browsing, querying, analyzing, and visualizing stored metabolic networks. PathCaseMAW editor, with its user-friendly interface, can be used to create a new metabolic network and/or update an existing metabolic network. The network can also be created from an existing genome-scale reconstructed network using the PathCaseMAW SBML parser. The metabolic network can be accessed through a web interface or an iPad application. For metabolomics analysis, Steady-State Metabolic Network Dynamics Analysis (SMDA) algorithm is implemented and integrated with the system. SMDA tool is accessible through both the web-based interface and the iPad application for metabolomics analysis based on a metabolic profile. PathCaseMAW is a comprehensive system with various data input and data access sub-systems. It is easy to work with by design, and is a promising tool for metabolomics research and for educational purposes. Copyright © 2007 by the Association for Computing Machinery.","Metabolic network database; SBML; SMDA","Analysis workbenches; Genome-scale metabolic network; Life sciences research; Metabolic network; Metabolic network analysis; SBML; SMDA; User friendly interface; Genes; Hand held computers; Mammals; Metabolism; Multimedia systems; Query processing; Tools; Bioinformatics",2-s2.0-84888143522
"Amagasa T., Seino M., Kitagawa H.","Energy-efficient XML stream processing through element-skipping parsing",2013,"Proceedings - International Workshop on Database and Expert Systems Applications, DEXA",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887926988&doi=10.1109%2fDEXA.2013.34&partnerID=40&md5=2008f9f96599c360d8ac6b5158b17141","This paper proposes a scheme of energy-efficient XML stream processing through element-skipping parsing. Although parsing is one of the most computationally heavy part in the process of XML stream processing, existing techniques do not focus on the efficiency of XML parsing. To this problem, in our scheme, we propose element-skipping parsing where the parsing of such XML elements that do not contribute to the final query result is skipped with the coordination of XML parser and query processor. We show that the scheme is effective in reducing the execution time as well as energy consumption of stream-based XML query processor. © 2013 IEEE.",,"Energy efficient; Query processor; Query results; Stream-based; XML parser; XML parsing; XML queries; XML stream; Energy efficiency; Energy utilization; Expert systems; Query processing; XML",2-s2.0-84887926988
"Monnin A.","The Web as Ontology: Web Architecture Between Rest, Resources, and Rules",2013,"Philosophical Engineering: Toward a Philosophy of the Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016341655&doi=10.1002%2f9781118700143.ch3&partnerID=40&md5=72a19e92015ee451bb4b54ded2214a7e",[No abstract available],,,2-s2.0-85016341655
"Nagi K., Halim D.","Creating facets hierarchy for unstructured arabic documents",2013,"IC3K 2013; KEOD 2013 - 5th International Conference on Knowledge Engineering and Ontology Development, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887709488&partnerID=40&md5=c7a3f7873d416141b9eb09f6edf93858","Faceted search is becoming the standard searching method on modern web sites. To implement a faceted search system, a well defined metadata structure for the searched items must exist. Unfortunately, online text documents are simple plain text, usually without any metadata to describe their content. Taking advantage of external lexical hierarchies, a variety of methods for extracting plain and hierarchical facets from textual content are recently introduced. Meanwhile, the size of Arabic documents that can be accessed online is increasing every day. However, the Arabic language is not as established as the English language on the web. In our work, we introduce a faceted search system for unstructured Arabic text. Since the maturity of Arabic processing tools is not as high as the English ones, we try two methods for building the facets hierarchy for the Arabic terms. We then combine these methods into a hybrid one to get the best out of both approaches. We assess the three methods using our prototype by searching in real-life articles extracted from two sources: The BBC Arabic edition website and the Arab Sciencepedia Website. Copyright ©2013 SCITEPRESS.","Arabic content on the internet; Faceted search; Indexing arabic content","Arabic document; Arabic languages; Arabic processing; English languages; Faceted search; Searching methods; Text document; Textual content; Knowledge engineering; Metadata; Safety devices; Websites",2-s2.0-84887709488
"Horpácsi D.","Extending erlang by utilising RefactorErl",2013,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887216696&doi=10.1145%2f2505305.2505314&partnerID=40&md5=f6582bb971b8da093721be57efd40fcd","In this paper, we present the idea of utilising a refactoring tool for implementing extensions to a programming language. We elaborate the correspondence between the main components of the compiler and the refactoring tool, and examine how analysis and transformation features of the tool can be exploited for turning its refactoring framework into a translation framework. The presented method allows one, for instance, to make the Erlang language supportive for embedding domain specific languages as well as to make its functions portable. Copyright © 2013.","Domain specific language; Erlang; Language extension; Precompiler; Preprocessor; Refactoring; Static code analysis; Transformation; Translation","Domain specific languages; Erlang; Language extensions; Precompiler; Preprocessor; Refactorings; Static code analysis; Transformation; Computer programming languages; Functional programming; Tools; XML; Translation (languages)",2-s2.0-84887216696
"Sutoyo R., Quix C., Kastrati F.","FactRunner: Fact extraction over wikipedia",2013,"WEBIST 2013 - Proceedings of the 9th International Conference on Web Information Systems and Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887052218&partnerID=40&md5=cd2e7598e83eee99fffa025719a60649","The increasing role of Wikipedia as a source of human-readable knowledge is evident as it contains an enormous amount of high quality information written in natural language by human authors. However, querying this information using traditional keyword based approaches requires often a time-consuming, iterative process to explore the document collection to find the information of interest. Therefore, a structured representation of information and queries would be helpful to be able to directly query for the relevant information. An important challenge in this context is the extraction of structured information from unstructured knowledge bases which is addressed by Information Extraction (IE) systems. However, these systems struggle with the complexity of natural language and produce frequently unsatisfying results. In addition to the plain natural language text,Wikipedia contains links between documents which directly link a term of one document to another document. In our approach for fact extraction fromWikipedia, we consider these links as an important indicator for the relevance of the linked information. Thus, our proposed system FactRunner focusses on extracting structured information from sentences containing such links. We show that a natural language parser combined with Wikipedia markup can be exploited for extracting facts in form of triple statements with a high accuracy. Copyright © 2013 SCITEPRESS.","Information extraction; Semantic search","Document collection; High quality information; Information extraction systems; Iterative process; Natural language text; Natural languages; Semantic search; Structured information; Semantics; World Wide Web; Information retrieval",2-s2.0-84887052218
"Lee D., Jeong M., Kim K., Ryu S., Lee G.G.","Unsupervised spoken language understanding for a multi-domain dialog system",2013,"IEEE Transactions on Audio, Speech and Language Processing",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886683449&doi=10.1109%2fTASL.2013.2280212&partnerID=40&md5=c0f08c81c8067a634cb1455dd5c0cae6","This paper proposes an unsupervised spoken language understanding (SLU) framework for a multi-domain dialog system. Our unsupervised SLU framework applies a non-parametric Bayesian approach to dialog acts, intents and slot entities, which are the components of a semantic frame. The proposed approach reduces the human effort necessary to obtain a semantically annotated corpus for dialog system development. In this study, we analyze clustering results using various evaluation metrics for four dialog corpora. We also introduce a multi-domain dialog system that uses the unsupervised SLU framework. We argue that our unsupervised approach can help overcome the annotation acquisition bottleneck in developing dialog systems. To verify this claim, we report a dialog system evaluation, in which our method achieves competitive results in comparison with a system that uses a manually annotated corpus. In addition, we conducted several experiments to explore the effect of our approach on reducing development costs. The results show that our approach be helpful for the rapid development of a prototype system and reducing the overall development costs. © 2006-2012 IEEE.","Dialog system; spoken language understanding; unsupervised learning","Clustering results; Development costs; Dialog systems; Evaluation metrics; Non-parametric Bayesian; Prototype system; Spoken language understanding; Unsupervised approaches; Bayesian networks; Clustering algorithms; Semantics; Speech recognition; Unsupervised learning; Cost reduction",2-s2.0-84886683449
"Orimaye S.O., Alhashmi S.M., Siew E.-G.","Can predicate-argument structures be used for contextual opinion retrieval from blogs?",2013,"World Wide Web",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885927474&doi=10.1007%2fs11280-012-0170-8&partnerID=40&md5=dab30d238f8dd23bb29880a0592fd7ed","We present the results of our investigation on the use of predicate-argument structures for contextual opinion retrieval. The use of predicate-argument structure for opinion retrieval is a novel approach that exploits the grammatical derivation of sentences to show contextual and subjective relevance. We do not use frequency of certain keywords as it is usually done in keyword-based opinion retrieval approaches. Rather, our novel solution is based on frequency of contextually relevant and subjective sentences. We use a linear relevance model that leverages semantic similarities among predicate-argument structures of sentences. Thus, this paper presents the evaluation results of the linear relevance model. The model does a linear combination of a popular relevance model, our proposed transformed terms similarity model, and the absolute value of a sentence subjectivity scoring scheme. The predicate-argument structures are derived from the grammatical derivations of natural language query topics and the well formed sentences from blog documents. The derived predicate-argument structures are then semantically compared to compute an opinion relevance score. Our scoring technique uses the highest frequency of semantically related predicate-argument structures enriched with the total subjectivity score from sentences. Evaluation and experimental results show that predicate-argument structures can indeed be used for contextual opinion retrieval as it improves performance of opinion retrieval task by 15% over the popular TREC baselines. © 2012 Springer Science+Business Media, LLC.","contextual opinion retrieval; linear relevance model; predicate-argument structures; sentence-level","Evaluation results; Linear combinations; Natural language queries; Opinion retrievals; Relevance models; Scoring techniques; Semantic similarity; sentence-level; Semantics; Internet",2-s2.0-84885927474
"Abebe S.L., Alicante A., Corazza A., Tonella P.","Supporting concept location through identifier parsing and ontology extraction",2013,"Journal of Systems and Software",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884166097&doi=10.1016%2fj.jss.2013.07.009&partnerID=40&md5=da1fc93acc055e74683b7b3aeb8acc72","Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily ""parse"" identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships. In this study, we have evaluated the use of off-the-shelf and trained natural language analyzers to parse identifier names, extract an ontology and use it to support concept location. In the evaluation, we assessed whether the concepts taken from the ontology can be used to improve the efficiency of queries used in concept location. We have also investigated if the use of different natural language analyzers has an impact on the ontology extracted and the support it provides to concept location. Results show that using the concepts from the ontology significantly improves the efficiency of concept location queries (e.g., in some cases, an improvement of 127% is observed). The results also indicate that the efficiency of concept location queries is not affected by the differences in the ontologies produced by different analyzers. © 2013 Elsevier Inc.","Concept location; Natural language parsing; Program understanding","Concept locations; Natural language parsing; Natural languages; Ontology Extraction; Program understanding; Semantic relationships; Support concepts; Support programs; Semantics; Efficiency",2-s2.0-84884166097
"Seidl H., Wilhelm R., Hack S.","Compiler design: Syntactic and semantic analysis",2013,"Compiler Design: Syntactic and Semantic Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929284843&doi=10.1007%2f978-3-642-17540-4&partnerID=40&md5=0686aa2076c2fa4f042d0530bc8ef4b6","While compilers for high-level programming languages are large complex software systems, they have particular characteristics that differentiate them from other software systems. Their functionality is almost completely well-defined - ideally there exist complete precise descriptions of the source and target languages. Additional descriptions of the interfaces to the operating system, programming system and programming environment, and to other compilers and libraries are often available. This book deals with the analysis phase of translators for programming languages. It describes lexical, syntactic and semantic analysis, specification mechanisms for these tasks from the theory of formal languages, and methods for automatic generation based on the theory of automata. The authors present a conceptual translation structure, i.e., a division into a set of modules, which transform an input program into a sequence of steps in a machine program, and they then describe the interfaces between the modules. Finally, the structures of real translators are outlined. The book contains the necessary theory and advice for implementation. This book is intended for students of computer science. The book is supported throughout with examples, exercises and program fragments. © Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.",,"Computer software; Formal languages; High level languages; Program translators; Semantics; Syntactics; Translation (languages); Automatic Generation; Complex software systems; High-level programming language; Program fragments; Programming environment; Programming system; Semantic analysis; Software systems; Program compilers",2-s2.0-84929284843
"Claveau V., Kijak E.","Nonsupervised morphological analysis in biomedical field: Application to information retrieval [Analyse morphologique non supervisée en domaine biomédical: Application á la recherche d'information]",2013,"TAL Traitement Automatique des Langues",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886629014&partnerID=40&md5=e7789756eeaa1e8954d8b58bd4ed41e1","In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system.","Alignment; Analogical learning; Biomedical information retrieval; Biomedical terminology; Morphology; Morphosemantic indexing",,2-s2.0-84886629014
"Saxena P., Kamal R.","System architecture and effect of depth of query on XML document filtering using PFilter",2013,"2013 6th International Conference on Contemporary Computing, IC3 2013",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886511383&doi=10.1109%2fIC3.2013.6612188&partnerID=40&md5=6c6a026d21b5377a71de474ccbce55d8","The XML stream filtering applications are gaining popularity in recent years. These Applications require a filtering system that queries on a continuous stream of XML documents and delivers matched content accordingly. A PFilter algorithm has been proposed recently by authors. It has been found to be more effective on a large number of XML streaming documents. The algorithm is used for extracting the information of user's interest in information systems. The present paper proposes an XML stream filtering system architecture based on PFilter algorithm. [10] The algorithm converts the XPath query expressions for the queries into sequences of nodes. The system provides efficient and fast search in the streaming XML document. Experimental results are given for the performance of the filtering system for thousands of queries on streaming XML documents. Performance versus depth of queries and probability of occurrence of XPath operators demonstrates that the proposed system performs better compared to earlier state-of-the-art YFilter system. The proposed system can be used in applications requiring data dissemination based on user interest. © 2013 IEEE.","SAX parser; XML filtering; XPath expression","Data dissemination; Document filtering; Filtering applications; Probability of occurrence; SAX parser; System architectures; XML filtering; XPath expressions; Algorithms; XML; Information retrieval systems",2-s2.0-84886511383
"Cimiano P., Lopez V., Unger C., Cabrio E., Ngonga Ngomo A.-C., Walter S.","Multilingual question answering over linked data (QALD-3): Lab overview",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886377008&doi=10.1007%2f978-3-642-40802-1_30&partnerID=40&md5=0755f696aa88d93c174444ef3482d669","The third edition of the open challenge on Question Answering over Linked Data (QALD-3) has been conducted as a half-day lab at CLEF 2013. Differently from previous editions of the challenge, has put a strong emphasis on multilinguality, offering two tasks: one on multilingual question answering and one on ontology lexicalization. While no submissions were received for the latter, the former attracted six teams who submitted their systems' results on the provided datasets. This paper provides an overview of QALD-3, discussing the approaches proposed by the participating systems as well as the obtained results. © 2013 Springer-Verlag.",,"Lexicalization; Linked datum; Multilinguality; Participating systems; Question Answering; Data handling; Natural language processing systems",2-s2.0-84886377008
"Gupta S., Malik S., Pollock L., Vijay-Shanker K.","Part-of-speech tagging of program identifiers for improved text-based software engineering tools",2013,"IEEE International Conference on Program Comprehension",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886050513&doi=10.1109%2fICPC.2013.6613828&partnerID=40&md5=e709e337c0ba100170379d3af21cb2d7","To aid program comprehension, programmers choose identifiers for methods, classes, fields and other program elements primarily by following naming conventions in software. These software 'naming conventions' follow systematic patterns which can convey deep natural language clues that can be leveraged by software engineering tools. For example, they can be used to increase the accuracy of software search tools, improve the ability of program navigation tools to recommend related methods, and raise the accuracy of other program analyses. After splitting multi-word names into their component words, the next step to extracting accurate natural language information is tagging each word with its part of speech (POS) and then chunking the name into natural language phrases. State-of-theart approaches, most of which rely on 'traditional POS taggers' trained on natural language documents, do not capture the syntactic structure of program elements. In this paper, we present a POS tagger and syntactic chunker for source code names that takes into account programmers' naming conventions to understand the regular, systematic ways a program element is named. We studied the naming conventions used in Object Oriented Programming and identified different grammatical constructions that characterize a large number of program identifiers. This study then informed the design of our POS tagger and chunker. Our evaluation results show a significant improvement in accuracy(11%-20%) of POS tagging of identifiers, over the current approaches. With this improved accuracy, both automated software engineering tools and developers will be able to better capture and understand the information available in code. © 2013 IEEE.","comprehension; identifiers; natural language processing; part-of-speech; Program understanding","comprehension; identifiers; NAtural language processing; Part Of Speech; Program understanding; Object oriented programming; Software engineering; Syntactics; Tools; Natural language processing systems",2-s2.0-84886050513
"Vera R., Perez-Riverol Y., Perez S., Ligeti B., Kertész-Farkas A., Pongor S.","JBioWH: an open-source Java framework for bioinformatics data integration.",2013,"Database : the journal of biological databases and curation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885940979&doi=10.1093%2fdatabase%2fbat051&partnerID=40&md5=719388e23d93d16f6dadfb52d85ad0cd","The Java BioWareHouse (JBioWH) project is an open-source platform-independent programming framework that allows a user to build his/her own integrated database from the most popular data sources. JBioWH can be used for intensive querying of multiple data sources and the creation of streamlined task-specific data sets on local PCs. JBioWH is based on a MySQL relational database scheme and includes JAVA API parser functions for retrieving data from 20 public databases (e.g. NCBI, KEGG, etc.). It also includes a client desktop application for (non-programmer) users to query data. In addition, JBioWH can be tailored for use in specific circumstances, including the handling of massive queries for high-throughput analyses or CPU intensive calculations. The framework is provided with complete documentation and application examples and it can be downloaded from the Project Web site at http://code.google.com/p/jbiowh. A MySQL server is available for demonstration purposes at hydrax.icgeb.trieste.it:3307. Database URL: http://code.google.com/p/jbiowh.",,"drug; article; biology; computer interface; computer language; data base; Computational Biology; Database Management Systems; Databases as Topic; Pharmaceutical Preparations; Programming Languages; User-Computer Interface",2-s2.0-84885940979
"Huang C.-C., Lin C.-Y., Chang C.-W., Tang C.Y.","Enzyme reaction annotation using cloud techniques",2013,"BioMed Research International",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885626457&doi=10.1155%2f2013%2f140237&partnerID=40&md5=ebc5e517ccb6ae1b15bca0a0962b4170","An understanding of the activities of enzymes could help to elucidate the metabolic pathways of thousands of chemical reactions that are catalyzed by enzymes in living systems. Sophisticated applications such as drug design and metabolic reconstruction could be developed using accurate enzyme reaction annotation. Because accurate enzyme reaction annotation methods create potential for enhanced production capacity in these applications, they have received greater attention in the global market. We propose the enzyme reaction prediction (ERP) method as a novel tool to deduce enzyme reactions from domain architecture. We used several frequency relationships between architectures and reactions to enhance the annotation rates for single and multiple catalyzed reactions. The deluge of information which arose from high-throughput techniques in the postgenomic era has improved our understanding of biological data, although it presents obstacles in the data-processing stage. The high computational capacity provided by cloud computing has resulted in an exponential growth in the volume of incoming data. Cloud services also relieve the requirement for large-scale memory space required by this approach to analyze enzyme kinetic data. Our tool is designed as a single execution file; thus, it could be applied to any cloud platform in which multiple queries are supported. © 2013 Chuan-Ching Huang et al.",,"enzyme; protein; algorithm; analytic method; article; chemical reaction; classification; cloud computing; controlled study; data mining; data processing; enzyme analysis; enzyme kinetics; enzyme mechanism; intermethod comparison; simulation; biology; chemistry; computer program; kinetics; metabolism; molecular genetics; Computational Biology; Enzymes; Kinetics; Molecular Sequence Annotation; Software",2-s2.0-84885626457
"Yilmaz L., Ören T.","Toward Replicability-Aware Modeling and Simulation: Changing the Conduct of M&S in the Information Age",2013,"Intelligent Systems Reference Library",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885663184&doi=10.1007%2f978-3-642-31140-6_11&partnerID=40&md5=048b7786b7f9da0cfdac79cbb8f8c634","The use of computational models in science end engineering is increasingly becoming pervasive. However, there is a growing credibility gap due to widespread, relaxed attitudes in communication of experiments, models, and validation of simulations used in computational research. Consequent disputes and article retractions due to unverified code and data suggest a pressing need for greater transparency. We introduce the e Portfolio concept, which is an ensemble documents that interweave the conceptual model, simulator design, experimental frames, and scientific workflow specifications. Strategies and potential mechanisms are delineated to enable authors, publishers, funding agencies, journals, and the broader scientific community to cooperate and establish a sustained model base, simulations, experiments, and documentation, so that scientists can build on each other's work and achievements. © Springer-Verlag Berlin Heidelberg 2013.",,,2-s2.0-84885663184
"Hakkani-Tur D., Heck L., Tur G.","Using a knowledge graph and query click logs for unsupervised learning of relation detection",2013,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890477722&doi=10.1109%2fICASSP.2013.6639289&partnerID=40&md5=e8babcb77b6bb20ed8edecdd50f697da","In this paper, we introduce a novel statistical language understanding paradigm inspired by the emerging semantic web: Instead of building models for the target application, we propose relying on the semantic space already defined and populated in the knowledge graph for the target domain. As a first step towards this direction, we present unsupervised methods for training relation detection models exploiting the semantic knowledge graphs of the semantic web. The detected relations are used to mine natural language queries against a back-end knowledge base. For each relation, we leverage the complete set of entities that are connected to each other in the graph with the specific relation, and search these entity pairs on the web. We use the snippets that the search engine returns to create natural language examples that can be used as the training data for each relation. We further refine the annotations of these examples using the knowledge graph itself and iterate using a bootstrap approach. Furthermore, we explot the URLs returned for these pairs by the search engine to mine additional examples from the search engine query click logs. In our experiments, we show that, we can achieve relation detection models that perform about 60% macro F-measure on the relations that are in the knowledge graph without any manual labeling, resulting in a comparable performance with supervised training. © 2013 IEEE.","knowledge graph; multi-class classification; search query click logs; semantic web; spoken language understanding","Knowledge graphs; Language understanding; Multi-class classification; Natural language queries; Search query click logs; Spoken language understanding; Supervised trainings; Unsupervised method; Knowledge based systems; Natural language processing systems; Semantic Web; Signal processing; Search engines",2-s2.0-84890477722
"Gillenwater J., He X., Gao J., Deng L.","End-to-end learning of parsing models for information retrieval",2013,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890478429&doi=10.1109%2fICASSP.2013.6638271&partnerID=40&md5=67274441b9f5125dcdf9d1fbe29b6a09","Parsers have been shown to be helpful in information retrieval tasks because they are able to model long-span word dependencies efficiently. While previous work focused on using traditional syntactic parse trees, this paper proposes a new approach where, unlike previous work, the parser parameters are discriminatively trained to directly optimize a non-convex and non-smooth IR measure. The relevance between a document and a query is then modeled by the weighted tree edit distance between their parses. We evaluate our method on a large scale web search task consisting of a real world query set. Results show that the new parser is more effective for document retrieval than using traditional syntactic parse trees. It gives significant improvement, especially for long queries where proper modeling of long-span dependencies is crucial. © 2013 IEEE.","end-to-end optimization; information retrieval; parsing model; tree edit distance","Document Retrieval; Long queries; New approaches; Proper modeling; Syntactic parse tree; Tree edit distance; Web searches; Weighted tree; Forestry; Optimization; Signal processing; Syntactics; Information retrieval; Forestry; Information Retrieval; Models; Optimization",2-s2.0-84890478429
"Chou C.-P., Jea K.-F.","Grammar-based matching of multiple continuous queries on XML streams",2013,"Proceedings - 2013 7th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2013",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885205858&doi=10.1109%2fCISIS.2013.49&partnerID=40&md5=2a67f0ef3e375923f8462a222f1797f7","This paper addresses the multiple-query matching problem on XML streams. The goal is to extract all XML data that match queries from the XML streams immediately and efficiently under the constraint of scanning the streams only once. A grammar-based, continuous-query matching method, namely GCQ, is proposed. GCQ generates a context-free grammar according to the queries being processed, and then produces a parser capable of parsing the grammar by using a compiler tool such as YACC. With the parser, GCQ matches multiple queries concurrently in XML data streams. With the matured compiler techniques and tools, we concentrate on solving the matching problem by a set of declarative grammar rules. Both of the algorithms for grammar generation and query matching have polynomial time complexity. Experiments were also conducted to show the efficiency and scalability of GCQ in various aspects. Consequently, GCQ is beneficial for building efficient publish/subscribe applications. © 2013 IEEE.","Continuous query; Syntactic pattern recognition; Twig query matching; XML; XML streams","Compiler techniques; Continuous queries; Matching problems; Polynomial time complexity; Publish/subscribe; Syntactic pattern recognition; Twig queries; XML stream; Data mining; Formal languages; Pattern recognition; Polynomial approximation; Program compilers; Query processing; Tools; XML",2-s2.0-84885205858
"Quesada L., Berzal F., Cubero J.-C.","A model-based multilingual natural language parser - Implementing chomsky's X-bar theory in ModelCC",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884937095&doi=10.1007%2f978-3-642-40769-7_26&partnerID=40&md5=41504578c54fb328685abdca450d84a5","Natural language support is a powerful feature that enhances user interaction with query systems. NLP requires dealing with ambiguities. Traditional probabilistic parsers provide a convenient means for disambiguation. However, they incorrigibly return wrong sequences of tokens, they impose hard constraints on the way lexical and syntactic ambiguities can be resolved, and they are limited in the mechanisms they allow for taking context into account. In comparison, model-based parser generators allow for flexible constraint specification and reference resolution, which facilitates the context consideration. In this paper, we explain how the ModelCC model-based parser generator supports statistical language models and arbitrary probability estimators. Then, we present the ModelCC implementation of a natural language parser based on the syntax of most Romance and Germanic languages. This natural language parser can be instantiated for a specific language by connecting it with a thesaurus (for lexical analysis), a linguistic corpus (for syntax-driven disambiguation), and an ontology or semantic database (for semantics-driven disambiguation). © 2013 Springer-Verlag Berlin Heidelberg.","disambiguation; Natural languages; query parsing","disambiguation; Flexible constraint; Natural languages; Probability estimator; query parsing; Reference resolution; Statistical language models; Syntactic Ambiguities; Natural language processing systems; Semantics; Syntactics; Query processing",2-s2.0-84884937095
"Majidi S., Crane G.","Committee-based active learning for dependency parsing",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884725158&doi=10.1007%2f978-3-642-40501-3_56&partnerID=40&md5=7d5c464e5645da0789eb37e55f17354f","Annotations on structured corpora provide a foundational instrument for emerging linguistic research. To generate annotations automatically, data-driven dependency parsers need a large annotated corpus to learn from. But these annotations are expensive to collect and require a labor intensive task. In order to reduce the costs of annotation, we provide a novel framework in which a committee of dependency parsers collaborate to improve their efficiency using active learning. © 2013 Springer-Verlag.","active learning; corpus annotation; dependency parsing","Active Learning; Corpus annotations; Dependency parser; Dependency parsing; Labor intensive; Artificial intelligence; Computer science; Digital libraries",2-s2.0-84884725158
"Lu J.","An introduction to XML query processing and keyword search",2013,"An Introduction to XML Query Processing and Keyword Search",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949180276&doi=10.1007%2f978-3-642-34555-5&partnerID=40&md5=2a830011a7622ddc041511ab971604fc","""An Introduction to XML Query Processing and Keyword Search"" systematically and comprehensively covers the latest advances in XML data searching. It presents an extensive overview of the current query processing and keyword search techniques on XML data, including XML labeling schemes, indexing, processing on order and un-order XML tree patterns, XML query optimization, results estimation, and XML keyword searches, which are elaborated in separate chapters. Graduate students and researchers in the field of XML data searching will find this book an invaluable resource. © Tsinghua University Press, Beijing and Springer-Verlag Berlin Heidelberg 2013. All rights are reserved.",,"Indexing (materials working); Query processing; Relational database systems; Students; Trees (mathematics); XML; Graduate students; Keyword search; XML data; Xml keyword searches; XML labeling; XML Query optimization; XML query processing; XML trees; Search engines",2-s2.0-84949180276
"Kwak M., Leroy G., Martinez J.D., Harwell J.","Development and evaluation of a biomedical search engine using a predicate-based vector space model",2013,"Journal of Biomedical Informatics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883806589&doi=10.1016%2fj.jbi.2013.07.006&partnerID=40&md5=cb7d426c1176eb5d48e91bf7c8969090","Although biomedical information available in articles and patents is increasing exponentially, we continue to rely on the same information retrieval methods and use very few keywords to search millions of documents. We are developing a fundamentally different approach for finding much more precise and complete information with a single query using predicates instead of keywords for both query and document representation. Predicates are triples that are more complex datastructures than keywords and contain more structured information. To make optimal use of them, we developed a new predicate-based vector space model and query-document similarity function with adjusted tf-idf and boost function. Using a test bed of 107,367 PubMed abstracts, we evaluated the first essential function: retrieving information. Cancer researchers provided 20 realistic queries, for which the top 15 abstracts were retrieved using a predicate-based (new) and keyword-based (baseline) approach. Each abstract was evaluated, double-blind, by cancer researchers on a 0-5 point scale to calculate precision (0 versus higher) and relevance (0-5 score). Precision was significantly higher ( p<. .001) for the predicate-based (80%) than for the keyword-based (71%) approach. Relevance was almost doubled with the predicate-based approach-2.1 versus 1.6 without rank order adjustment ( p<. .001) and 1.34 versus 0.98 with rank order adjustment ( p<. .001) for predicate-versus keyword-based approach respectively. Predicates can support more precise searching than keywords, laying the foundation for rich and sophisticated information search. © 2013 Elsevier Inc.","Information retrieval; Predicate; Search engine; Triple; Vector space model","Biomedical information; Biomedical search engines; Document Representation; Predicate; Similarity functions; Structured information; Triple; Vector space models; Abstracting; Diseases; Equipment testing; Search engines; Vector spaces; Information retrieval; article; biomedicine; cancer research; mathematical analysis; priority journal; search engine; Information retrieval; Predicate; Search engine; Triple; Vector space model; Computer Simulation; Search Engine",2-s2.0-84883806589
"Güngör T.","A machine learning approach for displaying query results in search engines",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884468704&doi=10.1007%2f978-3-642-40261-6_21&partnerID=40&md5=8b3d16ccc30f60e7f21d9234772a6c12","In this paper, we propose an approach that displays the results of a search engine query in a more effective way. Each web page retrieved by the search engine is subjected to a summarization process and the important content is extracted. The system consists of four stages. First, the hierarchical structures of documents are extracted. Then the lexical chains in documents are identified to build coherent summaries. The document structures and lexical chains are used to learn a summarization model by the next component. Finally, the summaries are formed and displayed to the user. Experiments on two datasets showed that the method significantly outperforms traditional search engines. © 2013 Springer-Verlag.",,"Document structure; Hierarchical structures; Lexical Chain; Machine learning approaches; Query results; Summarization models; Image analysis; Natural language processing systems; Search engines",2-s2.0-84884468704
"Willink E., Hoyos H., Kolovos D.","Yet Another Three QVT Languages",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884341055&doi=10.1007%2f978-3-642-38883-5_8&partnerID=40&md5=a26bc1280cd6e98664a6a873f75046ed","The early enthusiasm, in 2002, for model to model transformation languages led to eight submissions for an OMG standard[1] comprising three languages, yet no commercial products. The QVT Core language was intended as the foundation for QVT Relations but the available implementations have ignored the core language. Rather than ignoring the core language, we take the opposite approach and introduce three more core languages. Progressive program-to-program transformation through these core languages terminates in an easily implemented imperative language that supports declarative transformations. There are currently only two freely available but discouragingly stable implementations of QVTr. There are no implementations for QVTc. The Eclipse QVT Declarative project provides only models, editors and parsers for both QVTr and QVTc. We outline progress to remedy the execution deficiency. © 2013 Springer-Verlag.",,"Commercial products; Imperative languages; Model to model transformation; Artificial intelligence; Computer science; Mathematical models",2-s2.0-84884341055
"Van Rest O., Wachsmuth G., Steel J.R.H., Süß J.G., Visser E.","Robust Real-Time Synchronization between Textual and Graphical Editors",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884322576&doi=10.1007%2f978-3-642-38883-5_11&partnerID=40&md5=cf9e0e55eaf9b4c692478b26be13709b","In modern Integrated Development Environments (IDEs), textual editors are interactive and can handle intermediate, incomplete, or otherwise erroneous texts while still providing editor services such as syntax highlighting, error marking, outline views, and hover help. In this paper, we present an approach for the robust synchronization of interactive textual and graphical editors. The approach recovers from errors during parsing and text-to-model synchronization, preserves textual and graphical layout in the presence of erroneous texts and models, and provides synchronized editor services such as selection sharing and navigation between editors. It was implemented for synchronizing textual editors generated by the Spoofax language workbench and graphical editors generated by the Graphical Modeling Framework. © 2013 Springer-Verlag.",,"Graphical editors; Graphical modeling frameworks; Integrated development environment; Language workbenches; Robust synchronization; Spoofax; Mathematical models; Synchronization",2-s2.0-84884322576
"Campos D., Matos S., Oliveira J.L.","A modular framework for biomedical concept recognition",2013,"BMC Bioinformatics",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884469671&doi=10.1186%2f1471-2105-14-281&partnerID=40&md5=99e34d8f13d346692d78325c55e7bde9","Background: Concept recognition is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. The development of such solutions is typically performed in an ad-hoc manner or using general information extraction frameworks, which are not optimized for the biomedical domain and normally require the integration of complex external libraries and/or the development of custom tools.Results: This article presents Neji, an open source framework optimized for biomedical concept recognition built around four key characteristics: modularity, scalability, speed, and usability. It integrates modules for biomedical natural language processing, such as sentence splitting, tokenization, lemmatization, part-of-speech tagging, chunking and dependency parsing. Concept recognition is provided through dictionary matching and machine learning with normalization methods. Neji also integrates an innovative concept tree implementation, supporting overlapped concept names and respective disambiguation techniques. The most popular input and output formats, namely Pubmed XML, IeXML, CoNLL and A1, are also supported. On top of the built-in functionalities, developers and researchers can implement new processing modules or pipelines, or use the provided command-line interface tool to build their own solutions, applying the most appropriate techniques to identify heterogeneous biomedical concepts. Neji was evaluated against three gold standard corpora with heterogeneous biomedical concepts (CRAFT, AnEM and NCBI disease corpus), achieving high performance results on named entity recognition (F1-measure for overlap matching: species 95%, cell 92%, cellular components 83%, gene and proteins 76%, chemicals 65%, biological processes and molecular functions 63%, disorders 85%, and anatomical entities 82%) and on entity normalization (F1-measure for overlap name matching and correct identifier included in the returned list of identifiers: species 88%, cell 71%, cellular components 72%, gene and proteins 64%, chemicals 53%, and biological processes and molecular functions 40%). Neji provides fast and multi-threaded data processing, annotating up to 1200 sentences/second when using dictionary-based concept identification.Conclusions: Considering the provided features and underlying characteristics, we believe that Neji is an important contribution to the biomedical community, streamlining the development of complex concept recognition solutions. Neji is freely available at http://bioinformatics.ua.pt/neji. © 2013 Campos et al.; licensee BioMed Central Ltd.",,"Appropriate techniques; Biomedical information extractions; Command-line interfaces; Named entity recognition; NAtural language processing; Normalization methods; Open source frameworks; Part of speech tagging; Genes; Information retrieval; Learning algorithms; Linguistics; Natural language processing systems; Optimization; Proteins; Text processing; Biomedical engineering; article; automated pattern recognition; biology; computer program; factual database; information retrieval; methodology; natural language processing; Computational Biology; Databases, Factual; Information Storage and Retrieval; Natural Language Processing; Pattern Recognition, Automated; Software",2-s2.0-84884469671
"Omar C., Chung B., Kurilova D., Potanin A., Aldrich J.","Type-directed, whitespace-delimited parsing for embedded DSLs",2013,"Proceedings of the 1st Workshop on the Globalization of Domain Specific Languages, GlobalDSL 2013",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883056093&doi=10.1145%2f2489812.2489815&partnerID=40&md5=bb0de3bca584fb811be77aa65af90f08","Domain-specific languages improve ease-of-use, expressiveness and verifiability, but defining and using different DSLs within a single application remains difficult. We introduce an approach for embedded DSLs where 1) whitespace delimits DSL-governed blocks, and 2) the parsing and type checking phases occur in tandem so that the expected type of the block determines which domain-specific parser governs that block. We argue that this approach occupies a sweet spot, providing high expressiveness and ease-of-use while maintaining safe composability. We introduce the design, provide examples and describe an ongoing implementation of this strategy in theWyvern programming language. We also discuss how a more conventional keyword-directed strategy for parsing of DSLs can arise as a special case of this type-directed strategy.",,"Composability; Domain specific; Domain specific languages; Ease-of-use; Sweet spot; Typechecking; Verifiability; Problem oriented languages; Computer programming languages",2-s2.0-84883056093
"Llopis M., Ferrández A.","How to make a natural language interface to query databases accessible to everyone: An example",2013,"Computer Standards and Interfaces",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878286634&doi=10.1016%2fj.csi.2012.09.005&partnerID=40&md5=caa04d311db26f402a0b823db7712e62","Natural Language Interfaces to Query Databases (NLIDBs) have been an active research field since the 1960s. However, they have not been widely adopted. This article explores some of the biggest challenges and approaches for building NLIDBs and proposes techniques to reduce implementation and adoption costs. The article describes {AskMe*}, a new system that leverages some of these approaches and adds an innovative feature: query-authoring services, which lower the entry barrier for end users. Advantages of these approaches are proven with experimentation. Results confirm that, even when {AskMe*} is automatically reconfigurable against multiple domains, its accuracy is comparable to domain-specific NLIDBs. © 2012 Elsevier B.V. All rights reserved.","Concept hierarchy; Natural language interface; Ontology extraction; Query-authoring services; Relational database","Concept hierarchies; Natural language interfaces; Ontology Extraction; Query-authoring services; Relational Database; Database systems; Query processing; Natural language processing systems",2-s2.0-84878286634
"Sinkovics Á., Porkoláb Z.","Implementing monads for C++ template metaprograms",2013,"Science of Computer Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878836451&doi=10.1016%2fj.scico.2013.01.002&partnerID=40&md5=7ca93c7a8bebb80c585c6936fd168197","C++ template metaprogramming is used in various application areas, such as expression templates, static interface checking, active libraries, etc. Its recognized similarities to pure functional programming languages-like Haskell-make the adoption of advanced functional techniques possible. Such a technique is using monads, programming structures representing computations. Using them actions implementing domain logic can be chained together and decorated with custom code. C++ template metaprogramming could benefit from adopting monads in situations like advanced error propagation and parser construction. In this paper we present an approach for implementing monads in C++ template metaprograms. Based on this approach we have built a monadic framework for C++ template metaprogramming. As real world examples we present a generic error propagation solution for C++ template metaprograms and a technique for building compile-time parser generators. All solutions presented in this paper are implemented and available as an open source library. © 2012 Elsevier B.V. All rights reserved.","C++ template metaprogram; Exception handling; Monad; Monoid; Typeclass","C++ templates; Exception handling; Monad; Monoid; Typeclass; Computer programming; Software engineering; Functional programming",2-s2.0-84878836451
"Di Caro L., Grella M.","Sentiment analysis via dependency parsing",2013,"Computer Standards and Interfaces",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878317358&doi=10.1016%2fj.csi.2012.10.005&partnerID=40&md5=5273bbe26eed3d5aaf5cd27fd3ca8d6c","Nowadays, Sentiment Analysis (SA) is receiving huge attention because of the wide range of its direct applications like analyses of products, customer profiles, political trends, and so forth. Still, the availability of big amounts of data coming from the World Wide Web makes easier the study of both new techniques and evaluation methods. Current literature mainly focuses on two approaches which rely on sentiment lexicons (i.e., lists of words associated to scores of sentiment polarity) or on Natural Language Processing techniques (NLP). In this paper, on one hand, we introduce and evaluate a novel algorithm for SA that relies on a simple set of propagation rules applied at syntactic level within a dependency parse tree. On the other hand, we propose a context-based model where the users' sentiments (or opinions) are tuned according to some context of analysis. Finally, we present the system called SentiVis which implements these ideas through an orthogonal approach to SA that directly leans on Data Visualization. Extracted sentiments, with respect to some query of analysis, are ordered and represented graphically in a 2-dimensional space, conveying information about their strength and variability. This way, we avoid cumbersome rankings of objects and associated opinions by directly mapping such information on the screen. The user is then able to interact with the visualized data in order to discover interesting facts as well as removing false positive (or negative) opinions deriving by the used algorithm. We then evaluate the efficacy of the proposed system through several case studies. © 2012 Elsevier B.V. All rights reserved.","Data visualization; Sentiment analysis; User interfaces","Customer profiles; Dependency parsing; Evaluation methods; NAtural language processing; Novel algorithm; Propagation rule; Sentiment analysis; Sentiment lexicons; Algorithms; Data visualization; Natural language processing systems; User interfaces; World Wide Web; Data mining",2-s2.0-84878317358
"Li Y.-N., Li H.-S., Cai Q.","Research on knowledge recommendation for domain ontology based on semantic similarity",2013,"Complex Systems and Complexity Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887256864&partnerID=40&md5=d02baa28fdf097e237f0c65250dff257","With the explosive growth of web resource, it is difficult for keyword-based knowledge recommendation to meet the professional needs of users. In this paper, a knowledge recommandation calculation algorithm based on semantic similarity method is proposed. According to the style of user's input, we calculate similarity of concepts based on information content and similarity of sentences based on semantic similarity and structure similarity. Experiment results show that the user's inquiry request has been expanded its concept effectively, and the recall and accuracy of retrieval have been improved obviously.","Information content; Knowledge recommendation; Semantic; Similarity","Calculation algorithms; Domain ontologies; Information contents; Knowledge recommendation; Semantic similarity; Similarity; Similarity of concepts; Structure similarity; Systems engineering; Semantics",2-s2.0-84887256864
"Amaechi B.T., Porteous N., Ramalingam K., Mensinkai P.K., Ccahuana Vasquez R.A., Sadeghpour A., Nakamoto T.","Remineralization of artificial enamel lesions by theobromine",2013,"Caries Research",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876458124&doi=10.1159%2f000348589&partnerID=40&md5=7e85f44285b40e6f1568dd19bd3de69f","Objective: This study investigated the remineralization potential of theobromine in comparison to a standard NaF dentifrice. Methods: Three tooth blocks were produced from each of 30 teeth. Caries-like lesion was created on each block using acidified gel. A smaller block was cut from each block for baseline scanning electron microscopy imaging and electron-dispersive spectroscopy (EDS) analysis for surface Ca level. A tooth slice was cut from each lesion-bearing block for transverse microradiography (TMR) quantification of baseline mineral loss (Δz) and lesion depth (LD). Then baseline surface microhardness (SMH) of each lesion was measured. The three blocks from each tooth were assigned to three remineralizing agents: (1) artificial saliva; (2) artificial saliva with theobromine (0.0011 mol/l), and (3) NaF toothpaste slurry (0.0789 mol/l F). Remineralization was conducted using a pH cycling model with storage in artificial saliva. After a 28-day cycle, samples were analyzed using EDS, TMR, and SMH. Intragroup comparison of pre-and posttest data was performed using t tests (p < 0.05). Intergroup comparisons were performed by post hoc multistep comparisons (Tukey). Results: SMH indicated significant (p < 0.01) remineralization only with theobromine (38 ± 32%) and toothpaste (29 ± 16%). With TMR (Δz/lD), theobromine and toothpaste exhibited significantly (p < 0.01) higher mineral gain relative to artificial saliva. With SMH and TMR, remineralization produced by theobromine and toothpaste was not significantly different. With EDS, calcium deposition was significant in all groups, but not significantly different among the groups (theobromine 13 ± 8%, toothpaste 10 ± 5%, and artificial saliva 6 ± 8%). Conclusion: The present study demonstrated that theobromine in an apatite-forming medium can enhance the remineralization potential of the medium. © 2013 S. Karger AG, Basel.","Caries prevention; Remineralization; Theobromine","anticaries agent; calcium; lactic acid; saliva substitute; sodium fluoride; theobromine; toothpaste; article; comparative study; controlled clinical trial; controlled study; dental caries; dental procedure; drug effect; electron probe microanalysis; enamel; hardness; human; materials testing; methodology; microradiography; pathology; pH; randomized controlled trial; scanning electron microscopy; time; ultrastructure; Calcium; Cariostatic Agents; Dental Caries; Dental Enamel; Electron Probe Microanalysis; Hardness; Humans; Hydrogen-Ion Concentration; Lactic Acid; Materials Testing; Microradiography; Microscopy, Electron, Scanning; Saliva, Artificial; Sodium Fluoride; Theobromine; Time Factors; Tooth Remineralization; Toothpastes",2-s2.0-84876458124
"Bianconi A.","Shape resonances in superconducting gaps and complexity in superstripes",2013,"Journal of Superconductivity and Novel Magnetism",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882614206&doi=10.1007%2fs10948-013-2205-5&partnerID=40&md5=00e387bc44466caf923a824040747388","The emerging scenario of superstripes for high temperature superconductors is presented. The complexity of the electronic structure of doped cuprates results from two electronic components and nanoscale phase separation. In this lattice, charge and magnetic complexity the unconventional high temperature superconductivity emerges in a broken symmetry. Shape resonances in superconducting gaps are discussed. © 2013 Springer Science+Business Media New York.","Fano resonance; Lattice complexity; Layered superconducting materials; Shape resonances; Superstripes","Electronic component; Fano resonances; High-temperature superconductivity; Lattice complexity; Nano-scale phase separation; Shape resonance; Superconducting gaps; Superstripes; Electronic structure; High temperature superconductors; Phase separation; Superconductivity; Electron resonance",2-s2.0-84882614206
"Dobos L., Csabai I., Szalay A.S., Budavári T., Li N.","Graywulf: A platform for federated scientific databases and services",2013,"ACM International Conference Proceeding Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883015014&doi=10.1145%2f2484838.2484863&partnerID=40&md5=a97bf735aa212e2bcb0d2a5d495a2773","Many fields of science rely on relational database management systems to analyze, publish and share data. Since RDBMS are originally designed for, and their development directions are primarily driven by, business use cases they often lack features very important for scientific applications. Horizontal scalability is probably the most important miss- ing feature which makes it challenging to adapt traditional relational database systems to the ever growing data sizes. Due to the limited support of array data types and meta- data management, successful application of RDBMS in science usually requires the development of custom extensions. While some of these extensions are specific to the field of science, the majority of them could easily be generalized and reused in other disciplines. With the Graywulf project we intend to target several goals. We are building a generic platform that offers reusable components for efficient storage, transformation, statistical analysis and presentation of scientific data stored in Microsoft SQL Server. Graywulf also addresses the distributed computational issues arising from current RDBMS technologies. The current version sup- ports load balancing of simple queries and parallel execution of partitioned queries over a set of mirrored databases. Uniform user access to the data is provided through a web based query interface and a data surface for software clients. Queries are formulated in a slightly modified syntax of SQL that offers a transparent view of the distributed data. The software library consists of several components that can be reused to develop complex scientific data warehouses: a sys- tem registry, administration tools to manage entire database server clusters, a sophisticated workflow execution frame- work, and a SQL parser library. Copyright © 2013 ACM.","Management","Computational issues; Development directions; Microsoft SQL Server; Parallel executions; Relational database management systems; Reusable components; Scientific applications; Scientific database; Computer software reusability; Data warehouses; Information management; Management; Management information systems; Metadata; Query processing; Relational database systems; Digital storage",2-s2.0-84883015014
"Kröni D., Schweizer R.","Parsing graphs: Applying parser combinators to graph traversals",2013,"Proceedings of the 4th Workshop on Scala, SCALA 2013",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882374141&doi=10.1145%2f2489837.2489844&partnerID=40&md5=caf2dab07c4dade4de33e1d7b1912396","Connected data such as social networks or business process interactions are frequently modeled as graphs, and increas- ingly often, stored in graph databases. In contrast to rela- tional databases where SQL is the proven query language, there is no established counterpart for graph databases. One way to explore and extract data from a graph database is to specify the structure of paths (partial traversals) through the graph. We show how such traversals can be expressed by com- bining graph navigation primitives with familiar grammar constructions such as sequencing, choice and repetition { essentially applying the idea of parser combinators to graph traversals. The result is trails [6], a Scala combinator library that provides an implementation for the neo4j graph database [7] and for the generic graph API blueprints [8]. Copyright 2013 ACM.","Domain specific language; Graph database; Graph traversal; Parser combinators; Scala","Domain specific languages; Graph database; Graph traversals; Parser combinators; Scala; Query languages",2-s2.0-84882374141
"Pizzocaro D., Parizas C., Preece A., Braines D., Mott D., Bakdash J.Z.","CE-SAM: A conversational interface for ISR mission support",2013,"Proceedings of SPIE - The International Society for Optical Engineering",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881178245&doi=10.1117%2f12.2015686&partnerID=40&md5=509ca1fc2943a0b0285f15cce6b1eb2a","There is considerable interest in natural language conversational interfaces. These allow for complex user interactions with systems, such as fulfilling information requirements in dynamic environments, without requiring extensive training or a technical background (e.g. in formal query languages or schemas). To leverage the advantages of conversational interactions we propose CE-SAM (Controlled English Sensor Assignment to Missions), a system that guides users through refining and satisfying their information needs in the context of Intelligence, Surveillance, and Reconnaissance (ISR) operations. The rapidly-increasing availability of sensing assets and other information sources poses substantial challenges to effective ISR resource management. In a coalition context, the problem is even more complex, because assets may be ""owned"" by different partners. We show how CE-SAM allows a user to refine and relate their ISR information needs to pre-existing concepts in an ISR knowledge base, via conversational interaction implemented on a tablet device. The knowledge base is represented using Controlled English (CE) - a form of controlled natural language that is both human-readable and machine processable (i.e. can be used to implement automated reasoning). Users interact with the CE-SAM conversational interface using natural language, which the system converts to CE for feeding-back to the user for confirmation (e.g. to reduce misunderstanding). We show that this process not only allows users to access the assets that can support their mission needs, but also assists them in extending the CE knowledge base with new concepts. © 2013 SPIE.","Analyst; CE-SAM; Controlled English; Conversational interaction; Conversational interface; Decision support; Intelligence; ISR; Mobile app; Resource allocation; Sensor mission assignment","Analyst; CE-SAM; Controlled English; Conversational interaction; Conversational interface; Decision supports; Intelligence; ISR; Mobile app; Decision support systems; Information science; Knowledge based systems; Query languages; Resource allocation; Sensors; Information management",2-s2.0-84881178245
"Mlakar I., Rojc M.","A new distributed platform for client-side fusion of web applications and natural modalities-a multimodal web platform",2013,"Applied Artificial Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882436254&doi=10.1080%2f08839514.2013.813167&partnerID=40&md5=e03ab63a9fc44f2e37e9b29eae3334e8","Web-based solutions and interfaces should be easy, more intuitive, and should also adapt to the natural and cognitive information processing and presentation capabilities of humans. Today, human-controlled multimodal systems with multimodal interfaces are possible. They allow for a more natural and more advanced exchange of information between man and machine. The fusion of web-based solutions with natural modalities is therefore an effective solution for users who would like to access services and web content in a more natural way. This article presents a novel multimodal web platform (MWP) that enables flexible migration from traditionally closed and purpose-oriented multimodal systems to the wider scope offered by web applications. The MWP helps to overcome problems of interoperability, compatibility, and integration that usually accompany migrations from standard (task-oriented) applications to web-based solutions and multiservice networks, thus enabling the enrichment of general web-based user interfaces with several advanced natural modalities in order to communicate and exchange information. The MWP is a system in which all modules are embedded within generic network-based architecture. When using it, the fusion of user front ends with new modalities requires as little intervention to the code of the web application as possible. The fusion is implemented within user front ends and retains the web-application code and its functionalities intact. © 2013 Taylor & Francis Group, LLC.",,"Cognitive information processing; Distributed platforms; Effective solution; Exchange of information; Multi-modal interfaces; Multiservice network; Network-based architectures; Web-based solutions; Data processing; User interfaces; Websites; Applications",2-s2.0-84882436254
"Sleiman H.A., Corchuelo R.","A survey on region extractors from web documents",2013,"IEEE Transactions on Knowledge and Data Engineering",34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881073852&doi=10.1109%2fTKDE.2012.135&partnerID=40&md5=8c1fd7fea7ad10fde7e69a1e2f113509","Extracting information from web documents has become a research area in which new proposals sprout out year after year. This has motivated several researchers to work on surveys that attempt to provide an overall picture of the many existing proposals. Unfortunately, none of these surveys provide a complete picture, because they do not take region extractors into account. These tools are kind of preprocessors, because they help information extractors focus on the regions of a web document that contain relevant information. With the increasing complexity of web documents, region extractors are becoming a must to extract information from many websites. Beyond information extraction, region extractors have also found their way into information retrieval, focused web crawling, topic distillation, adaptive content delivery, mashups, and metasearch engines. In this paper, we survey the existing proposals regarding region extractors and compare them side by side. © 1989-2012 IEEE.","Enterprise information integration; Information extractors; Region extractors; Web documents; Wrappers","Enterprise information integration; Information extractors; Region extractors; Web document; Wrappers; Distillation; Information retrieval; Information retrieval systems; Surveys; World Wide Web",2-s2.0-84881073852
"Al-Khanjari Z.A., Baghdadi Y., Al-Hamdani A., Al-Kindi S.","DBSoft: A toolkit for testing database transactions",2013,"Journal of Emerging Technologies in Web Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883565079&doi=10.4304%2fjetwi.5.3.205-212&partnerID=40&md5=679ba1704ea83ba3d9fb7094dc8d6fc6","Databases (DBs) are used in all enterprise transactions, which require attention not only to the consistency of DB, but also to existence, accuracy and correctness of data required by the transactions. While the Atomicity, Consistency, Isolation, and Durability (ACID) properties of a transaction ensure that DB is consistent after the execution of each transaction, it is not sure that the transactions retrieve the correct data. Indeed, the testing phase of the transactions, in the development process, is often ignored. Therefore, there is a need for testing techniques and tools. This paper proposes an architecture, a design, and an implementation of a tester, we refer to as DBSoft, to test transactions, in terms of required data they need to access. The architecture of DBSoft is a layered one. It is made of five components having separate concerns and serving each other: (C1) a parser to collect information, specifically for the metadata, (C2) an input generator to generate test cases, (C3) an output generator to implement the test cases, (C4) an output validator to validate test cases, and (C5) a report generator to generate test reports. DBSoft aims at avoiding cost effective transaction run-time errors. ©2013 Academy Publisher.","Databases; Metadata; Testing Tools; Transactions; XML",,2-s2.0-84883565079
"Muchemi L., Popowich F.","An ontology-based architecture for natural language access to relational databases",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880720581&doi=10.1007%2f978-3-642-39188-0-53&partnerID=40&md5=529c96c21d707fe48d2df7de99740956","Natural language (NL) accessto databases is a problem that has interested researchers for many years. We demonstrate that an ontology-based approach is technically feasible to handle some of the challenges facing NL query processing for database access. This paper presents the architecture, algorithms and results from the prototype thereof which indicate a domain and language independent architecture with high precision and recall rates. Studies are conducted for each of English and Swahili queries, both for same language and cross-lingual retrieval, from which we demonstrate promising precision and recall rates, language and domain independence, and that for language pairs it is sufficient to incorporate a machine translation system at the gazetteer level. © 2013 Springer-Verlag Berlin Heidelberg.","Databases; Natural Language Interfaces; Ontologies","Crosslingual retrieval; Domain independences; Language independents; Machine translation systems; Natural language interfaces; Natural languages; Precision and recall; Relational Database; Database systems; Design; Natural language processing systems; Ontology; Tools; Human computer interaction",2-s2.0-84880720581
"Kaliszyk C., Urban J.","Automated reasoning service for HOL Light",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880708235&doi=10.1007%2f978-3-642-39320-4_8&partnerID=40&md5=c0b992e1b448ee4535770647b9522555","HOL(y)Hammer is an AI/ATP service for formal (computer-understandable) mathematics encoded in the HOL Light system, in particular for the users of the large Flyspeck library. The service uses several automated reasoning systems combined with several premise selection methods trained on previous Flyspeck proofs, to attack a new conjecture that uses the concepts defined in the Flyspeck library. The public online incarnation of the service runs on a 48-CPU server, currently employing in parallel for each task 25 AI/ATP combinations and 4 decision procedures that contribute to its overall performance. The system is also available for local installation by interested users, who can customize it for their own proof development. An Emacs interface allowing parallel asynchronous queries to the service is also provided. The overall structure of the service is outlined, problems that arise are discussed, and an initial account of using the system is given. © 2013 Springer-Verlag Berlin Heidelberg.",,"Automated reasoning; Decision procedure; Light systems; Proof development; Selection methods; Artificial intelligence; Computer science; Mathematical techniques",2-s2.0-84880708235
"Liao Z., Zhang Z.","Learning to map chinese sentences to logical forms",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880756498&doi=10.1007%2f978-3-642-39787-5-38&partnerID=40&md5=99b88e79fe5c36400f684e5b808d76d0","This paper addresses the problem of learning to map Chinese sentences to logical forms. The training data consist of Chinese natural language sentences paired with logical representations of their meaning. Although many approaches have been developed for learning to map from some western natural languages to two different meaning representations, there is no such approached for Chinese language. To this end, a Chinese dataset with 880 (Chinese sentence, logical form) pairs was developed. Then, the unification-based learning (UBL) approach which induces a probabilistic Combinatory Categorial Grammar (CCG) with higher-order unification is applied to the task of learning. Experimental results show high accuracy on benchmark datasets in Chinese language with two different meaning representations. © 2013 Springer-Verlag Berlin Heidelberg.",,"Benchmark datasets; Chinese language; Chinese natural languages; Chinese sentence; Combinatory categorial grammar (CCG); Higher-order unification; Logical representations; Natural languages; Computational linguistics; Natural language processing systems",2-s2.0-84880756498
"Wang T., Zhu Q., Wang S.","Fact statements verification based on semantic similarity",2013,"Jisuanji Xuebao/Chinese Journal of Computers",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883259813&doi=10.3724%2fSP.J.1016.2013.01668&partnerID=40&md5=900e3db1ab679cae76e4ece89927eac1","Untruthful fact statements mislead users and affect user experience. How to determine whether a fact statement is trustful is required. In this paper, we propose a method, called MFSV, to determine the truthfulness of a fact statement. MFSV is based on the features of fact statements. By collecting and analyzing information related to the fact statement whose truthfulness is needed to be determined, the semantic similarity between related information and the corresponding fact statement are computed. Considering popularity and importance of related information, the credibility ranking of related information is captured. Combining the credibility ranking and the corresponding semantic similarity, we measure the contributions of related information to the truthfulness determination of the fact statement and determine whether the fact statement is trustful or not. We run a set of experiments for evaluating MFSV and the results show the method is reasonable and the precision is higher.","Credibility ranking; Fact statements; Semantic similarity; Trustfulness determination","Credibility ranking; Fact statements; Semantic similarity; Trustfulness determination; User experience; Hardware; Software engineering",2-s2.0-84883259813
"Okhotin A.","Conjunctive and boolean grammars: The true general case of the context-free grammars",2013,"Computer Science Review",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880701745&doi=10.1016%2fj.cosrev.2013.06.001&partnerID=40&md5=31c104c547cbf9bd101f843406b39291","Conjunctive grammars extend the definition of a context-free grammar by allowing a conjunction operation in the rules; Boolean grammars are further equipped with an explicit negation. These grammars maintain the main principle of the context-free grammars, that of defining syntactically correct strings inductively from their substrings, but lift the restriction of using disjunction only. This paper surveys the results on conjunctive and Boolean grammars obtained over the last decade, comparing them to the corresponding results for ordinary context-free grammars and their main subfamilies. Much attention is given to parsing algorithms, most of which are inherited from the case of ordinary context-free grammars without increasing their computational complexity. The intended readership includes any computer scientists looking for a compact and accessible description of this formal model and its properties, as well as for a general outlook on formal grammars. The paper is also addressed to theoretical computer scientists seeking a subject for research; an account of pure theoretical research in the area presented in this paper is accompanied by a list of significant open problems, with an award offered for the first correct solution of each problem. Several directions for future investigation are proposed. © 2013 Elsevier Inc.","Boolean grammars; Conjunctive grammars; Context-free grammars; Formal languages; Language equations; Parsing","Boolean grammars; Computer scientists; Conjunctive grammars; Correct solution; Language equations; Parsing; Parsing algorithm; Theoretical research; Formal languages; Context free grammars",2-s2.0-84880701745
"Bollegala D., Weir D., Carroll J.","Cross-domain sentiment classification using a sentiment sensitive thesaurus",2013,"IEEE Transactions on Knowledge and Data Engineering",69,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897584660&doi=10.1109%2fTKDE.2012.103&partnerID=40&md5=3ad2e3f3c823d14b39d3a98fe57ad711","Automatic classification of sentiment is important for numerous applications such as opinion mining, opinion summarization, contextual advertising, and market analysis. Typically, sentiment classification has been modeled as the problem of training a binary classifier using reviews annotated for positive or negative sentiment. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is costly. Applying a sentiment classifier trained using labeled data for a particular domain to classify sentiment of user reviews on a different domain often results in poor performance because words that occur in the train (source) domain might not appear in the test (target) domain. We propose a method to overcome this problem in cross-domain sentiment classification. First, we create a sentiment sensitive distributional thesaurus using labeled data for the source domains and unlabeled data for both source and target domains. Sentiment sensitivity is achieved in the thesaurus by incorporating document level sentiment labels in the context vectors used as the basis for measuring the distributional similarity between words. Next, we use the created thesaurus to expand feature vectors during train and test times in a binary classifier. The proposed method significantly outperforms numerous baselines and returns results that are comparable with previously proposed cross-domain sentiment classification methods on a benchmark data set containing Amazon user reviews for different types of products. We conduct an extensive empirical analysis of the proposed method on single- and multisource domain adaptation, unsupervised and supervised domain adaptation, and numerous similarity measures for creating the sentiment sensitive thesaurus. Moreover, our comparisons against the SentiWordNet, a lexical resource for word polarity, show that the created sentiment-sensitive thesaurus accurately captures words that express similar sentiments. © 2013 IEEE.","Cross-domain sentiment classification; Domain adaptation; Thesauri creation","Automatic classification; Binary classifiers; Contextual advertisings; Distributional similarities; Domain adaptation; Negative sentiments; Sentiment classification; Similarity measure; Classification (of information); Text processing; Thesauri",2-s2.0-84897584660
"Sassaman L., Patterson M.L., Bratus S., Locasto M.E.","Security applications of formal language theory",2013,"IEEE Systems Journal",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880540238&doi=10.1109%2fJSYST.2012.2222000&partnerID=40&md5=5f77125f8a25c7455000226c2b20f0ca","We present a formal language theory approach to improving the security aspects of protocol design and message-based interactions in complex composed systems. We argue that these aspects are responsible for a large share of modern computing systems' insecurity. We show how our approach leads to advances in input validation, security modeling, attack surface reduction, and ultimately, software design and programming methodology. We cite examples based on real-world security flaws in common protocols, representing different classes of protocol complexity. We also introduce a formalization of an exploit development technique, the parse tree differential attack, made possible by our conception of the role of formal grammars in security. We also discuss the negative impact unnecessarily increased protocol complexity has on security. This paper provides a foundation for designing verifiable critical implementation components with considerably less burden to developers than is offered by the current state of the art. In addition, it offers a rich basis for further exploration in the areas of offensive analysis and, conversely, automated defense tools, and techniques. © 2012 IEEE.","Language-theoretic security; secure composition; secure protocol design","Development technique; Differential attacks; Formal language theory; Language-theoretic security; Programming methodology; Secure composition; Secure protocols; Security application; Formal languages; Network security",2-s2.0-84880540238
"Sun J., Jing R., Wang Y., Zhu T., Li M., Li Y.","PPM-Dom: A novel method for domain position prediction",2013,"Computational Biology and Chemistry",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880310067&doi=10.1016%2fj.compbiolchem.2013.06.002&partnerID=40&md5=48d95b45b9f8621b6592417d6c46a3ba","Domains are the structural basis of the physiological functions of proteins, and the prediction of which is an advantageous process on the study of protein structure and function. This article proposes a new complete automatic prediction method, PPM-Dom (Domain Position Prediction Method), for predicting the particular positions of domains in a target protein via its atomic coordinate. The presented method integrates complex networks, community division, and fuzzy mean operator (FMO). The whole sequences are divided into potential domain regions by the complex network and community division, and FMO allows the final determination for the domain position. This method will suffice to predict regions that will form a domain structure and those that are unstructured based on completely new atomic coordinate information of the query sequence, and be able to separate different domains in the same query sequence from each other. On evaluating the performance using an independent testing dataset, PPM-Dom reached 91.41% for prediction accuracy, 96.12% for sensitivity and 92.86% for specificity. The tool bag of PPM-Dom is freely available at http://cic.scu.edu.cn/bioinformatics/PPMDom.zip. © 2013 The Authors. Published by Elsevier Ltd. All rights reserved.","Community division; Complex network; Domain position prediction; Fuzzy mean operator; Protein structure","Automatic prediction; Community division; Different domains; Fuzzy means; Physiological functions; Position predictions; Prediction accuracy; Protein structures; Complex networks; Proteins; Statistical tests; Forecasting; protein; article; chemical structure; chemistry; Community division; complex network; Domain position prediction; Fuzzy mean operator; metabolism; methodology; protein structure; protein tertiary structure; sequence analysis; Community division; Complex network; Domain position prediction; Fuzzy mean operator; Protein structure; Models, Molecular; Protein Structure, Tertiary; Proteins; Sequence Analysis, Protein; Community division; Complex network; Domain position prediction; Fuzzy mean operator; Protein structure",2-s2.0-84880310067
"Pham P.T., Deschacht K., Tuytelaars T., Moens M.-F.","Naming persons in video: Using the weak supervision of textual stories",2013,"Journal of Visual Communication and Image Representation",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880069069&doi=10.1016%2fj.jvcir.2013.06.009&partnerID=40&md5=b8a24db04400efd1dbf8818f374f4a22","In this paper, we focus on the problem of automated video annotation. We report on the application of naming faces in soap series by using the weak supervision of narrative texts that describe the events in the video and that are drafted by fans. Several unsupervised methods that operate without any manual labeling of exemplar faces, and methods that use a limited number of labeled exemplars are presented and evaluated. All methods exploit the multiple co-occurrences between faces shown in the video and names mentioned in the texts to compute the strength of the linking and reinforce this coupling by means of an Expectation Maximization algorithm. We show that the unsupervised methods attain competitive results without any prior human effort. The results show F1 values between 80% and 86% for the recognition of the face-name pairs without any human supervision. These figures rise only slightly when a number of faces were manually labeled beforehand. The study gives insights in the benefits and bottlenecks of the proposed approaches, and an error analysis results in guidelines for the choice of a certain technique. © 2013 Elsevier Inc. All rights reserved.","Cross-media mining; Expectation; Maximization; Unsupervised alignment; Video annotation","Automated video; Cross-media; Expectation; Expectation-maximization algorithms; Human supervision; Manual labeling; Unsupervised method; Video annotations; Media streaming; Optimization; Visual communication; Face recognition",2-s2.0-84880069069
"Murray W.R., Harrison P., Singliar T.","Interpreting spatiotemporal expressions from english to fuzzy logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880982771&doi=10.1007%2f978-3-642-39617-5_21&partnerID=40&md5=7836a29c16a2f842bc87f6d06d0b4bcd","We discuss extensions to a controlled natural language allowing spatiotemporal expressions to be interpreted as fuzzy logic functions. The extensions first required new sentence templates. Next, changes to a GPSG parser modified its lexicon, and then extended its parsing and logical form rules to allow user-defined spatial and temporal constraints to be extracted. The sentence templates ground user-defined culturally-specific times and places to boundaries surrounding prototypical ideals. Query points, defined by location and time, are compared to these definitions using Gaussians centered at prototypical 'ideal' times or places. The Gaussians provide soft fall-off at the boundaries. Fuzzy logic operators allow larger expressions to be interpreted, analogous to Boolean combinations of terms. The mathematically-interpreted spatiotemporal terms act as domain features for a machine learning algorithm. They allow easy specification (compared to programming) of basis functions for an inverse reinforcement learning algorithm that detects anomalous vehicle tracks or suspicious agent behavior. © 2013 Springer-Verlag Berlin Heidelberg.","anomaly detection; controlled natural language; CPL; fuzzy logic; GPSG parsing; inverse reinforcement learning; spatiotemporal reasoning","Computer programming languages; Context free grammars; Learning algorithms; Reinforcement learning; Anomaly detection; Controlled natural language; CPL; GPSG parsing; Inverse reinforcement learning; Spatio-temporal reasoning; Fuzzy logic",2-s2.0-84880982771
"Miwa M., Ohta T., Rak R., Rowley A., Kell D.B., Pyysalo S., Ananiadou S.","A method for integrating and ranking the evidence for biochemical pathways by mining reactions from text",2013,"Bioinformatics",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879895059&doi=10.1093%2fbioinformatics%2fbtt227&partnerID=40&md5=5168a8b28720656437306585a92819d3","Motivation: To create, verify and maintain pathway models, curators must discover and assess knowledge distributed over the vast body of biological literature. Methods supporting these tasks must understand both the pathway model representations and the natural language in the literature. These methods should identify and order documents by relevance to any given pathway reaction. No existing system has addressed all aspects of this challenge.Method: We present novel methods for associating pathway model reactions with relevant publications. Our approach extracts the reactions directly from the models and then turns them into queries for three text mining-based MEDLINE literature search systems. These queries are executed, and the resulting documents are combined and ranked according to their relevance to the reactions of interest. We manually annotate document-reaction pairs with the relevance of the document to the reaction and use this annotation to study several ranking methods, using various heuristic and machine-learning approaches.Results: Our evaluation shows that the annotated document-reaction pairs can be used to create a rule-based document ranking system, and that machine learning can be used to rank documents by their relevance to pathway reactions. We find that a Support Vector Machine-based system outperforms several baselines and matches the performance of the rule-based system. The success of the query extraction and ranking methods are used to update our existing pathway search system, PathText. © The Author 2013.",,"algorithm; artificial intelligence; biochemistry; data mining; Medline; procedures; support vector machine; article; data mining; methodology; Algorithms; Artificial Intelligence; Biochemical Processes; Data Mining; MEDLINE; Support Vector Machines; Algorithms; Artificial Intelligence; Biochemical Processes; Data Mining; MEDLINE; Support Vector Machines",2-s2.0-84879895059
"Aggarwal C.C., Zhao P.","Towards graphical models for text processing",2013,"Knowledge and Information Systems",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878855154&doi=10.1007%2fs10115-012-0552-3&partnerID=40&md5=4ea439fd9eddd15f80e524e1f28efc84","The rapid proliferation of the World Wide Web has increased the importance and prevalence of text as a medium for dissemination of information. A variety of text mining and management algorithms have been developed in recent years such as clustering, classification, indexing, and similarity search. Almost all these applications use the well-known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve information about the relative ordering and distance between the words in the graphs and provide a much richer representation in terms of sentence structure of the underlying data. Recent advances in graph mining and hardware capabilities of modern computers enable us to process more complex representations of text. We will see that such an approach has clear advantages from a qualitative perspective. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation. We will apply this technique to a variety of mining and management applications and show its advantages and richness in exploring the structure of the underlying text documents. © 2012 Springer-Verlag London Limited.","Text classification; Text clustering; Text representation; Text search",,2-s2.0-84878855154
"Kahlaoui A., Abran A.","Demystifying domain specific languages",2013,"Progressions and Innovations in Model-Driven Software Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944878280&doi=10.4018%2f978-1-4666-4217-1.ch009&partnerID=40&md5=5ba27b1dc780fab19ff7c8b15ac4b3bd","Domain Specific Languages (DSLs) provide interesting characteristics that align well with the goals and mission of model-driven software engineering. However, there are still some issues that hamper widespread adoption. In this chapter, the authors discuss two of these issues. The first relates to the vagueness of the term DSL, which they address by studying the individual terms: domain, specificity, and language. The second is related to the difficulty of developing DSLs, which they address with a view to making DSL development more accessible via processes, standards, and tools.",,"Computer programming languages; Software engineering; Domain specific languages; Model driven software engineering; Digital subscriber lines",2-s2.0-84944878280
"Angluin D., Aspnes J., Eisenstat S., Kontorovich A.","On the learnability of shuffle ideals",2013,"Journal of Machine Learning Research",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884248690&partnerID=40&md5=2afd0bdacc8dc9af991e2d0b011ead15","PAC learning of unrestricted regular languages is long known to be a difficult problem. The class of shuffle ideals is a very restricted subclass of regular languages, where the shuffle ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shuffle ideals appears quite difficult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP ≠ NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution. © 2013 Dana Angluin, James Aspnes, Sarah Eisenstat and Aryeh Kontorovich.","Deterministic finite automata; PAC learning; Regular languages; Shuffle ideals; Statistical queries; Subsequences","Deterministic finite automata; PAC learning; Shuffle ideals; Statistical queries; Subsequences; Algorithms; Formal languages; Polynomial approximation; Automata theory",2-s2.0-84884248690
"Liang P., Jordan M.I., Klein D.","Learning dependency-based compositional semantics",2013,"Computational Linguistics",46,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877085938&doi=10.1162%2fCOLI_a_00127&partnerID=40&md5=eadddfc9c0668c30e38894189bcb3b36","Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive. Our goal is to instead learn a semantic parser from question-answer pairs, where the logical form is modeled as a latent variable. We develop a new semantic formalism, dependency-based compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The model parameters are estimated using a simple procedure that alternates between beam search and numerical optimization. On two standard semantic parsing benchmarks, we show that our system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms. © 2013 Association for Computational Linguistics.",,,2-s2.0-84877085938
"Fernández-Trujillo J.P., Lester G.E., Dos-Santos N., Juan A.M., Esteva J., Jifon J.L., Varó P.","Pre-and postharvest muskmelon fruit cracking: Causes and potential remedies",2013,"HortTechnology",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879287381&partnerID=40&md5=7d6c5949ed7515e151d545a3e2a7bd07","Fruit cracking is an important disorder that can cause severe loss of marketable yield and revenue in the muskmelon (Cucumis melo) fruit industry. The physiological and environmental factors causing cracking are poorly understood. Although generally considered a physiological disorder caused by fluctuating environmental conditions, current evidence indicates that this disorder also has a genetic as well as a genotype · environment component. Certain cultivars are more susceptible than others, but wide fluctuations in irrigation, temperature, and nutrition during late fruit maturation stages appear to predispose fruit to cracking. This article summarizes the current state of our understanding of the causes of fruit splitting in muskmelons.","Cucumis melo; Environmental factors; Fruit quality; Fruit splitting; Near-isogenic lines; Netting; Physiological disorder","Cucumis; Cucumis melo",2-s2.0-84879287381
"Hoelz B.W.P., Ralha C.G.","A framework for semantic annotation of digital evidence",2013,"Proceedings of the ACM Symposium on Applied Computing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877931401&doi=10.1145%2f2480362.2480729&partnerID=40&md5=de72e16294b2b6001a776848beb40102","Most tools used during the forensic examination process emphasize data and metadata extraction without a formal definition of the concepts used in their outputs. These vary not only in the terminology used, but also in the way values are represented. These differences hinder the adoption of computer-assisted analysis, since the elements to be analyzed are not well-defined, requiring ad hoc parsers to process and interpret the output of each tool. A framework for semantic annotation of digital evidence is presented in this work. Semantic annotations use concepts that are defined in an ontology to describe the annotated object. They can replace raw metadata, user-defined labels and tool-specific analysis results with computer-readable, formally defined terms that can be used in semantically advanced queries. The framework's components provide means to extract, analyze and index the contents of the digital evidence. The framework allows the augmentation of a base ontology, by adding domain and case-specific concepts to it. A prototype implementation is described and a case study is conducted to illustrate its potential uses and improvements to the forensic examination process. Copyright 2013 ACM.","Digital evidence; Metadata; Ontology; Semantic annotation","Computer-assisted analysis; Data and metadata; Digital evidence; Forensic examinations; Formal definition; Prototype implementations; Semantic annotations; Use concept; Computer aided analysis; Ontology; Semantics; Metadata",2-s2.0-84877931401
"Burghouts G.J., De Penning L., Kruithof M., Hanckmann P., Ten Hove J.-M., Landsmeer S., Van Den Broek S.P., Den Hollander R., Van Leeuwen C., Korzec S., Bouma H., Schutte K.","A search engine for retrieval and inspection of events with 48 human actions in realistic videos",2013,"ICPRAM 2013 - Proceedings of the 2nd International Conference on Pattern Recognition Applications and Methods",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877955638&partnerID=40&md5=2312958b77a91afe504e81f4c2c2f820","The contribution of this paper is a search engine that recognizes and describes 48 human actions in realistic videos. The core algorithms have been published recently, from the early visual processing (Bouma, 2012), discriminative recognition (Burghouts, 2012) and textual description (Hanckmann, 2012) of 48 human actions. We summarize the key algorithms and specify their performance. The novelty of this paper is that we integrate these algorithms into a search engine. In this paper, we add an algorithm that finds the relevant spatio-temporal regions in the video, which is the input for the early visual processing. As a result, metadata is produced by the recognition and description algorithms. The meta-data is filtered by a novel algorithm that selects only the most informative parts of the video. We demonstrate the power of our search engine by retrieving relevant parts of the video based on three different queries. The search results indicate where specific events occurred, and which actors and objects were involved. We show that events can be successfully retrieved and inspected by usage of the proposed search engine.","48 human actions; Action recognition; Human behavior understanding; Indexing; Meta data; Search engine; Textual description; Video retrieval","Action recognition; Human actions; Human behavior understanding; Textual description; Video retrieval; Algorithms; Image recognition; Indexing (of information); Metadata; Motion estimation; Search engines",2-s2.0-84877955638
"Orsini M., Carcangiu S., Cuccuru G., Uva P., Tramontano A.","The PARIGA Server for Real Time Filtering and Analysis of Reciprocal BLAST Results",2013,"PLoS ONE",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877116133&doi=10.1371%2fjournal.pone.0062224&partnerID=40&md5=9a7e2cc2dab0defcea23fde3b1c38a1f","BLAST-based similarity searches are commonly used in several applications involving both nucleotide and protein sequences. These applications span from simple tasks such as mapping sequences over a database to more complex procedures as clustering or annotation processes. When the amount of analysed data increases, manual inspection of BLAST results become a tedious procedure. Tools for parsing or filtering BLAST results for different purposes are then required. We describe here PARIGA (http://resources.bioinformatica.crs4.it/pariga/), a server that enables users to perform all-against-all BLAST searches on two sets of sequences selected by the user. Moreover, since it stores the two BLAST output in a python-serialized-objects database, results can be filtered according to several parameters in real-time fashion, without re-running the process and avoiding additional programming efforts. Results can be interrogated by the user using logical operations, for example to retrieve cases where two queries match same targets, or when sequences from the two datasets are reciprocal best hits, or when a query matches a target in multiple regions. The Pariga web server is designed to be a helpful tool for managing the results of sequence similarity searches. The design and implementation of the server renders all operations very fast and easy to use. © 2013 Orsini et al.",,"access to information; article; BLAST program; computer program; computer system; nucleic acid database; online system; PARIGA server; protein database; reference database; sequence analysis; sequence homology; web browser; Computational Biology; Databases, Genetic; Databases, Protein; Internet; Software; Time Factors",2-s2.0-84877116133
"Orimaye S.O., Alhashmi S.M., Siew E.-G.","Performance and trends in recent opinion retrieval techniques",2013,"Knowledge Engineering Review",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919828358&doi=10.1017%2fS0269888913000167&partnerID=40&md5=70c72efc5bb64e7921d07b1ab51c4b37","This paper presents trends and performance of opinion retrieval techniques proposed within the last 8 years. We identify major techniques in opinion retrieval and group them into four popular categories. We describe the state-of-the-art techniques for each category and emphasize on their performance and limitations. We then summarize with a performance comparison table for the techniques on different datasets. Finally, we highlight possible future research directions that can help solve existing challenges in opinion retrieval. © 2013 Cambridge University Press.",,"Opinion retrievals; Performance comparison; Possible futures; State-of-the-art techniques",2-s2.0-84919828358
"Cao W., Shasha D.","AppSleuth: A tool for database tuning at the application level",2013,"ACM International Conference Proceeding Series",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876804060&doi=10.1145%2f2452376.2452445&partnerID=40&md5=55840e29654685ff56b81092d5100578","Excellent work ([1]-[6]) has shown that memory management and transaction concurrency levels can often be tuned automatically by the database management systems. Other excellent work ([7]]-[14]) has shown how to use the optimizer to do automatic physical design or to make the optimizer itself more self-adaptive ([15]-[17]). Our performance tuning experience across various industries (finance, gaming, data warehouses, and travel) has shown that enormous additional tuning benefits (sometimes amounting to orders of magnitude) can come from reengineering application code and table design. The question is: can a tool help in this effort? We believe so. We present a tool called AppSleuth that parses application code and the tracing log for two popular database management systems in order to lead a competent tuner to the hot spots in an application. This paper discusses (i) representative application ""delinquent design patterns"", (ii) an application code parser to find them, (iii) a log parser to identify the patterns that are critical, and (iv) a display to give a global view of the issue. We present an extended sanitized case study from a real travel application to show the results of the tool at different stages of a tuning engagement, yielding a 300 fold improvement. This is the first tool of its kind that we know of. © 2013 ACM.","application-level optimization; database tuning; performance tool","Application codes; Application level; Different stages; Memory management; Orders of magnitude; Performance tools; Performance tuning; Physical design; Data warehouses; Management information systems; Management",2-s2.0-84876804060
"Su W., Wu H., Li Y., Zhao J., Lochovsky F.H., Cai H., Huang T.","Understanding query interfaces by statistical parsing",2013,"ACM Transactions on the Web",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879928149&doi=10.1145%2f2460383.24603&partnerID=40&md5=7aebafcfe2742edf0b009397b8214bb2","Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers. © 2013 ACM.","Maximum entropy; Query interface","Hierarchical queries; Maximum-entropy principle; Online database; Parsing methods; Query capabilities; Query interfaces; Statistical parser; Statistical parsing; Formal languages; Maximum entropy methods; Probability; Query languages; Query processing",2-s2.0-84879928149
"Kiani S., Shirazi H., Javanmard M.","Providing a new optimization structure for quickening the performance of the unstructured solicitations",2013,"Indian Journal of Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879256268&partnerID=40&md5=c6e60ea833fd110947c997820848ccce","Through this paper, we will demonstrate that how we can decrease the performance duration of the solicitations on XML large files; and why such files can't be read as a single file through the memory; or why doesn't the typical detection and derivation mechanism of XML (e.g. DOM - SAX) perform quickly for implementing solicitations, or why are they ineffective during processing. We defined a new concept as Skeletal Documents that is preserved and maintained in memory as a file, which by using of parser is read PULL. This affair is used to show the document structure and to select its components.","Query; Semantic Web; Skeletal Documents; Unstructured; XML-Documents",,2-s2.0-84879256268
"Tran D., Nguyen H.","API specification-based function search engine using natural language query",2013,"2013 International Conference on Computing, Management and Telecommunications, ComManTel 2013",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875922643&doi=10.1109%2fComManTel.2013.6482380&partnerID=40&md5=1385ddbad6b437b113842ac967fa014c","Programmers nearly always use existing functions while developing their applications. However, the functions have grown more numerous and more diverse while the applications have grown more dependent on them. Thus, it's difficult for programmers to find what functions they want and know how to call those functions [1]. This paper present two novel approaches to address these problems. The first is the approach to find right functions based on the API specification. This approach can search suitable functions by their functionalities described in the API specification. The second is approach to automatically generate code for 'function call'. In the second approach, programmer can call a function by natural language query. We have implemented a function search engine for Java, called FSE. Besides, we have also performed some evaluations to demonstrate that FSE is better than the existing online search engines in precision and recall. © 2013 IEEE.","API specification; Code search engine; natural language processing; software reuse","API specifications; Code search engine; Function calls; NAtural language processing; Natural language queries; Precision and recall; Computer software reusability; Search engines; Specifications; Natural language processing systems",2-s2.0-84875922643
"Liao H., Shan W., Gao H.","Automatic parallelization of XQuery programs",2013,"Journal of Software",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875750864&doi=10.4304%2fjsw.8.4.842-851&partnerID=40&md5=0798f2db63882c1ab1d26701cc859d88","XQuery is a functional language with implicit parallelism. It is an important approach to improve the efficiency of XML query by taking full advantage of multi-core environment in the parallel implementation of XQuery language. In this paper, we propose an implementation method for parallelizing XML query represented by XQuery programs automatically. According to the features of its functional language, an XQuery program is divided into a number of tasks that can be executed in parallel. Then, on the basis of the running cost evaluation, three kinds of parallelism are applied to different tasks and they are data parallelism, task parallelism and pipeline parallelism. Under the guidance of a novel scheduling strategy, the execution of the XQuery program is parallelized automatically. The experiments show that this approach improves the efficiency of the execution of XQuery programs and the computing resources of multi-core computer are used efficiently. © 2013 ACADEMY PUBLISHER.","Implicit parallelism; Multi-core; Task partitioning; Task scheduling; XML; XQuery","Implicit parallelisms; Multi core; Task partitioning; Task-scheduling; XQuery; XML; Multicore programming",2-s2.0-84875750864
"Mukherjee A., Sengupta S., Chakraborty D., Sen A., Garain U.","Text-to-diagram conversion: A method for formal representation of natural language geometry problems",2013,"IASTED Multiconferences - Proceedings of the IASTED International Conference on Artificial Intelligence and Applications, AIA 2013",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875488330&doi=10.2316%2fP.2013.793-042&partnerID=40&md5=3d2c178b8f993cedf59702fabd2097b9","Natural language geometry problems are translated into formal representation. This is done as an essential step involved in text to diagram conversion. A parser is designed that analyzes a problem statement in order to describe it as a language independent, unambiguous formal representation. Natural language processing tools and a lexical knowledge base are used to assist the parser that finally generates a graph as the parsing output. The parse graph is the formal representation of the input natural language problem. This graph is later translated into another intermediate representation consisting of a set of graphics-friendly statements. High school level geometry problems are used to develop and test the proposed methods. Experimental results show high accuracy of the approach in translating a natural language problem into a formal description.","Complexity analysis; Formal description; Knowledgebase; Natural language problems; Parse graph; Parsing algorithm; Text to diagram conversion","Complexity analysis; Formal Description; Knowledge base; Natural languages; Parse graph; Parsing algorithm; Artificial intelligence; Geometry; Knowledge based systems; Natural language processing systems",2-s2.0-84875488330
"Nguyen D., Trieschnigg D., Theune M.","Folktale classification using learning to rank",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875446028&doi=10.1007%2f978-3-642-36973-5_17&partnerID=40&md5=c8138290d75213cb738ecf3dfbeb3c90","We present a learning to rank approach to classify folktales, such as fairy tales and urban legends, according to their story type, a concept that is widely used by folktale researchers to organize and classify folktales. A story type represents a collection of similar stories often with recurring plot and themes. Our work is guided by two frequently used story type classification schemes. Contrary to most information retrieval problems, the text similarity in this problem goes beyond topical similarity. We experiment with approaches inspired by distributed information retrieval and features that compare subject-verb-object triplets. Our system was found to be highly effective compared with a baseline system. © 2013 Springer-Verlag.",,"Baseline systems; Distributed information retrieval; Fairy tales; Information retrieval problems; Learning to rank; Text similarity; Type classifications; Artificial intelligence; Information retrieval",2-s2.0-84875446028
"Sakaue D., Itoyama K., Ogata T., Okuno H.G.","Robust multipitch analyzer against initialization based on latent harmonic allocation using overtone corpus",2013,"Journal of Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876222903&doi=10.2197%2fipsjjip.21.246&partnerID=40&md5=f4fc65ee84d4cb92aa9d98cfead23007","We present a Bayesian analysis method that estimates the harmonic structure of musical instruments in music signals on the basis of psychoacoustic evidence. Since the main objective of multipitch analysis is joint estimation of the fundamental frequencies and their harmonic structures, the performance of harmonic structure estimation significantly affects fundamental frequency estimation accuracy. Many methods have been proposed for estimating the harmonic structure accurately, but no method has been proposed that satisfies all these requirements: robust against initialization, optimization-free, and psychoacoustically appropriate and thus easy to develop further. Our method satisfies these requirements by explicitly incorporating Terhardt's virtual pitch theory within a Bayesian framework. It does this by automatically learning the valid weight range of the harmonic components using a MIDI synthesizer. The bounds are termed ""overtone corpus."" Modeling demonstrated that the proposed overtone corpus method can stably estimate the harmonic structure of 40 musical pieces for a wide variety of initial settings. © 2013 Information Processing Society of Japan.","Harmonic clustering; Multipitch estimation; Musical instrument sounds; Overtone estimation",,2-s2.0-84876222903
"Burroughs S.K., Kaluz S., Wang D., Wang K., Van Meir E.G., Wang B.","Hypoxia inducible factor pathway inhibitors as anticancer therapeutics",2013,"Future Medicinal Chemistry",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876133472&doi=10.4155%2ffmc.13.17&partnerID=40&md5=c2331f301c92e9d84dd8d121f56278fa","Hypoxia is a significant feature of solid tumor cancers. Hypoxia leads to a more malignant phenotype that is resistant to chemotherapy and radiation, is more invasive and has greater metastatic potential. Hypoxia activates the hypoxia inducible factor (HIF) pathway, which mediates the biological effects of hypoxia in tissues. The HIF complex acts as a transcription factor for many genes that increase tumor survival and proliferation. To date, many HIF pathway inhibitors indirectly affect HIF but there have been no clinically approved direct HIF inhibitors. This can be attributed to the complexity of the HIF pathway, as well as to the challenges of inhibiting protein-protein interactions. © 2013 Future Science Ltd.",,"2 methoxyestra 1,3,5(10),16 tetraene 3 carboxamide; 2 methoxyestradiol; alvespimycin; ansamycin derivative; antineoplastic agent; bortezomib; camptothecin; echinomycin; geldanamycin; hypoxia inducible factor; hypoxia inducible factor 1alpha; hypoxia inducible factor 2alpha; irinotecan; miltefosine; mitogen activated protein kinase; perifosine; protein kinase B; protein p53; reactive oxygen metabolite; romidepsin; tanespimycin; temsirolimus; topotecan; ubiquitin protein ligase E3; acetylation; antiangiogenic activity; antiproliferative activity; binding affinity; breast cancer; cancer growth; cancer therapy; castration resistant prostate cancer; colorectal cancer; cutaneous T cell lymphoma; DNA supercoiling; down regulation; genetic transcription; human; IC 50; lung small cell cancer; metastasis; multiple myeloma; nonhuman; phase 1 clinical trial (topic); priority journal; prostate cancer; protein protein interaction; respiratory chain; review; skin lymphoma; tissue survival; transcription initiation; tumor growth; tumor microenvironment; ubiquitination; uterine cervix carcinoma; Animals; Antineoplastic Agents; Cell Hypoxia; Gene Expression Regulation, Neoplastic; Humans; Hypoxia-Inducible Factor 1; Molecular Targeted Therapy; Neoplasms; Signal Transduction",2-s2.0-84876133472
"Jörges S.","Construction and evolution of code generators: A model-driven and service-oriented approach",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874502400&partnerID=40&md5=3a3325ad07091d9b49a12a24c6c4ca6f","Automatic code generation is an essential cornerstone of model-driven approaches to software development. It closes the gap that emerges when models are used to abstract from a concrete software system, and thus is to models what compilers are to high-level languages. Consequently, the simple and fast development of code generators is a key requirement of today's approaches to model-driven development, which are increasingly characterized by a strong focus on agility and domain-specificity. Currently, many techniques are available that support the specification and implementation of code generators, such as engines based on templates or rule-based transformations. All these techniques have in common that code generators are either directly programmed or described by means of textual specifications. This monograph presents Genesys, a general approach, which advocates the graphical development of code generators for arbitrary source and target languages, on the basis of models and services. In particular, it is designed to support incremental language development on arbitrary metalevels. The use of models allows building of code generators in a truly platform-independent and domain-specific way. Furthermore, models are amenable to formal verification methods such as model checking, which increase the reliability and robustness of the code generators. Services enable the reuse and integration of existing code generation frameworks and tools regardless of their complexity, and at the same time manifest as easy-to-use building blocks that facilitate agile development through quick interchangeability. Both models and services are reusable and thus form a growing repository for the fast creation and evolution of code generators. This book shows these and further advantages arising from the Genesys approach by means of a full-fledged reference implementation, which has been field-tested in a large number of case studies.. © 2013 Springer-Verlag Berlin Heidelberg.",,"Agile development; Automatic code generations; Building blockes; Code Generation; Code generators; Domain specific; Formal verification methods; General approach; GENESYS; Graphical development; Language development; Meta levels; Model driven approach; Model driven development; Model-driven; Reference implementation; Reliability and robustness; Reuse and integrations; Rule-based transformation; Service-oriented approaches; Software systems; Target language; Computer programming languages; Model checking; Software engineering; Specifications; Systems analysis; Building codes",2-s2.0-84874502400
"Syeed M.M.M., Aaltonen T., Hammouda I., Systä T.","Tool assisted analysis of open source projects: A multi-faceted challenge",2013,"Open Source Software Dynamics, Processes, and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944937891&doi=10.4018%2f978-1-4666-2937-0.ch006&partnerID=40&md5=c6c3e037873c44821a66542758a296a8","Open Source Software (OSS) is currently a widely adopted approach to developing and distributing software. OSS code adoption requires an understanding of the structure of the code base. For a deeper understanding of the maintenance, bug fixing and development activities, the structure of the developer community also needs to be understood, especially the relations between the code and community structures. This, in turn, is essential for the development and maintenance of software containing OSS code. This paper proposes a method and support tool for exploring the relations of the code base and community structures of OSS projects. The method and proposed tool, Binoculars, rely on generic and reusable query operations, formal definitions of which are given in the paper. The authors demonstrate the applicability of Binoculars with two examples. The authors analyze a well-known and active open source project, FFMpeg, and the open source version of the IaaS cloud computing project Eucalyptus. © 2013 by IGI Global. All rights reserved.",,"Binoculars; Bins; Codes (symbols); Open source software; Social sciences; Software engineering; Community structures; Development activity; Formal definition; Iaas clouds; Open source projects; Open sources; Query operations; Support tool; Open systems",2-s2.0-84944937891
"Szczuka M., Janusz A.","Semantic clustering of scientific articles using explicit semantic analysis",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873389226&doi=10.1007%2f978-3-642-36505-8-6&partnerID=40&md5=53ebf91ffb344488cd1c3bac4ab8e4e9","This paper summarizes our recent research on semantic clustering of scientific articles. We present a case study which was focused on analysis of papers related to the Rough Sets theory. The proposed method groups the documents on the basis of their content, with an assistance of the DBpedia knowledge base. The text corpus is first processed using Natural Language Processing tools in order to produce vector representations of the content. In the second step the articles are matched against a collection of concepts retrieved from DBpedia. As a result, a new representation that better reflects the semantics of the texts, is constructed. With this new representation the documents are hierarchically clustered in order to form a partitioning of papers into semantically related groups. The steps in textual data preparation, the utilization of DBpedia and the employed clustering methods are explained and illustrated with experimental results. A quality of the resulting clustering is then discussed. It is assessed using feedback form human experts combined with typical cluster quality measures. These results are then discussed in the context of a larger framework that aims to facilitate search and information extraction from large textual repositories. © 2013 Springer-Verlag Berlin Heidelberg.","DBpedia; document grouping; rough sets; semantic clustering; Text mining","DBpedia; document grouping; Rough set; Semantic clustering; Text mining; Data mining; Knowledge based systems; Natural language processing systems; Rough set theory; Text processing; Semantics",2-s2.0-84873389226
"Marlet R.","Program Specialization",2013,"Program Specialization",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891585074&doi=10.1002%2f9781118576984&partnerID=40&md5=24debd0db31703d6a887c69dd72e3dfd","This book presents the principles and techniques of program specialization - a general method to make programs faster (and possibly smaller) when some inputs can be known in advance. As an illustration, it describes the architecture of Tempo, an offline program specializer for C that can also specialize code at runtime, and provides figures for concrete applications in various domains. Technical details address issues related to program analysis precision, value reification, incomplete program specialization, strategies to exploit specialized program, incremental specialization, and data specialization. The book, that targets both researchers and software engineers, also opens scientific and industrial perspectives. © 2013 by John Wiley & Sons, Inc.",,,2-s2.0-84891585074
"Medina-Aunon J.A., Krishna R., Ghali F., Albar J.P., Jones A.J.","A guide for integration of proteomic data standards into laboratory workflows",2013,"Proteomics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873988752&doi=10.1002%2fpmic.201200268&partnerID=40&md5=c2e56815379676b6cde1a8ac9c1c82ae","The development of the HUPO-Proteomics Standards Initiative standard data formats and Minimum Information About a Proteomics Experiment guidelines facilitate coordination within the scientific community. The data standards provide a framework to exchange and share data regardless of the source instrument or software. Nevertheless there remains a view that Proteomics Standards Initiative standards are challenging to use and integrate into routine laboratory pipelines. In this article, we review the tools available for integrating the different data standards and building compliant software. These tools are focused on a range of different data types and support different scenarios, intended for software developers or end users, allowing the standards to be used in a straightforward manner. © 2013 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.","Bioinformatics; Mass spectrometry and review; Proteomics Standards Initiative; Software evaluation","bioinformatics; computer program; laboratory test; mass spectrometry; molecular interaction; practice guideline; priority journal; protein analysis; proteomics; review; Automatic Data Processing; Guidelines as Topic; Information Dissemination; Information Management; Proteomics; Reference Standards; User-Computer Interface; Workflow",2-s2.0-84873988752
"Orsini M., Carcangiu S.","BlaSTorage: A fast package to parse, manage and store BLAST results",2013,"Source Code for Biology and Medicine",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873028158&doi=10.1186%2f1751-0473-8-4&partnerID=40&md5=af0cb5f4f3151eebf3a5184b2618b3d1","Background: Large-scale sequence studies requiring BLAST-based analysis produce huge amounts of data to be parsed. BLAST parsers are available, but they are often missing some important features, such as keeping all information from the raw BLAST output, allowing direct access to single results, and performing logical operations over them.Findings: We implemented BlaSTorage, a Python package that parses multi BLAST results and returns them in a purpose-built object-database format. Unlike other BLAST parsers, BlaSTorage retains and stores all parts of BLAST results, including alignments, without loss of information; a complete API allows access to all the data components.Conclusions: BlaSTorage shows comparable speed of more basic parser written in compiled languages as C++ and can be easily integrated into web applications or software pipelines. © 2013 Orsini and Carcangiu; licensee BioMed Central Ltd.","BLAST; Blast parser; Python-package; Serialized python object",,2-s2.0-84873028158
"Stevenson A., Cordy J.R.","Grammatical inference in software engineering: An overview of the state of the art",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872703858&doi=10.1007%2f978-3-642-36089-3_12&partnerID=40&md5=3ab5be7eef3a06bda946ecdc7875ea4b","Grammatical inference-used successfully in a variety of fields such as pattern recognition, computational biology and natural language processing-is the process of automatically inferring a grammar by examining the sentences of an unknown language. Software engineering can also benefit from grammatical inference. Unlike the aforementioned fields, which use grammars as a convenient tool to model naturally occuring patterns, software engineering treats grammars as first-class objects typically created and maintained for a specific purpose by human designers. We introduce the theory of grammatical inference and review the state of the art as it relates to software engineering. © 2013 Springer-Verlag Berlin Heidelberg.","grammar induction; grammatical inference; software engineering","Computational biology; Grammar induction; Grammatical inferences; Natural languages; State of the art; Bioinformatics; Natural language processing systems; Pattern recognition; Software engineering; Computational grammars",2-s2.0-84872703858
"Mihǎilǎ C., Ohta T., Pyysalo S., Ananiadou S.","BioCause: Annotating and analysing causality in the biomedical domain",2013,"BMC Bioinformatics",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872188560&doi=10.1186%2f1471-2105-14-2&partnerID=40&md5=a6d12198888ebdb94b6bf8ad30d2d9dd","Background: Biomedical corpora annotated with event-level information represent an important resource for domain-specific information extraction (IE) systems. However, bio-event annotation alone cannot cater for all the needs of biologists. Unlike work on relation and event extraction, most of which focusses on specific events and named entities, we aim to build a comprehensive resource, covering all statements of causal association present in discourse. Causality lies at the heart of biomedical knowledge, such as diagnosis, pathology or systems biology, and, thus, automatic causality recognition can greatly reduce the human workload by suggesting possible causal connections and aiding in the curation of pathway models. A biomedical text corpus annotated with such relations is, hence, crucial for developing and evaluating biomedical text mining.Results: We have defined an annotation scheme for enriching biomedical domain corpora with causality relations. This schema has subsequently been used to annotate 851 causal relations to form BioCause, a collection of 19 open-access full-text biomedical journal articles belonging to the subdomain of infectious diseases. These documents have been pre-annotated with named entity and event information in the context of previous shared tasks. We report an inter-annotator agreement rate of over 60% for triggers and of over 80% for arguments using an exact match constraint. These increase significantly using a relaxed match setting. Moreover, we analyse and describe the causality relations in BioCause from various points of view. This information can then be leveraged for the training of automatic causality detection systems.Conclusion: Augmenting named entity and event annotations with information about causal discourse relations could benefit the development of more sophisticated IE systems. These will further influence the development of multiple tasks, such as enabling textual inference to detect entailments, discovering new facts and providing new hypotheses for experimental work. © 2013 Mihǎilǎ et al.; licensee BioMed Central Ltd.",,"Annotation scheme; Biomedical domain; Biomedical journal; Causal relations; Domain-specific information; Event extraction; Infectious disease; Systems biology; Data mining; Natural language processing systems; article; computer program; data mining; methodology; statistical analysis; Data Interpretation, Statistical; Data Mining; Software",2-s2.0-84872188560
"Kuhn T.","A Principled Approach to Grammars for Controlled Natural Languages and Predictive Editors",2013,"Journal of Logic, Language and Information",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874443664&doi=10.1007%2fs10849-012-9167-z&partnerID=40&md5=5d4cce4d79c956159ec80d433e9957cc","Controlled natural languages (CNL) with a direct mapping to formal logic have been proposed to improve the usability of knowledge representation systems, query interfaces, and formal specifications. Predictive editors are a popular approach to solve the problem that CNLs are easy to read but hard to write. Such predictive editors need to be able to ""look ahead"" in order to show all possible continuations of a given unfinished sentence. Such lookahead features, however, are difficult to implement in a satisfying way with existing grammar frameworks, especially if the CNL supports complex nonlocal structures such as anaphoric references. Here, methods and algorithms are presented for a new grammar notation called Codeco, which is specifically designed for controlled natural languages and predictive editors. A parsing approach for Codeco based on an extended chart parsing algorithm is presented. A large subset of Attempto Controlled English has been represented in Codeco. Evaluation of this grammar and the parser implementation shows that the approach is practical, adequate and efficient. © 2012 Springer Science+Business Media Dordrecht.","Anaphoric references; Attempto Controlled English; Chart parsing; Controlled natural languages; Predictive editors",,2-s2.0-84874443664
"Kim T., Jeon H., Choi J.","A new document representation using a unified graph to document similarity search",2013,"Advanced Materials Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871700817&doi=10.4028%2fwww.scientific.net%2fAMR.601.394&partnerID=40&md5=61c879b23780370347fa327a9cdfc7c0","Document similarity search is to retrieve a ranked list of similar documents and find documents similar to a query document in a text corpus or a web page on the web. But most of the previous researches regarding searching for similar documents are focused on classifying documents based on the contents of documents. To solve this problem, we propose a novel retrieval approach based on undirected graphs to represent each document in corpus. In addition, this study also considers unified graph in conjunction with multiple graphs to improve the quality of searching for similar documents. Experimental results on the Reuters-21578 data demonstrate that the proposed system has better performance and success than the traditional approach. © (2013) Trans Tech Publications, Switzerland.","Document similarity search; Graph representation; Information retrieval; Text mining","Document Representation; Document similarity; Graph representation; Query documents; Reuters-21578; Text corpora; Text mining; Undirected graph; Data mining; Information retrieval; Text processing; Manufacture",2-s2.0-84871700817
"Bhagyashala J., Shefali S.","An XML parser of efficient updates for a binary string: A case study",2013,"Advances in Intelligent Systems and Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871332898&doi=10.1007%2f978-81-322-0740-5_84&partnerID=40&md5=9cd1c31ee34a8bdc77d2c0fd8de37131","In many emerging applications, such as XML publishing systems, electronic commerce and intelligent Web searching, ordered XML data are available in query processing. An XML query processing based on labeling schemes has been thoroughly studied in the past several years. However, all these techniques have high update cost, cannot completely avoid re-labeling in XML updates and increase the label size. This paper experiments a labeling scheme, called IBSL (Improved Binary String Labeling), which supports order sensitive updates without relabeling or recalculation. By using IBSL, University Web search has been considered as a separate case study using conventional Google search and applying IBSL algorithm along with the search. This paper reports that the IBSL algorithm is time efficient. © 2013 Springer.","dynamic XML; order-sensitive update; tree labeling; Web search","Algorithms; Information retrieval; Query processing; Websites; Binary string; Dynamic XML; Emerging applications; Intelligent web; Labeling scheme; order-sensitive update; Relabeling; Web searches; XML data; XML parser; XML publishing; XML query processing; XML update; XML",2-s2.0-84871332898
"Pradel C., Haemmerlé O., Hernandez N.","Natural language query interpretation into SPARQL using patterns",2013,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924787975&partnerID=40&md5=12f7b6ad14bb98988819416e82fe4679","Our purpose is to provide end-users with a means to query ontology based knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. In this article we justify the postulate supporting our work which claims that queries issued by real life end-users are variations of a few typical query families. We also explain how our approach is designed to be adaptable to different user languages. Evaluations on the QALD-3 data set have shown the relevancy of the approach.",,"Data handling; Natural language processing systems; Query languages; Semantic Web; Social networking (online); Data set; End users; Graph query language; Knowledge basis; Natural language queries; Ontology-based; Query patterns; Computational linguistics",2-s2.0-84924787975
"Su W., Wu H., Li Y., Zhao J., Lochovsky F.H., Cai H., Huang T.","Understanding Query Interfaces by Statistical Parsing",2013,"ACM Transactions on the Web",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884594906&doi=10.1145%2f2460383.2460387&partnerID=40&md5=e340e8a0bc3e1808c4d3d4cb72926ae3","Users submit queries to an online database via its query interface. Query interface parsing, which is important for many applications, understands the query capabilities of a query interface. Since most query interfaces are organized hierarchically, we present a novel query interface parsing method, StatParser (Statistical Parser), to automatically extract the hierarchical query capabilities of query interfaces. StatParser automatically learns from a set of parsed query interfaces and parses new query interfaces. StatParser starts from a small grammar and enhances the grammar with a set of probabilities learned from parsed query interfaces under the maximum-entropy principle. Given a new query interface, the probability-enhanced grammar identifies the parse tree with the largest global probability to be the query capabilities of the query interface. Experimental results show that StatParser very accurately extracts the query capabilities and can effectively overcome the problems of existing query interface parsers. © 2013, ACM. All rights reserved.","Algorithms; Experimentation Query Interface; Maximum Entropy; Performance",,2-s2.0-84884594906
"Kwiatkowski T., Choi E., Artzi Y., Zettlemoyer L.","Scaling semantic parsers with on-the-fly ontology matching",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",78,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926294305&partnerID=40&md5=86c5d87aaa1e672fed9770cec4659420","We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as 'daughter' and 'number of people living in' cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontologi-cal mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical-form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. © 2013 Association for Computational Linguistics.",,"Benchmarking; Natural language processing systems; Ontology; Semantics; Syntactics; Accuracy Improvement; Learning semantics; Number of peoples; Ontology matching; Question Answering; Question-answer pairs; Semantic parsing; State-of-the-art performance; Computational linguistics",2-s2.0-84926294305
"Liu K., Tan H.B.K., Chen X.","Automated insertion of exception handling for key and referential constraints",2013,"Journal of Database Management",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887458783&doi=10.4018%2fjdm.2013010101&partnerID=40&md5=a6a976b8ec34251ec7fa2546a4269450","Key and referential constraints are two main integrity constraints in database applications. These constraints can be automatically enforced by the Database Management System with their exception violation from these constraints handled by programmers. This paper proposes an approach to relieve the burden of programmers from mechanical coding for handling exceptions of these constraints violation by using program transformation. We first propose an extended abstract syntax tree to include SQL query semantics. Based on it, each code pattern that requires exception handling together with the exception handling code to be inserted is represented as a transformation rule. We provide two alternatives to handle the exceptions: one is to handle the exceptions in conjunction with the built-in enforcement feature in Database Management System; the other is handling them without using the feature provided in Database Management System. Hence, two types of transformation rules are provided accordingly. A tool GEHPHP (Generation of Exception Handling for PHP Systems) has been developed to implement the proposed approach. Experiments have also been conducted to evaluate the applicable of the approach. © 2013, IGI Global.","Constraints Violation; Database Query Aware AST (DB-AST); Exception Handling; Key and Referential Constrains; Structured Query Language (SQL) Query Pattern; Transformation Rule","Constraints Violation; Database queries; Exception handling; Key and Referential Constrains; Query patterns; Transformation rules; Management information systems; Query languages; Query processing; Semantics; XML; Management",2-s2.0-84887458783
"Lynn T., Foster J., Dras M., Van Genabith J.","Working with a small dataset - Semi-supervised dependency parsing for Irish",2013,"SPMRL 2013 - 4th Workshop on Statistical Parsing of Morphologically Rich Languages, Proceedings of the Workshop",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926228811&partnerID=40&md5=27c815ae7efc06837e039b35b471ce64","We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. We take two popular dependency parsers - one graph-based and one transition-based - and compare results for both. Results show that using semi-supervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. We also try to use morphological information in a targeted way and fail to see any improvements. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Graphic methods; Supervised learning; Co-training; Dependency parser; Dependency parsing; Graph-based; Morphological information; Self training; Semi- supervised learning; Semi-supervised; Syntactics",2-s2.0-84926228811
"Giordani A., Moschitti A.","Automatic Generation and Reranking of SQL-Derived Answers to NL Questions",2013,"Communications in Computer and Information Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904636807&doi=10.1007%2f978-3-642-45260-4_5&partnerID=40&md5=f15a73df71bac9a5105d66dee1d37166","In this paper, given a relational database, we automatically translate a natural language question into an SQL query retrieving the correct answer. We exploit the structure of the DB to generate a set of candidate SQL queries, which we rerank with a SVM-ranker based on tree kernels. In particular we use linguistic dependencies in the natural language question and the DB metadata to build a set of plausible SELECT, WHERE and FROM clauses enriched with meaningful joins. Then, we combine all the clauses to get the set of all possible SQL queries, producing candidate queries to answer the question. This approach can be recursively applied to deal with complex questions, requiring nested queries. We sort the candidates in terms of scores of correctness using a weighting scheme applied to the query generation rules. Then, we use a SVM ranker trained with structural kernels to reorder the list of question and query pairs, where both members are represented as syntactic trees. The f-measure of our model on standard benchmarks is in line with the best models (85% on the first question), which use external and expensive hand-crafted resources such as the semantic interpretation. Moreover, we can provide a set of candidate answers with a Recall of the answer of about 92% and 96% on the first 2 and 5 candidates, respectively. © Springer-Verlag Berlin Heidelberg 2013.",,"Forestry; Natural language processing systems; Semantics; Automatic Generation; Candidate query; Complex questions; Natural language questions; Query generation; Relational Database; Semantic interpretation; Syntactic trees; Query processing; Forestry; Languages; Models",2-s2.0-84904636807
"Burkhardt F., Nägeli H.U.","Voice search in mobile applications and the use of linked open data",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906252236&partnerID=40&md5=9181db41fdff6dfae987e4ffb1edb9bb","We describe our approach on voice seach in a mobile context by a TV guide search app that integrates linked open data to identify movies from a search query. A text parser to match keywords against vocabularies and numerical value descriptors is introduced. Copyright © 2013 ISCA.","Mobile applications; Query interpretation; Voice search","Computer applications; Computer simulation; Descriptors; Linked open datum; Mobile applications; Mobile context; Numerical values; Query interpretation; Search queries; Voice searches; Mobile computing",2-s2.0-84906252236
"Celikyilmaz A., Tur G., Hakkani-Tür D.","IsNL? A discriminative approach to detect natural language like queries for conversational understanding",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906246861&partnerID=40&md5=0ebb8369aa9fd4a94e53c4bfd7a185ed","While data-driven methods for spoken language understanding (SLU) provide state of the art performances and reduce maintenance and model adaptation costs compared to handcrafted parsers, the collection and annotation of domain-specific natural language utterances for training remains a time-consuming task. A recent line of research has focused on enriching the training data with in-domain utterances by mining search engine query logs to improve the SLU tasks. However genre mismatch is a big obstacle as search queries are typically keywords. In this paper, we present an efficient discriminative binary classification method that filters large collection of online web search queries only to select the natural language like queries. The training data used to build this classifier is mined from search query click logs, represented as a bipartite graph. Starting from queries which contain natural language salient phrases, random graph walk algorithms are employed to mine corresponding keyword queries. Then an active learning method is employed for quickly improving on top of this automatically mined data. The results show that our method is robust to noise in search queries by improving over a baseline model previously used for SLU data collection. We also show the effectiveness of detected natural language like queries in extrinsic evaluations on domain detection and slot filling tasks. Copyright © 2013 ISCA.","Keyword search; Natural language; Natural language understanding; Semantic parsing; Web search","Artificial intelligence; Classification (of information); Filtration; Graph theory; Information retrieval; Search engines; Semantics; Speech recognition; Text processing; World Wide Web; Keyword search; Natural language understanding; Natural languages; Semantic parsing; Web searches; Natural language processing systems",2-s2.0-84906246861
"Pradel C., Peyet G., Haemmerle O., Hernandez N.","SWIP at QALD-3: Results, criticisms and lesson learned",2013,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041568&partnerID=40&md5=0ab957a303877f1569bb8b85cbac1119","This paper presents the results obtained by the SWIP system while participating in the QALD-3 (Question Answering over Linked Data) challenge, co-located with CLEF 2013 (Conference and Labs of the Evaluation Forum). We tackled task 1, multilingual question answering, whose purpose is to interpret natural language questions in order to return the answers contained in a graph knowledge base. We answered queries of both proposed datasets (one concerning DBpedia, the other Musicbrainz) and took into consideration only questions in English. The system SWIP (Semantic Web Interface using Patterns) aims at automatically generating formal queries from user queries expressed in natural language. For this, it relies on the use of query patterns which enable the complex task of interpreting natural language queries. The results obtained on the Musicbrainz dataset (precision = 0:51, recall = 0:51, F-measure = 0:51) are very satisfactory and encouraging. The results on DBpedia (precision = 0:16, recall = 0:15, F-measure = 0:16) are more disappointing. In this paper, we present both the SWIP approach and its implementation. We then present the results of the challenge in more detail and their analysis. Finally we draw some conclusions on the strengths and weaknesses of our approach, and suggest ways to improve its performance.",,"Computational linguistics; Knowledge based systems; Semantic Web; Complex task; Knowledge base; Linked datum; Natural language queries; Natural language questions; Natural languages; Query patterns; Question Answering; Natural language processing systems",2-s2.0-84922041568
"Lavigne M., Shaban-Nejad A., Okhmatovskaia A., Mondor L., Buckeridge D.L.","A hybrid natural language approach to manage semantic interoperability for public health analytics",2013,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924705623&partnerID=40&md5=fa2e2accb442377f1a281714c484b261","This paper discusses the integration of an ontology with a natural language query engine to calculate and interpret epidemiological indicators for population health assessment. In this paper, we discuss the application of this approach to one type of possible query, which retrieves health determinants, causally associated with diabetes mellitus.","Causal inference; Epidemiology; Natural language interface; Ontology","Computational linguistics; Epidemiology; Health; Natural language processing systems; Ontology; Social networking (online); Causal inferences; Diabetes mellitus; Natural language interfaces; Natural language queries; Natural languages; Population health; Semantic interoperability; Semantic Web",2-s2.0-84924705623
"Kushman N., Barzilay R.","Using Semantic Unification to Generate Regular Expressions from Natural Language",2013,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926221440&partnerID=40&md5=cd63ca15558268ca280bf97f11dd41b1","We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof- the-Art semantic parsing baseline, yielding a 29% absolute improvement in accuracy. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Natural language processing systems; Pattern matching; Semantics; Translation (languages); Amazon mechanical turks; Level of abstraction; Natural language queries; Natural language representation; Natural language text; Natural languages; Regular expressions; State of the art; Computer programming languages",2-s2.0-84926221440
"Poon H.","Grounded unsupervised semantic parsing",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904366363&partnerID=40&md5=c08279c589f31fd7f84aee02259674cc","We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Database queries; Latent state; Probabilistic grammars; Question-answer pairs; Semantic parsing; Unsupervised approaches; Semantics",2-s2.0-84904366363
"Volkova S., Choudhury P., Quirk C., Dolan B., Zettlemoyer L.","Lightly supervised learning of Procedural dialog systems",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907335951&partnerID=40&md5=d2c0f07000c05d16c67bbc19ca426273","Procedural dialog systems can help users achieve a wide range of goals. However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowledge and dialog management strategies. In this paper, we demonstrate that it is possible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. We consider domains where the required task knowledge exists in textual form (e.g., instructional web pages) and where system builders have access to statements of user intent (e.g., search query logs or dialog interactions). To learn from such textual resources, we describe a novel approach that first automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. Evaluation in a Microsoft Office domain shows that the individual components are highly accurate and can be integrated into a dialog system that provides effective help to users. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Dialog management; Dialog systems; Domain specific; Highly accurate; Individual components; Microsoft Office; Search queries; System builders; Natural language processing systems",2-s2.0-84907335951
"Wang X., Dai L.","Design of wireless signal analysis system based on intelligent control",2013,"Proceedings of 2013 2nd International Conference on Measurement, Information and Control, ICMIC 2013",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897559182&doi=10.1109%2fMIC.2013.6758169&partnerID=40&md5=bc7c3aa6d6e4d9bc3a328ad4dc28f5bb","The wireless communication channel and electric interference for communication devices are simulated by the intelligent instruments. Compared with the problems of current signal analyzer versatility, complicated operation process, the error in the operation of instrument and the measuring efficiency, the wireless signal analysis system based on the idea of intelligent control with SCPI programmable instruments standard instruction, combining the signal analyzer and signal generator to develop. Using VC++6.0 as the development platform, the system develops automation control of signal analyzer, building a wireless signal analysis system and saves test results and design parameters to the database through the VISA interface function controlling of PC. The system with measuring efficiency and precision is simple in operation and easy to query and manage the test data. © 2013 IEEE.","Intelligent control; SCPI; Signal analyzer; VISA","Instruments; Intelligent control; Query processing; Communication device; Current signal analyzer; Intelligent instrument; SCPI; Signal analyzers; VC; VISA; Wireless communication channels; Signal analysis",2-s2.0-84897559182
"Gelernter J., Zhang W.","Cross-lingual geo-parsing for non-structured data",2013,"Proceedings of the 7th Workshop on Geographic Information Retrieval, GIR 2013",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893902694&doi=10.1145%2f2533888.2533943&partnerID=40&md5=17f07567378792f01b2c1de262dc447f","A geo-parser automatically identifies location words in a text. We have generated a geo-parser specifically to find locations in unstructured Spanish text. Our novel geo-parser architecture combines the results of four parsers: A lexico-semantic Named Location Parser, a rules-based building parser, a rules-based street parser, and a trained Named Entity Parser. Each parser has different strengths: The Named Location Parser is strong in recall, and the Named Entity Parser is strong in precision, and building and street parser finds buildings and streets that the others are not designed to do. To test our Spanish geo-parser performance, we compared the output of Spanish text through our Spanish geoparser, with that same Spanish text translated into English and run through our English geo-parser. The results were that the Spanish geo-parser identified toponyms with an F1 of .796, and the English geo-parser identified toponyms with an F1 of .861 (and this is despite errors introduced by translation from Spanish to English), compared to an F1 of .114 from a commercial off-theshelf Spanish geo-parser. Results suggest (1) geo-parsers should be built specifically for unstructured text, as have our Spanish and English geo-parsers, and (2) location entities in Spanish that have been machine translated to English are robust to geo-parsing in English.","Cross-language geographic information retrieval (CLGIR); Geo-parse; Georeference; Location; Microtext; Spanish; Translation; Twitter","Artificial intelligence; Computational linguistics; Location; Semantics; Translation (languages); Geo-parse; Geographic information retrieval; Georeference; Microtext; Spanish; Twitter; Information retrieval",2-s2.0-84893902694
"Gao X., Cui Z., Yu J., Cui W., Zhang S., Lv G.","Research on GML data validity verification based on stack mechanism",2013,"Proceedings - 6th International Symposium on Computational Intelligence and Design, ISCID 2013",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901050729&doi=10.1109%2fISCID.2013.202&partnerID=40&md5=5517de8ace800f2dd852c94cd53e6b75","GML schemas are metadata files, which define the structure, content and restriction of GML instances. GML parsing establishes a good basis for future GML data studies and applications such as storage, compression, query, index and share etc. The engine for GML grammar validation is an important component of GML parser, which includes validity verification and consistency verification. The objective of validity verification is to ensure GML data to be well-formed. In this paper, the functional structure of the GML schema-based parser is discussed, and an algorithm for GML data validity verification is developed and implemented. Experimental results show that the algorithm is effective and efficient. © 2013 IEEE.","Consistency verification; GML; Grammar validation; Schema-based Parsing; Validity verification","Artificial intelligence; Digital storage; Formal languages; Consistency verifications; GML; Grammar validation; Schema-based Parsing; Validity verifications; Search engines",2-s2.0-84901050729
"Heck L., Hakkani-Tür D., Tur G.","Leveraging knowledge graphs forweb-scale unsupervised semantic parsing",2013,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906262369&partnerID=40&md5=36e1ac1c8478ec59d79c09727bc43104","The past decade has seen the emergence of web-scale structured and linked semantic knowledge resources (e.g., Freebase, DBPedia). These semantic knowledge graphs provide a scalable ""schema for the web"", representing a significant opportunity for the spoken language understanding (SLU) research community. This paper leverages these resources to bootstrap a web-scale semantic parser with no requirement for semantic schema design, no data collection, and no manual annotations. Our approach is based on an iterative graph crawl algorithm. From an initial seed node (entity-type), the method learns the related entity-types from the graph structure, and automatically annotates documents that can be linked to the node (e.g., Wikipedia articles, web search documents). Following the branches, the graph is crawled and the procedure is repeated. The resulting collection of annotated documents is used to bootstrap webscale conditional random field (CRF) semantic parsers. Finally, we use a maximum-a-posteriori (MAP) unsupervised adaptation technique on sample data from a specific domain to refine the parsers. The scale of the unsupervised parsers is on the order of thousands of domains and entity-types, millions of entities, and hundreds of millions of relations. The precision-recall of the semantic parsers trained with our unsupervised method approaches those trained with supervised annotations. Copyright © 2013 ISCA.","Dialog; Natural language understanding; Semantic parsing; Semantic search; Semantic web","Computational linguistics; Iterative methods; Knowledge management; Conditional random field; Dialog; Maximum a posteriori; Natural language understanding; Semantic parsing; Semantic search; Spoken language understanding; Unsupervised adaptation; Semantic Web",2-s2.0-84906262369
[No author name available],"SPLASH 2013 - Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, and Applications: Software for Humanity",2013,"SPLASH 2013 - Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, and Applications: Software for Humanity",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897369707&partnerID=40&md5=6859933a2c754857c52f498e57af6210","The proceedings contain 52 papers. The topics discussed include: mining source code repositories with boa; Panini: a capsule-oriented programming language for implicitly concurrent program design; finding architectural flaws in android apps is easy; finding the missing eclipse perspective: the runtime perspective; automated assessment of students' testing skills for improving correctness of their code; implementing a scripting language parser with self-extensible syntax; program transformation techniques applied to languages used in high performance computing; a secure play store for android; refactoring multicore applications towards energy efficiency; the poor man's proof assistant: using prolog to develop formal language theoretic proofs; dictionary-based query recommendation for local code search; a screen-oriented representation for mobile applications; and source code management for projectional editing.",,,2-s2.0-84897369707
"Berant J., Chou A., Frostig R., Liang P.","Semantic parsing on freebase from question-answer pairs",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",155,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904308637&partnerID=40&md5=4611a3ffed162776aed34edddc6f4bc3","In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Knowledge based systems; Natural language processing systems; Knowledge base; Logical forms; Logical predicate; Question-answer pairs; Semantic parsing; State of the art; Text corpora; Two ways; Semantics",2-s2.0-84904308637
"Li P., Wang Y., Jiang J.","Automatically building templates for entity summary construction",2013,"Information Processing and Management",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870289405&doi=10.1016%2fj.ipm.2012.03.006&partnerID=40&md5=bcb79b3941c65d7ea3ade989e6e36929","In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles. We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Finally, we use the generated templates to construct summaries for new entities. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of template slots that need to be filled in. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We apply our method on five Wikipedia entity categories and compare our method with three baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method. © 2012 Elsevier Ltd. All rights reserved.","LDA; Pattern mining; Summary template","Automatic Generation; Automatic identification; Baseline methods; Dependency trees; Frequent subtrees; Human judgments; Key feature; LDA; Parse trees; Pattern mining; Quantitative evaluation; Sentence compression; Summary template; Wikipedia; Automation; Forestry; Linguistics; Trees (mathematics); Abstracts; Forestry; Languages; Pattern Recognition",2-s2.0-84870289405
"Angeli G., Uszkoreit J.","Language-independent discriminative parsing of temporal expressions",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907339265&partnerID=40&md5=fa491b0c4efabe789a38fefa16393ca7","Temporal resolution systems are traditionally tuned to a particular language, requiring significant human effort to translate them to new languages. We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. We achieve state-of-the-art accuracy on all languages in the TempEval-2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Language independents; Rich features; Temporal expressions; Temporal resolution; Temporal semantics; Semantics",2-s2.0-84907339265
"Briola D., Caccia R., Bozzano M., Locoro A.","Ontologica: Exploiting ontologies and natural language for railway management. Design, implementation and usage examples",2013,"International Journal of Knowledge-Based and Intelligent Engineering Systems",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013575461&doi=10.3233%2fKES-130262&partnerID=40&md5=c55a6849cd6309c85ee1bd7acd3561f9","This paper presents the ""Ontologica"" system, a forefront project born from a joint effort between the Department of Informatics, Bioengineering, Robotics and System Engineering of Genoa university and Ansaldo STS (from the same city) in the design of advanced information systems. The aim of the project is twofold: the adoption of ontologies to manage the Centralized Traffic Control (CTC) logics of a railway system; the improvement of the user interface through the exploitation of natural language queries. Being Ansaldo STS a leader in the railway and metro scope, this project aims to develop a system to be tested on a real and large dataset before being adopted in the railway stations using the CTC system. The first results we obtained are very promising and the system is currently under testing and improvement. In this paper we present the ""Ontologica"" rationale and architecture with some usage examples. © 2010 - IOS Press and the authors.","Knowledge representation; Natural language processing; Ontology",,2-s2.0-85013575461
"Saxena A., Bindal A., Wegmann A.","A cognitive reference based model for learning compositional hierarchies with whole-composite tags",2013,"IC3K 2013; KDIR 2013 - 5th International Conference on Knowledge Discovery and Information Retrieval and KMIS 2013 - 5th International Conference on Knowledge Management and Information Sharing, Proc.",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887749981&partnerID=40&md5=7e86e2fc13f75f26b737605f3f38b86f","A compositional hierarchy is the default organization of knowledge acquired for the purpose of specifying the design requirements of a service. Existing methods for learning compositional hierarchies from natural language text, interpret composition as an exclusively propositional form of part-whole relations. Nevertheless, the lexico-syntactic patterns used to identify the occurrence of part-whole relations fail to decode the experientially grounded information, which is very often embedded in various acts of natural language expression, e.g. construction and delivery. The basic idea is to take a situated view of conceptualization and model composition as the cognitive act of invoking one category to refer to another. Mutually interdependent set of categories are considered conceptually inseparable and assigned an independent level of abstraction in the hierarchy. Presence of such levels in the compositional hierarchy highlight the need to model these categories as a unified-whole wherein they can only be characterized in the context of the behavior of the set as a whole. We adopt an object-oriented representation approach that models categories as entities and relations as cognitive references inferred from syntactic dependencies. The resulting digraph is then analyzed for cyclic references, which are resolved by introducing an additional level of abstraction for each cycle.","Digraph analysis; Linguistic markers; Part-whole relations; Service design; Situated conceptualization","Abstracting; Directed graphs; Information retrieval; Knowledge management; Digraph analysis; Lexico-syntactic patterns; Natural language expressions; Object-oriented representation; Organization of knowledge; Part-whole relations; Service design; Situated conceptualization; Syntactics",2-s2.0-84887749981
"Morita H., Takamura H., Sasano R., Okumura M.","Subtree extractive summarization via submodular maximization",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906929030&partnerID=40&md5=8e77ec393e19201ec9cff2239aed820f","This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 1/2 (1 - e-1). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Extraction; Text processing; Approximation ratios; Dependency constraints; Extractive summarizations; Maximization problem; Sentence extraction; State-of-the-art algorithms; Test Collection; Text summarization; Approximation algorithms",2-s2.0-84906929030
"Searle G., O’Connor K.","New outcomes for Australian firms in the global it industry",2013,"The Economic Geography of the IT Industry in the Asia Pacific Region",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917299250&doi=10.4324%2f9780203097151&partnerID=40&md5=6f157df582f5acb7d5f3eb56ea395d06",[No abstract available],,,2-s2.0-84917299250
"Szugyi Z., Cséri T., Porkoláb Z.","Random number generator for C++ template metaprograms",2013,"13th Symposium on Programming Languages and Software Tools, SPLST 2013 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923609332&partnerID=40&md5=b08e0055dc87d42162de5c626bee0b69","Template metaprogramming is a widely used programming paradigm to develop libraries in C++. With the help of cleverly dened templates the programmer can execute algorithms at compilation time. C++ template metaprograms are proven to be Turing-complete, thus wide scale of algorithms can be executed in compilation time. Applying randomized algorithms and data structures is, however, troublesome due to the deterministic nature of template metaprograms. In this paper we describe a C++ template metaprogram library that generates pseudorandom numbers at compile time. Random number engines are responsible to generate pseudorandom integer sequences with a uniform distribution. Random number distributions transform the generated pseudorandom numbers into dierent statistical distributions. Our goal was to provide similar functionality to the run-time random generator module of the Standard Template Library, thus programmers familiar with STL can easily adopt our library.",,"C++ (programming language); Computer programming; Embedded systems; High level languages; Number theory; Programming paradigms; Pseudo-random numbers; Random number distribution; Random number generators; Randomized Algorithms; Standard template library; Statistical distribution; Template metaprogramming; Random number generation",2-s2.0-84923609332
"Morante R., Krallinger M., Valencia A., Daelemans W.","Machine reading of biomedical texts about Alzheimer's disease",2013,"CEUR Workshop Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041551&partnerID=40&md5=230112b45ff229a7191a9e67fdfda8cb","This report describes the task Machine reading of biomedical texts about Alzheimer's disease, which is a task of the Question Answering for Machine Reading Evaluation (QA4MRE) Lab at CLEF 2013. The task aims at exploring the ability of a machine reading system to answer questions about a scientific topic, namely Alzheimer's disease. As in the QA4MRE task, participant systems were asked to read a document and identify the answers to a set of questions about information that is stated or implied in the text. A background collection was provided for systems to acquire background knowledge. Three teams participated in the task submitting a total of 13 runs. The highest score obtained by a team was 0.42 c@1, which is clearly above baseline.",,"Alzheimer's disease; Back-ground knowledge; Biomedical text; Question Answering; Set of questions; Neurodegenerative diseases",2-s2.0-84922041551
"Dong N., Doʇruöz A.S.","Word level language identification in online multilingual communication",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926320416&partnerID=40&md5=0551733ddb9db29508aa231f13e5eaae","Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identification at the word level. For our experiments with multilingual online discussions, we first tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task. © 2013 Association for Computational Linguistics.",,"Natural language processing systems; Social networking (online); Automatic language identification; Language identification; Language model; Multilingual communications; Online discussions; Word level; Computational linguistics",2-s2.0-84926320416
"Burtsev A., Mishrikoti N., Eide E., Ricci R.","Weir: A streaming language for performance analysis",2013,"Proceedings of the 7th Workshop on Programming Languages and Operating Systems, PLOS 2013 - In Conjunction with the 24th ACM Symposium on Operating Systems Principles, SOSP 2013",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897369792&doi=10.1145%2f2525528.2525537&partnerID=40&md5=bed6c8c5700259a8e879b525136e6716","For modern software systems, performance analysis can be a challenging task. The software stack can be a complex, multi-layer, multi-component, concurrent, and parallel environment with multiple contexts of execution and multiple sources of performance data. Although much performance data is available, because modern systems incorporate many mature data-collection mechanisms, analysis algorithms suffer from the lack of a unifying programming environment for processing the collected performance data, potentially from multiple sources, in a convenient and script-like manner. This paper presents Weir, a streaming language for systems performance analysis. Weir is based on the insight that performance-analysis algorithms can be naturally expressed as stream-processing pipelines. In Weir, an analysis algorithm is implemented as a graph composed of stages, where each stage operates on a stream of events that represent collected performance measurements. Weir is an imperative streaming language with a syntax designed for the convenient construction of stream pipelines that utilize composable and reusable analysis stages. To demonstrate practical application, this paper presents the authors' experience in using Weir to analyze performance in systems based on the Xen virtualization platform. © 2013 ACM.",,"Algorithms; Computer hardware description languages; Computer software reusability; Hydraulic structures; Pipeline processing systems; Pipelines; Analysis algorithms; Multiple contexts; Parallel environment; Performance analysis; Performance data; Performance measurements; Programming environment; Systems performance analysis; Weirs",2-s2.0-84897369792
"Enríquez F., Cruz F.L., Ortega F.J., Vallejo C.G., Troyano J.A.","A comparative study of classifier combination applied to nlp tasks",2013,"Information Fusion",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885431390&doi=10.1016%2fj.inffus.2012.05.001&partnerID=40&md5=85ff925f0a305edc963533e346456c60","The paper is devoted to a comparative study of classifier combination methods, which have been successfully applied to multiple tasks including Natural Language Processing (NLP) tasks. There is variety of classifier combination techniques and the major difficulty is to choose one that is the best fit for a particular task. In our study we explored the performance of a number of combination methods such as voting, Bayesian merging, behavior knowledge space, bagging, stacking, feature sub-spacing and cascading, for the part-of-speech tagging task using nine corpora in five languages. The results show that some methods that, currently, are not very popular could demonstrate much better performance. In addition, we learned how the corpus size and quality influence the combination methods performance. We also provide the results of applying the classifier combination methods to the other NLP tasks, such as name entity recognition and chunking. We believe that our study is the most exhaustive comparison made with combination methods applied to NLP tasks so far. © 2012 Elsevier B.V.","Classifier combination; Natural language processing; Part-of-speech tagging; Text analysis","Computational linguistics; Linguistics; Behavior knowledge spaces; Classifier combination; Combination method; Comparative studies; Name entity recognition; NAtural language processing; Part of speech tagging; Text analysis; Natural language processing systems",2-s2.0-84885431390
"FitzGerald N., Artzi Y., Zettlemoyer L.","Learning distributions over logical forms for referring expression generation",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908171707&partnerID=40&md5=8af9979fb25ef028e3e15a3eb29ebced","We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87% of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35% relative error reduction over previous state of the art. © 2013 Association for Computational Linguistics.",,"Approximate inference; Density estimation; Effective learning; Linear distribution; Logical expressions; Referring expressions; Relative errors; State of the art; Natural language processing systems",2-s2.0-84908171707
"Pallotta V., Delmonte R.","Interaction mining: The new frontier of customer interaction analytics",2013,"Studies in Computational Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867967816&doi=10.1007%2f978-3-642-31546-6-6&partnerID=40&md5=2d617b3a365a0a38c86da8bb5cc9fba6","In this paper, we present our solution for argumentative analysis of call center conversations in order to provide useful insights for enhancing Customer Interaction Analytics to a level that will enable more qualitative metrics and key performance indicators (KPIs) beyond the standard approach used in Customer Interaction Analytics. These metrics rely on understanding the dynamics of conversations by highlighting the way participants discuss about topics. By doing that we can detect relevant situations such as social behaviors, controversial topics, customer oriented behaviors, and also predict customer satisfaction.",,,2-s2.0-84867967816
"Reimann J., Aßsmann U.","Quality-aware refactoring for early detection and resolution of energy deficiencies",2013,"Proceedings - 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing, UCC 2013",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901676583&doi=10.1109%2fUCC.2013.70&partnerID=40&md5=bb315068c496f065d187218e7118f9d4","Software development processes usually target requirements regarding particular qualities in late iteration phases. The developed system is optimised in terms of quality issues, such as, e.g., energy efficiency, without altering the software's behaviour. Bad structures in terms of specific qualities can be considered as bad smells and refactorings can be used to resolve them to preserve its semantics. The problem is that no explicit relationship between smells, qualities and refactorings exists. Without such a relation it is not possible to give evidence about which quality requirements are not satisfied by detected smells. It cannot be specified which smells are resolved by particular refactorings. Thus, developers are not supported in focusing specific qualities and cannot detect and resolve badly structured code in combination. In this paper we present an approach for correlating smells, qualities and refactorings explicitly which supports to focus on specific qualities in early development phases already. We introduce the new term quality smell and come up with a metamodel and architecture enabling developers to establish such relations. A small evaluation regarding energy efficiency in Java code and discussion completes this paper. © 2013 IEEE.","energy efficiency; quality smell; quality-aware refactoring","Cloud computing; Energy efficiency; Semantics; Software engineering; Development phasis; Energy deficiency; Iteration phasis; Quality issues; Quality requirements; Refactorings; Software development process; Structured codes; Odors",2-s2.0-84901676583
"Sarasa-Cabezuelo A., Sierra J.-L.","The grammatical approach: A syntax-directed declarative specification method for XML processing tasks",2013,"Computer Standards and Interfaces",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867866022&doi=10.1016%2fj.csi.2012.06.006&partnerID=40&md5=f7cbbacd753c5bdcd51f95e71dbf2994","This paper describes the grammatical approach, an approach to the specification of XML processing tasks based on attribute grammars. This approach describes how to provide task-specific context-free grammars for XML documents, as well as how to decompose complex processing tasks into simpler ones with attribute-grammar fragments. The result is a high-level, syntax-directed declarative specification for the processing of XML documents, which facilitates the development and maintenance of complex XML processing applications while preserving the flexibility of general-purpose XML processing models. The grammatical approach is illustrated using Chasqui, an e-learning platform for building educational digital libraries of learning objects. © 2012 Elsevier B.V. All rights reserved.","Attribute grammar; Learning object repository; Metadata; Syntax-directed translation; XML processing","Attribute grammars; Complex processing; E-learning platforms; Educational digital libraries; Learning object repositories; Learning objects; XML processing; Context sensitive grammars; Digital libraries; E-learning; Metadata; Syntactics; XML",2-s2.0-84867866022
"Thomas P., Starlinger J., Leser U.","Experiences from developing the domain-specific entity search engine gene view",2013,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922728598&partnerID=40&md5=f2b067f3ed89242f4dbf4578a8b88e1b","GeneView is a semantic search engine for the Life Sciences. Unlike traditional search engines, GeneView analyzes texts upon import to recognize and properly handle biomedical entities, relationships between those entities, and the structure of documents. This allows for a number of advanced features required to work effectively with scientific texts, such as entity disambiguation, ranking of documents by entity content, linking to structured knowledge about entities, userfriendly highlighting of entities etc. As of now, GeneView indexes approximately ~21,4M abstracts and ~358K full texts with more than 200M entities of 11 different types and more than 100K relationships. In this paper, we describe the architecture underlying the system with a focus on the complex pipeline of advanced NLP and information extraction tools necessary for achieving the above functionality. We also discuss open challenges in developing and maintaining a semantic search engine over a large (though not web-scale) corpus. © Gesellschaft für Informatik, Bonn 2013.",,"Natural language processing systems; Semantic Web; Semantics; Social networking (online); Domain specific; Entity disambiguation; Entity search; Information extraction tools; Life-sciences; Scientific texts; Semantic search engines; Structured knowledge; Search engines",2-s2.0-84922728598
"Seretan V., Wehrli E.","Syntactic concordancing and multi-word expression detection",2013,"International Journal of Data Mining, Modelling and Management",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877343941&doi=10.1504%2fIJDMMM.2013.053694&partnerID=40&md5=87f9a6f5dc40c430243d22223c2d1b48","Concordancers are tools that display the contexts of a given word in a corpus. Also called key word in context (KWIC), these tools are nowadays indispensable in the work of lexicographers, linguists, and translators. We present an enhanced type of concordancer that integrates syntactic information on sentence structure as well as statistical information on word cooccurrence in order to detect and display those words from the context that are most strongly related to the word under investigation. This tool considerably alleviates the users' task, by highlighting syntactically well-formed word combinations that are likely to form complex lexical units, i.e., multi-word expressions. One of the key distinctive features of the tool is its multilingualism, as syntax-based multi-word expression detection is available for multiple languages and parallel concordancing enables users to consult the version of a source context in another language, when multilingual parallel corpora are available. In this article, we describe the underlying methodology and resources used by the system, its architecture, and its recently developed online version. We also provide relevant performance evaluation results for the main system components, focusing on the comparison between syntax-based and syntax-free approaches. © 2013 Inderscience Enterprises Ltd.","collocation extraction; collocations; concordancers; key word in context; KWIC; lexical acquisition; lexical resources; linguistic analysis; multi-word expression detection; multi-word expressions; multilingualism; MWE; natural language processing; NLP; parallel concordancing tool; syntactic analysis; syntactic concordancing; syntax; translation; word cooccurrence",,2-s2.0-84877343941
"de la Higuera C.","Grammatical inference: Learning automata and grammars",2013,"Grammatical Inference: Learning Automata and Grammars",49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928365296&doi=10.1017%2fCBO9781139194655&partnerID=40&md5=1dda9ac6a76b329dc90d96d8008f8650","The problem of inducing, learning or inferring grammars has been studied for decades, but only in recent years has grammatical inference emerged as an independent field with connections to many scientific disciplines, including bio-informatics, computational linguistics and pattern recognition. This book meets the need for a comprehensive and unified summary of the basic techniques and results, suitable for researchers working in these various areas. In Part I, the objects of use for grammatical inference are studied in detail: strings and their topology, automata and grammars, whether probabilistic or not. Part II carefully explores the main questions in the field: What does learning mean? How can we associate complexity theory with learning? In Part III the author describes a number of techniques and algorithms that allow us to learn from text, from an informant, or through interaction with the environment. These concern automata, grammars, rewriting systems, pattern languages or transducers. © C. de la Higuera 2010.",,"Automata theory; Computational grammars; Pattern recognition; Complexity theory; Grammatical inferences; Learning Automata; Pattern languages; Rewriting systems; Scientific discipline; Techniques and results; Education",2-s2.0-84928365296
"Li C., Liakata M., Rebholz-Schuhmann D.","Biological network extraction from scientific literature: State of the art and challenges",2013,"Briefings in Bioinformatics",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900861593&doi=10.1093%2fbib%2fbbt006&partnerID=40&md5=e939dafaf95b59d0e1b2509619324722","Networks of molecular interactions explain complex biological processes, and all known information on molecular events is contained in a number of public repositories including the scientific literature. Metabolic and signalling pathways are often viewed separately, even though both types are composed of interactions involving proteins and other chemical entities. It is necessary to be able to combine data from all available resources to judge the functionality, complexity and completeness of any given network overall, but especially the full integration of relevant information from the scientific literature is still an ongoing and complex task. Currently, the text-mining research community is steadily moving towards processing the full body of the scientific literature by making use of rich linguistic features such as full text parsing, to extract biological interactions. The next step will be to combine these with information from scientific databases to support hypothesis generation for the discovery of new knowledge and the extension of biological networks. The generation of comprehensive networks requires technologies such as entity grounding, coordination resolution and co-reference resolution, which are not fully solved and are required to further improve the quality of results. Here, we analyse the state of the art for the extraction of network information from the scientific literature and the evaluation of extraction methods against reference corpora, discuss challenges involved and identify directions for future research. © The Author 2013.","Event extraction; Network extraction; Text mining","data mining; linguistics; signal transduction; Data Mining; Linguistics; Signal Transduction",2-s2.0-84900861593
"Johnson D.","Media franchising: Creative license and collaboration in the culture industries",2013,"Media Franchising: Creative License and Collaboration in the Culture Industries",53,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900788860&partnerID=40&md5=9a17f4388c6ecf22631333cb1ec6e150","""Johnson astutely reveals that franchises are not Borg-like assimilation machines, but, rather, complicated ecosystems within which creative workers strive to create compelling 'shared worlds.' This finely researched, breakthrough book is a must-read for anyone seeking a sophisticated understanding of the contemporary media industry."" -Heather Hendershot, author of What's Fair on the Air?: Cold War Right-Wing Broadcasting and the Public Interest. While immediately recognizable throughout the U.S. and many other countries, media mainstays like X-Men, Star Trek, and Transformers achieved such familiarity through constant reincarnation. In each case, the initial success of a single product led to a long-term embrace of media franchising-a dynamic process in which media workers from different industrial positions shared in and reproduced familiar cultureacross television, film, comics, games, and merchandising. In Media Franchising, Derek Johnson examines the corporate culture behind these production practices, as well as the collaborative and creative efforts involved in conceiving, sustaining, and sharing intellectual properties in media work worlds. Challenging connotations of homogeneity, Johnson shows how the cultural and industrial logic of franchising has encouraged media industries to reimagine creativity as an opportunity for exchange among producers, licensees, and evenconsumers. Drawing on case studies and interviews with media producers, he reveals the meaningful identities, cultural hierarchies, and struggles for distinction that accompany collaboration within these production networks. Media Franchising provides a nuanced portrait of the collaborative cultural production embedded in both the media industries and our own daily lives. © 2013 by New York University. All rights reserved.",,,2-s2.0-84900788860
"Xiao T., Zhu J., Liu T.","Bagging and Boosting statistical machine translation systems",2013,"Artificial Intelligence",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884907416&doi=10.1016%2fj.artint.2012.11.005&partnerID=40&md5=de1ae5d4f83c97c34a225caaceeb9f7f","In this article we address the issue of generating diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Unlike traditional approaches, we do not resort to multiple structurally different SMT systems, but instead directly learn a strong SMT system from a single translation engine in a principled way. Our approach is based on Bagging and Boosting which are two instances of the general framework of ensemble learning. The basic idea is that we first generate an ensemble of weak translation systems using a base learning algorithm, and then learn a strong translation system from the ensemble. One of the advantages of our approach is that it can work with any of current SMT systems and make them stronger almost ""for free"". Beyond this, most system combination methods are directly applicable to the proposed framework for generating the final translation system from the ensemble of weak systems. We evaluate our approach on Chinese-English translation in three state-of-the-art SMT systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. Experimental results on the NIST MT evaluation corpora show that our approach leads to significant improvements in translation accuracy over the baselines. More interestingly, it is observed that our approach is able to improve the existing system combination systems. The biggest improvements are obtained by generating weak systems using Bagging/Boosting, and learning the strong system using a state-of-the-art system combination method. © 2012 Elsevier B.V. All rights reserved.","Ensemble learning; Statistical machine translation; System combination","Computational linguistics; Computer aided language translation; Engines; Learning algorithms; Linguistics; Speech transmission; Ensemble learning; State-of-the-art system; Statistical machine translation; Statistical machine translation system; System combination; Traditional approaches; Translation engines; Translation systems; Hierarchical systems",2-s2.0-84884907416
"Grace H.","Culture, aesthetics and affect in ubiquitous media: The prosaic image",2013,"Culture, Aesthetics and Affect in Ubiquitous Media",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909317889&doi=10.4324%2f9781315883984&partnerID=40&md5=24bbf96be74d54623649ba0a78a91f7f","This book argues that ubiquitous media and user-created content establish a new perception of the world that can be called ‘particulate vision’, involving a different relation to reality that better represents the atomization of contemporary experience especially apparent in social media. Drawing on extensive original research including detailed ethnographic investigation of camera phone practices in Hong Kong, as well as visual analysis identifying the patterns, regularities and genres of such work, it shows how new distributed forms of creativity and subjectivity now work to shift our perceptions of the everyday. The book analyses the specific features of these new developments — the components of what can be called a ‘general aesthesia’ — and it focuses on the originality and innovation of amateur practices, developing a model for making sense of the huge proliferation of images in contemporary culture, discovering rhythms and tempo in this work and showing why it matters. © 2014 Helen Grace.",,,2-s2.0-84909317889
"Clement R.","Is it time for an evidence based uniform for doctors?",2012,"BMJ (Online)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872075542&doi=10.1136%2fbmj.e8286&partnerID=40&md5=aabb1c5305779ffefb27a04cc812c8c6",[No abstract available],,"aging; airborne infection; antisepsis; clothing; color; evidence based practice; hospital care; human; infection control; medical society; patient safety; physician uniform; practice guideline; priority journal; professional image; review; surgical gown; Clothing; Cross Infection; Evidence-Based Medicine; Great Britain; Guidelines as Topic; Humans; Infection Control; Physicians; Textiles; Wit and Humor as Topic",2-s2.0-84872075542
"Shiono Y., Arita T., Miyadera Y., Sugita K., Yaku T., Tsuchida K.","Automatic generation of XML files and their database registration from tabular form specifications",2012,"Journal of Information Processing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871244599&doi=10.2197%2fipsjjip.20.622&partnerID=40&md5=a6efb9e4b479abc1445f5a2c37a5ad00","Various forms of tables have been used as tools for visualizing and arranging information in many fields. In addition, XML is widely used as a language for exchanging data. We have studied how documents are formally processed with software development tools. In this paper, we propose a system to create and manage tabular specifications based on an attribute graph grammar. A tabular form specification is represented by a marked graph, and its syntax is defined by an attribute NCE graph grammar. We add a new attribute that contains XML source codes of the tabular form specifications. The XML source codes are generated by evaluating the attribute and are automatically registered to the database. The specifications are then retrieved from the database. Our system can perform a characteristic retrieval for software specifications. The results may lead to a considerable improvement in the efficiency of human labor due to the use of a unified formal methodology based on graph theory and advanced retrieval. © 2012 Information Processing Society of Japan.","Attribute graph grammar; Hiform specifications; Parser; Software information; XML database",,2-s2.0-84871244599
"Choi J., Kim D., Kim S., Lee J., Lim S., Lee S., Kang J.","CONSENTO: A new framework for opinion based entity search and summarization",2012,"ACM International Conference Proceeding Series",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871052205&doi=10.1145%2f2396761.2398547&partnerID=40&md5=7869e8c68f5522bc48c63395d892586f","Search engines have become an important decision making tool today. Decision making queries are often subjective, such as ""a good birthday present for my girlfriend"", ""best action movies in 2010"", to name a few. Unfortunately, such queries may not be answered properly by conventional search systems. In order to address this problem, we introduce Consento, a consensus search engine designed to answer subjective queries. Consento performs segment indexing, as opposed to document indexing, to capture semantics from user opinions more precisely. In particular, we define a new indexing unit, Maximal Coherent Semantic Unit (MCSU). An MCSU represents a segment of a document, which captures a single coherent semantic. We also introduce a new ranking method, called ConsensusRank that counts online comments referring to an entity as a weighted vote. In order to validate the efficacy of the proposed framework, we compare Consento with standard retrieval models and their recent extensions for opinion based entity ranking. Experiments using movie and hotel data show the effectiveness of our framework. © 2012 ACM.","consensus rank; consensus search; entity search; maximal coherent semantic unit; sentiment analysis","Coherent semantics; consensus rank; consensus search; Entity search; Sentiment analysis; Decision making; Indexing (of information); Knowledge management; Query processing; Search engines; Semantics; Management science",2-s2.0-84871052205
"Huang B., Xie T., Ma Y.","Anti sql injection with statements sequence digest",2012,"2012 Spring World Congress on Engineering and Technology, SCET 2012 - Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870702801&doi=10.1109%2fSCET.2012.6341889&partnerID=40&md5=a17702cf09ee84e70a1032ed1b618d11","SQL Injection (SQLI) is an attack method that is easy to achieve but often leads data leak and malicious control of the system, so anti SQLI is very important for applications using database. This paper proposes an algorithm for detecting SQLI based on statements sequence digest (SSD). Abstract SQL statement, SSD and its calculating method are defined; algorithms for building SSD and detection SQLI in application are given. For SSD is calculated from the SQL statements sequence context, the proposed algorithm is not only more reliable than filtering special characters and words, but also more exact than checking single SQL statement. Experiments show that the proposed algorithm is implementation feasible and performance efficient. © 2012 IEEE.","Abstract SQL statement; Hash; SQL injection; Statement sequence digest","Abstract SQL statement; Attack methods; Calculating methods; Hash; IS implementation; SQL injection; Statement sequence digest; Engineering; Technology; Algorithms",2-s2.0-84870702801
"Henderson T.C., Cohen E., Joshi A., Grant E., Draelos M., Deshpande N.","Symmetry as a basis for perceptual fusion",2012,"IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870616261&doi=10.1109%2fMFI.2012.6343065&partnerID=40&md5=7af3a1c8ffbab6331d96644c80174c9c","We propose that robot perception is enabled by means of a common sensorimotor semantics arising from a set of symmetry theories (expressed as symmetry detectors and parsers) embedded a priori in each robot. These theories inform the production of structural representations of sensorimotor processes, and these representations, in turn, permit perceptual fusion to broaden categories of activity. Although the specific knowledge required by a robot will depend on the particular application domain, there is a need for fundamental mechanisms which allow each individual robot to obtain the requisite knowledge. Current methods are too brittle and do not scale very well, and a new approach to perceptual knowledge representation is necessary. Our approach provides firm semantic grounding in the real world, provides for robust dynamic performance in real-time environments with a range of sensors and allows for communication of acquired knowledge in a broad community of other robots and agents, including humans. Our work focuses on symmetry based multisensor knowledge structuring in terms of: (1) symmetry detection in signals, and (2) symmetry parsing for knowledge structure, including structural bootstrapping and knowledge sharing. Operationally, the hypothesis is that group theoretic representations (G-Reps) inform cognitive activity. Our contributions here are to demonstrate symmetry detection and signal analysis and for 1D and 2D signals in a simple office environment; symmetry parsing based on these tokens is left for future work. © 2012 IEEE.",,"2-D signals; Cognitive activities; Dynamic performance; Knowledge structures; Knowledge structuring; Knowledge-sharing; Multi sensor; Office environments; Real-time environment; Robot perception; Specific knowledge; Structural representation; Symmetry detection; Detectors; Intelligent systems; Knowledge representation; Semantics; Signal detection; Robots",2-s2.0-84870616261
"Ma J., Zhang S., Hu T., Wu M., Chen T.","Parallel speculative dom-based XML parser",2012,"Proceedings of the 14th IEEE International Conference on High Performance Computing and Communications, HPCC-2012 - 9th IEEE International Conference on Embedded Software and Systems, ICESS-2012",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870458073&doi=10.1109%2fHPCC.2012.15&partnerID=40&md5=4d1c72e62d52f0eb89bc86f122d6c55d","The extensible markup language XML is a standard information representation tool and is playing an increasingly important role in many fields, like database and web services. XML parsing is a core task in XML processing, and many XML parsers are presented both in software and hardware community. In order to accelerate XML parsing, parallel XML parsing method is introduced. In this paper, we detail the design of a parallel speculative Dom-based XML parser (PSDXP) which is implemented of Field Programmable Gate Array (FPGA). Both two threads parallelism and four threads parallelism PSDXPs are implemented on a Xilinx Virtex-5 board. The former can achieve 0.5004 CPB and 1.998 Gbps, and the later capable of running at 0.2505 CPB and 3.992 Gbps. © 2012 IEEE.","DOM; FPGA; Parallel parsing; XML parser","DOM; Parallel parsing; Standard information; XML parser; XML parsing; XML processing; Embedded software; Field programmable gate arrays (FPGA); Web services; XML",2-s2.0-84870458073
"Krishnamurthy J., Mitchell T.M.","Weakly supervised training of semantic parsers",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",40,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883409094&partnerID=40&md5=c229a397faa64f82ae7e8d8a5ab897c1","We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependency-parsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-the-art accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form. © 2012 Association for Computational Linguistics.",,"Binary relation; Knowledge base; Logical forms; Natural language queries; Semantic structures; Text corpora; Weakly supervised trainings; Knowledge based systems; Knowledge representation; Natural language processing systems; Semantics",2-s2.0-84883409094
"Vitucci N., Neri M.A., Tedesco R., Gini G.","Semanticizing syntactic patterns in NLP processing using SPARQL-DL queries",2012,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892623770&partnerID=40&md5=b192e78864fc2b74ce6fcd6dae554dae","Some recent works on natural language semantic parsing make use of syntax and semantics together using different combination modeł. In our work we attempt to use SPARQL-DL as an interface between syntactic information given by the Stanford statistical parser (namely part-of-speech tagged text and typed dependency representation) and semantic information obtained from the FrameNet database. We use SPARQL-DL queries to check the presence of syntactic patterns within a sentence and identify their role as frame elements. The choice of SPARQL-DL is due to its usage as a common reference language for semantic applications and its high expressivity, which let rules to be generalized exploiting the inference capabilities of the underlying reasoner.",,"Combination modes; Frame elements; Natural language semantics; Part Of Speech; Semantic information; Statistical parser; Syntactic information; Syntactic patterns; Birds; Semantics; Syntactics",2-s2.0-84892623770
"Zou G., Peter-Paul R., Boley H., Riazanov A.","PSOATransRun: Translating and running PSOA RuleML via the TPTP interchange language for theorem provers",2012,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891752931&partnerID=40&md5=c7ccbbd180ac9eb52abbc5f3b20e40cb","PSOA RuleML is an object-relational rule language generalizing POSL, OO RuleML, F-logic, and RIF-BLD. In PSOA RuleML, the notion of positional-slotted, object-applicative (psoa) terms is used as a generalization of: (1) positional-slotted terms in POSL and OO RuleML and (2) frame and class-membership terms in F-logic and RIF-BLD. We demonstrate an online PSOA RuleML reasoning service, PSOATransRun, consisting of a translator and an execution engine. The translator, PSOA2TPTP, maps knowledge bases and queries in the PSOA RuleML presentation syntax to the popular TPTP interchange language, which is supported by many first-order logic theorem provers. The translated documents are then executed by the open-source VampirePrime reasoner to perform query answering. In our implementation, we use the ANTLR v3 parser generator tool to build the translator based on the grammars we developed. We wrap the translator and execution engine as resources into a RESTful Web API for convenient access. The presentation demonstrates PSOATransRun with a suite of examples that also constitute an online-interactive introduction to PSOA RuleML.",,"Execution engine; First order logic; Knowledge basis; Object-relational; Parser generators; Presentation syntax; Query answering; Theorem provers; Artificial intelligence; Engines; Theorem proving; Translation (languages); Computer programming languages",2-s2.0-84891752931
"Aygul F.A., Cicekli N.K., Cicekli I.","Natural language query processing in multimedia ontologies",2012,"KEOD 2012 - Proceedings of the International Conference on Knowledge Engineering and Ontology Development",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881404959&partnerID=40&md5=e1ad4a1781c83e4998c0e9e80ca54a2b","In this paper a natural language query interface is developed for semantic and spatio-temporal querying of MPEG-7 based domain ontologies. The underlying ontology is created by attaching domain ontologies to the core Rhizomik MPEG-7 ontology. The user can pose concept, complex concept, spatial, temporal, object trajectory and directional trajectory queries. Furthermore, the system handles the negative meaning in the user query. When the user enters a natural language query, it is parsed with the link parser. According to query type, the objects, attributes, spatial relation, temporal relation, trajectory relation, time filter and time information are extracted from the parser output by using predefined information extraction rules. After the information extraction, SPARQL queries are generated, and executed against the ontology by using an RDF API. The results are used to calculate spatial, temporal, and trajectory relations between objects. The results satisfying the required relations are displayed in a tabular format and the user can navigate through the multimedia content.","MPEG-7 ontology; Natural language querying; SPARQL; Spatio-temporal querying","Information extraction rules; Multimedia contents; Multimedia ontology; Natural language queries; Natural languages; Object trajectories; SPARQL; Spatio-temporal querying; Information retrieval; Knowledge engineering; Motion Picture Experts Group standards; Semantic Web; Semantics; Trajectories; Natural language processing systems",2-s2.0-84881404959
"Chouaib B., Zizette B.","Syntactico-semantic interpretation of natural language queries on a medical ontology",2012,"Proceedings of the 2012 IEEE 2nd International Workshop on Advanced Information Systems for Enterprises, IWAISE 2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874561838&doi=10.1109%2fIWAISE.2012.17&partnerID=40&md5=7e76771ebb2bd71a48660e31628ae98f","Nowadays, one of crucial problems of the Semantic Web is to offer a simple and convenient access to knowledge bases and ontologies. Advances in semantic search have been delayed because of the complexity of nRQL like query languages, as well as the ambiguities of the Natural Language (NL). To go beyond these difficulties, we propose an approach that aims to support mechanisms and techniques for analyzing and processing a natural language query. As result, we obtain an intermediate representation in description logics, easy to be interpreted in any query language oriented information retrieval, as nRQL. This system is composed of four basic modules that provide a sequential processing of the query expressed in NL until the expected results. © 2012 IEEE.","description logics; medical ontology; Natural Language Processing(NLP); nRQL language; semantic search","Description logic; Medical ontology; NAtural language processing; nRQL language; Semantic search; Data description; Formal languages; Industry; Information systems; Query languages; Natural language processing systems",2-s2.0-84874561838
"Chali Y., Hasan S.A.","On the effectiveness of using sentence compression models for query-focused multi-document summarization",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876807906&partnerID=40&md5=3596b7e5ebc9a28965ef510514727665","This paper applies sentence compression models for the task of query-focused multi-document summarization in order to investigate if sentence compression improves the overall summarization performance. Both compression and summarization are considered as global optimization problems and solved using integer linear programming (ILP). Three different models are built depending on the order in which compression and summarization are performed: 1) ComFirst (where compression is performed first), 2) SumFirst (where important sentence extraction is performed first), and 3) Combined (where compression and extraction are performed jointly via optimizing a combined objective function). Sentence compression models include lexical, syntactic and semantic constraints while summarization models include relevance, redundancy and length constraints. A comprehensive set of query-related and importance-oriented measures are used to define the relevance constraint whereas four alternative redundancy constraints are employed based on different sentence similarity measures using a) cosine similarity, b) syntactic similarity, c) semantic similarity, and d) extended string subsequence kernel (ESSK). Empirical evaluation on the DUC benchmark datasets demonstrates that the overall summary quality can be improved significantly using global optimization with semantically motivated models. © 2012 The COLING.","Integer linear programming (ILP); Query-focused multi-document summarization; Sentence compression","Empirical evaluations; Global optimization problems; Integer Linear Programming; Multi-document summarization; Semantic constraints; Sentence compression; Summarization models; Syntactic similarities; Benchmarking; Computational linguistics; Extraction; Global optimization; Integer programming; Redundancy; Semantics; Syntactics; Natural language processing systems",2-s2.0-84876807906
"Kolomiyets O., Bethard S., Moens M.-F.","Extracting narrative timelines as temporal dependency structures",2012,"50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878206996&partnerID=40&md5=b6cec29ed558b18945c628fbbdab7b9c","We propose a new approach to characterizing the timeline of a text: Temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children's stories with temporal dependency trees, achieving agreement (Krippendorff's Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions. © 2012 Association for computational Linguistics.",,"Dependency parser; Dependency structures; Dependency trees; Future research directions; Krippendorff's alphas; Maximum spanning tree; Ordering relations; Partial ordering; Computational linguistics",2-s2.0-84878206996
"Atutxa A., Agirre E., Sarasola K.","Contribution of complex lexical information to solve syntactic ambiguity in Basque",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876797436&partnerID=40&md5=e5e7f2f2dabc64806f217ee6ddc3fdf0","In this study, we explore the impact of complex lexical information to solve syntactic ambiguity, including verbal subcategorization in the form of verbal transitivity and verb-noun-case or verb-noun-case-auxiliary relations. The information was obtained from different sources, including a subcategorization dictionary extracted from a Basque corpus, the web as a corpus, an English corpus and a Basque dictionary. Functional ambiguity between subject and object is a widespread problem in Basque, where 22% of subjects and objects are ambiguous, and this ambiguity surfaces in 33% of the sentences. This problem is comparable to PP attachment ambiguities in other languages. Our results show that, using complex lexical information, our results are better than a state-of-the-art statistical parser, obtaining a statistically significant error reduction of 20%. The disambiguation system is independent on the actual parsing algorithm used. The analysis revealed that the most relevant information are the case carried by the noun and the transitivity of the verb. © 2012 The COLING.","Subcategorization; Syntactic ambiguity resolution; Web as a corpus","Error reduction; Lexical information; Parsing algorithm; Relevant informations; Statistical parser; Subcategorization; Syntactic Ambiguities; Web as a corpus; Computational linguistics; Syntactics",2-s2.0-84876797436
"Kang J., Pang S., Dong J., Du B., Huang J.","A key component extraction method based on HMM and dependency parsing",2012,"2012 6th International Conference on Application of Information and Communication Technologies, AICT 2012 - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872863749&doi=10.1109%2fICAICT.2012.6398516&partnerID=40&md5=cc0e93c6a18528a78fb8bcf4addeb1a3","Increasing attention has been paid for POI (Point of Interest) data query for travel information service. The correct extraction of key components in question is crucial for improving the accuracy of query results. The paper proposes a key component extraction method based on HMM (Hidden Markov Model) and dependency parsing. Firstly, the sentence pattern classifier is established by HMM. And then, questions are classified by classifier. Finally, combination of sentence pattern's structure, the four key components are extracted by dependency parsing. The results show that the F1-Measure is 0.83, which well proves the effectiveness of the method. © 2012 IEEE.","dependency parsing; HMM; key component; segmentation; sentence pattern","Component extraction; Data query; Dependency parsing; HMM; key component; Pattern classifier; Point of interest; Query results; sentence pattern; Hidden Markov models; Image segmentation; Information services; Information technology; Linguistics",2-s2.0-84872863749
"Jin Y., Shen X., Song C.","A filter-based approach for SQL injection attack detection",2012,"Proceedings of the ISCA 27th International Conference on Computers and Their Applications, CATA 2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871992504&partnerID=40&md5=bf8d69527f76704001de513e32d74c64","As database access increasingly occurs over the web, securing database becomes more important to consider for building a web application. Among different types of attacks, SQL Injection Attack is the one targeting the back-end database system by using malicious input. Different from existing SQL injection detection approaches that exam the query structure, this paper proposes an approach that analyzes users' inputs before forming a SQL statement. We design additional table schema at database level to allow synchronized encryption checking. Evaluation was performed and its results are reported in this paper.","Database; SQL injection attacks; Web","Back-end database; Database access; Database levels; Detection approach; Filter-based; Query structures; SQL injection; Sql injection attacks; Web; WEB application; Computer applications; Database systems; Query processing",2-s2.0-84871992504
"Bellandi A., Bellini P., Cappuccio A., Nesi P., Pantaleo G., Rauch N.","Assisted knowledge base generation, management and competence retrieval",2012,"International Journal of Software Engineering and Knowledge Engineering",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876245710&doi=10.1142%2fS021819401240013X&partnerID=40&md5=0a027d07dd47c2ab72381ac12dde607b","Despite the presence of many systems for developing and managing structured taxonomies and/or SKOS models for a given domain for which small documents set are accessible, the production and maintenance of these domain knowledge bases is still a very expensive and time consuming process. This paper proposes a solution for assisting expert users in the development and management of knowledge base, including SKOS and ontologies modeling structures and relationships. The proposed solution accelerates the knowledge production by crawling and exploiting different kinds of sources (in multiple languages and with several inconsistencies among them). The proposed tool supports the experts in defining relationships among the most recurrent concepts, reducing the time to SKOS production and allowing assisted production. The validity of the produced knowledge base has been assessed by using SPARQL query interface and a precision and recall model. The results have demonstrated better performance with respect to the state of the art. The solution has been developed for Open Space Innovative Mind project, with the aim of creating a portal to allow industries at posing semantic queries to discover potential competences in a large institution such as the University of Florence, in which several distinct domains are associated with its own departments. © 2012 World Scientific Publishing Company.","knowledge management; semantic queries; Semantic web; skills management system; SKOS; validation","Knowledge production; Potential competence; Precision and recall; Semantic query; Skills managements; SKOS; University of Florence; validation; Knowledge management; Semantic Web; Knowledge based systems",2-s2.0-84876245710
"McKnight B., Arpinar I.B.","Linking and querying genomic datasets using natural language",2012,"Proceedings - 2012 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872509953&doi=10.1109%2fBIBM.2012.6392724&partnerID=40&md5=6fb4b5173be8b47b235699a09625212b","The association of experimental data with domain knowledge expressed in ontologies facilitates information aggregation, meaningful querying and knowledge discovery to aid in the process of analyzing the extensive amount of interconnected data available for genome projects. TcruziKB is an ontology-driven problem solving system to describe and provide access to the data available for a traditional genome database for the parasite Trypanosoma Cruzi. The problem solving environment enables many advanced search and information presentation features that enable complex queries that would be difficult, if not impossible, to execute without semantic enhancements. However the problem solving features do not only improve the quality of the information retrieved but also reduces the strain on the user by improving usability over the standard system. © 2012 IEEE.","Genomics; Ontologies; Semantic Web; SPARQL","Complex queries; Data sets; Domain knowledge; Genome database; Genome projects; Genomics; Information aggregation; Information presentation; Natural languages; Parasite; Problem solving environments; Problem solving systems; SPARQL; Standard system; Trypanosoma cruzi; Digital storage; Genes; Ontology; Query processing; Semantic Web; Bioinformatics",2-s2.0-84872509953
"Wang X., Mao H., Luo Y.","Design model execution engine based on web services for distributed geography modeling environment",2012,"International Geoscience and Remote Sensing Symposium (IGARSS)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873209313&doi=10.1109%2fIGARSS.2012.6350863&partnerID=40&md5=7f87f819b67f3fdfbab547ecf4fb5c5f","The design of execution engine based on web services for distributed geography modeling environment relies on model contract, model management environment, execution node, data center and control node. The execution procedure of geography model execution engine would be like this: first, geographers construct model contract according to the rule of model contract, and then submit the model contract into control node; Second, the control node parsers the model contract, queries the geography model deployment information, entry point and model input/output information from model management environment, keeps them in memory; Third, control node prepares model data and calls the models which distributed on each execution node for simulation; Finally, the environment returns the result to geographers. The model execution engine adopts multi-thread technology, and could handle multi-job submitted by geographers. © 2012 IEEE.","Geography modeling; model contract; model execution engine","Construct models; Control nodes; Data centers; Design models; Entry point; Execution engine; Model data; Model executions; Model inputs; Model management; Modeling environments; Multi-thread technology; Engines; Environmental management; Geology; Query processing; Remote sensing; Web services; Websites; Computer simulation",2-s2.0-84873209313
"Battle S., Wood D., Leigh J., Ruth L.","The callimachus project: RDFa as a web template language",2012,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891941659&partnerID=40&md5=3a6fd0a4a1481540d67bb9da4fe14d6a","The uptake of semantic technology depends on the availability of simple tools that enable Web developers to build complete applications, with particular emphasis on the last mile, the user interface. RDFa is a vocabulary for adding semantic annotations to standard XHTML and HTML5 documents. This allows the page to contain machine-readable content that is easier to find and mashable with other content. This paper describes Callimachus, an Open Source project that turns this idea around, using RDFa as a template language for the bulk generation of human-readable Web pages from machine-readable RDF data. Most existing template engines generate Web pages by combining the template with query results from a relational database. In the Callimachus template engine, XHTML+RDFa templates pass through an RDFa parser and are then compiled into SPARQL for evaluation against an RDF database. The result set is combined with the template to produce a Web page populated with RDF data, retaining the embedded RDFa. This paper evaluates the benefits and shortcomings of RDFa as a template language, and examines the relationship between Callimachus and the adoption of Web standards such as RDFa.","Linked data; RDF; RDFa; Template engine","Linked datum; Open source projects; RDF; RDFa; Relational Database; Semantic annotations; Semantic technologies; Template engines; Data handling; Engines; Query processing; User interfaces; Websites; Semantic Web",2-s2.0-84891941659
"Guerrisi V., La Torre P., Quarteroni S.","Natural language interfaces to data services",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893689849&doi=10.1007%2f978-3-642-34213-4_6&partnerID=40&md5=008f53950fbb43a3565ee73581a99a28","Natural language interfaces to data services will be a key technology to access unstructured data repositories in a natural way. This involves solving the complex problem of recognizing relevant services given an ambiguous, potentially ungrammatical natural language question. In this paper, we address the requirements of natural language interfaces to data services. While current approaches deal with single-domain questions, we study both rule-based and machine learning methods to address multi-domain questions to support conjunctive queries over data services. Our results denote high accuracy with both approaches. © Springer-Verlag Berlin Heidelberg 2012.",,"Complex problems; Conjunctive queries; Key technologies; Machine learning methods; Natural language interfaces; Natural language questions; Single domains; Unstructured data; Learning systems; Natural language processing systems",2-s2.0-84893689849
"Zanibbi R., Blostein D.","Recognition and retrieval of mathematical expressions",2012,"International Journal on Document Analysis and Recognition",78,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864926316&doi=10.1007%2fs10032-011-0174-4&partnerID=40&md5=70a1e8a7166b269f30a20373639aaf91","Document recognition and retrieval technologies complement one another, providing improved access to increasingly large document collections. While recognition and retrieval of textual information is fairly mature, with wide-spread availability of optical character recognition and text-based search engines, recognition and retrieval of graphics such as images, figures, tables, diagrams, and mathematical expressions are in comparatively early stages of research. This paper surveys the state of the art in recognition and retrieval of mathematical expressions, organized around four key problems in math retrieval (query construction, normalization, indexing, and relevance feedback), and four key problems in math recognition (detecting expressions, detecting and classifying symbols, analyzing symbol layout, and constructing a representation of meaning). Of special interest is the machine learning problem of jointly optimizing the component algorithms in a math recognition system, and developing effective indexing, retrieval and relevance feedback algorithms for math retrieval. Another important open problem is developing user interfaces that seamlessly integrate recognition and retrieval. Activity in these important research areas is increasing, in part because math notation provides an excellent domain for studying problems common to many document and graphics recognition and retrieval applications, and also because mature applications will likely provide substantial benefits for education, research, and mathematical literacy. © 2011 Springer-Verlag.","Content-based image retrieval; Graphics recognition; Human-computer interaction; Math recognition; Mathematical information retrieval","Content based image retrieval; Document collection; Document recognition and retrieval; Graphics recognition; Machine learning problem; Math recognition; Mathematical expressions; Mathematical information; Query construction; Recognition systems; Relevance feedback; Retrieval applications; State of the art; Textual information; Algorithms; Content based retrieval; Human computer interaction; Indexing (of information); Optical character recognition; Pattern recognition; Search engines; User interfaces; Research",2-s2.0-84864926316
"Chiueh T.-C., Simha D.N., Saxena A., Bhola S., Lin P.-H., Pang C.-E.","Encryption domain text retrieval",2012,"CloudCom 2012 - Proceedings: 2012 4th IEEE International Conference on Cloud Computing Technology and Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874241496&doi=10.1109%2fCloudCom.2012.6427518&partnerID=40&md5=00c09188a4945e634858b38e77c2fb74","This paper proposes efficient indexing and querying services on encrypted user documents stored in the cloud. We develop few sophisticated techniques to ensure that any network intruder or even the cloud service provider itself is oblivious to both user data content and the type of operations performed over the data. All data operations, including encryption and decryption of data are performed in the client environment, thereby guaranteeing complete data security. By providing, such a secure system to store and access data on the cloud, end users will be free of any security concerns and hence can easily migrate their data to the cloud. We have implemented an Encryption Domain Text Retrieval (EDTR) System that has a server positioned in the cloud to provide indexing, storage and querying services and a client tool on the end user system that processes the user documents. In order to enable the cloud server to work on encrypted data, the client tool orchestrates the user data into sets of keywords which are again encrypted. We further provide an unique measurement technique to identify unfaithful execution of user requested services by the cloud service provider. Our experiments demonstrate the practical applicability of such a sophisticated system. Though we logically explain the security implications and how the EDTR system overcomes typical cloud security issues, we do not focus on providing concrete, theoretical security proofs. © 2012 IEEE.","Cloud; cloud computing; distributed consistent hashing; encryption domain text retrieval; index and query; privacy protection","Client tools; Cloud securities; Cloud servers; Cloud service providers; Consistent hashing; Data operations; Encrypted data; Encryption and decryption; End user system; End users; index and query; Measurement techniques; Network intruders; Privacy protection; Querying services; Secure system; Security implications; Security proofs; Sophisticated system; Text retrieval; User data; Cloud computing; Clouds; Cryptography; Digital storage; Distributed database systems; Indexing (of information); Information retrieval; Security of data",2-s2.0-84874241496
"Tur G., Jeong M., Wang Y.-Y., Hakkani-Tür D., Heck L.","Exploiting the semantic web for unsupervised natural language semantic parsing",2012,"13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878381853&partnerID=40&md5=dd54d3ad994cad54ee8a97b56898f06c","In this paper, we propose to bring together the semantic web experience and statistical natural language semantic parsing modeling. The idea is that, the process for populating knowledgebases by semantically parsing structured web pages may provide very valuable implicit annotation for language understanding tasks. We mine search queries hitting to these web pages in order to semantically annotate them for building statistical un-supervised slot filling models, without even a need for a semantic annotation guideline. We present promising results demonstrating this idea for building an unsupervised slot filling model for the movies domain with some representative slots. Furthermore, we also employ unsupervised model adaptation for cases when there are some in-domain unannotated sentences available. Another key contribution of this work is using implicitly annotated natural-language-like queries for testing the performance of the models, in a totally unsupervised fashion. We believe, such an approach also ensures consistent semantic representation between the semantic parser and the backend knowledge-base.","Dialog; Natural language understanding; Semantic parsing; Semantic search; Semantic web","Dialog; Language understanding; Natural language semantics; Natural language understanding; Semantic annotations; Semantic parsing; Semantic representation; Semantic search; Computer applications; Computer simulation; Semantic Web; Websites",2-s2.0-84878381853
"Le P., Zuidema W.","Learning compositional semantics for open domain semantic parsing",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876805315&partnerID=40&md5=05f243fdfb99e7e58472e420e36a8f87","This paper introduces a new approach to learning compositional semantics for open domain semantic parsing. Our approach is called Dependency-based Semantic Composition using Graphs (DeSCoG) and deviates from existing approaches in several ways. First, we remove the need of the lambda calculus by using a graph-based variant of Discourse Representation Structures to represent semantic building blocks and defining new combinatory operations for our graph structures. Second, we propose a probability model to approximate probability distributions over possible semantic compositions. And third, we use a variant of alignment algorithms from machine translation to learn a lexicon. On the Groningen Meaning Bank (a recently released, large-scale, domain-general, semantically annotated corpus; Basile et al. (2012)), where we preprocess sentences with an existing dependency parser, we achieve results significantly better than the baseline. On Geoquery we obtain performance comparable to semantic parsers that were developed specifically for that domain. © 2012 The COLING.","Dependency structure; Graph; Parsing; Semantics","Alignment algorithms; Compositional semantics; Dependency structures; Discourse representation; Graph; Machine translations; Parsing; Semantic composition; Computational linguistics; Differentiation (calculus); Graphic methods; Probability distributions; Semantics",2-s2.0-84876805315
"Kim D.S., Verma K., Yeh P.Z.","Building a lightweight semantic model for unsupervised information extraction on short listings",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883425059&partnerID=40&md5=71d0d43486b1a6892a5251388684b276","Short listings such as classified ads or product listings abound on the web. If a computer can reliably extract information from them, it will greatly benefit a variety of applications. Short listings are, however, challenging to process due to their informal styles. In this paper, we present an unsupervised information extraction system for short listings. Given a corpus of listings, the system builds a semantic model that represents typical objects and their attributes in the domain of the corpus, and then uses the model to extract information. Two key features in the system are a semantic parser that extracts objects and their attributes and a listing-focused clustering module that helps group together extracted tokens of same type. Our evaluation shows that the semantic model learned by these two modules is effective across multiple domains. © 2012 Association for Computational Linguistics.",,"Classified ads; Extract informations; Key feature; Multiple domains; Semantic Model; Unsupervised information extractions; Information retrieval; Information retrieval systems; Linguistics; Semantics; Natural language processing systems",2-s2.0-84883425059
"Sinkovics A., Porkoláb Z.","Domain-specific language integration with c++ template metaprogramming",2012,"Formal and Practical Aspects of Domain-Specific Languages: Recent Developments",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899376577&doi=10.4018%2f978-1-4666-2092-6.ch002&partnerID=40&md5=e2ea6bf98e20cdfbd64b3cc73400610b","Domain specific language integration has to provide the right balance between the expressive power of the DSL and the implementation and maintenance cost of the applied integration techniques. External solutions may perform poorly as they depend on third party tools which should be implemented, tested and then maintained during the whole lifetime of the project. Ideally a self-contained solution can minimize third-party dependencies. The authors propose the use of C++ template metaprograms to develop a domain specific language integration library based on only the standard C++ language features. The code in the domain specific language is given as part of the C++ source code wrapped into C++ templates. When the C++ source is compiled, the C++ template metaprogram library implementing a full-featured parser infrastructure is executed. As the authors' approach does not require other tool than a standard C++ compiler, it is highly portable. To demonstrate their solution, the chapter implements a type-safe printf as a domain specific language. The library is fully implemented and downloadable as an open source project. © 2013, IGI Global.",,,2-s2.0-84899376577
"Celikyilmaz A., Hakkani-Tur D., Tur G.","Statistical semantic interpretation modeling for spoken language understanding with enriched semantic features",2012,"2012 IEEE Workshop on Spoken Language Technology, SLT 2012 - Proceedings",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874255045&doi=10.1109%2fSLT.2012.6424225&partnerID=40&md5=cef1920f662b75d8424f9f9da1700fd7","In natural language human-machine statistical dialog systems, semantic interpretation is a key task typically performed following semantic parsing, and aims to extract canonical meaning representations of semantic components. In the literature, usually manually built rules are used for this task, even for implicitly mentioned non-named semantic components (like genre of a movie or price range of a restaurant). In this study, we present statistical methods for modeling interpretation, which can also benefit from semantic features extracted from large in-domain knowledge sources. We extract features from user utterances using a semantic parser and additional semantic features from textual sources (online reviews, synopses, etc.) using a novel tree clustering approach, to represent unstructured information that correspond to implicit semantic components related to targeted slots in the user's utterances. We evaluate our models on a virtual personal assistance system and demonstrate that our interpreter is effective in that it does not only improve the utterance interpretation in spoken dialog systems (reducing the interpretation error rate by 36% relative compared to a language model baseline), but also unveils hidden semantic units that are otherwise nearly impossible to extract from purely manual lexical features that are typically used in utterance interpretation. © 2012 IEEE.","graphical models; semantic interpretation; semi-supervised clustering; spoken language understanding","Assistance system; Dialog systems; GraphicaL model; Hidden semantics; Human-machine; Implicit semantics; Interpretation errors; Knowledge sources; Language model; Lexical features; Natural languages; Online reviews; Semantic components; Semantic features; Semantic interpretation; Semantic parsing; Semi-supervised Clustering; Spoken dialog systems; Spoken language understanding; Tree clustering; Clustering algorithms; Computational linguistics; Machine components; Speech recognition; Semantics",2-s2.0-84874255045
"Hall D., Klein D.","Training factored PCFGs with expectation propagation",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883435877&partnerID=40&md5=d54e6626d0b91fc6f665e6e2b76a87dd","PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguistically-motivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the naïve approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank. © 2012 Association for Computational Linguistics.",,"Combinatorial explosion; Expectation Propagation; F1 scores; Latent structures; Latent variable; Lexicalization; Orders of magnitude; Per-symbol; Forestry; Natural language processing systems",2-s2.0-84883435877
"Quochi V., Frontini F., Rubino F.","A MWE acquisition and lexicon builder web service",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876805860&partnerID=40&md5=2f99b5529ac6d8f5962ff3c98b7d5252","This paper describes the development of a web-service tool for the automatic extraction of Multi-word expressions lexicons, which has been integrated in a distributed platform for the automatic creation of linguistic resources. The main purpose of the work described is thus to provide a (computationally ""light"") tool that produces a full lexical resource: multi-word terms/items with relevant and useful attached information that can be used for more complex processing tasks and applications (e.g. parsing, MT, IE, query expansion, etc.). The output of our tool is a MW lexicon formatted and encoded in XML according to the Lexical Mark-up Framework. The tool is already functional and available as a service. Evaluation experiments show that the tool precision is of about 80%. © 2012 The COLING.","Lexical resources; LMF; Multiword extraction; Web services","Automatic extraction; Distributed platforms; Evaluation experiments; Lexical resources; Linguistic resources; LMF; Multi-word; Multi-word expressions; Computational linguistics; Extraction; Websites; Web services",2-s2.0-84876805860
"Varma V., Mogadala A.","Issues and challenges in building multilingual information access systems",2012,"Emerging Applications of Natural Language Processing: Concepts and New Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899375646&doi=10.4018%2f978-1-4666-2169-5.ch008&partnerID=40&md5=084dc82e7aaaa1278040f8b210b91eda","In this chapter, the authors start their discussion highlighting the importance of Cross Lingual and Multilingual Information Retrieval and access research areas. They then discuss the distinction between Cross Language Information Retrieval (CLIR), Multilingual Information Retrieval (MLIR), Cross Language Information Access (CLIA), and Multilingual Information Access (MLIA) research areas. In addition, in further sections, issues and challenges in these areas are outlined, and various approaches, including machine learning-based and knowledge-based approaches to address the multilingual information access, are discussed. The authors describe various subsystems of a MLIA system ranging from query processing to output generation by sharing their experience of building a MLIA system and discuss its architecture. Then evaluation aspects of the MLIA and CLIA systems are discussed at the end of this chapter. © 2013, IGI Global.",,,2-s2.0-84899375646
"Lee M.C., Chang J.W., Hsieh T.C., Wang T.I., Su C.Y., Chen H.H., Chen C.H.","A syntactic based approach for evaluating semantics of texts",2012,"International Journal of Advancements in Computing Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872945387&doi=10.4156%2fijact.vol4.issue21.26&partnerID=40&md5=f1bbae01b8c98258734ee35aafc3c7e9","This study introduced a novel algorithm to compute similarities between natural languages. Using syntactical relationships derived from natural languages, the algorithm proposed a semantic structural model and quantified natural languages using the word similarity method based on WordNet and lexical databases. The experimental results indicated that the algorithm could yield optimal results in semantic recognition when applied to sentences or short texts that are grammatically complex or relatively long. The contribution of this study is in its conversion of the grammar of different natural languages into a unified semantic structure, through which the semantic similarity of two sentences or short texts can be obtained by comparison. This study aimed to enhance the capability of computers for fuzzy concept processing, which can be applied to the fields of search engines and artificial intelligence. For instance, in search engines, sentences or short text-based concepts may be semantically structured to replace key-word based queries when executing search tasks. In the field of artificial intelligence, this capability may be applied to intelligent agents to smooth the process of interaction between humans and machines.","Semantics; Similarity; Syntactic; WordNet","Fuzzy concept; Lexical database; Natural languages; Novel algorithm; Optimal results; Search tasks; Semantic recognition; Semantic similarity; Semantic structures; Similarity; Structural models; Syntactical relationships; Word similarity; Wordnet; Algorithms; Artificial intelligence; Intelligent agents; Model structures; Natural language processing systems; Ontology; Search engines; Syntactics; Semantics",2-s2.0-84872945387
"Xue Y., Xing Z., Jarzabek S.","Feature location in a collection of product variants",2012,"Proceedings - Working Conference on Reverse Engineering, WCRE",31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872312622&doi=10.1109%2fWCRE.2012.24&partnerID=40&md5=2279e28693d0fbdd6c5462186ce3819e","Companies often develop and maintain a collection of product variants that share some common features but also support different, customer-specific features. To reengineering such legacy product variants for systematic reuse, one must identify features and their implementing code units (e.g. functions, files) in different product variants. Information retrieval (IR) techniques may be applied for that purpose. In this paper, we discuss problems that hinder direct application of IR techniques to a collection of product variants. To counter these problems, we present an approach to support effective feature location in product variants. The novelty of our approach is that we exploit commonalities and differences of product variants by software differencing and FCA techniques so that IR technique can achieve satisfactory results for feature location in product variants. We have implemented our approach and conducted evaluation with a collection of nine Linux kernel product variants. Our evaluation shows that our approach always significantly outperforms a direct application of IR technique in the subject product variants. © 2012 IEEE.","feature location; formal concept analysis; latent semantic analysis; software differencing; software product variants","Feature location; IR techniques; Latent Semantic Analysis; Legacy products; Linux kernel; Product variants; Software products; Formal concept analysis; Semantics; Reverse engineering",2-s2.0-84872312622
"Henriksson A., Moen H., Skeppstedt M., Eklund A.-M., Daudaravičius V., Hassel M.","Synonym extraction of medical terms from clinical text using combinations of word space models",2012,"SMBM 2012 - Proceedings of the 5th International Symposium on Semantic Mining in Biomedicine",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874243708&doi=10.5167%2fuzh-64476&partnerID=40&md5=31ee1edab19eb07579a19e66bc1d4fcb","In information extraction, it is useful to know if two signifiers have the same or very similar semantic content. Maintaining such information in a controlled vocabulary is, however, costly. Here it is demonstrated how synonyms of medical terms can be extracted automatically from a large corpus of clinical text using distributional semantics. By combining Random Indexing and Random Permutation, different lexical semantic aspects are captured, effectively increasing our ability to identify synonymic relations between terms. 44% of 340 synonym pairs from MeSH are successfully extracted in a list of ten suggestions. The models can also be used to map abbreviations to their full-length forms; simple pattern-based filtering of the suggestions yields substantial improvements.",,"Distributional semantics; Information Extraction; Large corpora; Lexical semantics; Medical terms; Random indexing; Random permutations; Semantic content; Word spaces; Semantics",2-s2.0-84874243708
"Panchenko A., Adeykin S., Romanov A., Romanov P.","Extraction of semantic relations between concepts with KNN algorithms on Wikipedia",2012,"CEUR Workshop Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891951097&partnerID=40&md5=cd146eace7792e01aa80bb16aca6b7ac","This paper presents methods for extraction of semantic relations between words. The methods rely on the k-nearest neighbor algorithms and two semantic similarity measures to extract relations from the abstracts of Wikipedia articles. We analyze the proposed methods and evaluate their performance. Precision of the extraction with the best method achieves 83%. We also present an open source system which effectively implements the described algorithms.","Computational lexical semantics; Information extraction; KNN; MKNN; Semantic relations; Semantic similarity measure; Wikipedia","Computational lexical semantics; KNN; MKNN; Semantic relations; Semantic similarity measures; Wikipedia; Algorithms; Formal concept analysis; Information analysis; Information retrieval; Open systems; Pattern recognition; Semantics",2-s2.0-84891951097
"Takase S., Okazaki N., Inui K.","Set expansion using sibling relations between semantic categories",2012,"Proceedings of the 26th Pacific Asia Conference on Language, Information and Computation, PACLIC 2012",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883326575&partnerID=40&md5=78a6415d57ba613b2c8223c2f6eb5d29","Most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories. However, in the setting of set expansion with multiple semantic categories, we might leverage other types of prior knowledge about semantic categories. In this paper, we present a method of set expansion when ontological information related to target semantic categories is available. More specifically, the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner. © 2012 The PACLIC.",,"Prior knowledge; Semantic category; Semi-automatics; Set expansions; Wikipedia; Knowledge management; Semantics",2-s2.0-84883326575
"Pauls A., Klein D.","Large-scale syntactic language modeling with treelets",2012,"50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878194222&partnerID=40&md5=0c283a28b74d82ea3e3b01a528334728","We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment. © 2012 Association for Computational Linguistics.",,"Discriminative models; Machine translations; N-gram language models; N-gram models; Overlapping window; Positive data; Single- machines; Syntactic languages; Forestry; Natural language processing systems; Syntactics; Computational linguistics; Mathematical Models; Programing Languages; Translation; Trees",2-s2.0-84878194222
"Wen D., Cuzzola J., Brown L., Kinshuk","Instructor-aided asynchronous question answering system for online education and distance learning",2012,"International Review of Research in Open and Distance Learning",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874682463&partnerID=40&md5=070deaf385e2dac600e2a2ef3b99e31f","Question answering systems have frequently been explored for educational use. However, their value was somewhat limited due to the quality of the answers returned to the student. Recent question answering (QA) research has started to incorporate deep natural language processing (NLP) in order to improve these answers. However, current NLP technology involves intensive computing and thus it is hard to meet the real-time demand of traditional search. This paper introduces a question answering (QA) system particularly suited for delayed-answered questions that are typical in certain asynchronous online and distance learning settings. We exploit the communication delay between student and instructor and propose a solution that integrates into an organization's existing learning management system. We present how our system fits into an online and distance learning situation and how it can better assist supporting students. The prototype system and its running results show the perspective and potential of this research.","Automated question answering; Distance education; Information retrieval; LMS; Natural language processing; Online learning",,2-s2.0-84874682463
"Ji S., Wang W., Ye C., Wei J., Liu Z.","Constructing a data accessing layer for in-memory data grid",2012,"4th Asia-Pacific Symposium on Internetware, Internetware 2012",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445787&doi=10.1145%2f2430475.2430490&partnerID=40&md5=75e9d3a44fbf8a81faecbde150fb0d43","In-memory data grid (IMDG) is a novel data processing middleware for Internetware. It provides higher scalability and performance compared with traditional rational database. However, because the data stored in IMDG must follow the key/value data model, new challenges have been proposed. One important aspect is that IMDG does not support standard data accessing languages such as JPA and SQL, and application developers must design their programs according to the peculiarities of an IMDG product. This results in complex and error-prone code, especially for the programmers who have no deep understanding of IMDG. In this paper, we propose a data accessing reference architecture for IMDG and a methodology to design and implement its data accessing layer. In this methodology, data accessing engine construction, data model designation and join operation supporting are presented. Moreover, following this methodology, we develop and implement a JPA compatible data accessing engine for Hazelcast as a case study, which proves the feasibility of our approach. Copyright 2012 ACM.","Data accessing; In-memory data grid (IMDG); Key/value data model","Application developers; Data accessing; Data Grid; Error prones; Internetware; ITS data; Join operation; Reference architecture; Data processing; Middleware; Product design; Models",2-s2.0-84873445787
"Kim J., Mooney R.J.","Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883389933&partnerID=40&md5=84c52632d7b2ed57be891fdcb8dea2b6","""Grounded"" language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. Börschinger et al. (2011) introduced an approach to grounded language learning based on unsupervised PCFG induction. Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011). This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision. Experimental results on the navigation task demonstrates the effectiveness of our approach. © 2012 Association for Computational Linguistics.",,"Language learning; Navigation tasks; Training data; Natural language processing systems",2-s2.0-84883389933
"Yilmaz L.","Reproducibility in M&S research: Issues, strategies and implications for model development environments",2012,"Journal of Experimental and Theoretical Artificial Intelligence",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867045274&doi=10.1080%2f0952813X.2012.693842&partnerID=40&md5=c45ded273811e1901d7791e1937f49b4","As the use of computer simulation is increasingly becoming central to scientific enterprise, lack of proper documentation, validation and distribution of models and experiments may hamper reproducibility and hence cause a credibility gap. Building on prior research requires ability to reproduce simulation experiments and to further extend models to address increasingly challenging problems. In this article, we delineate three dimensions of reproducible research. For each dimension, issues, strategies and implications for simulation model development are identified. From a technical infrastructure point of view, we introduce the e-Portfolio concept, which is an ensemble of integrated active documents that interweave published manuscript, computer code, data and scientific workflow specification. From the process point of view, strategies and potential mechanisms are delineated to enable authors, publishers, funding agencies, journals and the broader scientific community to cooperate and establish a sustained model base, simulations, experiments and documentation, so that scientists can build on each other's work and achievements. © 2012 Copyright Taylor and Francis Group, LLC.","computational reproducibility; modelling and simulation; replicability; reproducible research; simulation tools","Active documents; Computer codes; E-portfolios; Funding agencies; Model base; Model development; Modelling and simulations; Potential mechanism; Replicability; Reproducibilities; Reproducible research; Scientific community; Scientific enterprise; Scientific workflows; Simulation experiments; Simulation model; Technical infrastructure; Three dimensions; Experiments; Planning; Computer simulation",2-s2.0-84867045274
"Morgan M.S., Knuuttila T.","Models and Modelling in Economics",2012,"Philosophy of Economics",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882461056&doi=10.1016%2fB978-0-444-51676-3.50003-8&partnerID=40&md5=5c1d60100d9e2471473cf92a8c885e69",[No abstract available],,,2-s2.0-84882461056
"Günther S.","Design patterns and design principles for internal domain-specific languages",2012,"Formal and Practical Aspects of Domain-Specific Languages: Recent Developments",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899271541&doi=10.4018%2f978-1-4666-2092-6.ch007&partnerID=40&md5=d0a6b28ce508bffd3aef11298eff66e1","Internal DSLs are a special kind of DSLs that use an existing programming language as their host. To build them successfully, knowledge regarding how to modify the host language is essential. In this chapter, the author contributes six DSL design principles and 21 DSL design patterns. DSL Design principles provide guidelines that identify specific design goals to shape the syntax and semantic of a DSL. DSL design patterns express proven knowledge about recurring DSL design challenges, their solution, and their connection to each other - forming a rich vocabulary that developers can use to explain a DSL design and share their knowledge. The chapter presents design patterns grouped into foundation patterns (which provide the skeleton of the DSL consisting of objects and methods), notation patterns (which address syntactic variations of host language expressions), and abstraction patterns (which provide the domain-specific abstractions as extensions or even modifications of the host language semantics). © 2013, IGI Global.",,,2-s2.0-84899271541
"Theijssen D., Boves L., van Halteren H., Oostdijk N.","Evaluating automatic annotation: Automatically detecting and enriching instances of the dative alternation",2012,"Language Resources and Evaluation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870429737&doi=10.1007%2fs10579-011-9156-x&partnerID=40&md5=0627c76696a06a72f79ceb0c62ef1581","In this article, we automatically create two large and richly annotated data sets for studying the English dative alternation. With an intrinsic and an extrinsic evaluation, we address the question of whether such data sets that are obtained and enriched automatically are suitable for linguistic research, even if they contain errors. The extrinsic evaluation consists of building logistic regression models with these data sets. We conclude that the automatic approach for detecting instances of the dative alternation still needs human intervention, but that it is indeed possible to annotate the instances with features that are syntactic, semantic and discourse-related in nature. Only the automatic classification of the concreteness of nouns is problematic. © 2011 The Author(s).","Automatic annotation; Dative alternation; Intrinsic and extrinsic evaluation; Logistic regression; Syntactic alternation",,2-s2.0-84870429737
"Jones B., Andreas J., Bauer D., Hermann K.M., Knight K.","Semantics-based machine translation with hyperedge replacement grammars",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876807738&partnerID=40&md5=5455e7ef066f75daf1337a9f4089fc9e","We present an approach to semantics-based statistical machine translation that uses synchronous hyperedge replacement grammars to translate into and from graph-shaped intermediate meaning representations, to our knowledge the first work in NLP to make use of synchronous context free graph grammars. We present algorithms for each step of the semantics-based translation pipeline, including a novel graph-to-word alignment algorithm and two algorithms for synchronous grammar rule extraction. We investigate the influence of syntactic annotations on semantics-based translation by presenting two alternative rule extraction algorithms, one that requires only semantic annotations and another that additionally relies on syntactic annotations, and explore the effect of syntax and language bias in meaning representation structures by running experiments with two different meaning representations, one biased toward an English syntax-like structure and another that is language neutral. While preliminary work, these experiments show promise for semantically-informed machine translation. © 2012 The COLING.",,"Alignment algorithms; Context-free graph grammars; Hyperedge replacement; Rule extraction algorithms; Semantic annotations; Statistical machine translation; Synchronous grammars; Syntactic annotation; Algorithms; Computational linguistics; Computer aided language translation; Experiments; Formal languages; Syntactics; Semantics",2-s2.0-84876807738
"Chali Y., Hasan S.A.","Towards automatic topical question generation",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876788307&partnerID=40&md5=69eebcd70635fdd7813587dc0c5cc1bc","We address the challenge of automatically generating questions from topics. We consider that each topic is associated with a body of texts containing useful information about the topic. Questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts. To measure the importance of the generated questions, we use Latent Dirichlet Allocation (LDA) to identify the sub-topics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) to calculate their similarity with the questions. We also propose the use of syntactic tree kernels for computing the syntactic correctness of the questions. The questions are ranked by considering their importance (in the context of the given body of texts) and syntactic correctness. To the best of our knowledge, no other study has accomplished this task in our setting before. Experiments show that our approach can significantly outperform the state-of-the-art results. © 2012 The COLING.","Extended string subsequence kernel (ESSK); Latent dirichlet allocation (LDA); Named entity information; Predicate argument structures; Question generation; Syntactic tree kernel","Argument structures; Latent dirichlet allocations; Named entities; Question generation; Subsequence kernels; Syntactic trees; Computational linguistics; Forestry; Statistics; Syntactics; Experimentation; Statistics; Trees",2-s2.0-84876788307
"Radinsky K., Davidovich S., Markovitch S.","Learning to predict from textual data",2012,"Journal of Artificial Intelligence Research",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875154589&doi=10.1613%2fjair.3865&partnerID=40&md5=51dae09c6ca8ac916ae3f53f1c39eebd","Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns. For generalization, the model uses a vast number of world knowledge ontologies. Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans. © 2012 AI Access Foundation.",,"Empirical evaluations; Model use; Natural languages; News articles; Textual data; World knowledge; Computational linguistics; Natural language processing systems; Semantics; Algorithms",2-s2.0-84875154589
"Ning B., Liu C.","XML filtering with XPath expressions containing parent and ancestor axes",2012,"Information Sciences",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862676955&doi=10.1016%2fj.ins.2012.04.035&partnerID=40&md5=ee6e779c93a205fb8c89ad1bcfbef79f","More and more XML data is generated and used for data exchange. In this paper, we address the problem of filtering XML documents with large number of XPath expressions, which may contain 'ancestor' and 'parent' axes. XPath expressions with these axes are more powerful and flexible for users to describe their interests in publish/subscribe systems. First, we analyze the characteristics of the 'parent' axis and propose a series of rules to eliminate it in XPath expressions. Then we propose a new index structure called NIndex, which is designed to efficiently store and index large number of XPath expressions. NIndex offers several features which make it especially attractive for the large scale selective dissemination of information, including the ability to handle complex XPath expressions with 'ancestor' and 'parent' axes, and efficient pruning. Based on NIndex, we design a new filtering algorithm with low complexity for our problem. Our experiment results show that our algorithm performs well across a range of XPath expressions and documents. © 2012 Elsevier Inc. All rights reserved.","Information filtering; Publish/subscribe System; Selective dissemination of information; XML; XPath axes","Filtering algorithm; Information filtering; Low complexity; New indices; Publish/Subscribe system; Selective dissemination of information; XML data; XML filtering; XPath axes; XPath expressions; Electronic data interchange; Information retrieval systems; Information services; XML",2-s2.0-84862676955
"Papaemmanouil O.","Supporting extensible performance SLAs for cloud databases",2012,"Proceedings - 2012 IEEE 28th International Conference on Data Engineering Workshops, ICDEW 2012",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869031095&doi=10.1109%2fICDEW.2012.71&partnerID=40&md5=e3786052a165142be52f9d0764201d2e","Despite the fast growth and increased adoption of cloud databases the lack of application-specific Service-Level-Agreements (SLAs) hinders the adoption of cloud data services by large-scale enterprises. Defining application-specific QoS objectives and constraints, monitoring the performance factors to ensure acceptable QoS levels and isolating the source of QoS degradation, are some of the critical tasks that are still addressed through custom, ad-hoc tools at the application level, which drastically increases the application development and maintenance overhead. In this work-in-progress paper, we argue that performance management of data management applications should itself be offered as a service. Towards this goal, we present XCloud, a suite of SLA management services for cloud databases that enables the definition and monitoring of application-specific performance criteria and customizable performance SLAs. © 2012 IEEE.",,"Application development; Application level; Cloud data; Cloud database; Critical tasks; Customizable; Maintenance overhead; Management applications; Performance criterion; Performance factors; Performance management; SLA management; Work-in-progress; Information management; Technical presentations; Database systems",2-s2.0-84869031095
"Lockwood J.W., Gupte A., Mehta N., Blott M., English T., Vissers K.","A low-latency library in FPGA hardware for High-Frequency Trading (HFT)",2012,"Proceedings - 2012 IEEE 20th Annual Symposium on High-Performance Interconnects, HOTI 2012",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868346827&doi=10.1109%2fHOTI.2012.15&partnerID=40&md5=3f0ae8231abff22d1d30ff572579f73c","Current High-Frequency Trading (HFT) platforms are typically implemented in software on computers with high-performance network adapters. The high and unpredictable latency of these systems has led the trading world to explore alternative ""hybrid"" architectures with hardware acceleration. In this paper, we survey existing solutions and describe how FPGAs are being used in electronic trading to approach the goal of zero latency. We present an FPGA IP library which implements networking, I/O, memory interfaces and financial protocol parsers. The library provides pre-built infrastructure which accelerates the development and verification of new financial applications. We have developed an example financial application using the IP library on a custom 1U FPGA appliance. The application sustains 10Gb/s Ethernet line rate with a fixed end-to-end latency of 1μs - up to two orders of magnitude lower than comparable software implementations. © 2012 IEEE.","Algorithmic; FPGA; HFT; latency; trading","10 Gb/ S; Algorithmic; Electronic trading; End-to-end latency; Hardware acceleration; HFT; High frequency HF; High performance networks; IP library; latency; Line rate; Low-latency; Memory interface; Orders of magnitude; Software implementation; trading; Zero latency; Commerce; Computer hardware; Hardware; Internet protocols; Field programmable gate arrays (FPGA)",2-s2.0-84868346827
"Missier P., Belhajjame K.","A PROV encoding for provenance analysis using deductive rules",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868277795&doi=10.1007%2f978-3-642-34222-6_6&partnerID=40&md5=9faca3273a90700318876f9b939052fd","PROV is a specification, promoted by the World Wide Web consortium, for recording the provenance of web resources. It includes a schema, consistency constraints and inference rules on the schema, and a language for recording provenance facts. In this paper we describe a implementation of PROV that is based on the DLV Datalog engine. We argue that the deductive databases paradigm, which underpins the Datalog model, is a natural choice for expressing at the same time (i) the intensional features of the provenance model, namely its consistency constraints and inference rules, (ii) its extensional features, i.e., sets of provenance facts (called a provenance graph), and (iii) declarative recursive queries on the graph. The deductive and constraint solving capability of DLV can be used to validate a graph against the constraints, and to derive new provenance facts. We provide an encoding of the PROV rules as Datalog rules and constraints, and illustrate the use of deductive capabilities both for queries and for constraint validation, namely to detect inconsistencies in the graphs. The DLV code along with a parser to map the PROV assertion language to Datalog syntax, are publicly available. © 2012 Springer-Verlag.",,"Assertion language; Consistency constraints; Constraint Solving; Datalog; Deductive database; Deductive rule; Inference rules; Recording provenance; Web resources; World wide web consortiums; World Wide Web; Encoding (symbols)",2-s2.0-84868277795
"Cakmak M., Lopes M.","Algorithmic and human teaching of sequential decision tasks",2012,"Proceedings of the National Conference on Artificial Intelligence",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868291108&partnerID=40&md5=e0971887f291bd75c2858d4bd0f75ac9","A helpful teacher can significantly improve the learning rate of a learning agent. Teaching algorithms have been formally studied within the field of Algorithmic Teaching. These give important insights into how a teacher can select the most informative examples while teaching a new concept. However the field has so far focused purely on classification tasks. In this paper we introduce a novel method for optimally teaching sequential decision tasks. We present an algorithm that automatically selects the set of most informative demonstrations and evaluate it on several navigation tasks. Next, we explore the idea of using this algorithm to produce instructions for humans on how to choose examples when teaching sequential decision tasks. We present a user study that demonstrates the utility of such instructions. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Classification tasks; Decision task; Learning agents; Learning rates; Navigation tasks; Teaching algorithms; User study; Artificial intelligence; Algorithms",2-s2.0-84868291108
"Ghayoomi M.","Word clustering for persian statistical parsing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868101515&doi=10.1007%2f978-3-642-33983-7_13&partnerID=40&md5=0699c17c2ec79dd24b5043883549ad29","Syntactically annotated data like a treebank are used for training the statistical parsers. One of the main aspects in developing statistical parsers is their sensitivity to the training data. Since data sparsity is the biggest challenge in data oriented analyses, parsers have a malperformance if they are trained with a small set of data, or when the genre of the training and the test data are not equal. In this paper, we propose a word-clustering approach using the Brown algorithm to overcome these problems. Using the proposed class-based model, a more coarser level of the lexicon is created compared to the words. In addition, we propose an extension to the clustering approach in which the POS tags of the words are also taken into the consideration while clustering the words. We prove that adding this information improves the performance of clustering specially for homographs. In usual word clusterings, homographs are treated equally; while the proposed extended model considers the homographs distinct and causes them to be assigned to different clusters. The experimental results show that the class-based approach outperforms the word-based parsing in general. Moreover, we show the superiority of the proposed extension of the class-based parsing to the model which only uses words for clustering. © 2012 Springer-Verlag Berlin Heidelberg.","Statistical Parsing; the Persian Language; Word Clustering","Brown algorithm; Class-based; Clustering approach; Clusterings; Data sparsity; Extended model; Persians; Statistical parser; Statistical Parsing; Test data; Training data; Treebanks; Word clustering; Artificial intelligence; Natural language processing systems",2-s2.0-84868101515
"Xue W., Deng H.","Unstructured queries based on mobile user context",2012,"International Journal of Pervasive Computing and Communications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893427770&doi=10.1108%2f17427371211283038&partnerID=40&md5=9191180b78e9b1f51d4d5f29f3937f3e","Purpose: Many mobile devices today are equipped with diversified sensors that enable the acquisition of rich user context (e.g. GPS location, phone activity) for application utilization. With the growing usage of mobile devices in daily life, the problem of conveniently and promptly searching a piece of content that a user has viewed on his/her device before becomes more and more crucial. This paper aims to propose a context-based query processing framework called UCQP that supports unstructured queries for content search in a user's access history. Design/methodology/approach: Beyond the keywords related to the content properties, a context query in the framework is specified with freeform phrases that describe high-level mobile contexts of the user at a previous time when the user viewed the searched content. Findings: Experimental results on a prototype system of the framework illustrate its good accuracy and small response time. Originality/value: To tolerate the incompleteness and inaccuracy in user query texts caused by fading human memory, the authors develop several semantic query parsers that are tailored for different types of contexts using natural language processing and information retrieval techniques. The authors further propose a similarity model to rank the multiple result contents of a query by comparing context entities specified in the query and historical context values associated with each result. © Emerald Group Publishing Limited.","Mobile context; Mobile technology; Query languages; Semantic query parsing; Semantics; Unstructured context query",,2-s2.0-84893427770
"Gregor D., Toral S.L., Ariza T., Barrero F.","An ontology-based semantic service for cooperative urban equipments",2012,"Journal of Network and Computer Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867547268&doi=10.1016%2fj.jnca.2012.08.002&partnerID=40&md5=c6fba232cc7ab7519ddd6f1722baa695","The development of SOA (service oriented architecture) applications is a paradigm to consider for the integration of services which usually requires the incorporation of distributed artificial intelligence technologies or multi agent systems (MAS) to achieve their objectives. This is the case of transportation field, where the improvement of urban data networks and embedded systems allow the implementation of complex distributed services based on intelligent transportation systems. One of the challenges of this kind of systems is the discovery of services. Typically, discovery of services lacks of intelligence, or the result of this process returns a lot of nonsense information. However, the field of transportation requires quick and accurate requests and answers to deal with emergencies or incidents in the traffic flow. For this purpose, this paper proposes the development of a specific service called semantic service (an ontology-based semantic communication service) developed in TAO (The ACE ORB) of CORBA (common object request broker architecture). This service is able to provide a communication support for distributed environment in conjunction with a set of base libraries like Redland (RDF language bindings) for interacting with ontologies written in RDF and RDFS format. A parser Raptor (RDF Syntax Library) is used for analyzing sequences of symbols to determine the grammatical structure, and a syntax query language, Rasqal (RDF Query Library) is used to build and execute queries. Both, Raptor and Rasqal were designed to work with the Redland library. The main objective is to manage ontological information and interoperate with implemented services in embedded urban devices. Obtained results demonstrate the feasibility and effectiveness of the proposed approach. © 2012 Elsevier Ltd. All rights reserved.","CORBA; Embedded systems; Intelligent transportation systems; Ontologies; Semantic service","Communication support; CORBA (common object request broker architecture); Data network; Distributed Artificial Intelligence technology; Distributed environments; Distributed service; Execute query; Grammatical structure; Integration of services; Intelligent transportation systems; Language bindings; Multi agent system (MAS); Ontology-based; RDF query; Semantic communication; Semantic service; Traffic flow; Common object request broker architecture (CORBA); Communication; Embedded systems; Information services; Intelligent systems; Multi agent systems; Ontology; Query languages; Semantic Web; Semantics; Service oriented architecture (SOA); Syntactics; Urban transportation",2-s2.0-84867547268
"Gedik B., Andrade H.","A model-based framework for building extensible, high performance stream processing middleware and programming language for IBM InfoSphere Streams",2012,"Software - Practice and Experience",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867097696&doi=10.1002%2fspe.1139&partnerID=40&md5=198ff8d1d1fc04174c5e4381ab96688d","This work presents an extensive case study on the model-based design of a commercial-grade stream processing middleware (IBM's InfoSphere Streams) its runtime and language (SPL) compiler. The model-based underpinnings are pervasive throughout the whole environment, from describing inter-process communication interfaces and objects to the design of the extensibility mechanism in the runtime and language. In addition to many software engineering advantages such as consistent, uniform, and self-documented integration among the different parts of the system, we show intrinsic performance benefits to the platform derived from this design approach. First, we demonstrate how an incremental compilation strategy employed by the SPL compiler and rooted on the model description of the application, extracted by the compiler as part of the application building process, leads to better compile-time performance. Second, we discuss how the model-based code generation strategy employed by the SPL compiler also leads to increased runtime performance, by specializing the generated code to particular characteristics of the runtime environment. Finally, we show how the extensibility strategy used in the SPL language leads to automatic syntactic and semantic checks at compile time, while enabling behavioral reasoning and specific optimizations at runtime. Copyright © 2011 John Wiley & Sons, Ltd. Copyright © 2011 John Wiley & Sons, Ltd.","data stream processing; model-based code generation","Behavioral reasoning; Building process; Code Generation; Commercial grade; Compile time; Data stream processing; Design approaches; Incremental compilation; Infosphere; Interprocess communication; Intrinsic performance; Model description; Model-based design; Runtime environments; Runtime performance; Runtimes; Stream processing; Computer hardware description languages; Middleware; Network components; Program compilers; Semantics; Software engineering; Structural design",2-s2.0-84867097696
"Avancha S., Baxi A., Kotz D.","Privacy in mobile technology for personal healthcare",2012,"ACM Computing Surveys",71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860308263&doi=10.1145%2f2379776.2379779&partnerID=40&md5=dc155f0d6e4e98db13f200d266f6a75a","Information technology can improve the quality, efficiency, and cost of healthcare. In this survey, we examine the privacy requirements of mobile computing technologies that have the potential to transform healthcare. Such mHealth technology enables physicians to remotely monitor patients' health and enables individuals to manage their own health more easily. Despite these advantages, privacy is essential for any personal monitoring technology. Through an extensive survey of the literature, we develop a conceptual privacy framework for mHealth, itemize the privacy properties needed in mHealth systems, and discuss the technologies that could support privacy-sensitive mHealth systems. We end with a list of open research questions. © 2012 ACM.",,"mHealth; Mobile computing technology; Mobile Technology; Personal health care; Personal monitoring; Privacy requirements; Research questions; Health care; Information technology; Patient monitoring; Surveys; Medical computing",2-s2.0-84860308263
"Yanagisawa T., Miura T.","Context-based query using dependency structures based on latent topic model",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867861069&doi=10.1007%2f978-3-642-33609-6_12&partnerID=40&md5=b50e498d9a1cb9482197a367f75c13a5","To improve and enhance information retrieval techniques, there have been many approaches proposed so far, but few investigation which capture semantic aspects of queries directly. Here we propose a new approach to retrieve contextual dependencies in Japanese based on latent topics. We examine some experimental results to see the effectiveness. © 2012 Springer-Verlag.","Dependency Structure; Latent Dirichlet Allocation; Query","Context-based; Dependency structures; Latent Dirichlet allocation; Latent topic model; Query; Artificial intelligence; Statistics",2-s2.0-84867861069
"Angluin D., Aspnes J., Kontorovich A.","On the learnability of shuffle ideals",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867855580&doi=10.1007%2f978-3-642-34106-9_12&partnerID=40&md5=8cb79da2aa3490733dedc6577988bff6","Although PAC learning unrestricted regular languages is long known to be a very difficult problem, one might suppose the existence (and even an abundance) of natural efficiently learnable sub-families. When our literature search for a natural efficiently learnable regular family came up empty, we proposed the shuffle ideals as a prime candidate. A shuffle ideal generated by a string u is simply the collection of all strings containing u as a (discontiguous) subsequence. This fundamental language family is of theoretical interest in its own right and also provides the building blocks for other important language families. Somewhat surprisingly, we discovered that even a class as simple as the shuffle ideals is not properly PAC learnable, unless RP=NP. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution. © 2012 Springer-Verlag.",,"Building blockes; Learnability; Literature search; PAC learning; Statistical queries; Uniform distribution; Artificial intelligence; Algorithms",2-s2.0-84867855580
"Pradel C.","Allowing end users to query graph-based knowledge bases",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867673222&doi=10.1007%2f978-3-642-33876-2_2&partnerID=40&md5=4372210d53cbd31a5063682b6e9ba8f3","Our purpose is to provide end users a means to query knowledge bases using natural language queries and thus hide the complexity of formulating a query expressed in a graph query language such as SPARQL. The main originality of our approach lies in the use of query patterns. Our contribution is materialized in a system named SWIP, standing for Semantic Web Interface Using Patterns, which is situated in the Semantic Web framework. This paper presents the main issues addressed by our work and establishes the list of the important steps (to be) carried out in order to make SWIP a fully functional system. © 2012 Springer-Verlag.",,"End users; Functional systems; Graph query language; Graph-based; Knowledge basis; Natural language queries; Query patterns; Web interface; Interfaces (materials); Knowledge engineering; Query languages; Knowledge management",2-s2.0-84867673222
"Zhao J., Chevalier F., Collins C., Balakrishnan R.","Facilitating discourse analysis with interactive visualization",2012,"IEEE Transactions on Visualization and Computer Graphics",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867644146&doi=10.1109%2fTVCG.2012.226&partnerID=40&md5=3a0c32218be83dcab9883feb00ba571f","A discourse parser is a natural language processing system which can represent the organization of a document based on a rhetorical structure tree-one of the key data structures enabling applications such as text summarization, question answering and dialogue generation. Computational linguistics researchers currently rely on manually exploring and comparing the discourse structures to get intuitions for improving parsing algorithms. In this paper, we present DAViewer, an interactive visualization system for assisting computational linguistics researchers to explore, compare, evaluate and annotate the results of discourse parsers. An iterative user-centered design process with domain experts was conducted in the development of DAViewer. We report the results of an informal formative study of the system to better understand how the proposed visualization and interaction techniques are used in the real research environment. © 1995-2012 IEEE.","computational linguisitics; Discourse structure; interaction techniques; tree comparison; visual analytics","computational linguisitics; Discourse structure; Interaction techniques; Tree comparison; Visual analytics; Computational linguistics; Data structures; Forestry; Natural language processing systems; Research; Text processing; Visualization; Data; Forestry; Languages; Processing; Research; Structures",2-s2.0-84867644146
"Gopinath D., Zaeem R.N., Khurshid S.","Improving the effectiveness of spectra-based fault localization using specifications",2012,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866916142&doi=10.1145%2f2351676.2351683&partnerID=40&md5=5f83cb0b167a05304c7def29dc5a894b","Fault localization i.e., locating faulty lines of code, is a key step in removing bugs and often requires substantial manual effort. Recent years have seen many automated localization techniques, specifically using the program's passing and failing test runs, i.e., test spectra. However, the effectiveness of these approaches is sensitive to factors such as the type and number of faults, and the quality of the test-suite. This paper presents a novel technique that applies spectra-based localization in synergy with specification-based analysis to more accurately locate faults. Our insight is that unsatisfiability analysis of violated specifications, enabled by SAT technology, could be used to (1) compute unsatisfiable cores that contain likely faulty statements and (2) generate tests that help spectrabased localization. Our technique is iterative and driven by a feedback loop that enables more precise fault localization. Our framework SAT-TAR embodies our technique for Java programs, including those with multiple faults. An experimental evaluation using a suite of widely-studied data structure programs, including the ANTLR and JTopas parser applications, shows that our technique localizes faults more accurately than state-of-the-art approaches. Copyright 2012 ACM.","Alloy; Automated debugging; Fault localization; KODKOD; Minimal unsat cores; Tarantula","Automated debugging; Fault localization; KODKOD; Minimal unsat cores; Tarantula; Kodkod; Minimal UNSAT cores; Alloys; Automation; Cerium alloys; Data structures; Iterative methods; Java programming language; Software engineering; Software testing; Specifications; Application programs; Tracking (position)",2-s2.0-84866916142
"Zou G., Peter-Paul R., Boley H., Riazanov A.","PSOA2TPTP: A reference translator for interoperating PSOA RuleML with TPTP reasoners",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866714152&doi=10.1007%2f978-3-642-32689-9_22&partnerID=40&md5=28878a25d68c4ee243100cc02451cc65","PSOA RuleML is a recently specified rule language combining relational and object-oriented modeling. In order to provide reasoning services for PSOA RuleML, we have implemented a reference translator, PSOA2TPTP, to map knowledge bases and queries in the PSOA RuleML presentation syntax (PSOA/PS) to the popular TPTP format, supported by many first-order logic reasoners. In particular, PSOA RuleML reasoning has become available using the open-source VampirePrime reasoner, enabling query answering and entailment as well as consistency checking. The translator, currently composed of a lexer, a parser, and tree walkers, is generated by the ANTLR v3 parser generator tool from the grammars we developed. We discuss how to rewrite the original PSOA/PS grammar into an LL(1) grammar, thus demonstrating that PSOA/PS can be parsed efficiently. We also present a semantics-preserving mapping from PSOA RuleML to TPTP through a normalization and a translation phase. We wrap the translation and querying code into RESTful Web services for convenient remote access and provide a demo Web site. © 2012 Springer-Verlag.",,"Consistency checking; First order logic; Knowledge basis; Objectoriented modeling; Open-source; Parser generators; Presentation syntax; Query answering; Reasoner; Remote access; RESTful Web services; Semantics-preserving mapping; Artificial intelligence; Formal languages; Semantics; Web services; Websites; Computer programming languages",2-s2.0-84866714152
"Xu Q., Guo J., Xiao B.","The study of content security for Mobile Internet",2012,"Wireless Personal Communications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867337349&doi=10.1007%2fs11277-012-0738-8&partnerID=40&md5=3568e16fbb87790e17b1abdf23755cbe","The vast amount of information carried overMobile Internet and the high speed are providing unprecedented convenience for users, Mobile Internet is facing growing threat of lack of security. It is crucial to maintain and improve safety and security for Mobile Internet for it to thrive and develop. At content level, users are facing increasing amount of malicious or spam content, jeopardizing public's interest in legitimate internet content. Therefore, Mobile Internet information security has become an important research topic. In this paper we first propose a framework for content security management system for Mobile Internet, and then discuss how to acquire relevant information from Mobile Internet in a fast and efficient manner, how to process and analyze the vast amount of information collected, howto quickly discover negative or illegal information within the network, and provide detection and early warnings for potential hot topics. At the same time, we study how to perform audit and evaluation on the information content so that the relevant security management actions can be done. © Springer Science+Business Media, LLC. 2012.","Content auditing; Content security; Detection and filtration; Mobile Internet","Amount of information; Content auditing; Content level; Content security; Early warning; Information contents; Internet content; Mobile Internet; Research topics; Security management; Copyrights; Facings; Industrial management; Internet; Mobile devices; Security of data; Information management",2-s2.0-84867337349
"Severyn A., Moschitti A.","Structural relationships for large-scale learning of answer re-ranking",2012,"SIGIR'12 - Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866604545&doi=10.1145%2f2348283.2348383&partnerID=40&md5=20637cc61a2dda06e3fcf3bd8f53f3fb","Supervised learning applied to answer re-ranking can highly improve on the overall accuracy of question answering (QA) systems. The key aspect is that the relationships and properties of the question/answer pair composed of a question and the supporting passage of an answer candidate, can be efficiently compared with those captured by the learnt model. In this paper, we define novel supervised approaches that exploit structural relationships between a question and their candidate answer passages to learn a re-ranking model. We model structural representations of both questions and answers and their mutual relationships by just using an off-the-shelf shallow syntactic parser. We encode structures in Support Vector Machines (SVMs) by means of sequence and tree kernels, which can implicitly represent question and answer pairs in huge feature spaces. Such models together with the latest approach to fast kernel-based learning enabled the training of our rerankers on hundreds of thousands of instances, which previously rendered intractable for kernelized SVMs. The results on two different QA datasets, e.g., Answerbag and Jeopardy! data, show that our models deliver large improvement on passage re-ranking tasks, reducing the error in Recall of BM25 baseline by about 18%. One of the key findings of this work is that, despite its simplicity, shallow syntactic trees allow for learning complex relational structures, which exhibits a steep learning curve with the increase in the training size. © 2012 ACM.","kernel methods; large-scale learning; question answering; structural kernels; support vector machines","Data sets; Feature space; Kernel methods; Kernel-based learning; large-scale learning; Question Answering; Question answering systems; Re-ranking; Relational structures; Steep learning curve; structural kernels; Structural relationship; Structural representation; Syntactic parsers; Syntactic trees; Training size; Tree kernels; Forestry; Information retrieval; Support vector machines; Algorithms; Forestry; Information Retrieval",2-s2.0-84866604545
"Lioma C., Larsen B., Lu W.","Rhetorical relations for information retrieval",2012,"SIGIR'12 - Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866598102&doi=10.1145%2f2348283.2348407&partnerID=40&md5=f0fce8ec6c4ada9011123a29771f61d5","Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to the overall semantics of the text. Rhetorical relations, e.g. contrast, cause, explanation, describe how the parts of a text are linked to each other. Knowledge about this so-called discourse structure has been applied successfully to several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval (IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a document's rhetorical relations be useful to IR? We present a language model modification that considers rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of different versions of our model on TREC settings shows that certain rhetorical relations can benefit retrieval effectiveness notably (>10% in mean average precision over a state-of-the-art baseline). © 2012 ACM.","discourse structure; probabilistic retrieval model; rhetorical relations","Discourse structure; Empirical evaluations; Language model; NAtural language processing; Probabilistic retrieval; Retrieval effectiveness; Retrieval performance; Rhetorical relations; Work study; Computational linguistics; Natural language processing systems; Semantics; Information retrieval",2-s2.0-84866598102
"Yildiz T., Yildirim S.","Association rule based acquisition of hyponym and hypernym relation from a Turkish corpus",2012,"INISTA 2012 - International Symposium on INnovations in Intelligent SysTems and Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866599220&doi=10.1109%2fINISTA.2012.6246944&partnerID=40&md5=c415c6816ff87d837ea3739b483b99c9","In this paper, we propose a method for the automatic acquisition of hypernym/hyponymy relations from a Turkish raw text. Once the model has extracted prospective hyponyms by using lexico-syntactic patterns, an Apriori algorithm is applied to eliminate faulty hyponyms and increase precision. We show that a model based on a particular lexico-syntactic pattern and association rules for Turkish language can successfully retrieve many is-a relation with high precision. © 2012 IEEE.","Apriori algorithm; hypernym/hyponym; lexicon","Apriori algorithms; Automatic acquisition; High precision; hypernym/hyponym; Hyponyms; Lexico-syntactic patterns; lexicon; Model-based OPC; Turkish language; Turkishs; Intelligent systems; Learning algorithms; Syntactics; Association rules",2-s2.0-84866599220
"Chen C.-M., Chu F.-S., Chen P.-S.","Compiler support for effective XSL transformation",2012,"Concurrency Computation Practice and Experience",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866049509&doi=10.1002%2fcpe.1901&partnerID=40&md5=de8aeb2b7d23ecbf99e1c2f88b3e722d","XML is a markup language used to describe data or documents. The main goal of XML is to facilitate the sharing of data across diverse information systems, especially via the Internet. XML Stylesheet Transformations (XSLT) is a standard approach to describing how to transform an XML document into another data format. The ever-increasing number of Web technologies being used in our everyday lives commonly employs XSLT to support data exchange among heterogeneous environments, and the associated increasing burdens on XSLT processors have increased the demand for high-performance XSLT processors. In this paper, we present an XSLT compiler, named Zebu, which can transform an XSLT stylesheet into the corresponding C program. The compiled program can be used to transform documents without the processing of XSLT stylesheets. The results of experimental testing using standard benchmarks show that the proposed XSLT compiler performs well in processing XML transformations. Copyright © 2011 John Wiley & Sons, Ltd.","Compiler; XML; XSLT","C programs; Compiled programs; Compiler; Data format; Experimental testing; Heterogeneous environments; Style sheets; Web technologies; XML transformation; XSLT; Electronic data interchange; Hypertext systems; XML; Program compilers",2-s2.0-84866049509
"Tzoukermann E., Klavans J.L., Strzalkowski T.","Information Retrieval",2012,"The Oxford Handbook of Computational Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924030373&doi=10.1093%2foxfordhb%2f9780199276349.013.0029&partnerID=40&md5=624d6e397b29eb955a0ea82624b97e10","Information retrieval (IR) involves retrieving information from stored data, through user queries or pre-formulated user profiles. The information can be in any format. IR typically advances over four broad stages viz., identification of text types, document preprocessing, document indexing, and query processing and matching the same to documents. Although NLP has a role to play in IR, the procedural complexities of the latter impede determination of the stage of incorporation of the former into the latter. Earliest attempts at connecting NLP with IR, were extremely ambitious, proposing concepts instead of terms, as complex structures, to be compared using sophisticated algorithms. In its current state, IR still comes in handy, to retrieve information from various thesauri and ontologies, both in general-purpose lexical databases, as well as those categorizing knowledge in particular scientific and trade domains. However, NLP has yet to prove a better compatibility with IR, in enhancing the latter. © Editorial matter and organization Ruslan Mitkov 2003. All rights reserved.","Concept; Database; Information; Knowledge; Processing; Text; User",,2-s2.0-84924030373
"Mooney R.J.","Machine Learning",2012,"The Oxford Handbook of Computational Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924045685&doi=10.1093%2foxfordhb%2f9780199276349.013.0020&partnerID=40&md5=53161ccd5a6f4775a7fceec2ce1f65ca","This article introduces the type of symbolic machine learning in which decision trees, rules, or case-based classifiers are induced from supervised training examples. It describes the representation of knowledge assumed by each of these approaches and reviews basic algorithms for inducing such representations from annotated training examples and using the acquired knowledge to classify future instances. Machine learning is the study of computational systems that improve performance on some task with experience. Most machine learning methods concern the task of categorizing examples described by a set of features. These techniques can be applied to learn knowledge required for a variety of problems in computational linguistics ranging from part-of-speech tagging and syntactic parsing to word-sense disambiguation and anaphora resolution. Finally, this article reviews the applications to a variety of these problems, such as morphology, part-of-speech tagging, word-sense disambiguation, syntactic parsing, semantic parsing, information extraction, and anaphora resolution. © Editorial matter and organization Ruslan Mitkov 2003. All rights reserved.","Anaphora resolution; Information extraction; Part-of-speech tagging; Semantic parsing; Supervised training examples; Symbolic machine learning; Syntactic parsing",,2-s2.0-84924045685
"Hirschman L., Mani I.","Evaluation",2012,"The Oxford Handbook of Computational Linguistics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924037271&doi=10.1093%2foxfordhb%2f9780199276349.013.0022&partnerID=40&md5=ee5b9f66e8ffcd4d65c6d02a66177b93","The commercial success of natural language (NL) technology has raised the technical criticality of evaluation. Choices of evaluation methods depend on software life cycles, typically charting four stages - research, advance prototype, operational prototype, and product. At the prototype stage, embedded evaluation can prove helpful. Analysis components can be loose grouped viz., segmentation, tagging, extracting information, and document threading. Output technologies such as text summarization can be evaluated in terms of intrinsic and extrinsic measures, the former checking for quality and informativeness and the latter, for efficiency and acceptability, in some tasks. 'Post edit measures' commonly used in machine translation, determine the amount of correction required to obtain a desirable output. Evaluation of interactive systems typically evaluates the system and the user as one team and deploys subject variability, which runs enough subjects to obtain statistical validity hence, incurring substantial costs. Evaluation being a social activity, creates a community for internal technical comparison, via shared evaluation criteria. © Editorial matter and organization Ruslan Mitkov 2003. All rights reserved.","Analysis; Interactive; Measure; Output; Prototype; Software life cycles",,2-s2.0-84924037271
"Kaplan R.B.","The Oxford Handbook of Applied Linguistics, (2 Ed.)",2012,"The Oxford Handbook of Applied Linguistics, (2 Ed.)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923306324&doi=10.1093%2foxfordhb%2f9780195384253.001.0001&partnerID=40&md5=091fa826b3f442ce3bc5eb23bb2e439c","The Oxford Handbook of Applied Linguistics contains thirty-nine articles on a broad range of topics in applied linguistics written by a diverse group of contributors. Its goal is to provide a comprehensive survey of the field, the many connections among its various sub-disciplines, and the likely directions of its future development. The book addresses a broad audience: applied linguists; educators and other scholars working in language acquisition, language learning, language planning, teaching, and testing; and linguists concerned with the applications of their work. The volume systematically encompasses the major areas of applied linguistics and draws from a wide range of disciplines such as education, language policy, bi- and multi-lingualism, literacy, language and gender, psycholinguistics/cognition, language and computers, discourse analysis, language and concordinances, ecology of language, pragmatics, translation, psycholinguistics and cognition, and many other fields. This second edition includes five new articles, and the remaining articles have been revised and updated to give a clear picture of the state of applied linguistics. © 2010 by Oxford University Press, Inc. All rights reserved.","Bi-lingualism; Cognition; Dis; Language acquisition; Language and computers; Language and gender; Language education; Language learning; Language planning; Language policy; Language teaching; Language testing; Literacy; Multi-lingualism; Psycholinguistics",,2-s2.0-84923306324
"Saravanakumar K., Deepa K.","Hybrid approach for construction of summaries and clusters of blog data for improved blog search results",2012,"Communications in Computer and Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865965112&doi=10.1007%2f978-3-642-29216-3_75&partnerID=40&md5=1cd391f3936b9f9e07302f66914fdec5","The data are noisy in blogs, because blog entries are unstructured and might cover a wide variety of topics. However, because of the number of blogs exist, manually viewing and examining them is a difficult and time-consuming task. Intuitively, you could apply existing text and Web mining techniques to blog analysis and mining. But, because of the existence of various challenges, we can't directly apply these techniques. The bloggers update their information content on the blogs much more frequently than Web masters update traditional Web pages, often daily or even hourly. Above all, bloggers cover very diverse topics, so maybe only one paragraph in a particular entry could relate to someone's topic of interest. In this paper we propose an architecture which takes a query from the user, process through blog parser and extract content from the blog page. Then we identify the sentences which should be taken for further processing using a blog analyzer and finally summarizing the content based on the analysis results. The process is repeated for all the blogs and results in the summarized output of clustered blogs. © 2012 Springer-Verlag.","Blog mining; Information Retrieval; text summarization","Blog mining; Blog search; Bloggers; Content-based; Hybrid approach; Information contents; Text summarization; Time-consuming tasks; Web Mining; Information retrieval; Information systems; Websites; Blogs",2-s2.0-84865965112
"Yoshinaka R., Clark A.","Polynomial time learning of some multiple context-free languages with a minimally adequate teacher",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865477742&doi=10.1007%2f978-3-642-32024-8_13&partnerID=40&md5=313f3496162aa607bfc86bb065440133","We present an algorithm for the inference of some Multiple Context-Free Grammars from Membership and Equivalence Queries, using the Minimally Adequate Teacher model of Angluin. This is an extension of the congruence based methods for learning some Context-Free Grammars proposed by Clark (ICGI 2010). We define the natural extension of the syntactic congruence to tuples of strings, and demonstrate we can efficiently learn the class of Multiple Context-Free Grammars where the non-terminals correspond to the congruence classes under this relation. © 2012 Springer-Verlag.",,"Congruence class; Equivalence queries; Multiple context-free grammar; Natural extension; Polynomial-time; Syntactic congruence; Context free grammars; Inference engines; Polynomial approximation; Automata theory",2-s2.0-84865477742
"Baric S., Storti A., Hofer M., Dalla Via J.","Resolving the Parentage of the Apple Cultivar 'Meran' [Klärung der Elternschaft der Apfelsorte 'Meran']",2012,"Erwerbs-Obstbau",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868534193&doi=10.1007%2fs10341-012-0167-6&partnerID=40&md5=2d78da1368df1f2c9a5e42369133e070","In the mid-1970s, a new apple variety named 'Meran' was discovered in South Tyrol (northern Italy), which harbours the largest continuous apple growing area in Europe. The cultivar was registered for varietal protection and patented in several countries, and was declared to be a cross of the varieties 'Golden Delicious' and 'Morgenduft' (synonym 'Rome Beauty'). The parentage of 'Meran' has, however, been questioned, and the present study aimed to assess the descent of this cultivar by the combined use of molecular genetic and bioinformatic tools. Five accessions of 'Meran' were collected from three different European germplasm collections and analysed at 14 variable microsatellite DNA loci. Subsequently, computer software was used to allocate the most likely parent pair from a set of cultivars representative for the apple growing area of South Tyrol in 1975. The molecular genetic data clearly excluded 'Morgenduft' as a gene donor to 'Meran' and provided strong evidence that 'Meran' is a cross of the cultivars 'Golden Delicious' and 'Jonathan', confirming previous assumptions based on morphological traits of the tree and fruit. © 2012 The Author(s).","Apple cultivar; Breeding; Malus × domestica; Meran; Microsatellite DNA; Parentage allocation","Malus; Malus x domestica",2-s2.0-84868534193
"Severyn A., Moschitti A.","Fast support vector machines for convolution tree kernels",2012,"Data Mining and Knowledge Discovery",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864558154&doi=10.1007%2fs10618-012-0276-8&partnerID=40&md5=888c86eddc3b76cb7180e5b3c9c348f9","Feature engineering is one of the most complex aspects of system design in machine learning. Fortunately, kernel methods provide the designer with formidable tools to tackle such complexity. Among others, tree kernels (TKs) have been successfully applied for representing structured data in diverse domains, ranging from bioinformatics and data mining to natural language processing. One drawback of such methods is that learning with them typically requires a large number of kernel computations (quadratic in the number of training examples) between training examples. However, in practice substructures often repeat in the data which makes it possible to avoid a large number of redundant kernel evaluations. In this paper, we propose the use of Directed Acyclic Graphs (DAGs) to compactly represent trees in the training algorithm of Support Vector Machines. In particular, we use DAGs for each iteration of the cutting plane algorithm (CPA) to encode the model composed by a set of trees. This enablesDAGkernels to efficiently evaluate TKs between the current model and a given training tree. Consequently, the amount of total computation is reduced by avoiding redundant evaluations over shared substructures.We provide theory and algorithms to formally characterize the above idea, which we tested on several datasets. The empirical results confirm the benefits of the approach in terms of significant speedups over previous state-of-the-art methods. In addition, we propose an alternative sampling strategy within the CPA to address the class-imbalance problem, which coupled with fast learning methods provides a viable TK learning framework for a large class of real-world applications. © The Author(s) 2012.","Kernel methods; Large scale learning; Natural language processing; Tree kernels","Current models; Cutting plane algorithms; Data sets; Directed acyclic graphs; Diverse domains; Fast learning; Kernel methods; Large scale learning; Learning frameworks; NAtural language processing; Real-world application; Sampling strategies; State-of-the-art methods; Structured data; Training algorithms; Training example; Tree kernels; Bioinformatics; Computational linguistics; Forestry; Learning algorithms; Natural language processing systems; Support vector machines; Trees (mathematics); Algorithms; Computation; Data Processing; Forestry; Systems Engineering",2-s2.0-84864558154
"Xu Y., Tsujii J., Chang E.I.-C.","Named entity recognition of follow-up and time information in 20 000 radiology reports",2012,"Journal of the American Medical Informatics Association",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872259831&doi=10.1136%2famiajnl-2012-000812&partnerID=40&md5=3623fb39ec84edeb3162bf9726b24c29","Objective To develop a system to extract follow-up information from radiology reports. The method may be used as a component in a system which automatically generates follow-up information in a timely fashion. Methods A novel method of combining an LSP (labeled sequential pattern) classifier with a CRF (conditional random field) recognizer was devised. The LSP classifier filters out irrelevant sentences, while the CRF recognizer extracts follow-up and time phrases from candidate sentences presented by the LSP classifier. Measurements The standard performance metrics of precision (P), recall (R), and F measure (F) in the exact and inexact matching settings were used for evaluation. Results Four experiments conducted using 20 000 radiology reports showed that the CRF recognizer achieved high performance without time-consuming feature engineering and that the LSP classifier further improved the performance of the CRF recognizer. The performance of the current system is P=0.90, R=0.86, F=0.88 in the exact matching setting and P=0.98, R=0.93, F=0.95 in the inexact matching setting. Conclusion The experiments demonstrate that the system performs far better than a baseline rule-based system and is worth considering for deployment trials in an alert generation system. The LSP classifier successfully compensated for the inherent weakness of CRF, that is, its inability to use global information.",,"article; effect size; follow up; human; machine learning; medical information; medical record; radiology; artificial intelligence; classification; data mining; electronic medical record; feasibility study; hospital information system; methodology; natural language processing; Artificial Intelligence; Data Mining; Electronic Health Records; Feasibility Studies; Humans; Natural Language Processing; Radiology Information Systems",2-s2.0-84872259831
"Maybury M.T.","Multimedia Information Extraction: Advances in Video, Audio, and Imagery Analysis for Search, Data Mining, Surveillance, and Authoring",2012,"Multimedia Information Extraction: Advances in Video, Audio, and Imagery Analysis for Search, Data Mining, Surveillance, and Authoring",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891569116&doi=10.1002%2f9781118219546&partnerID=40&md5=eda246dfb383516c1cf56fd9dc46a45f","The advent of increasingly large consumer collections of audio (e.g., iTunes), imagery (e.g., Flickr), and video (e.g., YouTube) is driving a need not only for multimedia retrieval but also information extraction from and across media. Furthermore, industrial and government collections fuel requirements for stock media access, media preservation, broadcast news retrieval, identity management, and video surveillance. While significant advances have been made in language processing for information extraction from unstructured multilingual text and extraction of objects from imagery and video, these advances have been explored in largely independent research communities who have addressed extracting information from single media (e.g., text, imagery, audio). And yet users need to search for concepts across individual media, author multimedia artifacts, and perform multimedia analysis in many domains. This collection is intended to serve several purposes, including reporting the current state of the art, stimulating novel research, and encouraging cross-fertilization of distinct research disciplines. The collection and integration of a common base of intellectual material will provide an invaluable service from which to teach a future generation of cross disciplinary media scientists and engineers. © 2012 IEEE Computer Society.",,,2-s2.0-84891569116
"Verspoor K., Cohen K.B., Lanfranchi A., Warner C., Johnson H.L., Roeder C., Choi J.D., Funk C., Malenkiy Y., Eckert M., Xue N., Baumgartner Jr W.A., Bada M., Palmer M., Hunter L.E.","A corpus of full-text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools",2012,"BMC Bioinformatics",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865060319&doi=10.1186%2f1471-2105-13-207&partnerID=40&md5=0feb1d026c96a938d6ade88ab1114393","Background: We introduce the linguistic annotation of a corpus of 97 full-text biomedical publications, known as the Colorado Richly Annotated Full Text (CRAFT) corpus. We further assess the performance of existing tools for performing sentence splitting, tokenization, syntactic parsing, and named entity recognition on this corpus.Results: Many biomedical natural language processing systems demonstrated large differences between their previously published results and their performance on the CRAFT corpus when tested with the publicly available models or rule sets. Trainable systems differed widely with respect to their ability to build high-performing models based on this data.Conclusions: The finding that some systems were able to train high-performing models based on this corpus is additional evidence, beyond high inter-annotator agreement, that the quality of the CRAFT corpus is high. The overall poor performance of various systems indicates that considerable work needs to be done to enable natural language processing systems to work well when the input is full-text journal articles. The CRAFT corpus provides a valuable resource to the biomedical natural language processing community for evaluation and training of new models for biomedical full text publications. © 2012 Verspoor et al.; licensee BioMed Central Ltd.",,"Evaluation tool; Journal articles; Linguistic annotations; Named entity recognition; NAtural language processing; Natural Language Processing Tools; Poor performance; Rule set; Syntactic parsing; Tokenization; Trainable system; Data mining; Natural language processing systems; Publishing; Linguistics; article; computer program; data mining; methodology; natural language processing; Data Mining; Natural Language Processing; Software",2-s2.0-84865060319
"Yang Z., Geng X.","Toward information integration efficient XML data code scheme",2012,"International Review on Computers and Software",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864334633&partnerID=40&md5=fc938a83bd0c311abd70d15800dbe23a","In this paper, we propose an efficient XML data code scheme to improve the processing efficiency of information integration. We introduce an identification mechanism called bit pattern which indicates the information of decode. So we can directly obtain the bits of node's layer ID through it rather than recursive way of DLN. We design the improved code scheme based on it. This code scheme has the characters of dictionary order, certainty and compression, which can meet the requirements of efficient XML code scheme. Code generation algorithms of node for static parser and dynamic update are also proposed. Finally we analyze and verify the compression performance of this code scheme by experiment. The improved code scheme we proposed has more effective compression while maintains the original advantages of the DLN. So it can adapt to the demand of information integration. © 2012 Praise Worthy Prize S.r.l. -All rights reserved. -All rights reserved.","Bit pattern; Code algorithm; Compression code; XML data","Bit patterns; Code generation algorithm; Code scheme; Compression codes; Compression performance; Dynamic update; Identification mechanism; Information integration; XML data; Information retrieval; XML",2-s2.0-84864334633
"Grebenshchikov S., Lopes N.P., Popeea C., Rybalchenko A.","Synthesizing software verifiers from proof rules",2012,"ACM SIGPLAN Notices",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866388419&doi=10.1145%2f2345156.2254112&partnerID=40&md5=6e0de88664a0dee6510fb484f6ac7c06","Automatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multi-Threaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing state-of-The-art verification tools confirms the practicality of the approach. Copyright © 2012 ACM.","Proof rules; Software model checking; Software verification; Verification tool synthesis","Automatic synthesis; Automatically generated; Dataflow; Experimental comparison; Functional programs; Inductive loops; Iterative process; Liveness; Multi-threaded programs; Programmer productivity; Proof rules; Recursive equations; Software model checking; Software verification; Software verification tools; Synthesis procedure; Verification tools; Computer programming; Computer science; Model checking",2-s2.0-84866388419
"Gonzalez Granadillo G., Ben Mustapha Y., Hachem N., Debar H.","An ontology-driven approach to model SIEM information and operations using the SWRL formalism",2012,"International Journal of Electronic Security and Digital Forensics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864671603&doi=10.1504%2fIJESDF.2012.048412&partnerID=40&md5=3b13affda0564d839baeeea4815cb08f","The management of security events, from the risk analysis to the selection of appropriate countermeasures, has become a major concern for security analysts and IT administrators. Furthermore, the fact that network and system devices are heterogeneous, increases the difficulty of these administrative tasks. This paper introduces an ontology-driven approach to address the aforementioned problems. The proposed model takes into account two aspects: the information and the operations that are manipulated by SIEM environments in order to reach the desired goals. The model uses ontologies to provide simplicity on the description of concepts, relationships and instances of the security domain. The semantics web rule languages are used to describe the logic rules needed to infer relationships among individuals and classes. A case study on Botnets is presented at the end of this paper to illustrate a concrete utilisation of our model. Copyright © 2012 Inderscience Enterprises Ltd.","Data model; Event management; Heterogeneity; Logic rules; Ontology; Reasoning; Security events; Security information and event management; Semantics; SIEM; SWRL","Event management; Heterogeneity; Logic rules; Reasoning; Security events; SIEM; SWRL; Data structures; Ontology; Semantics; Semantic Web",2-s2.0-84864671603
"Chesher C.","Navigating sociotechnical spaces: Comparing computer games and sat navs as digital spatial media",2012,"Convergence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864883023&doi=10.1177%2f1354856512442762&partnerID=40&md5=4df6810a2f0938bf4091657f2038370c","Digital media increasingly mediate everyday spatial and navigational practices. From in-car satellite navigation (sat navs) to computer games, overpowered gadgets are combining multiple sources of abstract information to give users spatial guidance and experiences of movement. For example, open world computer games such as Grand Theft Auto IV render rich fictional spaces, and include intricate maps and indicators that allow players to navigate large gamespaces. Sat navs such as the TomTom Navigator follow similar practices of automated navigation in helping to guide cars through actual spaces. Their calculated routes display on personalized maps, including live data and visualizations that complement, or even override, what the driver sees through the windscreen. Games and sat navs are harbingers of historical shifts in technosocial space, suggesting that Henri Lefebvre's (1991) influential critical analysis of space deserves to be revised. Digital spatial media open up abstract relationships to space, but not from the distance that Lefebvre associates with 'conceived' spaces. Instead, they work in 'lived space', which is becoming dominant. They calculate space in real time, and open up new political and aesthetic questions. The article examines three characteristics of navigation with digital spatial media: (1) they reify routes as persuasive data and procedures; (2) their maps become subjective and privatized; and (3) they offer an array of spatial information that become incorporated into the user's 'perceived' space. These examples show that critical understandings of social space need increasingly to incorporate readings of digitally mediated spatiality. © 2012 The Author(s).","Computer games; digital media; global positioning systems; Grand Theft Auto; Lefebvre; space; spatiality",,2-s2.0-84864883023
"Feng J., Wang J., Li G.","Trie-join: A trie-based method for efficient string similarity joins",2012,"VLDB Journal",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864286297&doi=10.1007%2fs00778-011-0252-8&partnerID=40&md5=f901f88aca9ff8906a4e9a3c3dcc8484","A string similarity join finds similar pairs between two collections of strings. Many applications, e. g., data integration and cleaning, can significantly benefit from an efficient string-similarity-join algorithm. In this paper, we study string similarity joins with edit-distance constraints. Existing methods usually employ a filter-and-refine framework and suffer from the following limitations: (1) They are inefficient for the data sets with short strings (the average string length is not larger than 30); (2) They involve large indexes; (3) They are expensive to support dynamic update of data sets. To address these problems, we propose a novel method called trie-join, which can generate results efficiently with small indexes. We use a trie structure to index the strings and utilize the trie structure to efficiently find similar string pairs based on subtrie pruning. We devise efficient trie-join algorithms and pruning techniques to achieve high performance. Our method can be easily extended to support dynamic update of data sets efficiently. We conducted extensive experiments on four real data sets. Experimental results show that our algorithms outperform state-of-the-art methods by an order of magnitude on the data sets with short strings. © 2011 Springer-Verlag.","Data integration and cleaning; Edit distance; String similarity joins; Subtrie pruning; Trie index","Data integration; Edit distance; String similarity; Subtrie pruning; Trie index; Hardware; Information systems; Algorithms",2-s2.0-84864286297
"Joannou S., Raman R.","Dynamizing succinct tree representations",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864326366&doi=10.1007%2f978-3-642-30850-5_20&partnerID=40&md5=529d9fed467d4bb259917fc66f6ec392","We consider succinct, or space-efficient, representations of ordinal trees. Representations exist that take 2n + o(n) bits to represent a static n-node ordinal tree - close to the information-theoretic minimum - and support navigational operations in O(1) time on a RAM model; and some implementations have good practical performance. The situation is different for dynamic ordinal trees. Although there is theoretical work on succinct dynamic ordinal trees, there is little work on the practical performance of these data structures. Motivated by applications to representing XML documents, in this paper, we report on a preliminary study on dynamic succinct data structures. Our implementation is based on representing the tree structure as a sequence of balanced parentheses, with navigation done using the min-max tree of Sadakane and Navarro (SODA '10). Our implementation shows promising performance for update and navigation, and our findings highlight two issues that we believe will be important to future implementations: the difference between the finger model of (say) Farzan and Munro (ICALP '09) and the parenthesis model of Sadakane and Navarro, and the choice of the balanced tree used to represent the min-max tree. © 2012 Springer-Verlag.",,"Balanced trees; Min-max; ON dynamics; RAM model; Succinct data structure; Tree representation; Tree structures; Algorithms; Data structures; Information theory; Trees (mathematics); Forestry; Algorithms; Data; Forestry; Information Retrieval; Mathematics; Structures; Trees",2-s2.0-84864326366
"Yang Y., Zhan J., Zhao H., Zhou Y.","A new size-independent score for pairwise protein structure alignment and its application to structure classification and nucleic-acid binding prediction",2012,"Proteins: Structure, Function and Bioinformatics",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863784145&doi=10.1002%2fprot.24100&partnerID=40&md5=58ca87d35759a9bab89ef2ff32c1039a","A structure alignment program aligns two structures by optimizing a scoring function that measures structural similarity. It is highly desirable that such scoring function is independent of the sizes of proteins in comparison so that the significance of alignment across different sizes of the protein regions aligned is comparable. Here, we developed a new score called SP-score that fixes the cutoff distance at 4 Å and removed the size dependence using a normalization prefactor. We further built a program called SPalign that optimizes SP-score for structure alignment. SPalign was applied to recognize proteins within the same structure fold and having the same function of DNA or RNA binding. For fold discrimination, SPalign improves sensitivity over TMalign for the chain-level comparison by 12% and over DALI for the domain-level comparison by 13% at the same specificity of 99.6%. The difference between TMalign and SPalign at the chain level is due to the inability of TMalign to detect single domain similarity between multidomain proteins. For recognizing nucleic acid binding proteins, SPalign consistently improves over TMalign by 12% and DALI by 31% in average value of Mathews correlation coefficients for four datasets. SPalign with default setting is 14% faster than TMalign. SPalign is expected to be useful for function prediction and comparing structures with or without domains defined. The source code for SPalign and the server are available at http://sparks.informatics.iupui.edu. © 2012 Wiley Periodicals, Inc.","DNA binding proteins; Function prediction; RNA binding proteins; Structure alignment; Structure classification","DNA binding protein; nucleic acid binding protein; RNA binding protein; article; particle size; priority journal; protein binding; protein DNA binding; protein domain; protein folding; protein function; protein RNA binding; protein structure; protein structure alignment; scoring system; sequence alignment; SP score; Computational Biology; DNA-Binding Proteins; Protein Conformation; Protein Folding; Protein Structure, Tertiary; Proteins; RNA-Binding Proteins; Software; Structural Homology, Protein; Structure-Activity Relationship",2-s2.0-84863784145
"Wong W., Liu W., Bennamoun M.","Ontology learning from text: A look back and into the future",2012,"ACM Computing Surveys",134,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866505340&doi=10.1145%2f2333112.2333115&partnerID=40&md5=f144c9a5cb6ecff67e2fc6dad80ad099","Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future © 2012 ACM.","Application of ontologies; Concept discovery; Ontology learning; Semantic relation acquisition; Term recognition","Automatic ontology; Concept discovery; NAtural language processing; Ontology learning; Research directions; Semantic relations; Term Recognition; Textual information; Computational linguistics; Natural language processing systems; Research",2-s2.0-84866505340
"Ozyurt I.B., Condit C., Gupta A.","Processing semantic keyword queries for scientific literature",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864015431&doi=10.1007%2f978-3-642-31178-9_51&partnerID=40&md5=30fa05c15dfcc493b50bd927cd6db4d9","In this short paper, we present early results from an ongoing research on creating a new graph-based representation from NLP analysis of scientific documents so that the graph can be utilized for answering structured queries on NL-processed data. We present a sketch of the data model and the query language to show how scientifically meaningful queries can be posed against this graph structure. © 2012 Springer-Verlag.",,"Graph structures; Graph-based representations; Keyword queries; Scientific documents; Scientific literature; Structured queries; Computational linguistics; Information systems; Query languages; Query processing; Semantics; Natural language processing systems",2-s2.0-84864015431
"Giordani A., Moschitti A.","Generating SQL queries using natural language syntactic dependencies and metadata",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863995903&doi=10.1007%2f978-3-642-31178-9_16&partnerID=40&md5=165001ec2665b1fd9cf84d1eb47e52a8","This research concerns with translating natural language questions into SQL queries by exploiting the MySQL framework for both hypothesis construction and thesis verification in the task of question answering. We use linguistic dependencies and metadata to build sets of possible SELECT and WHERE clauses. Then we exploit again the metadata to build FROM clauses enriched with meaningful joins. Finally, we combine all the clauses to get the set of all possible SQL queries, producing an answer to the question. Our algorithm can be recursively applied to deal with complex questions, requiring nested SELECT instructions. Additionally, it proposes a weighting scheme to order all the generated queries in terms of probability of correctness. Our preliminary results are encouraging as they show that our system generates the right SQL query among the first five in the 92% of the cases. This result can be greatly improved by re-ranking the queries with a machine learning methods. © 2012 Springer-Verlag.","Information Schema; Metadata; Natural Language Processing; Question Answering; SQL","Complex questions; Information schema; Machine learning methods; NAtural language processing; Natural language questions; Natural languages; Probability of correctness; Question Answering; Re-ranking; SQL; SQL query; Weighting scheme; Computational linguistics; Information systems; Learning algorithms; Learning systems; Metadata; Natural language processing systems",2-s2.0-84863995903
"Bloom N.","Using natural language processing to improve document categorization with associative networks",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864004298&doi=10.1007%2f978-3-642-31178-9_18&partnerID=40&md5=1a4f19616fdcca020604708bb0df2719","Associative networks are a connectionist language model with the ability to handle large sets of documents. In this research we investigated the use of natural language processing techniques (part-of-speech tagging and parsing) in combination with Associative Networks for document categorization and compare the results to a TF-IDF baseline. By filtering out unwanted observations and preselecting relevant data based on sentence structure, natural language processing can pre-filter information before it enters the associative network, thus improving results. © 2012 Springer-Verlag.","Associative Networks; Document Categorization; Natural Language Processing; Stanford Natural Language Parser; WordNet","Associative network; Document categorization; NAtural language processing; Natural languages; Wordnet; Computational linguistics; Information systems; Natural language processing systems",2-s2.0-84864004298
"Dorosz K., Korzycki M.","Latent semantic analysis evaluation of conceptual dependency driven focused crawling",2012,"Communications in Computer and Information Science",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864011863&doi=10.1007%2f978-3-642-30721-8_8&partnerID=40&md5=ec80ec2b62399a09c3fd8447bbb238f1","In this paper we study a focused crawler driven by deep semantic analysis provided by the Conceptual Dependency (CD) theory. We test in practice the application of CD scripts as an approach of defining topics (queries) in a focused crawler and its robustness in evaluating real text structures extracted from HTML documents. In order to benchmark its efficiency in comparison to classical approaches, apart from human evaluation we also provide an evaluation of the result set based on its internal similarity using Latent Semantic Analysis (LSA). The performed measurement brings us to the conclusion that the CD theory is well suited for evaluating the similarity of HTML documents provided a specific query, as it achieves a high precision measured through human evaluation. At the same time we observe the drawbacks of LSA used in the same context. © 2012 Springer-Verlag.","conceptual dependency; focused crawling; LSA; topic crawling","Classical approach; conceptual dependency; Focused crawler; Focused crawling; High precision; HTML documents; Human evaluation; Its efficiencies; Latent Semantic Analysis; LSA; Semantic analysis; Text structure; topic crawling; HTML; Multimedia systems; Semantics; Websites",2-s2.0-84864011863
"Nguyen L.M., Shimazu A.","A semi supervised learning model for mapping sentences to logical form with ambiguous supervision",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863993393&doi=10.1007%2f978-3-642-31178-9_11&partnerID=40&md5=7cbaa54436f0651aec6e98dfde93b51b","Semantic parsing is the task of mapping a natural sentence to a meaning representation. The limitation of semantic parsing is that it is very difficult to obtain annotated training data in which a sentence is paired with a semantic representation. To deal with this problem, we introduce a semi supervised learning model for semantic parsing with ambiguous supervision. The main idea of our method is to utilize a large amount of data, to enrich feature space with the maximum entropy model using our semantic learner. We evaluate the proposed models on standard corpora to show that our methods are suitable for semantic parsing problem. Experimental results show that the proposed methods work efficiently and well on ambiguous data and it is comparable to the state of the art method. © 2012 Springer-Verlag.","Semantic Parsing; Support Vector Machines","Ambiguous Data; Annotated training data; Feature space; Logical forms; Maximum entropy models; Semantic parsing; Semantic representation; Semi-supervised learning; State-of-the-art methods; Computational linguistics; Information systems; Learning algorithms; Maximum entropy methods; Natural language processing systems; Semantics; Support vector machines; Context free grammars",2-s2.0-84863993393
"Pawlas P., Domański A., Domańska J.","Universal web pages content parser",2012,"Communications in Computer and Information Science",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863926971&doi=10.1007%2f978-3-642-31217-5_14&partnerID=40&md5=0dc0389e8c14d779a72323ab153b75bf","This article describes the universal web pages content parser - cross-platform application enhancing the process of data extraction from the web pages. In this implementation user friendly interface, possibility of significant automation and reusability of already created patterns had been the key elements. Moreover, the original approach to the issue of parsing the not well-formed HTML, stating the application's core, is precisely presented. Universal web pages content parser shows that the simplified web scrapping utility may be available to masses and not well-formed HTML sources may feed useful tree-like data structures as well as the well-formed ones. © 2012 Springer-Verlag.","content parser; HTML; web; XML","content parser; Cross-platform; Data extraction; HTML sources; Key elements; User friendly interface; web; Data structures; HTML; Reusability; XML; Websites",2-s2.0-84863926971
"Dantuluri P., Davis B., Ludwick P., Handschuh S.","Engineering a controlled natural language into semantic MediaWiki",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863910676&doi=10.1007%2f978-3-642-31175-8_4&partnerID=40&md5=9e225645d7953e7bdf746dec01e94f5e","The Semantic Web is yet to gain mainstream recognition. In part this is caused by the relative complexity of the various semantic web formalisms, which act as a major barrier of entry to naive web users. In addition, in order for the Semantic Web to become a reality, we need semantic metadata. While controlled natural language research has sought to address these challenges, in the context of user friendly ontology authoring for domain experts, there has been little focus on how to adapt controlled languages for novice social web users. The paper describes an approach to using controlled languages for fact creation and management as opposed to ontology authoring, focusing on the domain of meeting minutes. For demonstration purposes, we developed a plug-in to the Semantic MediaWiki, which adds a controlled language editor extension. This editor aids the user while authoring or annotating in a controlled language in a user friendly manner. Controlled content is sent to a parsing service which generates semantic metadata from the sentences which are subsequently displayed and stored in the Semantic MediaWiki. The semantic metadata generated by the parser is grounded against a project documents ontology. The controlled language modeled covers a wide variety of sentences and topics used in the context of a meeting minute. Finally this paper provides a architectural overview of the annotation system. © 2012 Springer-Verlag.",,"Annotation systems; Controlled natural language; Domain experts; Language editors; MediaWiki; Plug-ins; Project documents; Relative complexity; Semantic metadata; User friendly; Web users; Computer software; Websites; Metadata",2-s2.0-84863910676
"Dayarathna M., Houngkaew C., Suzumura T.","Introducing ScaleGraph: An X10 library for billion scale graph analytics",2012,"Proceedings of the ACM SIGPLAN 2012 X10 Workshop, X10 2012",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863896247&doi=10.1145%2f2246056.2246062&partnerID=40&md5=1fefc05a23a06295fdcc11e6be49a394","Highly Productive Computing Systems (HPCS) and PGAS languages are considered as important ways in achieving the exascale computational capabilities. Most of the current large graph processing applications are custom developed using non-HPCS/PGAS techniques such as MPI, MapReduce. This paper introduces Scale-Graph, an X10 library targeting billion scale graph analysis scenarios. Compared to non-PGAS alternatives, ScaleGraph defines concrete, simple abstractions for representing massive graphs. We have designed ScaleGraph from ground up considering graph structural property analysis, graph clustering and community detection. We describe the design of the library and provide some initial performance evaluation results of the library using a twitter graph with 1.47 billion edges. Copyright © 2012 ACM.","Distributed computing; HPCS; Large graph analytics; PGAS; Programming techniques; Reusable libraries; X10","HPCS; Large graphs; PGAS; Programming technique; Reusable library; X10; Distributed computer systems; Computer programming languages",2-s2.0-84863896247
"Kop C.","Checking feasible completeness of domain models with natural language queries",2012,"Conferences in Research and Practice in Information Technology Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863576304&partnerID=40&md5=e1aee3523f8acbb018b813c7450d506b","During the design of information systems it is important to know when a conceptual model of a database is complete. Completing the conceptual database model is a communication process between end users and designers. At the end of the process, both kinds of stakeholders agree that the model has reached a state where it can fulfill the purpose for which it is designed. Natural language queries play an important role as test cases in this process. They are understandable by the end user and help to discuss the model. Hence, this paper focuses on natural language queries as one mean (among others) to test if a conceptual model (domain model) is complete. © 2012, Australian Computer Society, Inc.","Completensess; Controlled natural language queries; Domain models; End user participation; Quality","Communication process; Completensess; Conceptual model; Controlled natural language; Database models; Domain model; End users; Natural language queries; Test case; Communication; Image quality; Query languages",2-s2.0-84863576304
"Grebenshchikov S., Lopes N.P., Popeea C., Rybalchenko A.","Synthesizing software verifiers from proof rules",2012,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",93,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863494447&doi=10.1145%2f2254064.2254112&partnerID=40&md5=9e46600a1b1c32540d1c0cc736f04bb3","Automatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multi-threaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing state-of-the-art verification tools confirms the practicality of the approach. Copyright © 2012 ACM.","Proof rules; Software model checking; Software verification; Verification tool synthesis","Automatic synthesis; Automatically generated; Dataflow; Experimental comparison; Functional programs; Inductive loops; Iterative process; Liveness; Multi-threaded programs; Programmer productivity; Proof rules; Recursive equations; Software model checking; Software verification; Software verification tools; Synthesis procedure; Verification tools; Model checking; Computer programming languages",2-s2.0-84863494447
"Currim F.A., Currim S.A., Dyreson C.E., Snodgrass R.T., Thomas S.W., Zhang R.","Adding temporal constraints to XML schema",2012,"IEEE Transactions on Knowledge and Data Engineering",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863464332&doi=10.1109%2fTKDE.2011.74&partnerID=40&md5=5bd2d8187992ba7af126602044ba42f8","If past versions of XML documents are retained, what of the various integrity constraints defined in XML Schema on those documents? This paper describes how to interpret such constraints as sequenced constraints, applicable at each point in time. We also consider how to add new variants that apply across time, so-called nonsequenced constraints. Our approach supports temporal documents that vary over both valid and transaction time, whose schema can vary over transaction time. We do this by replacing the schema with a (possibly time-varying) temporal schema and replacing the document with a temporal document, both of which are upward compatible with conventional XML and with conventional tools like XMLLINT, which we have extended to support the temporal constraints introduced here. © 1989-2012 IEEE.","Cardinality constraint; key constraint; referential integrity; temporal data; XML Schema constraint; XML validation","Cardinality constraints; Key constraints; Referential integrity; Temporal Data; XML schemas; Computational methods; Information systems; XML",2-s2.0-84863464332
"Alghamdi N.S., Rahayu W., Pardede E.","OXDP & OXiP: The notion of objects for efficient large XML data queries",2012,"International Journal of Grid and Utility Computing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863907603&doi=10.1504%2fIJGUC.2012.047762&partnerID=40&md5=8e4634df5ff82123ce7bc0e877f5bdcb","Due to the rapid growth of XML representation for information exchange, XML databases have been widely adopted in a variety of applications. This paper presents two layers of optimisation for dealing with large XML databases: (1) OXDP (Object-Based Methodology for XML Data Partitioning) which has been developed to partition XML data efficiently and (2) OXiP (Object-Based XML Indexing for Partitions) which is an indexing and linking mechanism for partitioned data. OXDP provides optimal XML data partitioning based on an object's semantic features which improves XML data query performance. The OXiP method tokenises all rooted label paths and preserves the pathways within each XML object partition. The semanticbased data partition ultimately enhances the notion of a frequently accessed data subset which is an advantageous feature in our proposed methods to decrease the time to answer queries. Experimentally, OXDP and OXiP can achieve an order of magnitude performance improvement for querying XML data. Copyright © 2012 Inderscience Enterprises Ltd.","Indexing; Optimisation; Partitioning; Path query; Semantic-based query processing; XML database","Data partition; Data subsets; Information exchanges; Object based; Optimisations; Partitioning; Path queries; Performance improvements; Rapid growth; Semantic features; XML data; XML database; XML indexing; XML representation; Database systems; Indexing (of information); Optimization; Query processing; Semantics; XML; Data handling",2-s2.0-84863907603
"Kong J., Barkol O., Bergman R., Pnueli A., Schein S., Zhang K., Zhao C.","Web interface interpretation using graph grammars",2012,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862550142&doi=10.1109%2fTSMCC.2011.2171335&partnerID=40&md5=239cf350a35059f5bd7c541dbec24e0a","With the advent of the Internet, it is desirable to interpret and extract useful information from the Web. One major challenge in Web interface interpretation is to discover the semantic structure underlying a Web interface. Many heuristic approaches have been developed to discover and group semantically related interface objects. However, those approaches cannot solve the problem of nonuniformity satisfactorily and are not able to tag the semantic role of each object. Distinct from existing approaches, this paper develops a robust and formal approach to recovering interface semantics using graph grammars. Because of the distinct capability of spatial specifications in the abstract syntax, the spatial graph grammar (SGG) is selected to perform the semantic grouping and interpretation of segmented screen objects. Instead of analyzing HTML source codes, we apply an efficient image-processing technology to recognize atomic interface objects from the screenshot of an interface and produce a spatial graph, which records significant spatial relations among recognized objects. A spatial graph is more concise than its corresponding document object model structure and, thus, facilitates interface analysis and interpretation. Based on the spatial graph, the SGG parser recovers the hierarchical relations among interface objects. © 2012 IEEE.","Data extraction; graph grammar; image processing; page segmentation","Abstract syntax; Data extraction; Document object model; Formal approach; Graph grammar; Heuristic approach; Hierarchical relations; HTML sources; Interface analysis; Nonuniformity; Page segmentation; Semantic grouping; Semantic roles; Semantic structures; Spatial graphs; Spatial relations; Web interface; Context sensitive grammars; Formal languages; Graph theory; Heuristic methods; Image processing; Semantics; Semantic Web",2-s2.0-84862550142
"Petrotchenko E.V., Serpa J.J., Hardie D.B., Berjanskii M., Suriyamongkol B.P., Wishart D.S., Borchers C.H.","Use of proteinase K nonspecific digestion for selective and comprehensive identification of interpeptide cross-links: Application to prion proteins",2012,"Molecular and Cellular Proteomics",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863807078&doi=10.1074%2fmcp.M111.013524&partnerID=40&md5=731513c47a6d83ad71297364613bf8f3","Chemical cross-linking combined with mass spectrometry is a rapidly developing technique for structural proteomics. Cross-linked proteins are usually digested with trypsin to generate cross-linked peptides, which are then analyzed by mass spectrometry. The most informative cross-links, the interpeptide cross-links, are often large in size, because they consist of two peptides that are connected by a cross-linker. In addition, trypsin targets the same residues as amino-reactive cross-linkers, and cleavage will not occur at these cross-linker-modified residues. This produces high molecular weight crosslinked peptides, which complicates their mass spectrometric analysis and identification. In this paper, we examine a nonspecific protease, proteinase K, as an alternative to trypsin for cross-linking studies. Initial tests on a model peptide that was digested by proteinase K resulted in a ""family"" of related cross-linked peptides, all of which contained the same cross-linking sites, thus providing additional verification of the cross-linking results, as was previously noted for other post-translational modification studies. The procedure was next applied to the native (PrPC) and oligomeric form of prion protein (PrPβ). Using proteinase K, the affinity-purifiable CID-cleavable and isotopically coded cross-linker cyanurbiotindipropionylsuccinimide and MALDI-MS cross-links were found for all of the possible cross-linking sites. After digestion with proteinase K, we obtained a mass distribution of the crosslinked peptides that is very suitable for MALDI-MS analysis. Using this new method, we were able to detect over 60 interpeptide cross-links in the native PrPC and PrPβprion protein. The set of cross-links for the native form was used as distance constraints in developing a model of the native prion protein structure, which includes the 90-124-amino acid N-terminal portion of the protein. Several cross-links were unique to each form of the prion protein, including a Lys 185-Lys220 cross-link, which is unique to the PrPβ and thus may be indicative of the conformational change involved in the formation of prion protein oligomers. © 2012 by The American Society for Biochemistry and Molecular Biology, Inc.",,"cyanurbiotindipropionylsuccinimide; prion protein; proteinase K; succinimide derivative; unclassified drug; article; matrix assisted laser desorption ionization time of flight mass spectrometry; priority journal; protein cross linking; protein processing",2-s2.0-84863807078
"Settles B.","Active learning",2012,"Synthesis Lectures on Artificial Intelligence and Machine Learning",106,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863817669&doi=10.2200%2fS00429ED1V01Y201207AIM018&partnerID=40&md5=83b3725badcc9e3fabad8898ac709ff0","The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns. An active learner may pose ""queries,"" usually in the form of unlabeled data instances to be labeled by an ""oracle"" (e.g., a human annotator) that already understands the nature of the problem. This sort of approach is well-motivated in many modern machine learning and data mining applications, where unlabeled data may be abundant or easy to come by, but training labels are difficult, time-consuming, or expensive to obtain. This book is a general introduction to active learning. It outlines several scenarios in which queries might be formulated, and details many query selection algorithms which have been organized into four broad categories, or ""query selection frameworks."" We also touch on some of the theoretical foundations of active learning, and conclude with an overview of the strengths and weaknesses of these approaches in practice, including a summary of ongoing work to address these open challenges and opportunities. Copyright © 2012 by Morgan & Claypool.","active learning; expected error reduction; hierarchical sampling; optimal experimental design; query by committee; query by disagreement; query learning; uncertainty sampling; variance reduction","Active Learning; Error reduction; Hierarchical sampling; Optimal experimental designs; Query by committees; query by disagreement; Query learning; Variance reductions; Learning systems; Learning algorithms",2-s2.0-84863817669
"Ahmed S.T., Davulcu H., Tikves S., Nair R., Zhao Z.","BioEve search: A novel framework to facilitate interactive literature search",2012,"Advances in Bioinformatics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862275854&doi=10.1155%2f2012%2f509126&partnerID=40&md5=f3fa2e154eb43f20406df35db53299b6","Background. Recent advances in computational and biological methods in last two decades have remarkably changed the scale of biomedical research and with it began the unprecedented growth in both the production of biomedical data and amount of published literature discussing it. An automated extraction system coupled with a cognitive search and navigation service over these document collections would not only save time and effort, but also pave the way to discover hitherto unknown information implicitly conveyed in the texts. Results. We developed a novel framework (named ""BioEve"") that seamlessly integrates Faceted Search (Information Retrieval) with Information Extraction module to provide an interactive search experience for the researchers in life sciences. It enables guided step-by-step search query refinement, by suggesting concepts and entities (like genes, drugs, and diseases) to quickly filter and modify search direction, and thereby facilitating an enriched paradigm where user can discover related concepts and keywords to search while information seeking. Conclusions. The BioEve Search framework makes it easier to enable scalable interactive search over large collection of textual articles and to discover knowledge hidden in thousands of biomedical literature articles with ease. Copyright © 2012 Syed Toufeeq Ahmed et al.",,,2-s2.0-84862275854
"Thuraisingham B., Khadilkar V., Rachapalli J., Cadenhead T., Kantarcioglu M., Hamlen K., Khan L., Husain F.","Cloud-centric assured information sharing",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862230564&doi=10.1007%2f978-3-642-30428-6_1&partnerID=40&md5=aa7045ce51c6828064aa74c607badcbc","In this paper we describe the design and implementation of cloud-based assured information sharing systems. In particular, we will describe our current implementation of a centralized cloud-based assured information sharing system and the design of a decentralized hybrid cloud-based assured information sharing system of the future. Our goal is for coalition organizations to share information stored in multiple clouds and enforce appropriate policies. © 2012 Springer-Verlag.",,"Information sharing; Information sharing systems; Artificial intelligence; Information analysis",2-s2.0-84862230564
"Rakthanmanon T., Zhu Q., Keogh E.J.","Efficiently finding near duplicate figures in archives of historical documents",2012,"Journal of Multimedia",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861846954&doi=10.4304%2fjmm.7.2.109-123&partnerID=40&md5=081a19ae4f9e74e3470dae1a1837e5f4","The increasing interest in archiving all of humankind's cultural artifacts has resulted in the digitization of millions of books, and soon a significant fraction of the world's books will be online. Most of the data in historical manuscripts is text, but there is also a significant fraction devoted to images. This fact has driven much of the recent increase in interest in query-by-content systems for images. While querying/indexing systems can undoubtedly be useful, we believe that the historical manuscript domain is finally ripe for true unsupervised discovery of patterns and regularities. To this end, we introduce an efficient and scalable system that can detect approximately repeated occurrences of shape patterns both within and between historical texts. We show that this ability to find repeated shapes allows automatic annotation of manuscripts, and allows users to trace the evolution of ideas. We demonstrate our ideas on datasets of scientific and cultural manuscripts dating back to the fourteenth century. © 2012 ACADEMY PUBLISHER.","Component; Cultural artifacts; Duplication detection; Repeated patterns","Automatic annotation; Component; Cultural artifacts; Data sets; Duplication detection; Historical documents; Near-duplicates; Repeated patterns; Scalable systems; Shape patterns; Image recording; History",2-s2.0-84861846954
"Nilsson A., Hedin G.","Metacompiling OWL ontologies",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861633230&doi=10.1007%2f978-3-642-28830-2_20&partnerID=40&md5=a80bd60e4e0d1263aa568db1bf86c05d","Ontologies, formal knowledge representation, and reasoning are technologies that have begun to gain substantial interest in recent years. We present a high-level declarative approach to writing application programs for specific ontologies, based on viewing the ontology as a domain-specific language. Our approach is based on declarative meta-compilation techniques. We have implemented a tool using this approach that allows typed frontends to be generated for specific ontologies, and to which the desired functionality can be added as separate aspects. Our tool makes use of the JastAdd meta-compilation system which is based on reference attribute grammars. We describe the architecture of our tool and evaluate the approach on applications in industrial robotics. © 2012 Springer-Verlag.",,"Application programs; Attribute grammars; Domain specific languages; Industrial robotics; OWL ontologies; Problem oriented languages; Knowledge representation",2-s2.0-84861633230
"Andrade D., Matsuzaki T., Tsujii J.","Statistical extraction and comparison of pivot words for bilingual lexicon extension",2012,"ACM Transactions on Asian Language Information Processing",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863640864&doi=10.1145%2f2184436.2184439&partnerID=40&md5=3044331bdcc7ee4bd4926a7e74419784","Bilingual dictionaries can be automatically extended by new translations using comparable corpora. The general idea is based on the assumption that similar words have similar contexts across languages. However, previous studies have mainly focused on Indo-European languages, or use only a bag-of-words model to describe the context. Furthermore, we argue that it is helpful to extract only the statistically significant context, instead of using all context. The present approach addresses these issues in the following manner. First, based on the context of a word with an unknown translation (query word), we extract salient pivot words. Pivot words are words for which a translation is already available in a bilingual dictionary. For the extraction of salient pivot words, we use a Bayesian estimation of the point-wise mutual information to measure statistical significance. In the second step, we match these pivot words across languages to identify translation candidates for the query word. We therefore calculate a similarity score between the query word and a translation candidate using the probability that the same pivots will be extracted for both the query word and the translation candidate. The proposed method uses several context positions, namely, a bag-of-words of one sentence, and the successors, predecessors, and siblings with respect to the dependency parse tree of the sentence. In order to make these context positions comparable across Japanese and English, which are unrelated languages, we use several heuristics to adjust the dependency trees appropriately. We demonstrate that the proposed method significantly increases the accuracy of word translations, as compared to previous methods. © 2012 ACM.","Bayesian statistical methods; Bilingual dictionary creation; Comparable corpora; Dependency parse tree information","Bag of words; Bag-of-words models; Bayesian estimations; Bayesian statistical method; Bilingual dictionary; Bilingual lexicons; Comparable corpora; Dependency trees; Mutual informations; Parse trees; Query words; Similarity scores; Statistical significance; Word translation; Bayesian networks; Forestry; Translation (languages); Forestry; Languages; Translation; Trees",2-s2.0-84863640864
"Sarasa-Cabezuelo A., Temprado-Battad B., Rodríguez-Cerezo D., Sierra J.-L.","Building XML-driven application generators with compiler construction tools",2012,"Computer Science and Information Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865216134&doi=10.2298%2fCSIS110505002S&partnerID=40&md5=1a3dfc6e4ab68825b127c41e38157b76","This paper describes how to use conventional compiler construction tools, and parser generators in particular, to build XML-driven application generators. In our approach, the document interface is provided by a standard stream-oriented XML processing framework (e.g., SAX or StAX). This framework is used to program a generic, customizable XML scanner that transforms documents into streams of suitable tokens (opening and closing tags, character data, etc.). The next step is to characterize the syntactic structure of these streams in terms of generation-specific context-free grammars. By adding suitable semantic attributes and semantic actions to these grammars, developers obtain generation-oriented translation schemes: high-level specifications of the generation tasks. These specifications are then turned into working application generators by using standard parser generation technology. We illustrate the approach with <e-Subway>, an XML-driven generator of shortest-route search applications in subway networks.","Application generators; Compiler construction tools; Software development approach; XML processing",,2-s2.0-84865216134
"Alcaraz Calero J.M., Ortega A.M., Perez G.M., Botia Blaya J.A., Gomez Skarmeta A.F.","A non-monotonic expressiveness extension on the semantic web rule language",2012,"Journal of Web Engineering",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861432197&partnerID=40&md5=89436b680903d77a4b21685998b5217d","SWRL (Semantic Web Rule Language) extends OWL syntax and semantics by enabling the description of Horn-like rules. However, the current SWRL speciffcation lacks support for, among others, negative expressions, missing values and priority relationships between rules, which are frequently needed when modeling realistic scenarios. This paper motivates the necessity of surpassing some of these problems and provides an extension over the original SWRL aimed to deffne more expressive rules. Hence, the following four operators have been added to SWRL: Not operator (i.e., classical negation) to express negative facts; NotExists quantiffer to ask for missing facts in the knowledge base (when used in the antecedent of the rule) and remove facts (when used in the consequent); Dominance operator to establish priorities among rules; and Mutex operator to establish exclusions during rule executions. The syntax and semantics of these four operators are described in this proposal. Moreover, the non-monotonicity added to the rule-based inference process by means of such elements is also explained. An implementation of the four operators has been developed as a plug-in for the Jena generic rule engine, which enables the execution of Horn-like rules, together with a parser to translate SWRL rules to the Jena speciffc rule language. Finally, the proposed SWRL extension and its implementation have been validated in a real scenario centered on call forwarding management in an intelligent building. © Rinton Press.","Non-monotonicity; Ontology Web Language (OWL); Rule-based inference process; Semantic Web Rule Language (SWRL)","Generic rules; Knowledge base; Missing values; Non-monotonicity; Ontology web language; Plug-ins; Priority relationship; Realistic scenario; Rule-based inference; Semantic Web rule language (SWRL); Intelligent buildings; Knowledge based systems; Syntactics",2-s2.0-84861432197
"Barbau R., Krima S., Rachuri S., Narayanan A., Fiorentini X., Foufou S., Sriram R.D.","OntoSTEP: Enriching product model data using ontologies",2012,"CAD Computer Aided Design",86,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857929258&doi=10.1016%2fj.cad.2012.01.008&partnerID=40&md5=e492184a285e8d385054270dd3a4ad30","The representation and management of product lifecycle information is critical to any manufacturing organization. Different modeling languages are used at different lifecycle stages, for example STEP's EXPRESS may be used at a detailed design stage, while UML may be used for initial design stages. It is necessary to consolidate product information created using these different languages to build a coherent knowledge base. In this paper, we present an approach to enable the translation of STEP schema and its instances to Ontology Web Language (OWL). This gives a modelwhich we call OntoSTEPthat can easily be integrated with any OWL ontologies to create a semantically rich model. As an example, we combine geometry information represented in STEP with non-geometry information, such as function and behavior, represented using the NIST's Core Product Model (CPM). A plug-in for Protégé is developed to automate the different steps of the translation. As additional benefits, reasoning, inference procedures, and queries can be performed on enriched legacy CAD models. We describe the rules for the translation from EXPRESS to OWL, and illustrate the benefits of OWL translation with an example. We will also describe how these mapping rules can be implemented through meta-model based transformations, which can be used to map other languages to OWL.","Automated model transcription; EXPRESS; Meta-model; OWL; Product lifecycle information; Product modeling; Semantic; STEP","EXPRESS; Meta model; OWL; Product modeling; Product-life-cycle; STEP; Knowledge based systems; Life cycle; Semantics; Translation (languages); Computer aided design",2-s2.0-84857929258
"Lukács A., Nagy Z.","Dynamic log analysis",2012,"Infocommunications Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865768356&partnerID=40&md5=15f6ff8e12076dfceefebbeb2a269ff9","This article reviews a new log analysis solution based on multidimensional data cubes. It introduces the process of dynamic log analysis, including real-time log compression during the log management, a new log parsing language, and the efficient regular expression processing engine for this language. The article assesses the new online analytic processing (OLAP) tool implemented for log analysis. The algorithm and software technology developments appearing in the resulting system are creating a new class of real time log processing and analysis tools.","Bitmap index; Log analysis; OLAP; Real-time compression; Regular expression","Bitmap indexes; Log analysis; OLAP; Real-time compression; Regular expressions; Data compression; Pattern matching; Software engineering; Well logging",2-s2.0-84865768356
"Li M., Zhang H., Wu R., Zhou Z.-H.","Sample-based software defect prediction with active and semi-supervised learning",2012,"Automated Software Engineering",52,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856674640&doi=10.1007%2fs10515-011-0092-1&partnerID=40&md5=5c657b241cb2470833cba5c7b70bb8b8","Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice. © 2011 Springer Science+Business Media, LLC.","Active semi-supervised learning; Machine learning; Quality assurance; Sampling; Software defect prediction","Active sampling; Control software; Conventional machines; Data sets; Defect prediction; Defect prediction models; Historical data; Industrial practices; Large software systems; Machine-learning; New projects; Prediction model; Project data; Random sampling; Semi-supervised; Semi-supervised learning; Semi-supervised learning methods; Software defect prediction; Computer software selection and evaluation; Forecasting; Mathematical models; Quality assurance; Sampling; Supervised learning; Defects",2-s2.0-84856674640
"Fok C.-L., Roman G.-C., Lu C.","Servilla: A flexible service provisioning middleware for heterogeneous sensor networks",2012,"Science of Computer Programming",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858997440&doi=10.1016%2fj.scico.2010.11.006&partnerID=40&md5=b0d8f6b2995df4ac03730f855381584f","Device heterogeneity in wireless sensor networks is rendering such networks increasingly difficult to program. To address this problem, we present Servilla, a novel middleware that enables applications to be both platform-independent and efficient despite executing over a diverse and dynamic set of devices. It achieves this by using service-oriented computing and requiring all platform-specific functionality be encapsulated behind services, which are dynamically discovered by applications. Novel forms of service bindings and invocation semantics enable flexible yet energy-efficient in-network collaboration among heterogeneous devices. To support a wide range of devices, Servilla introduces the concept of middleware asymmetry, enabling resource-constrained devices to only provide services that can be leveraged by more powerful devices running applications. Servilla has been implemented and evaluated on two disparate hardware platforms, the Imote2 and TelosB. Microbenchmarks demonstrate Servilla's feasibility while a structural health monitoring application case study demonstrates its efficacy. © 2010 Elsevier B.V. All rights reserved.","Coordination model; Middleware; Service-oriented computing; Wireless sensor networks","Coordination model; Energy efficient; Flexible service provisioning; Hardware platform; Heterogeneous devices; Heterogeneous sensor networks; Microbenchmarks; Powerful devices; Resourceconstrained devices; Running applications; Service binding; Service oriented computing; Structural health; Wireless sensor network (WSN); Middleware; Quality of service; Semantics; Equipment",2-s2.0-84858997440
"Bai Y., Wang F., Cao Y.","Visual flexible coding system design based on rule knowledge and XML",2012,"Proceedings - 2012 International Conference on Computer Science and Electronics Engineering, ICCSEE 2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861079978&doi=10.1109%2fICCSEE.2012.450&partnerID=40&md5=6f4773829285bb017a756cc977ecb4c8","Based on the analysis of enterprise coding demand, a Visual Flexible Coding System (VFCS) architecture is put forward. Its function modules are determined that include coding rule design module, code inputting and parsing module, and code query module. First, development platform selection, man-machine interface design, and data structure description are completed. Then, the key problems and corresponding solutions are studied, such as, rule knowledge representation and expansion, general code segment parser, segment-based composite code query, etc. Finally, the key technologies are solved, such as, instant event responding, GUI plotting, XML parsing, general code segment positioning, etc, and the VFCS is developed. © 2012 IEEE.","man-machine interface; Rule knowledge; Visual flexible coding; XML","Code segments; Coding rule; Coding system; Corresponding solutions; Design module; Development platform; Function module; Key technologies; Man-machine interface; Query modules; Rule knowledge; Segment-based; Visual flexible coding; XML parsing; Computer science; Data structures; Electronics engineering; Knowledge representation; Signal encoding; XML",2-s2.0-84861079978
"Deng Z., Liao H., Gao H.","Twig pattern matching running on XML streams",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860606445&doi=10.1007%2f978-3-642-29426-6_6&partnerID=40&md5=f3d90cf3a7b60aa06f96e7e72ae68cce","Twig pattern matching plays an important role in XML query processing, holistic twig pattern matching algorithms have been proposed and are considered to be effective since they avoid producing large number of intermediate results. Meanwhile, automaton-based approaches are naturally used in filtering XML streams, because Finite State Machines(FSMs) are driven by events which conform to event-based XML parser SAX. In this paper, we proposed a hybrid approach combining FSM and holistic twig matching algorithm to find occurrences of twig pattern in XML streams. That is, we locate the lowest common ancestor(LCA) of return node(s) in twig pattern, decompose the twig pattern according to the LCA, use automaton-based approach for processing the sub twig pattern above LCA, and regular holistic twig pattern matching algorithm for the sub twig pattern below LCA. It only needs to buffer the elements between the start and end tag of LCA. Experiments show the effectiveness of this approach. © 2012 Springer-Verlag Berlin Heidelberg.","FSM; twig pattern matching; XML query processing; XML streams","Event-based; FSM; Holistic twig pattern matching; Hybrid approach; Intermediate results; Lowest common ancestors; Twig matching; Twig pattern; Twig pattern matching; XML parser; XML query processing; XML stream; Algorithms; Life cycle; Pattern matching; Query processing; XML",2-s2.0-84860606445
"De Lusignan S., Liaw S.-T., Michalakidis G., Jones S.","Defining datasets and creating data dictionaries for quality improvement and research in chronic disease using routinely collected data: An ontology-driven approach",2012,"Informatics in Primary Care",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863222653&partnerID=40&md5=614ebd2ebf137ca267bcf35fae0d21f7","Background: The burden of chronic disease is increasing, and research and quality improvement will be less effective if case finding strategies are suboptimal. Objective: To describe an ontology-driven approach to case finding in chronic disease and how this approach can be used to create a data dictionary and make the codes used in case finding transparent. Method: A five-step process: (1) identifying a reference coding system or terminology; (2) using an ontology-driven approach to identify cases; (3) developing metadata that can be used to identify the extracted data; (4) mapping the extracted data to the reference terminology; and (5) creating the data dictionary. Results: Hypertension is presented as an exemplar. A patient with hypertension can be represented by a range of codes including diagnostic, history and administrative. Metadata can link the coding system and data extraction queries to the correct data mapping and translation tool, which then maps it to the equivalent code in the reference terminology. The code extracted, the term, its domain and subdomain, and the name of the data extraction query can then be automatically grouped and published online as a readily searchable data dictionary. An exemplar online is: www.clininf.eu/qickd-datadictionary. html Conclusion: Adopting an ontology-driven approach to case finding could improve the quality of disease registers and of research based on routine data. It would offer considerable advantages over using limited datasets to define cases. This approach should be considered by those involved in research and quality improvement projects which utilise routine data. © 2011 PHCSG, British Computer Society.","Classification; Computerised; Medical informatics; Medical records systems","article; chronic disease; classification; coding; factual database; human; information retrieval; medical informatics; methodology; total quality management; Chronic Disease; Clinical Coding; Databases, Factual; Humans; Information Storage and Retrieval; Medical Informatics Applications; Quality Improvement",2-s2.0-84863222653
"Zheng Y.-L., He Q.-Y., Qian P., Li Z.","Construction of the Ontology-Based Agricultural Knowledge Management System",2012,"Journal of Integrative Agriculture",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862505145&doi=10.1016%2fS2095-3119%2812%2960059-8&partnerID=40&md5=a688d26618e9f8ca5a17c13d12a7be1a","Ontology is the formal representation of concepts and their mutual relations. It has wide application potential in the classification of agricultural information, the construction of information and knowledge database, the research and development of intelligent search engine, as well as the realization of cooperative information service, etc. In this research, an ontology-based agricultural knowledge management system framework is proposed, which includes modules of ontology-based knowledge acquisition, knowledge representation, knowledge organization, and knowledge mining, etc. The key technologies, building tools and applications of the framework are explored. Future researches on the theoretical refinement and intelligent simulation knowledge service are also envisioned. © 2012 Chinese Academy of Agricultural Sciences.","Agriculture; Construction; Knowledge management; Ontology; System",,2-s2.0-84862505145
"Stárka J., Svoboda M., Sochna J., Schejbal J., Mlýnková I., Bednárek D.","Analyzer: A complex system for data analysis",2012,"Computer Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860676246&doi=10.1093%2fcomjnl%2fbxr103&partnerID=40&md5=3f6b286fdea4582dc7258c678e3ef37e","Recently eXtensible Markup Language (XML) has achieved the leading role among languages for data representation and, thus, we can witness a massive boom of corresponding techniques for managing XML data. Most of the processing techniques, however, suffer from various bottlenecks worsening their time and/or space efficiency. We assume that the main reason is they consider XML collections too globally, involving all their possible features, although real-world data are often much simpler. Even though some techniques do restrict the input data, the restrictions are mostly unnatural. This paper aims to introduce Analyzer-a complex framework for performing statistical analyses of real-world documents. Exploitation of results of these analyses is a classical way how data processing can be optimized in many areas. Although this intent is legitimate, ad hoc and dedicated analyses soon become obsolete, they are usually built on insufficiently extensive collections and are difficult to repeat. Analyzer represents an easily extensible framework, which helps the user with gathering documents, managing analyses and browsing computed reports. © The Author 2011. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.","structural analysis; XML data analysis; XML data correction; XML data crawling; XQuery analysis","Data representations; Extensible framework; Input datas; Processing technique; Real world data; Space efficiencies; XML data; XQuery analysis; Data processing; Structural analysis; XML",2-s2.0-84860676246
"Nawab R.M.A., Stevenson M., Clough P.","Retrieving candidate plagiarised documents using query expansion",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860112209&doi=10.1007%2f978-3-642-28997-2_18&partnerID=40&md5=80f3c8d421527f77b8e2584410b5ff29","External plagiarism detection systems compare suspicious texts against a reference collection to identify the original one(s). The suspicious text may not contain a verbatim copy of the reference collection since plagiarists often try to disguise their behaviour by altering the text. For large reference collections, such as those accessible via the internet, it is not practical to compare the suspicious text with every document in the reference collection. Consequently many approaches to plagiarism detection begin by identifying a set of candidate documents from the reference collection. We report an IR-based approach to the candidate document selection problem that uses query expansion to identify candidates which have been altered. The reported system outperforms a previously reported approach and is also robust to changes in the reference collection text. © 2012 Springer-Verlag Berlin Heidelberg.","external plagiarism detection; information retrieval; query expansion","Document selection; Plagiarism detection; Query expansion; Reference collections; Information retrieval; Intellectual property; Expansion",2-s2.0-84860112209
"Pannarale P., Catalano D., De Caro G., Grillo G., Leo P., Pappadà G., Rubino F., Scioscia G., Licciulli F.","GIDL: a rule based expert system for GenBank Intelligent Data Loading into the Molecular Biodiversity database",2012,"BMC Bioinformatics",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864927195&doi=10.1186%2f1471-2105-13-S4-S4&partnerID=40&md5=b43e623065e0b55b710b8a22972044d4","Background: In the scientific biodiversity community, it is increasingly perceived the need to build a bridge between molecular and traditional biodiversity studies. We believe that the information technology could have a preeminent role in integrating the information generated by these studies with the large amount of molecular data we can find in bioinformatics public databases. This work is primarily aimed at building a bioinformatic infrastructure for the integration of public and private biodiversity data through the development of GIDL, an Intelligent Data Loader coupled with the Molecular Biodiversity Database. The system presented here organizes in an ontological way and locally stores the sequence and annotation data contained in the GenBank primary database.Methods: The GIDL architecture consists of a relational database and of an intelligent data loader software. The relational database schema is designed to manage biodiversity information (Molecular Biodiversity Database) and it is organized in four areas: MolecularData, Experiment, Collection and Taxonomy. The MolecularData area is inspired to an established standard in Generic Model Organism Databases, the Chado relational schema. The peculiarity of Chado, and also its strength, is the adoption of an ontological schema which makes use of the Sequence Ontology.The Intelligent Data Loader (IDL) component of GIDL is an Extract, Transform and Load software able to parse data, to discover hidden information in the GenBank entries and to populate the Molecular Biodiversity Database. The IDL is composed by three main modules: the Parser, able to parse GenBank flat files; the Reasoner, which automatically builds CLIPS facts mapping the biological knowledge expressed by the Sequence Ontology; the DBFiller, which translates the CLIPS facts into ordered SQL statements used to populate the database. In GIDL Semantic Web technologies have been adopted due to their advantages in data representation, integration and processing.Results and conclusions: Entries coming from Virus (814,122), Plant (1,365,360) and Invertebrate (959,065) divisions of GenBank rel.180 have been loaded in the Molecular Biodiversity Database by GIDL. Our system, combining the Sequence Ontology and the Chado schema, allows a more powerful query expressiveness compared with the most commonly used sequence retrieval systems like Entrez or SRS. © 2012 Pannarale et al.; licensee BioMed Central Ltd.",,"Biodiversity datum; Data representations; Extract , transform and loads; Flat files; GenBank; Generic models; Hidden information; Intelligent data; Large amounts; Main module; Molecular biodiversity; Molecular data; Public database; Reasoner; Relational Database; Relational database schemata; Relational schemas; Rule based expert systems; Semantic Web technology; Sequence retrieval systems; SQL statements; Biodiversity; Bioinformatics; Database systems; Expert systems; Genes; Information technology; Loaders; Loading; Ocean habitats; Ontology; Query processing; Viruses; Data integration; animal; article; biodiversity; biology; computer program; expert system; Internet; methodology; nucleic acid database; Animals; Biodiversity; Computational Biology; Databases, Nucleic Acid; Expert Systems; Internet; Software",2-s2.0-84864927195
"Parameswarappa S., Narayana V.N., Bharathi G.N.","A novel approach to build Kannada web Corpus",2012,"2012 International Conference on Computer Communication and Informatics, ICCCI 2012",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858721139&doi=10.1109%2fICCCI.2012.6158824&partnerID=40&md5=7797e1bc1738b869c595bab0552e9927","This paper introduces the Kannada Corpus tool, a suite of Perl (Program Extraction and Reporting Language) programs implementing an iterative procedure to build Kannada corpora from the web. The procedure requires is, first a set of ""seed"" words list is built and later a set of ""seed"" URLs (Uniform Resource Locator) containing documents in the Kannada language is collected by sending queries to commercial search engines (Google and Yahoo). The obtained seeds are then used to start a crawling job using the open-source, command-line based downloading tool ""wget"". The downloaded documents are then processed in various ways in order to build Kannada raw corpora such as HTML (Hyper Text Markup Language) code removal, boilerplate stripping, and language identification, duplicate and near duplicate detection. We conducted an evaluation of the tool by applying it to the construction of Kannada corpora from the domains such as Recent Discussions, Articles, Recent Activities, Proverbs, Recent Feedback's, Poems and Fifteen Books, Novels, News paper, Dictionary, Blogs and Informal Chats. The results illustrate the potential usefulness of the tool. © 2012 IEEE.","Corpora; Kannada corpus; Part-of-Speech (POS) tagging; Tokenizer; wget; World Wide Web","Corpora; Kannada corpus; Part of speech tagging; Tokenizer; wget; Hypertext systems; Information science; Natural language processing systems; Search engines; Speech communication; World Wide Web; Software agents",2-s2.0-84858721139
"Bajwa I.S., Lee M., Bordbar B.","Resolving syntactic ambiguities in natural language specification of constraints",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863350375&doi=10.1007%2f978-3-642-28604-9_15&partnerID=40&md5=763f119ec41266a729bf9aba599b7bcd","In the NL2OCL project, we aim to translate English specification of software constraints to formal constraints such as OCL (Object Constraint Language). In the used approach, the Stanford POS tagger and the Stanford Parser are employed for syntactic analysis of English specification and the output of syntactic analysis is given to our semantic analyzer for the detailed semantic analysis. However, in few cases, the Stanford POS tagger and parser are not able to handle particular syntactic ambiguities in English specifications of software constraints. In this paper, we highlight the identified cases of syntactic ambiguities and we also present a novel technique to automatically resolve the identified syntactic ambiguities. By addressing the identified cases of syntactic ambiguities, we can generate more accurate and complete formal (OCL) specifications. © 2012 Springer-Verlag.","Ambiguity resolution; Attachment Ambiguity; English constraints; Syntactic Ambiguities","Ambiguity resolution; Attachment Ambiguity; English constraints; Natural language specifications; Novel techniques; Object Constraint Language; PoS taggers; Semantic analysis; Stanford; Syntactic analysis; Computational linguistics; Semantics; Specifications; Text processing; Syntactics",2-s2.0-84863350375
"Paukkeri M.-S., Väyrynen J., Arppe A.","Exploring extensive linguistic feature sets in near-synonym lexical choice",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858306336&doi=10.1007%2f978-3-642-28601-8_1&partnerID=40&md5=2ae732e2aa15761e804e464011e9bce1","In the near-synonym lexical choice task, the best alternative out of a set of near-synonyms is selected to fill a lexical gap in a text. We experiment on an approach of an extensive set, over 650, linguistic features to represent the context of a word, and a range of machine learning approaches in the lexical choice task. We extend previous work by experimenting with unsupervised and semi-supervised methods, and use automatic feature selection to cope with the problems arising from the rich feature set. It is natural to think that linguistic analysis of the word context would yield almost perfect performance in the task but we show that too many features, even linguistic, introduce noise and make the task difficult for unsupervised and semi-supervised methods. We also show that purely syntactic features play the biggest role in the performance, but also certain semantic and morphological features are needed. © 2012 Springer-Verlag.","linguistic features; Near-synonym lexical choice","Automatic feature selection; Feature sets; Lexical choice; Linguistic analysis; Linguistic features; Machine-learning; Morphological features; Semi-supervised method; Syntactic features; Word contexts; Computational linguistics; Text processing; Semantics",2-s2.0-84858306336
"Nguyen H.S., Ślȩzak D., Skowron A., Bazan J.G.","Semantic search and analytics over large repository of scientific articles",2012,"Studies in Computational Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858021894&doi=10.1007%2f978-3-642-24809-2_1&partnerID=40&md5=04727767b649d9983bfba983cf7720c6","We present the architecture of the system aimed at search and synthesis of information within document repositories originating from different sources, with documents provided not necessarily in the same format and the same level of detail. The system is expected to provide domain knowledge interfaces enabling the internally implemented algorithms to identify relationships between documents (as well as authors, institutions et cetera) and concepts (such as, e.g., areas of science) extracted from various types of knowledge bases. The system should be scalable by means of scientific content storage, performance of analytic processes, and speed of search. In case of compound computational tasks (such as production of richer semantic indexes for the search improvements), it should follow the paradigms of hierarchical modeling and computing, designed as an interaction between domain experts, system experts, and appropriately implemented intelligent modules. © 2012 Springer-Verlag GmbH Berlin Heidelberg.","behavioral patterns; decision support; document analytics; document repositories; interactive and hierarchical computing; semantic information retrieval and synthesis; semantic search; wisdom technology",,2-s2.0-84858021894
"Moors A., Rompf T., Haller P., Odersky M.","Scala-Virtualized",2012,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857858103&partnerID=40&md5=8f1e76bd0449ae2d60df2504c8cfb28d","Scala-Virtualized extends the Scala language to better support hosting embedded DSLs. Embedding a DSL in Scala-Virtualized comes with all the benefits of a shallow embedding thanks to Scala's flexible syntax, without giving up analyzing and manipulating the domain program - typically exclusive to deep embeddings. Through lightweight modular staging, implemented in standard Scala, the benefits of a deep embedding are recovered with little overhead. Scala-Virtualized lifts more of the language's built-in constructs and static information to complete this support and make it more convenient. We illustrate how Scala-Virtualized makes Scala an even better host for embedded DSLs along three axes of customizing the language: syntax, run-time behavior and static semantics. Copyright © 2012 ACM.","Languages","Embeddings; Runtimes; Shallow embedding; Static information; Static semantics; Three axes; Query languages; Semantics; Syntactics; Computer programming languages",2-s2.0-84857858103
"Hovland D.","The membership problem for regular expressions with unordered concatenation and numerical constraints",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857828079&doi=10.1007%2f978-3-642-28332-1_27&partnerID=40&md5=02ccc998bc9d7336b44ad00056dbac08","We study the membership problem for regular expressions extended with operators for unordered concatenation and numerical constraints. The unordered concatenation of a set of regular expressions denotes all sequences consisting of exactly one word denoted by each of the expressions. Numerical constraints are an extension of regular expressions used in many applications, e.g. text search (e.g., UNIX grep), document formats (e.g. XML Schema). Regular expressions with unordered concatenation and numerical constraints denote the same languages as the classical regular expressions, but, in certain important cases, exponentially more succinct. We show that the membership problem for regular expressions with unordered concatenation (without numerical constraints) is already NP-hard. We show a polynomial-time algorithm for the membership problem for regular expressions with numerical constraints and unordered concatenation, when restricted to a subclass called strongly 1-unambiguous. © 2012 Springer-Verlag.","Automata; Interleaving; Numerical Constraints; Regular Expressions; SGML; Unordered Concatenation; XML","Automata; Interleaving; Numerical constraints; Regular expressions; Unordered Concatenation; SGML; XML; Automata theory",2-s2.0-84857828079
"Yang C.","Computational models of syntactic acquisition",2012,"Wiley Interdisciplinary Reviews: Cognitive Science",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863116441&doi=10.1002%2fwcs.1154&partnerID=40&md5=d5e05f47fd21d580535fa032d35df6e4","The computational approach to syntactic acquisition can be fruitfully pursued by integrating results and perspectives from computer science, linguistics, and developmental psychology. In this article, we first review some key results in computational learning theory and their implications for language acquisition. We then turn to examine specific learning models, some of which exploit distributional information in the input while others rely on a constrained space of hypotheses, yet both approaches share a common set of characteristics to overcome the learning problem. We conclude with a discussion of how computational models connects with the empirical study of child grammar, making the case for computationally tractable, psychologically plausible and developmentally realistic models of acquisition. © 2011 John Wiley & Sons, Ltd.",,,2-s2.0-84863116441
"Rompf T., Amin N., Moors A., Haller P., Odersky M.","Scala-Virtualized: linguistic reuse for deep embeddings",2012,"Higher-Order and Symbolic Computation",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889849208&doi=10.1007%2fs10990-013-9096-9&partnerID=40&md5=565dcc31b481cd7ff9868e906e76f634","Scala-Virtualized extends the Scala language to better support hosting embedded DSLs. Scala is an expressive language that provides a flexible syntax, type-level computation using implicits, and other features that facilitate the development of embedded DSLs. However, many of these features work well only for shallow embeddings, i.e. DSLs which are implemented as plain libraries. Shallow embeddings automatically profit from features of the host language through linguistic reuse: any DSL expression is just as a regular Scala expression. But in many cases, directly executing DSL programs within the host language is not enough and deep embeddings are needed, which reify DSL programs into a data structure representation that can be analyzed, optimized, or further translated. For deep embeddings, linguistic reuse is no longer automatic. Scala-Virtualized defines many of the language’s built-in constructs as method calls, which enables DSLs to redefine the built-in semantics using familiar language mechanisms like overloading and overriding. This in turn enables an easier progression from shallow to deep embeddings, as core language constructs such as conditionals or pattern matching can be redefined to build a reified representation of the operation itself. While this facility brings shallow, syntactic, reuse to deep embeddings, we also present examples of what we call deep linguistic reuse: combining shallow and deep components in a single DSL in such a way that certain features are fully implemented in the shallow embedding part and do not need to be reified at the deep embedding level. © Springer Science+Business Media New York 2013","Code generation; Domain-specific languages; Language virtualization; Linguistic reuse","Computer programming languages; Linguistics; Pattern matching; Problem oriented languages; Semantics; Syntactics; Code Generation; Domain specific languages; Embeddings; Language constructs; Scala languages; Shallow embedding; Digital subscriber lines",2-s2.0-84889849208
"Bosse T., Both F., Gerritsen C., Hoogendoorn M., Treur J.","Methods for model-based reasoning within agent-based Ambient Intelligence applications",2012,"Knowledge-Based Systems",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855970948&doi=10.1016%2fj.knosys.2011.09.011&partnerID=40&md5=6bef4e658bb8c0b2214d3f042b100369","Within agent-based Ambient Intelligence applications agents react to humans based on information obtained by sensoring and their knowledge about human functioning. Appropriate types of reactions depend on the extent to which an agent understands the human and is able to interpret the available information (which is often incomplete, and hence multi-interpretable) in order to create a more complete internal image of the environment, including humans. Such an understanding requires that the agent has knowledge to a certain depth about the human's physiological and mental processes in the form of an explicitly represented model of the causal and dynamic relations describing these processes. In addition, given such a model representation, the agent needs reasoning methods to derive conclusions from the model and interpret the (partial) information available by sensoring. This paper presents the development of a toolbox that can be used by a modeller to design Ambient Intelligence applications. This toolbox contains a number of model-based reasoning methods and approaches to control such reasoning methods. Formal specifications in an executable temporal format are offered, which allows for simulation of reasoning processes and automated verification of the resulting reasoning traces in a dedicated software environment. A number of such simulation experiments and their formal analysis are described. The main contribution of this paper is that the reasoning methods in the toolbox have the possibility to reason using both quantitative and qualitative aspects in combination with a temporal dimension, and the possibility to perform focused reasoning based upon certain heuristic information. © 2011 Elsevier B.V. All rights reserved.","Agent-based Ambient Intelligence applications; Default logic; Formal analysis; Model-based reasoning; Simulation","Ambient intelligence; Default logic; Formal analysis; Model-based reasoning; Simulation; Artificial intelligence; Automata theory; Computer software; Formal methods; Physiological models; Verification; Heuristic methods",2-s2.0-84855970948
"Lawson K.R., Lawson J.","LICSS - A chemical spreadsheet in Microsoft Excel",2012,"Journal of Cheminformatics",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862683650&doi=10.1186%2f1758-2946-4-3&partnerID=40&md5=9bf65f1f3f02a00b729ebf08163f5777","Background: Representations of chemical datasets in spreadsheet format are important for ready data assimilation and manipulation. In addition to the normal spreadsheet facilities, chemical spreadsheets need to have visualisable chemical structures and data searchable by chemical as well as textual queries. Many such chemical spreadsheet tools are available, some operating in the familiar Microsoft Excel environment. However, within this group, the performance of Excel is often compromised, particularly in terms of the number of compounds which can usefully be stored on a sheet. Summary: LICSS is a lightweight chemical spreadsheet within Microsoft Excel for Windows. LICSS stores structures solely as Smiles strings. Chemical operations are carried out by calling Java code modules which use the CDK, JChemPaint and OPSIN libraries to provide cheminformatics functionality. Compounds in sheets or charts may be visualised (individually or en masse), and sheets may be searched by substructure or similarity. All the molecular descriptors available in CDK may be calculated for compounds (in batch or on-the-fly), and various cheminformatic operations such as fingerprint calculation, Sammon mapping, clustering and R group table creation may be carried out. We detail here the features of LICSS and how they are implemented. We also explain the design criteria, particularly in terms of potential corporate use, which led to this particular implementation. Conclusions: LICSS is an Excel-based chemical spreadsheet with a difference: • It can usefully be used on sheets containing hundreds of thousands of compounds; it doesn't compromise the normal performance of Microsoft Excel • It is designed to be installed and run in environments in which users do not have admin privileges; installation involves merely file copying, and sharing of LICSS sheets invokes automatic installation • It is free and extensible LICSS is open source software and we hope sufficient detail is provided here to enable developers to add their own features and share with the community. © 2012 Lawson and Lawson; licensee Chemistry Central Ltd.",,,2-s2.0-84862683650
"Alzahrani S., Palade V., Salim N., Abraham A.","Using structural information and citation evidence to detect significant plagiarism cases in scientific publications",2012,"Journal of the American Society for Information Science and Technology",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857361390&doi=10.1002%2fasi.21651&partnerID=40&md5=865833a1a3e657cbbdc8fadc752f0356","In plagiarism detection (PD) systems, two important problems should be considered: the problem of retrieving candidate documents that are globally similar to a document q under investigation, and the problem of side-by-side comparison of q and its candidates to pinpoint plagiarized fragments in detail. In this article, the authors investigate the usage of structural information of scientific publications in both problems, and the consideration of citation evidence in the second problem. Three statistical measures namely Inverse Generic Class Frequency, Spread, and Depth are introduced to assign a degree of importance (i.e., weight) to structural components in scientific articles. A term-weighting scheme is adjusted to incorporate component-weight factors, which is used to improve the retrieval of potential sources of plagiarism. A plagiarism screening process is applied based on a measure of resemblance, in which component-weight factors are exploited to ignore less or nonsignificant plagiarism cases. Using the notion of citation evidence, parts with proper citation evidence are excluded, and remaining cases are suspected and used to calculate the similarity index. The authors compare their approach to two flat-based baselines, TF-IDF weighting with a Cosine coefficient, and shingling with a Jaccard coefficient. In both baselines, they use different comparison units with overlapping measures for plagiarism screening. They conducted extensive experiments using a dataset of 15,412 documents divided into 8,657 source publications and 6,755 suspicious queries, which included 18,147 plagiarism cases inserted automatically. Component-weight factors are assessed using precision, recall, and F -measure averaged over a 10- fold cross-validation and compared using the ANOVA statistical test. Results from structural-based candidate retrieval and plagiarism detection are evaluated statistically against the flat baselines using paired-t tests on 10-fold cross-validation runs, which demonstrate the efficacy achieved by the proposed framework. An empirical study on the system's response shows that structural information, unlike existing plagiarism detectors, helps to flag significant plagiarism cases, improve the similarity index, and provide human-like plagiarism screening results. © 2011 ASIS&T.",,"10-fold cross-validation; Comparison unit; Cosine coefficient; Cross validation; Data sets; Empirical studies; Generic class; Jaccard coefficients; Plagiarism detection; Potential sources; Scientific articles; Scientific publications; Screening process; Similarity indices; Statistical measures; Structural component; Structural information; TF-IDF weighting; Detectors; Publishing; Statistical tests; Intellectual property",2-s2.0-84857361390
"Pérez-Castillo R., García-Rodríguez De Guzmán I., Piattini M., Places A.S.","A case study on business process recovery using an e-government system",2012,"Software - Practice and Experience",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856262817&doi=10.1002%2fspe.1057&partnerID=40&md5=60e2ef6e19c923129552074ed9657d88","Business processes have become one of the key assets of organization, since these processes allow them to discover and control what occurs in their environments, with information systems automating most of an organization's processes. Unfortunately, and as a result of uncontrolled maintenance, information systems age over time until it is necessary to replace them with new and modernized systems. However, while systems are aging, meaningful business knowledge that is not present in any of the organization's other assets gradually becomes embedded in them. The preservation of this knowledge through the recovery of the underlying business processes is, therefore, a critical problem. This paper provides, as a solution to the aforementioned problem, a model-driven procedure for recovering business processes from legacy information systems. The procedure proposes a set of models at different abstraction levels, along with the model transformations between them. The paper also provides a supporting tool, which facilitates its adoption. Moreover, a real-life case study concerning an e-government system applies the proposed recovery procedure to validate its effectiveness and efficiency. The case study was carried out by following a formal protocol to improve its rigor and replicability. Copyright © 2011 John Wiley & Sons, Ltd.","ADM; business process mining; case study; legacy system; model transformation","Abstraction level; Business knowledge; Business Process; business process mining; Critical problems; E-government systems; Legacy information systems; model transformation; Model-driven; Recovery procedure; Replicability; Supporting tool; Delta modulation; Embedded systems; Government data processing; Information systems; Recovery; Research; Legacy systems",2-s2.0-84856262817
"Mouheb D., Alhadidi D., Nouh M., Debbabi M., Wang L., Pourzandi M.","Aspect weaving in UML activity diagrams: A semantic and algorithmic framework",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862915063&doi=10.1007%2f978-3-642-27269-1_11&partnerID=40&md5=fb8adfa05b788a80b8a545169fd6a700","Aspect-Oriented Modeling (AOM) is an emerging solution for handling crosscutting concerns at the software modeling level in order to reduce the complexity of software models and application code. Most existing work on weaving aspects into UML design models is presented from a practical perspective and lacks formal syntax and semantics. In this paper, we propose formal specifications for aspect weaving into UML activity diagrams and the implementation strategies of the proposed weaving semantics. To this end, we define syntax for activity diagrams and UML aspects. We also show the correctness and the completeness of the matching and the weaving processes in terms of the semantics and the algorithms provided in this paper. Finally, we demonstrate the viability and the relevance of our propositions using a case study. © 2012 Springer-Verlag.","Aspect-Oriented Modeling (AOM); Operational Semantics; UML Activity Diagram; Weaving","Activity diagram; Algorithmic framework; Application codes; Aspect weaving; Aspect-Oriented Modeling; Crosscutting concern; Formal Specification; Implementation strategies; Operational semantics; Software model; Software modeling; UML activity diagrams; UML design; Algorithms; Computer programming languages; Computer software; Parallel processing systems; Semantics; Syntactics; Systems analysis; Weaving",2-s2.0-84862915063
"Kwak M., Leroy G., Kim M.","Development and evaluation of a triple parser to enable visual searching with a biomedical search engine",2012,"International Journal of Biomedical Engineering and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945726550&doi=10.1504%2fIJBET.2012.052398&partnerID=40&md5=b60d5e78e4621c00eb61ada74eab53af","We describe a new biomedical search engine that enables visual searching and utilises predicates instead of phrases. We report on the development and evaluation of the triple parser, the most essential component of the search engine, which extracts the necessary predicates from the biomedical text. Using texts from three biomedical related sites (N = 180), we compared the parser’s output with a gold standard independently created by a medical expert. The parser achieved more than 91% precision and recall. Its individual components showed different strengths with Finite State Automata being excellent for achieving high recall, while Support Vector Machines improved the precision. © 2012 Inderscience Enterprises Ltd.","finite state automata; kernel methods; search engine; support vector machines; text mining; triple",,2-s2.0-84945726550
"Kwak M., Leroy G., Kim M.","Development and evaluation of a triple parser to enable visual searching with a biomedical search engine",2012,"International Journal of Biomedical Engineering and Technology",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883796242&doi=10.1504%2fIJBET.2012.052397&partnerID=40&md5=dd0ace7fb91cec802f578c13d2af5261","We describe a new biomedical search engine that enables visual searching and utilises predicates instead of phrases. We report on the development and evaluation of the triple parser, the most essential component of the search engine, which extracts the necessary predicates from the biomedical text. Using texts from three biomedical related sites (N = 180), we compared the parser's output with a gold standard independently created by a medical expert. The parser achieved more than 91% precision and recall. Its individual components showed different strengths with Finite State Automata being excellent for achieving high recall, while Support Vector Machines improved the precision. Copyright © 2012 Inderscience Enterprises Ltd.","Finite State Automata; Kernel methods; Search engine; Support Vector Machines; Text mining; Triple","Data mining; Finite automata; Natural language processing systems; Support vector machines; Biomedical search engines; Biomedical text; Individual components; Kernel methods; Medical experts; Precision and recall; Text mining; Triple; Search engines; Article; finite state automata; gold standard; machine learning; medical research; search engine; statistical analysis; support vector machine",2-s2.0-84883796242
"Lee I., Jeong S., Yeo S., Moon J.","A novel method for SQL injection attack detection based on removing SQL query attribute values",2012,"Mathematical and Computer Modelling",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82755194883&doi=10.1016%2fj.mcm.2011.01.050&partnerID=40&md5=1281d41e224a6127435a9d2874c4a92e","SQL injection or SQL insertion attack is a code injection technique that exploits a security vulnerability occurring in the database layer of an application and a service. This is most often found within web pages with dynamic content. This paper proposes a very simple and effective detection method for SQL injection attacks. The method removes the value of an SQL query attribute of web pages when parameters are submitted and then compares it with a predetermined one. This method uses combined static and dynamic analysis. The experiments show that the proposed method is very effective and simple than any other methods. © 2011 Elsevier Ltd.","A combined dynamic and static method; DBMS; SQL injection attack; SQL query; Web application","Attack detection; Attribute values; Code injection; Database layer; Detection methods; Dynamic content; Security vulnerabilities; SQL injection; SQL query; Static and dynamic analysis; Static method; WEB application; Database systems; Network security; Dynamic analysis",2-s2.0-82755194883
"Chali Y., Hasan S.A.","Query-focused multi-document summarization: Automatic data annotations and supervised learning approaches",2012,"Natural Language Engineering",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84155164450&doi=10.1017%2fS1351324911000167&partnerID=40&md5=f43f55922486a9054acd6ec8a758c91e","In this paper, we apply different supervised learning techniques to build query-focused multi-document summarization systems, where the task is to produce automatic summaries in response to a given query or specific information request stated by the user. A huge amount of labeled data is a prerequisite for supervised training. It is expensive and time-consuming when humans perform the labeling task manually. Automatic labeling can be a good remedy to this problem. We employ five different automatic annotation techniques to build extracts from human abstracts using ROUGE, Basic Element overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel. The supervised methods we use are Support Vector Machines, Conditional Random Fields, Hidden Markov Models, Maximum Entropy, and two ensemble-based approaches. During different experiments, we analyze the impact of automatic labeling methods on the performance of the applied supervised methods. To our knowledge, no other study has deeply investigated and compared the effects of using different automatic annotation techniques on different supervised learning approaches in the domain of query-focused multi-document summarization. © Copyright Cambridge University Press 2011.",,"Automatic annotation; Automatic labeling; Basic elements; Conditional random field; Data annotation; Labeled data; Maximum entropy; Multi-document summarization; Semantic similarity measures; Similarity measure; Specific information; Hidden Markov models; Natural language processing systems; Semantics; Supervised learning; Search engines",2-s2.0-84155164450
"Furui S.","Selected topics from LVCSR research for Asian languages at Tokyo tech",2012,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860603432&doi=10.1587%2ftransinf.E95.D.1182&partnerID=40&md5=457672363e285695c807e449c1d3ea70","This paper presents our recent work in regard to building Large Vocabulary Continuous Speech Recognition (LVCSR) systems for the Thai, Indonesian, and Chinese languages. For Thai, since there is no word boundary in the written form, we have proposed a new method for automatically creating word-like units from a text corpus, and applied topic and speaking style adaptation to the language model to recognize spoken-style utterances. For Indonesian, we have applied proper noun-specific adaptation to acoustic modeling, and rule-based English-to- Indonesian phoneme mapping to solve the problem of large variation in proper noun and English word pronunciation in a spoken-query information retrieval system. In spoken Chinese, long organization names are frequently abbreviated, and abbreviated utterances cannot be recognized if the abbreviations are not included in the dictionary. We have proposed a new method for automatically generating Chinese abbreviations, and by expanding the vocabulary using the generated abbreviations, we have significantly improved the performance of spoken query-based search. Copyright © 2012 The Institute of Electronics, Information and Communication Engineers.","Asian languages; Automatic speech recognition; LVCSR; Spoken query","Character recognition; Continuous speech recognition; Deep neural networks; Search engines; Asian languages; Automatic speech recognition; Large vocabulary continuous speech recognition; LVCSR; Phoneme mappings; Query information; Specific adaptations; Spoken query; Speech recognition",2-s2.0-84860603432
"Hijazi I.H., Ehlers M., Zlatanova S.","Nibu: A new approach to representing and analysing interior utility networks within 3D geo-information systems",2012,"International Journal of Digital Earth",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859485048&doi=10.1080%2f17538947.2011.564661&partnerID=40&md5=ef5e67a306e6725aea72919fdab2f752","Facility management departments' responsibilities include monitoring and maintenance of building infrastructure, such as water, gas or electricity. Very often these tasks are completed using paper maps, which make integrated analysis of networks challenging. Ability to consider interior network structure and provide semantic and connectivity information supporting the required analysis operations are thus crucial. This paper presents an approach relying on Building Information Model (BIM) as a data source for obtaining information about interior utilities. The semantic and connectivity information of BIM is mapped onto a new model called Network for Interior Building Utilities (NIBU). NIBU is based on the semantic categorisation of utilities, and the spatial functions that have to be performed. Three scenarios ('maintenance operation', 'emergency response' and 'inspection operation') are developed to test the proposed approach. The model and its functions are implemented in spatial DBMS. The model is populated directly from a BIM server applying an Industrial Foundation Class (IFC) parser developed in-house. Five analysis functions are implemented to support spatial operations: trace upstream, trace downstream, find ancestors, find source and find disconnected. The investigation proves that BIM provides both the required semantics and attributes, and connectivity information that can facilitate analysis of interior utility networks. NIBU provides a simple yet flexible way to manage interior network information, which can be integrated into Digital Earth. © 2012 Taylor & Francis.","Building information model (bim); Building service system; Graph; Industrial foundation class (ifc); Spatial dbms; Utility network",,2-s2.0-84859485048
"Jarada T.N., Elsheikh A.M., Naser T., Chung K., Shimoon A., Karampelas P., Rokne J., Ridley M., Alhajj R.","Rules for effective mapping between two data environments: Object database language and XML",2012,"Recent Trends in Information Reuse and Integration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948098169&doi=10.1007%2f978-3-7091-0738-6_7&partnerID=40&md5=9880a16a38b58fba56acc31e81e9120a","The rapid development in automated communication and the diversity of computing platforms necessitated and motivated for the development of platform independent data format that could smoothly provide for portability and extensibility. Intensive research efforts over the past two decades have produced XML as the de facto standard for platform independent sharing of data which is the most valuable commodity for maintaining successful and competitive performance. Data is the most valuable source of knowledge. Once data is acquired, it can be queried to retrieve explicit content and it can be mined to extract and predict implicit content. XML has been embraced as a data model mainly due to its simplicity, readability, and portability, i.e., its ability to be transported over well established protocols, such as HTTP. XML is extremely similar to HTML in structure, making it an ideal data format to be used in conjunction with HTTP. Furthermore, HTML parsers can be easily adapted for dealing with XML data. However, XML serves a purpose different from that of HTML. While the latter is intended for data formatting, the former specifies and describes structure and context for the data by allowing the user to decide on his/her own tags, structure, nesting, etc. Finally, XML documents can be highly structured, based on an accompanying XML Schema. © 2012 Springer-Verlag/Wien.",,"Computer software portability; HTML; HTTP; Object-oriented databases; Competitive performance; Computing platform; Data environment; Data formatting; Database language; De facto standard; Intensive research; Platform independent; XML",2-s2.0-84948098169
"Bhattacharya S., Toldo L.","Question answering for Alzheimer disease using information retrieval",2012,"CEUR Workshop Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922023902&partnerID=40&md5=b7c3c0ce3717605b7600e3bba527fc3e","With the tremendous growth of biomedical literature and data, it's no longer feasible for researchers to manually sift through this information for answering questions on specific topics. The ""Machine Reading of Biomedical Texts about Alzheimer Disease"" task of CLEF QA4MRE encouraged the development of systems that can automatically find answers to questions on Alzheimer disease. To this end, we developed several information retrieval(IR) and semantic web-based strategies. Our best performing strategy used a combination of query processing followed by IR on the background corpus, distributed by the organizers, to find correct answers. Using our systems, the highest cumulative and individual c@1 scores achieved were 0.47 and 0.66 respectively.","Information retrieval; Question answering; Semantic web","Neurodegenerative diseases; Semantic Web; Social networking (online); Alzheimer disease; Biomedical literature; Biomedical text; Question Answering; Information retrieval",2-s2.0-84922023902
"Srinivasan A., Faruquie T.A., Joshi S.","Data and task parallelism in ILP using MapReduce",2012,"Machine Learning",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855678613&doi=10.1007%2fs10994-011-5245-8&partnerID=40&md5=6db6d5d3777cdfb00bee568aa17c2cd2","Nearly two decades of research in the area of Inductive Logic Programming (ILP) have seen steady progress in clarifying its theoretical foundations and regular demonstrations of its applicability to complex problems in very diverse domains. These results are necessary, but not sufficient, for ILP to be adopted as a tool for data analysis in an era of very large machine-generated scientific and industrial datasets, accompanied by programs that provide ready access to complex relational information in machine-readable forms (ontologies, parsers, and so on). Besides the usual issues about the ease of use, ILP is now confronted with questions of implementation. We are concerned here with two of these, namely: can an ILP system construct models efficiently when (a) Dataset sizes are too large to fit in the memory of a single machine; and (b) Search space sizes becomes prohibitively large to explore using a single machine. In this paper, we examine the applicability to ILP of a popular distributed computing approach that provides a uniform way for performing data and task parallel computations in ILP. The MapReduce programming model allows, in principle, very large numbers of processors to be used without any special understanding of the underlying hardware or software involved. Specifically, we show how the MapReduce approach can be used to perform the coverage-test that is at the heart of many ILP systems, and to perform multiple searches required by a greedy set-covering algorithm used by some popular ILP systems. Our principal findings with synthetic and real-world datasets for both data and task parallelism are these: (a) Ignoring overheads, the time to perform the computations concurrently increases with the size of the dataset for data parallelism and with the size of the search space for task parallelism. For data parallelism this increase is roughly in proportion to increases in dataset size; (b) If a MapReduce implementation is used as part of an ILP system, then benefits for data parallelism can only be expected above some minimal dataset size, and for task parallelism can only be expected above some minimal search-space size; and (c) The MapReduce approach appears better suited to exploit data-parallelism in ILP. © 2011 The Author(s).","ILP; MapReduce; Parallelism; Scaling-up","Complex problems; Construct models; Data parallelism; Data set size; Data sets; Diverse domains; Ease of use; ILP; Machine readable form; Map-reduce; Multiple search; Parallelism; Programming models; Real-world datasets; Scaling-up; Search space size; Search spaces; Set coverings; Task parallel; Task parallelism; Theoretical foundations; Computer programming; Ontology; Search engines; Inductive logic programming (ILP)",2-s2.0-84855678613
"Attardi G., Atzori L., Simi M.","Index expansion for Machine Reading and Question Answering",2012,"CEUR Workshop Proceedings",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922022080&partnerID=40&md5=4b6b0bb4a2b68370ebe17557bca68e10","The paper reports our experiments in tackling the CLEF 2012 Pilot Task on Machine Reading for Question Answering. We introduce the technique of index expansion, which relies on building a search index enriched with in-formation gathered from a linguistic analysis of texts. The index provides a highly tangled representation of the sentences where each word is directly con-nected to others representing both meaning and relations. Instead of keeping the knowledge base separate, the relevant knowledge gets embedded within the text. We can hence use efficient indexing techniques to represent such knowledge and query it very effectively. We explain how index expansion was used in the task and describe the experiments that we performed. The results achieved are quite positive and a final error analysis shows how the technique can be further improved.","Index expansion; Information retrieval; Machine Reading; Question Answering","Information retrieval; Knowledge based systems; Linguistics; Indexing techniques; Knowledge base; Linguistic analysis; On-machines; Pilot tasks; Question Answering; Expansion",2-s2.0-84922022080
"Tian F., Yuan C., Ren F.","Hyponym extraction from the web by bootstrapping",2012,"IEEJ Transactions on Electrical and Electronic Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83155192867&doi=10.1002%2ftee.21696&partnerID=40&md5=2360a7455da9df184804447692476e63","This paper proposes an effective method to automatically extract hyponym from the Web for Chinese. The method extracts hyponyms for a given hypernym through weak supervision in two stages: the first stage is submitting a hypernym and a seed hyponym as a query to Web search engine, and automatically extracting hyponyms matching with a Chinese doubly anchored hyponymy pattern from the Web by bootstrapping. In order to reduce noise data in bootstrapping extraction, we propose a set of filtering rules to ensure matching of the proper hypernym in the extracted sentence. The second stage is ranking all the extracted candidate hyponyms by an integrated ranking algorithm which takes into account measures both of linkage frequency between coordinate hyponyms and of semantic similarity between the hypernym and candidate hyponym based on co-occurrence statistics. © 2011 Institute of Electrical Engineers of Japan.","Bootstrapping; Hypernym; Hyponym; Hyponym acquisition; Web search engine","Information retrieval; Natural language processing systems; Search engines; Semantics; Bootstrapping; Co-occurrence statistics; Filtering rules; Hypernym; Hyponym; Hyponym acquisition; Hyponyms; Hyponymy; Integrated ranking; Noise data; Semantic similarity; Two stage; World Wide Web",2-s2.0-83155192867
"Dimitrios G., Isabella K., Athanasios M.","Producing content semantics to enhance mobile learners browsing usability",2012,"Proceedings of the IADIS International Conference Mobile Learning 2012, ML 2012",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944043007&partnerID=40&md5=95ec4c0c01850f62a204310fc89aa2ed","Mobile devices are the current trend in present days. The objective of this work is to present and evaluate a methodology for producing content semantics from the learning material. The proposed approach results in recommending relevant links to the users' interests and improving the mobile learners' web experience. © 2012 IADIS.","Mobile browsing; Ontology learning; Recommendation systems","E-learning; Mobile devices; Recommender systems; Content semantics; Learning materials; Mobile browsing; Mobile-learners; Ontology learning; Users' interests; Web experiences; Semantics",2-s2.0-84944043007
"Mithun S., Kosseim L.","A dependency relation-based method to identify attributive relations and its application in text summarization",2012,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893322347&partnerID=40&md5=6d18af17e4b1b0f30665bf560cf3bcdd","In this paper, we propose a domain and genre-independent approach to identify the discourse relation called attributive, included in Grimes' relation list [7]. An attributive relation provides details about an entity or an event or can be used to illustrate a particular feature about a concept or an entity. Since attributive relations describe attributes or features of an object or an event, they are often used in text summarization (e.g. [2]) and question answering systems (e.g. [12]). However, to our knowledge, no previous work has focused on tagging attributive relations automatically. We propose an automatic domain and genre-independent approach to tag attributive relations by utilizing dependency relations of words based on dependency grammars [3]. In this paper, we also show how attributive relations can be utilized in text summarization. By using a subset of the BLOG061 corpus, we have evaluated the accuracy of our attributive classifier and compared it to a baseline and human performance using precision, recall, and F-Measure. The evaluation results show that our approach compares favorably with human performance.",,"Natural language processing systems; Dependency grammar; Dependency relation; Evaluation results; Human performance; ITS applications; Question answering systems; Relation-based; Text summarization; Text processing",2-s2.0-84893322347
"Morante R., Krallinger M., Valencia A., Daelemans W.","Machine reading of biomedical texts about Alzheimer's disease",2012,"CEUR Workshop Proceedings",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922051494&partnerID=40&md5=d0656b6003944bfa0ecd92bbcbc33432","This report describes the task Machine reading of biomedical texts about Alzheimer's disease, which is a pilot task of the Question Answering for Machine Reading Evaluation (QA4MRE) Lab at CLEF 2012. The task aims at exploring the ability of a machine reading system to answer questions about a scientific topic, namely Alzheimer's disease. As in the QA4MRE task, participant systems were asked to read a document and identify the answers to a set of questions about information that is stated or implied in the text. A background collection was provided for systems to acquire background knowledge. The background collection is a corpus newly compiled for this task, the Alzheimer's Disease Literature Corpus. Seven teams participated in the task submitting a total of 43 runs. The highest score obtained by a team was 0.55 c@1, which is clearly above baseline.",,"Alzheimer's disease; Back-ground knowledge; Biomedical text; Pilot tasks; Question Answering; Set of questions; Neurodegenerative diseases",2-s2.0-84922051494
"Cadilhac A., Asher N., Benamara F., Popescu V., Seck M.","Preference extraction from negotiation dialogues",2012,"Frontiers in Artificial Intelligence and Applications",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878772907&doi=10.3233%2f978-1-61499-098-7-211&partnerID=40&md5=f394a051b06172707381f5af4071163a","This paper presents an NLP-based approach to extracting preferences from negotiation dialogues. We propose a new annotation scheme to study how preferences are linguistically expressed on two different corpus genres. We then automatically extract preferences in two steps: first, we extract the set of outcomes; then, we identify how these outcomes are ordered. We finally assess the reliability of our method on each corpus genre. © 2012 The Author(s).",,"Annotation scheme; Preference extractions; Artificial intelligence",2-s2.0-84878772907
"Saluja A., Lane I., Zhang Y.","Machine translation with binary feedback: A large-margin approach",2012,"AMTA 2012 - Proceedings of the 10th Conference of the Association for Machine Translation in the Americas",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911950489&partnerID=40&md5=77631bdf2a8254f8a569b40b19c142fe","Viewing machine translation as a structured classification problem has provided a gateway for a host of structured prediction techniques to enter the field. In particular, large-margin structured prediction methods for discriminative training of feature weights, such as the structured perceptron or MIRA, have started to match or exceed the performance of existing methods such as MERT. One issue with structured problems in general is the difficulty in obtaining fully structured labels, e.g., in machine translation, obtaining reference translations or parallel sentence corpora for arbitrary language pairs. Another issue, more specific to the translation domain, is the difficulty in online training of machine translation systems, since existing methods often require bilingual knowledge to correct translation output online. We propose a solution to these two problems, by demonstrating a way to incorporate binary-labeled feedback (i.e., feedback on whether a translation hypothesis is a ""good"" or understandable one or not), a form of supervision that can be easily integrated in an online manner, into a machine translation framework. Experimental results show marked improvement by incorporating binary feedback on unseen test data, with gains exceeding 5.5 BLEU points.",,"Bins; Computational linguistics; Online systems; Arbitrary languages; Binary feedback; Discriminative training; Machine translation systems; Machine translations; Online training; Structured prediction; Structured problems; Computer aided language translation",2-s2.0-84911950489
"Angeli G., Manning C.D., Jurafsky D.","Parsing time: Learning to interpret time expressions",2012,"NAACL HLT 2012 - 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890490115&partnerID=40&md5=f1c777d710b8bf0bbc0646858168c787","We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modification in time expressions motivates our use of a compositional grammar of time expressions. This grammar is used to construct a latent parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. We achieve an accuracy of 72% on an adapted TempEval-2 task - comparable to state of the art systems. © 2012 Association for Computational Linguistics.",,"Computational linguistics; Syntactics; Linear patterns; Probabilistic approaches; Probabilistic framework; Regular expressions; State-of-the-art system; Formal languages",2-s2.0-84890490115
"Faro A., Giordano D., Spampinato C.","Combining literature text mining with microarray data: Advances for system biology modeling",2012,"Briefings in Bioinformatics",44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855710474&doi=10.1093%2fbib%2fbbr018&partnerID=40&md5=cab5b1bb13f16d86c712a0c5f49e4895","A huge amount of important biomedical information is hidden in the bulk of research articles in biomedical fields. At the same time, the publication of databases of biological information and of experimental datasets generated by high-throughput methods is in great expansion, and a wealth of annotated gene databases, chemical, genomic (including microarray datasets), clinical and other types of data repositories are now available on the Web. Thus a current challenge of bioinformatics is to develop targeted methods and tools that integrate scientific literature, biological databases and experimental data for reducing the time of database curation and for accessing evidence, either in the literature or in the datasets, useful for the analysis at hand. Under this scenario, this article reviews the knowledge discovery systems that fuse information from the literature, gathered by text mining, with microarray data for enriching the lists of down and upregulated genes with elements for biological understanding and for generating and validating new biological hypothesis. Finally, an easy to use and freely accessible tool, GeneWizard, that exploits text mining and microarray data fusion for supporting researchers in discovering gene-disease relationships is described. © The Author 2011.","Biological databases; Knowledge discovery; Literature text mining; Microarray data","article; biology; data mining; factual database; genomics; Medline; methodology; microarray analysis; systems biology; Computational Biology; Data Mining; Databases, Factual; Genomics; MEDLINE; Microarray Analysis; Systems Biology",2-s2.0-84855710474
"De Silva L., Balasubramaniam D.","Controlling software architecture erosion: A survey",2012,"Journal of Systems and Software",60,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755136651&doi=10.1016%2fj.jss.2011.07.036&partnerID=40&md5=a2e50361d326ce03280b6b01f0c59c83","Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented. We discuss the merits and weaknesses of each strategy and argue that no single strategy can address the problem of erosion. Further, we explore the possibility of combining strategies and present a case for further work in developing a holistic framework for controlling architecture erosion. © 2011 Elsevier Inc.","Architecture erosion; Controlling architecture erosion; Design erosion; Software architecture; Software decay; Survey","Architectural principles; Architecture designs; Architecture erosion; Broken down; Business conditions; Controlling architecture erosion; Design constraints; Design erosion; Process-oriented architecture; Restoration techniques; Self adaptation; Software adaptation; Software architects; Software decay; Software systems; Useful lifetime; Design; Erosion; Restoration; Surveys; Software architecture",2-s2.0-80755136651
"Wu Z., Chen H., Jiang X.","Modern Computational Approaches to Traditional Chinese Medicine",2012,"Modern Computational Approaches to Traditional Chinese Medicine",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013843725&doi=10.1016%2fC2011-0-09058-0&partnerID=40&md5=72efd374a80b6520c16fbf4f3e47df62","Recognized as an essential component of Chinese culture, Traditional Chinese Medicine (TCM) is both an ancient medical system and one still used widely in China today. TCM's independently evolved knowledge system is expressed mainly in the Chinese language and the information is frequently only available through ancient classics and confidential family records, making it difficult to utilize. The major concern in TCM is how to consolidate and integrate the data, enabling efficient retrieval and discovery of novel knowledge from the dispersed data. Computational approaches such as data mining, semantic reasoning and computational intelligence have emerged as innovative approaches for the reservation and utilization of this knowledge system. Typically, this requires an inter-disciplinary approach involving Chinese culture, computer science, modern healthcare and life sciences. This book examines the computerization of TCM information and knowledge to provide intelligent resources and supporting evidences for clinical decision-making, drug discovery, and education. Recent research results from the Traditional Chinese Medicine Informatics Group of Zhejiang University are presented, gathering in one resource systematic approaches for massive data processing in TCM. These include the utilization of modern Semantic Web and data mining methods for more advanced data integration, data analysis and integrative knowledge discovery. This book will appeal to medical professionals, life sciences students, computer scientists, and those interested in integrative, complementary, and alternative medicine. Interdisciplinary book bringing together Traditional Chinese Medicine and computer scientists Introduces novel network technologies to Traditional Chinese Medicine informatics Provides theory and practical examples and case studies of new techniques. © 2012 Zhejiang University Press Co., Ltd. Published by Elsevier Inc. All rights reserved.",,,2-s2.0-85013843725
"Shatkay H., Craven M.","Mining the Biomedical Literature",2012,"Mining the Biomedical Literature",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888150250&partnerID=40&md5=76c5dcf3f79d8468aaec6c193b5e8317","A concise introduction to fundamental methods for finding and extracting relevant information from the ever-increasing amounts of biomedical text available. © 2012 Massachusetts Institute of Technology. All rights reserved.",,,2-s2.0-84888150250
"Renals S.","Multimodal signal processing: Human interactions in meetings",2012,"Multimodal Signal Processing: Human Interactions in Meetings",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924139705&doi=10.1017%2fCBO9781139136310&partnerID=40&md5=b09547e89444ffd8a5e9e224d13d3716","Bringing together experts in multimodal signal processing, this book provides a detailed introduction to the area, with a focus on the analysis, recognition and interpretation of human communication. The technology described has powerful applications. For instance, automatic analysis of the outputs of cameras and microphones in a meeting can make sense of what is happening - who spoke, what they said, whether there was an active discussion and who was dominant in it. These analyses are layered to move from basic interpretations of the signals to richer semantic information. The book covers the necessary analyses in a tutorial manner, going from basic ideas to recent research results. It includes chapters on advanced speech processing and computer vision technologies, language understanding, interaction modeling and abstraction, as well as meeting support technology. This guide connects fundamental research with a wide range of prototype applications to support and analyze group interactions in meetings. © Cambridge University Press 2012.",,"Modeling languages; Semantics; Speech processing; Computer vision technology; Fundamental research; Human communications; Human interactions; Language understanding; Meeting support technologies; Multi-modal signal processing; Semantic information; Signal processing",2-s2.0-84924139705
"Lassalle E., Denis P.","Leveraging different meronym discovery methods for bridging resolution in French",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455203151&doi=10.1007%2f978-3-642-25917-3_4&partnerID=40&md5=bd133e6c108244457875c54962a9ea61","This paper presents a statistical system for resolving bridging descriptions in French, a language for which current lexical resources have a very low coverage. The system is similar to that developed for English by [22], but it was enriched to integrate meronymic information extracted automatically from both web queries and raw text using syntactic patterns. Through various experiments on the DEDE corpus [8], we show that although still mediocre the performance of our system compare favorably to those obtained by [22] for English. In addition, our evaluation indicates that the different meronym extraction methods have a cumulative effect but that the text pattern-based extraction method is more robust and leads to higher accuracy than the Web-based approach. © 2011 Springer-Verlag Berlin Heidelberg.","bridging anaphora resolution; relation extraction; syntactic patterns","Anaphora resolution; Cumulative effects; Extraction method; Lexical resources; Relation extraction; Statistical systems; syntactic patterns; Web-based approach; Artificial intelligence; Syntactics",2-s2.0-84455203151
"Sinkovics A.","Nested lambda expressions with let expressions in C++ template metaprograms",2011,"Electronic Notes in Theoretical Computer Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855716612&doi=10.1016%2fj.entcs.2011.11.036&partnerID=40&md5=8c58380a587cb2d71037b0e6cb8c54a2","More and more C++ applications use template metaprograms directly or indirectly by using libraries based on them. Since C++ template metaprograms follow the functional paradigm, the well known and widely used tools of functional programming should be available for developers of C++ template metaprograms as well. Many functional languages support let expressions to bind expressions to names locally. It simplifies the source code, reduces duplications and avoids the pollution of the namespace or namespaces. In this paper we present how let expressions can be introduced in C++ template metaprograms. We also show how let expressions can be used to implement lambda expressions. The Boost Metaprogramming Library provides lambda expressions for template metaprograms, we show their limitations with nested lambda expressions and how our implementation can handle those cases as well. © 2011 Elsevier B.V. All rights reserved.","boost::mpl; C++; functional programming; template metaprogramming","boost::mpl; C++; C++ templates; Functional languages; Meta Programming; Metaprograms; Namespaces; Source codes; Template metaprogramming; Computer applications; Functional programming",2-s2.0-84855716612
"Liu X., Xu J., Li F.","Domain-specific ontology construction from hierarchy web documents",2011,"Proceedings - 7th International Conference on Semantics, Knowledge, and Grids, SKG 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84055213226&doi=10.1109%2fSKG.2011.21&partnerID=40&md5=2324a06d9b6c8d656f3feb40a56d4d38","Ontology, the base of Semantic Web, plays a vital role in knowledge representation and knowledge reasoning. There are many tools providing management interfaces to create, query and edit knowledge of ontology. However, most of them are still arduous time-consuming manual work. Therefore, how to automatically construct ontology has attracted many researchers' attention. With the development of WEB, there is influent information we can take advantage of. According to the structure of Web, An automatically construction method for domain-specific ontology is proposed. Firstly, some special web sites which have relatively structure or semi-structure are selected. Then, web documents are clawed with Jsoap. Secondly, all contained knowledge is extracted and organized together to form the domain-specific ontology. Finally, the Jena platform is employed to create, delete, read, write ontology model in the form of RDF and query by SPARQL. The experimental results in ""Entertainment"" and ""Sport"" domain show concept hierarchy structures are reasonable with the overall precision of domain-specific ontology being up to 97%. © 2011 IEEE.",,"Concept hierarchies; Construction method; Domain-specific ontologies; Knowledge reasoning; Management interfaces; Manual work; Ontology model; Web document; Knowledge representation; Semantic Web; World Wide Web; Ontology",2-s2.0-84055213226
"Kraas A.","A model-based formalization of the textual notation for SDL-UML",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83755187964&doi=10.1007%2f978-3-642-25264-8_17&partnerID=40&md5=de29e575394db96bc41e910371dc6194","The Specification and Description Language (SDL) is a domain specific language that is well-established in the telecommunication sector since many years, but only a small set of SDL tools is available. In contrast, for the Unified Modeling Language (UML) a wide range of different kinds of tools can be used for various purposes, such as model transformation. In order to makes it possible to specify SDL compliant models with UML, a profile for the combined use of SDL and UML was standardized. This profile also embraces a textual notation for the action language of SDL-UML, which is a subset of the concrete syntax of SDL. Unfortunately, a formal specification of that textual notation is not specified. In order to remedy this gap, in this paper, a model-based approach for the formalization of the textual notation for SDL-UML is presented. © 2011 Springer-Verlag.","Formalization; Profile; SDL-UML; Textual Notation","Action language; Compliant model; Concrete syntax; Domain specific languages; Formal Specification; Formalization; Model based approach; Model transformation; Profile; SDL; SDL-UML; Specification and description languages; Telecommunication sector; Textual notation; Specifications; Systems analysis; Unified Modeling Language",2-s2.0-83755187964
"Dede E., Fadika Z., Gupta C., Govindaraju M.","Scalable and distributed processing of scientific XML data",2011,"Proceedings - 2011 12th IEEE/ACM International Conference on Grid Computing, Grid 2011",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455248630&doi=10.1109%2fGrid.2011.24&partnerID=40&md5=2605f1a421f89aff6eaa42c9b71a27a5","A seamless and intuitive search capability for the vast amount of datasets generated by scientific experiments is critical to ensure effective use of such data by domain specific scientists. Currently, searches on enormous XML datasets is done manually via custom scripts or by using hard-to-customize queries developed by experts in complex and disparate XML query languages. Such approaches however do not provide acceptable performance for large-scale data since they are not based on a scalable distributed solution. Furthermore, it has been shown that databases are not optimized for queries on XML data generated by scientific experiments, as term kinship, range based queries, and constraints such as conjunction and negation need to be taken into account. There exists a critical need for an easy-to-use and scalable framework, specialized for scientific data, that provides natural-language-like syntax along with accurate results. As most existing search tools are designed for exact string matching, which is not adequate for scientific needs, we believe that such a framework will enhance the productivity and quality of scientific research by the data reduction capabilities it can provide. This paper presents how the MapReduce model should be used in XML metadata indexing for scientific datasets, specifically TeraGrid Information Services and the NeXus datasets generated by the Spallation Neutron Source (SNS) scientists. We present an indexing structure that scales well for large-scale MapReduce processing. We present performance results using two MapReduce implementations, Apache Hadoop and LEMO-MR, to emphasize the flexibility and adaptability of our framework in different MapReduce environments. © 2011 IEEE.",,"Data sets; Distributed processing; Distributed solutions; Domain specific; Indexing structures; Map-reduce; Range-based; Scientific data; Scientific researches; Search capabilities; Search tools; Spallation neutron sources; String matching; TeraGrid; XML data; XML metadata; XML query language; Computer software; Data reduction; Experiments; Indexing (materials working); Indexing (of information); Information services; Metadata; Neutron sources; Query languages; XML; Grid computing",2-s2.0-83455248630
"Kolomiyets O., Moens M.-F.","A survey on question answering technology from an information retrieval perspective",2011,"Information Sciences",41,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054683036&doi=10.1016%2fj.ins.2011.07.047&partnerID=40&md5=0ed3ac55a91f1af2625a0754d95db5bf","This article provides a comprehensive and comparative overview of question answering technology. It presents the question answering task from an information retrieval perspective and emphasises the importance of retrieval models, i.e., representations of queries and information documents, and retrieval functions which are used for estimating the relevance between a query and an answer candidate. The survey suggests a general question answering architecture that steadily increases the complexity of the representation level of questions and information objects. On the one hand, natural language queries are reduced to keyword-based searches, on the other hand, knowledge bases are queried with structured or logical queries obtained from the natural language questions, and answers are obtained through reasoning. We discuss different levels of processing yielding bag-of-words-based and more complex representations integrating part-of-speech tags, classification of the expected answer type, semantic roles, discourse analysis, translation into a SQL-like language and logical representations. © 2011 Elsevier Inc. All rights reserved.","Information retrieval; Natural language interfaces; Question answering; Retrieval and ranking models","Discourse analysis; Information object; Keyword-based search; Knowledge basis; Logical representations; Natural language interfaces; Natural language queries; Natural language questions; Part-of-speech tags; Question Answering; Question Answering Task; Question answering technology; Ranking model; Retrieval models; Semantic roles; Natural language processing systems; Query processing; Semantics; Surveys; Information retrieval",2-s2.0-80054683036
"Choi J., Kim D., Kim S., Lee S., Lee K., Kang J.","BOSS: A biomedical object search system",2011,"International Conference on Information and Knowledge Management, Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83255166529&doi=10.1145%2f2064696.2064702&partnerID=40&md5=237cc76dd847ba8abdf9b2c6ec914e66","It has never been easy to search relevant information from ever increasing corpus of academic literatures. Large volume of research exists concerning this problem. The previous solutions are put on either ends of spectrum: general-purpose search and domain-specific ""deep"" search systems. The general-purpose search systems such as PubMed offer flexible query interface, but churn out a list of matching documents that users have to digest in order to find the answers to their queries. On the other hand, the ""deep"" search systems such as PPI Finder and iHOP return the precompiled results in a structured way. Their results, however, are often found only within some predefined contexts. In order to address this problem, we introduce a new search engine, BOSS, for search on biomedical objects. Unlike the conventional search systems, BOSS indexes segments, rather than documents. A segment refers to a minimal semantic unit such as phrase, clause or sentence that is semantically coherent in the given context (e.g., biomedical objects or their relations). For a user query, BOSS finds all matching segments, identifies the objects appearing in the segments, and aggregates the segments for each object. Finally, it turns up for the user the ranked list of the objects along with their matching segments. BOSS fills the gap between either ends of the spectrum by allowing users to pose context-free queries and by returning a structured set of results. Furthermore, BOSS exhibits the characteristic of good scalability, just as with conventional document search engines, because as it is designed to use a standard document-indexing model with minimal modifications. Considering the features, BOSS is believed to notch up the technological level of traditional solutions for search on biomedical information. BOSS is accessible at http://boss.korea.ac.kr. © 2011 ACM.","biomedical object search engine; biomedical text mining; information retrieval","Academic literature; Biomedical information; Biomedical objects; Biomedical text minings; Context-free; Document search; Domain specific; Flexible queries; Search system; Semantic units; Technological level; User query; Bioinformatics; Data mining; Information retrieval; Information science; Knowledge management; Natural language processing systems; Safety devices; Semantics; Search engines",2-s2.0-83255166529
"Ottaviano G., Grossi R.","Semi-indexing semi-structured data in tiny space",2011,"International Conference on Information and Knowledge Management, Proceedings",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055191150&doi=10.1145%2f2063576.2063790&partnerID=40&md5=4290c257490d85ce124a118fd5341fd4","Semi-structured textual formats are gaining increasing popularity for the storage of document collections and rich logs. Their flexibility comes at the cost of having to load and parse a document entirely even if just a small part of it needs to be accessed. For instance, in data analytics massive collections are usually scanned sequentially, selecting a small number of attributes from each document. We propose a technique to attach to a raw, unparsed document (even in compressed form) a ""semi-index"": a succinct data structure that supports operations on the document tree at speed comparable with an in-memory deserialized object, thus bridging textual formats with binary formats. After describing the general technique, we focus on the JSON format: our experiments show that avoiding the full loading and parsing step can give speedups of up to 12 times for on-disk documents using a small space overhead. © 2011 ACM.","semi-index; semi-structured data; succinct data structures","At-speed; Binary format; Data analytics; Document collection; Document trees; Semi structured data; semi-index; Semi-structured; Space overhead; Succinct data structure; Textual format; Data structures; Knowledge management; Trees (mathematics)",2-s2.0-83055191150
"Chenlo J.M., Losada D.E.","Effective and efficient polarity estimation in blogs based on sentence-level evidence",2011,"International Conference on Information and Knowledge Management, Proceedings",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055191921&doi=10.1145%2f2063576.2063634&partnerID=40&md5=1c0df800ae5b755cfcbb86994f9b98f6","One of the core tasks in Opinion Mining consists of estimating the polarity of the opinionated documents found. In some scenarios (e.g. blogs), this estimation is severely affected by sentences that are off-topic or that simply do not express any opinion. In fact, the key sentiments in a blog post often appear in specific locations of the text. In this paper we propose several effective and robust polarity detection methods based on different sentence features. We show that we can successfully determine the polarity of documents guided by a sentence-level analysis that takes into account topicality and the location in the blog post of the subjective sentences. Our experimental results show that some of our proposed variants are both highly effective and computationally-lightweight. © 2011 ACM.","blog retrieval; efficiency; opinion mining; polarity estimation; sentence retrieval","blog retrieval; Detection methods; Opinion mining; Polarity estimation; sentence retrieval; Specific location; Efficiency; Estimation; Information retrieval; Knowledge management; Blogs",2-s2.0-83055191921
"Mishra D., Pujari N.","Cross-domain query answering: Using Web scrapper and data integration",2011,"2011 2nd International Conference on Computer and Communication Technology, ICCCT-2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955234133&doi=10.1109%2fICCCT.2011.6075193&partnerID=40&md5=d512de3bd0fd92121fdfa28bb38b05bd","Information drives today's businesses and the internet is a powerhouse of information. So data integration gives the user with a unified view of all heterogeneous data sources. The basic service provided by data integration is query processing. But if we are considering a query that involves multiple domains, then we find that general purpose search engines fail to answer such queries and domain specific search services cover only one domain. Hence currently the only solution to this problem is to pose the query separately to dedicated services and feed the result of one as input to another. Our consideration could be drawn from the work in data integration, where two basic approaches has been proposed to specify the mapping between global ontology and a set of services, that are GAV and LAV. This paper presents a model, providing fully automated support to multi domain queries. This model (a) integrates different kind of services into a global ontology (b) covers query formulation aspects over global ontology and query rewriting in terms of local services (c) Several web related services that helps to conquer the problem and the package that we have offered here is XAMPP Control panel. © 2011 IEEE.","Cross Domain; Data and Screen Scrapper; Data integration; GAV; GLAV; LAV; Parser; Web Scrapping","Cross-domain; Data and Screen Scrapper; Data integration; GAV; GLAV; LAV; Parser; Web Scrapping; Communication; Ontology; Query processing; Search engines; Data handling",2-s2.0-82955234133
"Parhi M., Acharya B.M., Puthal B.","Discovery of sensor web registry services for WSN with Multi-layered SOA framework",2011,"2011 2nd International Conference on Computer and Communication Technology, ICCCT-2011",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955237810&doi=10.1109%2fICCCT.2011.6075183&partnerID=40&md5=7b63f5f392c57b4e094eede5a6b93473","Sensor Web Services has potential usage in a wide range of application domain. However due to advancement of OGC SWE standard, there are still challenges left that has to be addressed within the context of discovery of Sensor Web Registry services throughout heterogeneous environment which raises several concerns like performance, reliability, and robustness. Many approaches and frameworks have been proposed to discover the sensor web registry services and some of the approaches assume that the requests are placed in SOAP compatible formats while others focus on GUI based parametric query processing. In this paper an approach has been proposed that uses the Natural Language Query Processing which is a convenient and easy method of sensor data access in comparison to SQL or XML based Query Language like XQuery and XPath. An architecture based on Multi-layered SOA Framework that organizes the method of sensor web registry service discovery in an efficient and structured manner has been suggested by adding some new layers like Request Parser & Query Generator (RPQ), Service Verifier and Certifier (SVC) and Service Rank Calculator (SRC). A typical weather sensor web service discovery where RPQ facilitates the processing of plain text request query to a most appropriate weather sensor web service and also an algorithm with implementation for a complete cycle of suitable sensor web registry service discovery according to requester's functional and QoS requirements has been provided. © 2011 IEEE.","Natural Language Processing; QoS; Request Parser & Query Generator (RPQ); Sensor Web Enablement Standard (SWE); Sensor Web Registry; Service Rank Calculator (SRC); Service Verifier and Certifier (SVC); Wireless Sensor Network (WSN)","NAtural language processing; Sensor Web Enablement Standard (SWE); Sensor Web Registry; Service Rank Calculator (SRC); Service Verifier and Certifier (SVC); Wireless sensor; Communication; Computational linguistics; Mathematical instruments; Natural language processing systems; Quality of service; Query languages; Query processing; Wireless sensor networks; Web services",2-s2.0-82955237810
"Sárosi G., Mozsolics T., Tarján B., Balog A., Mihajlik P., Fegyó T.","Recognition of multiple language voice navigation queries in traffic situations",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955189897&doi=10.1007%2f978-3-642-25775-9_20&partnerID=40&md5=8bee37b40eab8ad892a702901a9cc4aa","This paper introduces our work and results related to a multiple language continuous speech recognition task. The aim was to design a system that introduces tolerable amount of recognition errors for point of interest words in voice navigational queries even in the presence of real-life traffic noise. Additional challenges were that no task-specific training databases were available for language and acoustic modeling. Instead, general purpose acoustic database were obtained and (probabilistic) context free grammars were constructed for the acoustic and language models, respectively. Public pronunciation lexicon was used for the English language, whereas rule- and exception dictionary based pronunciation modeling was applied for French, German, Italian, Spanish and Hungarian. For the last four languages the classical phoneme-based pronunciation modeling approach was compared to grapheme-based pronunciation modeling technique, as well. Noise robustness was addressed by applying various feature extraction methods. The results show that achieving high word recognition accuracy is feasible if cooperative speakers can be assumed. © 2011 Springer-Verlag.","context free grammar; feature extraction; multiple languages; navigation system; noise robustness; Point of interest; speech recognition","Acoustic database; Acoustic modeling; English languages; Feature extraction methods; General purpose; Hungarians; Language model; Multiple languages; Navigational queries; Noise robustness; Point of interest; Pronunciation lexicon; Pronunciation modeling; Recognition error; Traffic noise; Traffic situations; Training database; Word recognition; Acoustic noise; Computational linguistics; Context free grammars; Continuous speech recognition; Feature extraction; Navigation systems; Noise pollution; Speech communication; Speech recognition; Context free languages",2-s2.0-82955189897
"Giorgidze G., Grust T., Schweinsberg N., Weijers J.","Bringing back monad comprehensions",2011,"Haskell'11 - Proceedings of the 2011 ACM SIGPLAN Haskell Symposium",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82655188170&doi=10.1145%2f2034675.2034678&partnerID=40&md5=d2537e835afd8fcb248bcecaac3d7bc8","This paper is about a Glasgow Haskell Compiler (GHC) extension that generalises Haskell's list comprehension notation to monads. The monad comprehension notation implemented by the extension supports generator and filter clauses, as was the case in the Haskell 1.4 standard. In addition, the extension generalises the recently proposed parallel and SQL-like list comprehension notations to monads. The aforementioned generalisations are formally defined in this paper. The extension will be available in GHC 7.2. This paper gives several instructive examples that we hope will facilitate wide adoption of the extension by the Haskell community. We also argue why the do notation is not always a good fit for monadic libraries and embedded domain-specific languages, especially for those that are based on collection monads. Should the question of how to integrate the extension into the Haskell standard arise, the paper proposes a solution to the problem that led to the removal of the monad comprehension notation from the language standard. © 2011 ACM.","comprehension; haskell; monad","comprehension; Domain specific languages; Glasgow haskell compilers; Haskell; Language standards; monad; Computer programming languages; Problem oriented languages; Standards",2-s2.0-82655188170
"Marin-Castro H.M., Sosa-Sosa V.J., Lopez-Arevalo I.","Automatic identification of web query interfaces",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82555167509&doi=10.1007%2f978-3-642-25330-0_26&partnerID=40&md5=3da170ac4076fc8319734e2b2cd546a1","The amount of information contained in databases in the Web has grown explosively in the last years. This information, known as the Deep Web, is dynamically obtained from specific queries to these databases through Web Query Interfaces (WQIs). The problem of finding and accessing databases in the Web is a great challenge due to the Web sites are very dynamic and the information existing is heterogeneous. Therefore, it is necessary to create efficient mechanisms to access, extract and integrate information contained in databases in the Web. Since WQIs are the only means to access databases in the Web, the automatic identification of WQIs plays an important role facilitating traditional search engines to increase the coverage and access interesting information not available on the indexable Web. In this paper we present a strategy for automatic identification of WQIs using supervised learning and making an adequate selection and extraction of HTML elements in the WQIs to form the training set. We present two experimental tests over a corpora of HTML forms considering positive and negative examples. Our proposed strategy achieves better accuracy than previous works reported in the literature. © 2011 Springer-Verlag.","classification; Databases; Deep Web; information extraction; Web query interfaces","Amount of information; Automatic identification; Deep web; Experimental test; Information Extraction; Interesting information; Negative examples; Query interfaces; Training sets; Artificial intelligence; Automation; Classification (of information); Database systems; HTML; Soft computing; Web services; Search engines",2-s2.0-82555167509
"Hafiz R., Frost R.A.","Modular natural language processing using declarative attribute grammars",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82555184023&doi=10.1007%2f978-3-642-25324-9_25&partnerID=40&md5=0421e3fcbc587cdf2874ce471a6b5f55","A system based on a general top-down parsing algorithm has been developed which allows language processors to be created as executable specifications of arbitrary attribute grammars. Declarative notation of attribute grammars allows modular construction of executable language definitions. Syntax is defined through general context-free grammar rules, and meaning is defined by associated semantic rules with arbitrary dependencies. An innovative technique allows parses to be pruned by arbitrary semantic constraints. This new technique is useful in modelling natural-language phenomena by imposing unification-like restrictions, and accommodating long-distance and cross-serial dependencies, which cannot be handled by context-free rules alone. © 2011 Springer-Verlag.","Attribute grammars; Compositional Semantics; Constraint-based Formalism; Lazy evaluation; Top-down parsing","Attribute grammars; Compositional semantics; Constraint-based; Lazy evaluation; Top-down parsing; Artificial intelligence; Computational linguistics; Context sensitive grammars; Modular construction; Natural language processing systems; Semantics; Context free languages",2-s2.0-82555184023
"Zanibbi R., Yu L.","Math spotting: Retrieving math in technical documents using handwritten query images",2011,"Proceedings of the International Conference on Document Analysis and Recognition, ICDAR",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355186445&doi=10.1109%2fICDAR.2011.96&partnerID=40&md5=672593db788ff1384b3ed9c1d6da1457","A method for locating mathematical expressions in document images without the use of optical character recognition is presented. An index of document regions is produced from recursive X-Y trees produced for each page in the corpus. Queries are provided as images of handwritten expressions, for which an X-Y tree is computed. During retrieval, the query is looked up in the document region index using features of its X-Y tree, producing a set of candidate regions. Candidate regions are ranked by the similarity of vertical pixel projections in their upper and lower halves with those of the query image, as computed using Dynamic Time Warping of the image columns. In an experiment, ten participants each wrote twenty queries from a 200-page corpus. On average, the top-10 retrieval candidates included a candidate covering 43.3% of the test query image (σ = 14.0), with the correct page being returned between 30.0% and 85.0% of the time across participants (μ = 63.2%, σ = 14.9%). When testing using the original query images, 90.0% of the queries were retrieved correctly. © 2011 IEEE.","Keyword Spotting; Math Recognition; Mathematical Information Retrieval","Document images; Dynamic time warping; Keyword spotting; Math Recognition; Mathematical expressions; Mathematical information; Pixel projections; Query images; Technical documents; Forestry; Information retrieval; Optical character recognition; Trees (mathematics); Coding; Image Quality; Indexing; Information Retrieval; Mathematics",2-s2.0-82355186445
"Saini D.K., Hasson N.N., Hasoon F.N., Hasan M.","Review of query processing in distributed systems",2011,"Proceedings of the IADIS International Conferences - Informatics 2011, Wireless Applications and Computing 2011, Telecommunications, Networks and Systems 2011, Part of the IADIS, MCCSIS 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865088683&partnerID=40&md5=d4e810f271dbf2833f064c0b9065d60c","In this paper we reviewed the distributed query execution architecture and query processing mechanism. Distributed data processing is becoming a reality. In real world complications databases are growing and the need for distributed query processing is also growing. In today's business world it is highly demanded to stay competitive. Distributed query processing is a complex process because it involves many heterogeneous sites, various client machines and many servers. The load of sites varies over time and new sites keep adding in the distributed environment. All the systems are to be integrated-such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. In this paper we tried to review the art of state of distributed databases query processing. © 2011 IADIS.","Architecture; Distributed database system; Parser; Query processor","Client machine; Complex Processes; Distributed data processing; Distributed database; Distributed environments; Distributed query execution; Distributed query processing; Distributed systems; Parser; Query processor; Architecture; Computer architecture; Data processing; Distributed database systems; Information science; Legacy systems; Network architecture; Wireless telecommunication systems; Query processing",2-s2.0-84865088683
"Jamil H.M.","Toward a cooperative natural language query interface for biological databases",2011,"Proceedings - 2011 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856044029&doi=10.1109%2fBIBM.2011.122&partnerID=40&md5=a111a156228435d81233a702c0d7c284","One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. From a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. This is partly due to biologists' tendency to seek simpler interfaces and partly due to the fact that questions in biology involve numerous high level concepts that are open to interpretations computed using sophisticated tools. In such highly interpretive environments, rigidly structured databases do not always perform well. In this paper, our goal is to propose a semantic correspondence plug-in to aid natural language query processing over arbitrary biological database schema with an aim to providing cooperative responses to queries tailored to users' interpretations. We demonstrate the feasibility of our system with a practical example. © 2011 IEEE.","biological database; cooperative query answering; natural language query; query transformation","Biological database; Cooperative response; Ground data; Like queries; Natural language queries; Natural languages; Plug-ins; Query answering; Query response; query transformation; Semantic correspondence; Structured database; Structured language; Unique features; Bioinformatics; Query processing; Semantics; Query languages",2-s2.0-84856044029
"Gruenheid A., Omiecinski E., Mark L.","Query optimization using column statistics in Hive",2011,"ACM International Conference Proceeding Series",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855313448&doi=10.1145%2f2076623.2076636&partnerID=40&md5=2976ae6686de006759443b4286b2c8ff","Hive is a data warehousing solution on top of the Hadoop MapReduce framework that has been designed to handle large amounts of data and store them in tables like a relational database management system or a conventional data warehouse while using the parallelization and batch processing functionalities of the Hadoop MapReduce framework to speed up the execution of queries. Data inserted into Hive is stored in the Hadoop FileSystem (HDFS), which is part of the Hadoop MapReduce framework. To make the data accessible to the user, Hive uses a query language similar to SQL, which is called HiveQL. When a query is issued in HiveQL, it is translated by a parser into a query execution plan that is optimized and then turned into a series of map and reduce iterations. These iterations are then executed on the data stored in the HDFS, writing the output to a file. The goal of this work is to to develop an approach for improving the performance of the HiveQL queries executed in the Hive framework. For that purpose, we introduce an extension to the Hive MetaStore which stores metadata that has been extracted on the column level of the user database. These column level statistics are then used for example in combination with join ordering algorithms which are adapted to the specific needs of the Hadoop MapReduce environment to improve the overall performance of the HiveQL query execution. © 2011 ACM.","column statistics; Hadoop MapReduce; hive; query optimization","Filesystem; hive; Large amounts of data; Level statistics; Map-reduce; Ordering algorithms; Parallelizations; Query execution; Query execution plan; Query optimization; Relational database management systems; User database; Batch data processing; Data warehouses; Information management; Metadata; Multiprocessing systems; Optimization; Query languages; Search engines",2-s2.0-84855313448
"Bendersky M., Croft W.B., Smith D.A.","Joint annotation of search queries",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859088388&partnerID=40&md5=96353a0283acf8f3cd3a5a90f63816c5","Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries. © 2011 Association for Computational Linguistics.",,"Keyword queries; Natural language queries; NLP tools; Part-of-speech tags; Probabilistic approaches; Pseudo relevance feedback; Search queries; Training data; Web searches; Classification (of information); Computational linguistics; Information retrieval systems; Natural language processing systems; Query processing; Information retrieval",2-s2.0-84859088388
"Schejbal J., Stárka J., Mlýnková I.","XQConverter: A system for XML query analysis",2011,"Proceedings - International Workshop on Database and Expert Systems Applications, DEXA",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880861083&doi=10.1109%2fDEXA.2011.90&partnerID=40&md5=3f90e6f0205ad296f6e3d2d169bb8134","As XQuery programs became one of the standard instruments of XML manipulation, for the optimization purposes it is important to identify commonly used constructs and patterns. In this paper we describe the design and implementation of a system for the analysis of XQuery programs, XQConverter, as the extension of our previously described analytic framework Analyzer. The XQuery program is converted to an XML representation which allows to formulate analytical queries in XPath. The analysis is based on the frequency of occurrence of various language constructs. © 2011 IEEE.","Analysis; Data representation; Xml; XQuery","Analysis; Analytical queries; Data representations; Design and implementations; Language constructs; XML queries; XML representation; XQuery; Expert systems; XML",2-s2.0-84880861083
"Sun W., Celli F., Morshed A., Jaques Y., Keizer J.","Design and implementation of a SOLR plug-in for Chinese-English cross-language query expansion based on SKOS thesauri",2011,"Lecture Notes in Electrical Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856889354&doi=10.1007%2f978-3-642-25992-0_51&partnerID=40&md5=748eae0a3b5aa1c7b35f698d8b7eb6fa","Given that existing studies for query expansion techniques for Chinese-English are relatively few and their level of standardization low, in order to improve efficiency of Chinese-English cross-language retrieval, this paper discusses the design and implementation of a SOLR plug-in for Chinese-English cross-language query expansion based on SKOS thesauri and used within the AGRIS agricultural bibliographic system. The paper also elaborates the key techniques involved in the plug-in. Finally, taking the AGRIS data resources as an example, the paper shows application examples for segmentation of mixed Chinese and English, user query parsing and AGRIS retrieval system etc., techniques that have improved the Chinese-English cross-language retrieval efficiency to a certain extent, and laid a technical foundation for research about knowledge retrieval and discovery in related fields. © 2011 Springer-Verlag.","Index; Linked open data; Query Expansion; SKOS; SOLR","Index; Linked open data; Query expansion; SKOS; SOLR; Expansion; Linguistics; Thesauri; Robotics",2-s2.0-84856889354
"Yu J., Zha Z.-J., Wang M., Chua T.-S.","Aspect ranking: Identifying important product aspects from online consumer reviews",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",88,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859089280&partnerID=40&md5=1401c026eb32293e8c120c83e77a58c6","In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers' opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers' opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers' opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of documentlevel sentiment classification, and improve the performance significantly. © 2011 Association for Computational Linguistics.",,"Dependency parser; Online consumer reviews; Ranking algorithm; Sentiment classification; Computational linguistics; Electronic commerce",2-s2.0-84859089280
"Gupta C., Govindaraju M.","Automatic creation of an ontological knowledge base from grid and cloud-based wikipages",2011,"Proceedings - 2011 3rd IEEE International Conference on Cloud Computing Technology and Science, CloudCom 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857174237&doi=10.1109%2fCloudCom.2011.123&partnerID=40&md5=1a621c123b289d923427a7f111aeddb5","As the size of experimental data, documents, and web pages grows larger, it will be difficult for scientists to search for appropriate data and grid web-services through keyword-based grid search portal interfaces. A well designed and dynamically updated knowledge base can help understand user queries when they are structurally different, but semantically correlated, with actual data stored in a grid or cloud. In this paper we discuss the implementation of a knowledge base creation tool for grid and cloud based wikipages. In order to achieve accurate results in the query matchmaking process, we store the knowledge base in the rich OWL format and update it automatically with subsequent inference of new facts. Our framework extracts grid and cloud related wikipages and webpages using syntactic Link Grammar Parser and creates core ontology models specific to the grid and cloud domain. In this paper we describe two core components of the ontology generation framework -an information extraction framework and a core ontology model. We present the accuracy of search in terms of precision and recall and its relationship with the dynamically updated ontological domain knowledge base. © 2011 IEEE.",,"Automatic creations; Core components; Domain knowledge base; Experimental data; Grid search; Information Extraction; Knowledge base; Link grammar; Ontology generation; Ontology model; Portal interface; Precision and recall; User query; Web page; Cloud computing; Ontology; Knowledge based systems",2-s2.0-84857174237
"Kent C.K., Salim N.","Web based cross language semantic plagiarism detection",2011,"Proceedings - IEEE 9th International Conference on Dependable, Autonomic and Secure Computing, DASC 2011",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856082597&doi=10.1109%2fDASC.2011.180&partnerID=40&md5=cdf8cc37c78d9825a72130d8ced4e360","As the Internet help us cross language and cultural border and with different types of translation tools, cross language plagiarism is bound to rise. Besides that, semantic plagiarism, where the student reconstructs the sentence or changes some terms into its corresponding synonyms, also raises concerns in the academic field. Both of this plagiarism is hardly detected due to the difference in their fingerprints. Plagiarism detection tools available are not capable to detect such plagiarism cases. In this research, we propose a new approach in detecting both cross language and semantic plagiarism. We consider Bahasa Melayu as the input language of the submitted document and English as a target language of similar, possibly plagiarised documents. In this system we shorten the query document by utilising fuzzy swarm-based summarisation approach. Our point of view is that using the summary will give us the most important keywords in the document. Input summary documents are translated into English using Google Translate Application Programming Interface (API) before the words are stemmed and the stop words are removed. Tokenized documents are sent to the Google AJAX Search API to detect similar documents throughout the World Wide Web. We integrate the use of Stanford Parser and Word Net to determine the semantic similarity level between the suspected documents with candidate source documents. Stanford parser assigns each terms in the sentence to their corresponding roles such as Nouns, Verbs and Adjectives. Based on these roles, we represent each sentence in a predicate form and similarity is measured based on those predicates using information content value from Word Net taxonomy. Our testing dataset is built up from two sets of Malay documents which are produced based on different plagiarism techniques. The result of our proposed semantic based similarity measurement shows that it can achieve higher precision, recall and F-Measure compared to the conventional Longest Common Subsequence (LCS) approach. © 2011 IEEE.","cross language; fuzzy swarm based summarization; plagiarism detection; semantic","Academic fields; cross language; Data sets; F-measure; fuzzy swarm based summarization; Information contents; Language semantics; Longest common subsequences; Plagiarism detection; Query documents; Search API; Semantic similarity; Similarity measurements; Stanford; Stop word; Target language; Translation tools; Web based; Word net; Application programming interfaces (API); Embedded software; Intellectual property; Natural language processing systems; Pattern recognition; Semantic Web; Semantics; Statistical tests; World Wide Web; Translation (languages)",2-s2.0-84856082597
"Liang P., Jordan M.I., Klein D.","Learning dependency-based compositional semantics",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",105,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859072748&partnerID=40&md5=9775470b27d8cb54710b91a4d6c1313d","Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. © 2011 Association for Computational Linguistics.",,"Compositional semantics; Learning problem; Logical forms; Question Answering; Question-answer pairs; Semantic parsing; Semantic representation; Computational linguistics; Semantics",2-s2.0-84859072748
"Goldwasser D., Reichart R., Clarke J., Roth D.","Confidence driven unsupervised semantic parsing",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859018994&partnerID=40&md5=0246450292c8edc23fb02adb8e8cb894","Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task. © 2011 Association for Computational Linguistics.",,"Confidence estimation; Data sets; Scaling-up; Self training; Semantic parsing; Training data; Unsupervised approaches; Computational linguistics; Learning algorithms; Semantics",2-s2.0-84859018994
"Doh K.-G., Kim H., Schmidt D.A.","Abstract LR-parsing",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863081376&doi=10.1007%2f978-3-642-24933-4_6&partnerID=40&md5=fd2641a0f827272e6d0ca1ea4ba1f4ca","We explain and illustrate abstract parsing, a static-analysis technique based on abstract interpretation, LR-parsing, and partial evaluation for validating PHP-like scripts that generate HTML/XML-style documents. A validated script is guaranteed to generate documents that are well formed with respect to the document language's LR(k)-grammar. In this way, abstract parsing resembles compiler data-type checking: a validated script will ""not go wrong"" and output a malformed, dynamically generated document. After presenting abstract parsing for LR(k)-grammars, we handle these important extensions: (i) String-replacement operations are analyzed by composing the finite-state automaton defined by a string replacement with the finite-state control of the LR(k)-parser. (ii) Conditional-test expressions are implemented by filter automata, which are also composed with the parser's finite-state control. (iii) Dynamically supplied and potentially malicious user input is predicted by characterizing it with an LR(k)-grammar and analyzing the strings generated by the grammar. (iv) Synthesized-attribute grammars are employed to calculate the semantics of the dynamically generated documents. © 2011 Springer-Verlag Berlin Heidelberg.",,"Abstract interpretations; Data type; Document languages; Finite state control; Finite-state automata; Partial evaluation; User input; Abstracting; Automata theory; Biological systems; Semantics; Formal languages",2-s2.0-84863081376
"Siddharthan A.","Text simplification using typed dependencies: A comparison of the robustness of different generation strategies",2011,"ENLG 2011 - 13th European Workshop on Natural Language Generation, Proceedings",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860003047&partnerID=40&md5=20750610014b08df9f855a2d08f631a1","We present a framework for text simplification based on applying transformation rules to a typed dependency representation produced by the Stanford parser. We test two approaches to regeneration from typed dependencies: (a) gen-light, where the transformed dependency graphs are linearised using the word order and morphology of the original sentence, with any changes coded into the transformation rules, and (b) gen-heavy, where the Stanford dependencies are reduced to a DSyntS representation and sentences are generating formally using the RealPro surface realiser. The main contribution of this paper is to compare the robustness of these approaches in the presence of parsing errors, using both a single parse and an n-best parse setting in an overgenerate and rank approach. We find that the gen-light approach is robust to parser error, particularly in the n-best parse setting. On the other hand, parsing errors cause the realiser in the genheavy approach to order words and phrases in ways that are disliked by our evaluators. © 2011 Association for Computational Linguistics.",,"Dependency graphs; Stanford; Transformation rules; Word orders; Errors; Context free grammars",2-s2.0-84860003047
"Choi Y.S.","TPEMatcher: A tool for searching in parsed text corpora",2011,"Knowledge-Based Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051471698&doi=10.1016%2fj.knosys.2011.04.009&partnerID=40&md5=f6d93f9fee63030b40df26d5e9431221","Recently, due to the widespread on-line availability of syntactically annotated text corpora, some automated tools for searching in such text corpora have gained great attention. Generally, those conventional corpus search tools use a decomposition-matching-merging method based on relational predicates for matching a tree pattern query to the desired parts of text corpora. Thus, their query formulation and expressivity are often complicated due to poorly understood query formalisms, and their searching tasks may require a big computational overhead due to a large number of repeated trials of matching tree patterns. To overcome these difficulties, we present TPEMatcher, a tool for searching in parsed text corpora. TPEMatcher provides not only an efficient way of query formulation and searching but also a good query expressivity based on concise syntax and semantics of tree pattern query. We also demonstrate that TPEMatcher can be effectively used for a text mining in practice with its useful interface providing in-depth details of search results. © 2011 Elsevier B.V. All rights reserved.","Corpus search tool; Parsed text corpora; Text mining; Tree pattern matching; Tree pattern querying","Search tools; Text corpora; Text mining; Tree pattern; Tree pattern matching; Pattern matching; Plant extracts; Semantics; Software agents",2-s2.0-80051471698
"Rüd S., Ciaramita M., Müller J., Schütze H.","Piggyback: Using search engines for robust cross-domain named entity recognition",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859055809&partnerID=40&md5=f5743bea6fff2056e187d2ce5654ffa2","We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries. © 2011 Association for Computational Linguistics.",,"Cross-domain; Language processing; Named entity recognition; Search engine results; Search results; Computational linguistics; Web services; Search engines",2-s2.0-84859055809
"Do T.T.-T., Nguyen C.T.","Natural language information retrieval system using phrase structure grammar analysis for English",2011,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856893664&doi=10.1145%2f2095536.2095653&partnerID=40&md5=4735c59fa0f682a718ce9b7adb170019","In this paper, we would like to represent our research in Natural Language Information Retrieval (NLIR) system using grammar analysis. In our system, the queries are small texts of description in natural language. The system tries to get searching conditions from these queries by grammar analyzing and searches for the expected documents. In many NLIR system, the descriptions are analyzed by either annotations[5] or predefined patterns[8]. These techniques can lead to miss-understanding the user's request and find inappropriate results. Therefore, the research in NLIR system focuses on understanding the user's descriptions in natural language by grammar analysis technique should be conducted to make found results more accurate. © 2011 Authors.","English sentence analysis; natural language information retrieval system; semantic analysis; web application","Analysis techniques; English sentences; Natural languages; Phrase structure; semantic analysis; System focus; WEB application; Natural language processing systems; Semantics; Web services; Information retrieval",2-s2.0-84856893664
"Lin F., Krizhanovsky A.","Multilingual ontology matching based on wiktionary data accessible via SPARQL endpoint",2011,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891765294&partnerID=40&md5=034a13e9ab766ede64d5ed46e9166be8","Interoperability is a feature required by the Semantic Web. It is provided by the ontology matching methods and algorithms. But now ontologies are presented not only in English, but in other languages as well. It is important to use an automatic translation for obtaining correct matching pairs in multilingual ontology matching. The translation into many languages could be based on the Google Translate API, the Wiktionary database, etc. From the point of view of the balance of presence of many languages, of manually crafted translations, of a huge size of a dictionary, the most promising resource is the Wiktionary. It is a collaborative project working on the same principles as the Wikipedia. The parser of the Wiktionary was developed and the machine-readable dictionary was designed. The data of the machinereadable Wiktionary are stored in a relational database, but with the help of D2R server the database is presented as an RDF store. Thus, it is possible to get lexicographic information (definitions, translations, synonyms) from web service using SPARQL requests. In the case study, the problem entity is a task of multilingual ontology matching based on Wiktionary data accessible via SPARQL endpoint. Ontology matching results obtained using Wiktionary were compared with results based on Google Translate API.",,"Automatic translation; Collaborative projects; D2R servers; Google translate; Machine-readable dictionaries; Multilingual ontologies; Ontology matching; Relational Database; Database systems; Digital libraries; Ontology; Web services; Translation (languages)",2-s2.0-84891765294
"Kwak M., Leroy G., Martinez J.D.","A pilot study of a predicate-based vector space model for a biomedical search engine",2011,"2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBMW 2011",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855989827&doi=10.1109%2fBIBMW.2011.6112537&partnerID=40&md5=a49c7f581553c95c829e9d4829c664b1","A search engine that supports finding precise biomedical statements, but also complementary, and contrasting information would greatly help biomedical researchers. We propose the use of predicates in a search engine's underlying data structure to accomplish this. A predicate is a triple that combines two phrases with a predicate. We report on the development and evaluation of a search engine that includes the predicates in its underlying data structure. The evaluation of the search engine was conducted by comparing three different approaches: keyword-based search, triple-based search, and an additive search that combines keywords and predicates. Cancer researchers provided the queries, relevant to their ongoing work, and evaluated the outcome in a double-blind fashion. The results showed that the combined approach, which combines triple-based and keyword-based approaches, always outperformed the 2 other approaches. © 2011 IEEE.","biomedical search engine; predicate; triple; vector space model","Keyword-based search; Pilot studies; predicate; triple; vector space model; Vector space models; Data structures; Vector spaces; Search engines",2-s2.0-84855989827
"Rakthanmanon T., Zhu Q., Keogh E.J.","Mining historical documents for near-duplicate figures",2011,"Proceedings - IEEE International Conference on Data Mining, ICDM",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857153484&doi=10.1109%2fICDM.2011.102&partnerID=40&md5=03153ff2a445d120f3fa8ff856186ec1","The increasing interest in archiving all of humankind's cultural artifacts has resulted in the digitization of millions of books, and soon a significant fraction of the world's books will be online. Most of the data in historical manuscripts is text, but there is also a significant fraction devoted to images. This fact has driven much of the recent increase in interest in query-by-content systems for images. While querying/indexing systems can undoubtedly be useful, we believe that the historical manuscript domain is finally ripe for true unsupervised discovery of patterns and regularities. To this end, we introduce an efficient and scalable system which can detect approximately repeated occurrences of shape patterns both within and between historical texts. We show that this ability to find repeated shapes allows automatic annotation of manuscripts, and allows users to trace the evolution of ideas. We demonstrate our ideas on datasets of scientific and cultural manuscripts dating back to the fourteenth century. © 2011 IEEE.","Cultural artifacts; Duplication detection; Repeated patterns","Automatic annotation; Cultural artifacts; Data sets; Duplication detection; Historical documents; Repeated patterns; Scalable systems; Shape patterns; Data mining; Image recording; History",2-s2.0-84857153484
"Wang K., Thrasher C., Hsu B.-J.","Web scale NLP: A case study on URL word breaking",2011,"Proceedings of the 20th International Conference on World Wide Web, WWW 2011",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052123854&doi=10.1145%2f1963405.1963457&partnerID=40&md5=f0884c46e20c2b2cf89b5c9a1bb92dd2","This paper uses the URL word breaking task as an example to elaborate what we identify as crucialin designingstatistical natural language processing (NLP) algorithmsfor Web scale applications: (1) rudimentary multilingual capabilities to cope with the global nature of the Web, (2) multi-style modeling to handle diverse language styles seen in the Web contents, (3) fast adaptation to keep pace with the dynamic changes of the Web, (4) minimal heuristic assumptions for generalizability and robustness, and (5) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost. We first show that the state-of-the-art word breaking techniquescan be unified and generalized under the Bayesian minimum risk (BMR) framework that, using a Web scale N-gram, can meet the first three requirements. We discuss how the existing techniques can be viewed asintroducing additional assumptions to the basic BMR framework, and describe a generic yet efficient implementation called word synchronous beam search.Testing the framework and its implementation on a series of large scale experiments reveals the following. First, the language style used to build the modelplays a critical role in the word breaking task, and the most suitable for the URL word breaking task appears to be that ofthe document title where the best performance is obtained. Models created from other language styles, such as from document body, anchor text, and even queries, exhibit varying degrees of mismatch. Although all styles benefit from increasing modeling power which, in our experiments, corresponds tothe use of a higher order N-gram, the gain is most recognizable for the title model. The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models, but are less effective and, in many cases,lead to poorer performance than the matched model with minimal assumptions. For the matched model based on document titles, an accuracy rate of 97.18% can already be achieved using simple trigram without any heuristics. Copyright © 2011 by the Association for Computing Machinery, Inc. (ACM).","Compound splitting; Multi-style language model; URL segmentation; Web scale word breaking; Word segmentation","Accuracy rate; Breaking performance; Compound splitting; Dynamic changes; Efficient implementation; Fast adaptations; Language model; Large scale experiments; Matched models; Modeling power; Multilingual capability; NAtural language processing; Tri grams; Web content; Web scale word breaking; Word segmentation; Computational linguistics; Experiments; Natural language processing systems; World Wide Web",2-s2.0-80052123854
"Giorgidze G., Grust T., Schweinsberg N., Weijers J.","Bringing back monad comprehensions",2011,"ACM SIGPLAN Notices",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860270094&doi=10.1145%2f2096148.2034678&partnerID=40&md5=a3cb5e592331099de156408351f70ba9","This paper is about a Glasgow Haskell Compiler (GHC) extension that generalises Haskell's list comprehension notation to monads. The monad comprehension notation implemented by the extension supports generator and filter clauses, as was the case in the Haskell 1.4 standard. In addition, the extension generalises the recently proposed parallel and SQL-like list comprehension notations to monads. The aforementioned generalisations are formally defined in this paper. The extension will be available in GHC 7.2. This paper gives several instructive examples that we hope will facilitate wide adoption of the extension by the Haskell community. We also argue why the do notation is not always a good fit for monadic libraries and embedded domain-specific languages, especially for those that are based on collection monads. Should the question of how to integrate the extension into the Haskell standard arise, the paper proposes a solution to the problem that led to the removal of the monad comprehension notation from the language standard. Copyright © 2011 ACM.","Comprehension; Haskell; Monad","Comprehension; Domain specific languages; Glasgow haskell compilers; Haskell; Language standards; Monad; Computer programming; Computer science; Problem oriented languages",2-s2.0-84860270094
"Pattinson C., Hajdarevic K., Hadzic A.","The time element in proactive network defense systems",2011,"Proceedings of the International Conference on Information Technology Interfaces, ITI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859065185&partnerID=40&md5=cbdf63504693c9d079d1daab4fcba42f","Identifying and detecting security threats such as Trojans, resource starvation, and Denial of Service (DoS) attacks in their early stages are major challenges in delivering computer security because starvation of specific resource such as hard disk as an example does not necessary deny specific service on the network. As with any computer application, installation of a Trojan leaves a ""footprint"" on the systems resources such as MIB data base explained and referenced in this paper. Effects of resource starvation and DoS attacks can be resolved proactively by monitoring communication traffic. The detector must be able to recognize the symptoms against a background of a range of other (""safe"") activities, which also consume system resources. Therefore, we wished to explore the potential of an economical approach that explicitly takes into account resources used. We presented general phases of above described attacks which can be used for creating metrics to measure proactive capabilities of similar systems for intrusion detection / prevention.","Anomaly detection; Management information base; Network security","Anomaly detection; Denial of service attacks; DoS attacks; Management information base; Proactive networks; Resource starvation; Security threats; System resources; Trojans; Computer applications; Distributed computer systems; Intrusion detection; Network security; Security systems; Telecommunication traffic; Information technology",2-s2.0-84859065185
"Vadas D., Curran J.R.","Parsing noun phrases in the Penn Treebank",2011,"Computational Linguistics",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255162277&partnerID=40&md5=843250d4aa68622f3b47fd98df24f22c","Noun phrases (NPs) are a crucial part of natural language, and can have a very complex structure. However, this NP structure is largely ignored by the statistical parsing field, as the most widely used corpus is not annotated with it. This lack of gold-standard data has restricted previous efforts to parse NPs, making it impossible to perform the supervised experiments that have achieved high performance in so many Natural Language Processing (NLP) tasks. © 2011 Association for Computational Linguistics.",,,2-s2.0-82255162277
"Theijssen D., Van Halteren H., Boves L., Oostdijk N.","On the diffculty of making concreteness concrete",2011,"Computational Linguistics in the Netherlands Journal",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874424881&partnerID=40&md5=e055df36bdc66357902c9436885a80dc","The use of labels of semantic properties like 'concreteness' is quite common in studies in syntax, but their exact meaning is often unclear. In this article, we compare different definitions of concreteness, and use them in different implementations to annotate nouns in two data sets: (1) all nouns with word sense annotations in the SemCor corpus, and (2) nouns in a particular lexico-syntactic context, viz. the theme (e.g. a book) in prepositional dative (gave a book to him) and double object (gave him a book) constructions. The results show that the definition and implementation used in different approaches differ greatly, and can considerably affect the conclusions drawn in syntactic research. A followup crowdsourcing experiment showed that there are instances that are clearly concrete or abstract, but also many instances for which humans disagree. Therefore, results concerning concreteness in syntactic research can only be interpreted when taking into account the annotation scheme used and the type of data that is being analysed. © 2011 Daphne Theijssen, Hans van Halteren, Lou Boves, Nelleke Oostdijk.",,"Annotation scheme; Crowdsourcing; Semantic properties; Word sense; Abstracting; Computational linguistics; Semantics; Syntactics; Concretes",2-s2.0-84874424881
"Raj R.G., Balakrishnan V.","A model for determining the degree of contradictions in information",2011,"Malaysian Journal of Computer Science",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857133611&partnerID=40&md5=b141de338d04d3ff3f662644e1766a50","Conversational systems are gaining popularity rapidly. Consequently, the believability of the conversational systems or chatterbots is becoming increasingly important. Recent research has proven that learning chatterbots tend to be rated as being more believable by users. Based on Raj's Model for Chatterbot Trust, we present a model for allowing chatterbots to determine the degree of contradictions in contradictory statements when learning thereby allowing them to potentially learn more accurately via a form of discourse. Some information that is learnt by a chatterbot may be contradicted by other information presented subsequently. Choosing correctly which information to use is critical in chatterbot believability. Our model uses sentence structures and patterns to compute contradiction degrees that can be used to overcome the limitations of Raj's Trust Model, which takes any contradictory information as being equally contradictory as opposed to some contradictions being greater than others and therefore having a greater impact on the actions that the chatterbot should take. This paper also presents the relevant proofs and tests of the contradiction degree model as well as a potential implementation method to integrate our model with Raj's Trust Model.","Contradiction; Information; Vector space model",,2-s2.0-84857133611
"Alvarez J.M., Polo L., Jimenez W., Abella P., Labra J.E.","Application of the spreading activation technique for recommending concepts of well-known ontologies in medical systems",2011,"2011 ACM Conference on Bioinformatics, Computational Biology and Biomedicine, BCB 2011",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858967123&doi=10.1145%2f2147805.2147913&partnerID=40&md5=457a250411064aa8700fafc0774120d4","The aim of this paper is to present the application of the Spreading Activation Technique in the scope of medical systems. This technique is implemented through the ONTOSPREAD framework for the development, configuration, customization and execution of the Spreading Activation technique over graph-based structures, more specifically over RDF graphs and ontologies arising from the Semantic Web area. It has been used to the efficient exploration and querying of large and heterogeneous knowledge bases based on semantic networks in the Information and Document Retrieval domains. ONTOSPREAD implements the double process of activation and spreading of concepts in ontologies applying different restrictions of the original model like weight degradation according to the distance or others coming from the extension of this technique like the converging paths reward. It is considered to be relevant to support the recommendation of concepts for tagging clinical records and to provide a tool for decision-support in clinical diagnosis. Finally an evaluation methodology and two examples using the well-known ontologies Galen and SNOMED CT are presented to validate the goodness, the improvement and the capabilities of this technique applied to medical systems. Copyright © 2011 ACM.","Algorithms; Information retrieval; Recommending system; Spreading activation","Activation techniques; Clinical diagnosis; Clinical records; Document Retrieval; Evaluation methodologies; Graph-based; Heterogeneous Knowledge; Medical systems; Original model; RDF graph; Recommending system; Semantic network; Algorithms; Decision support systems; Diagnosis; Information retrieval; Semantic Web; Bioinformatics",2-s2.0-84858967123
"Dahl V.","Informing datalog through language intelligence - A personal perspective",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856559764&doi=10.1007%2f978-3-642-24206-9_10&partnerID=40&md5=f3c7452795af370f234a79e4653f4bb8","Despite AI's paramount aim of developing convincing similes of true natural language ""understanding"", crucial knowledge that is increasingly becoming available to computers in text form on web repositories remains in fact decipherable only by humans. In this position paper, we present our views on the reasons for this failure, and we argue that for bringing computers closer to becoming true extensions of the human brain, we need to endow them with a cognitively-informed web by integrating new methodologies in the inter-disciplines involved, around the pivot of Logic Programming and Datalog. © 2011 Springer-Verlag.","cognitive sciences; computational linguistics; Datalog; knowledge extraction; logic grammars; logic programming; semantic web; web search","Cognitive science; Datalog; Knowledge extraction; Logic grammar; Web searches; Computational linguistics; Computer system recovery; Logic programming; Semantic Web; Personal computers",2-s2.0-84856559764
"Kiso T., Shimbo M., Komachi M., Matsumoto Y.","HITS-based seed selection and stop list construction for bootstrapping",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859023478&partnerID=40&md5=75dce3ec128cc1e6dbdd450a77b0d70b","In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti's Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg's HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. © 2011 Association for Computational Linguistics.",,"Bootstrapping algorithm; Graph-based; HITS algorithms; Human supervision; Seed selection; Seed set; Semantic drifts; Algorithms; Semantics; Computational linguistics",2-s2.0-84859023478
"Goldwasser D., Roth D.","Learning from natural instructions",2011,"IJCAI International Joint Conference on Artificial Intelligence",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865492589&doi=10.5591%2f978-1-57735-516-8%2fIJCAI11-301&partnerID=40&md5=c758e969fba3226df85f6c50b961e751","Machine learning is traditionally formalized and researched as the study of learning concepts and decision functions from labeled examples, requiring a representation that encodes information about the domain of the decision function to be learned. We are interested in providing a way for a human teacher to interact with an automated learner using natural instructions, thus allowing the teacher to communicate the relevant domain expertise to the learner without necessarily knowing anything about the internal representations used in the learning process. In this paper we suggest to view the process of learning a decision function as a natural language lesson interpretation problem instead of learning from labeled examples. This interpretation of machine learning is motivated by human learning processes, in which the learner is given a lesson describing the target concept directly, and a few instances exemplifying it. We introduce a learning algorithm for the lesson interpretation problem that gets feedback from its performance on the final task, while learning jointly (1) how to interpret the lesson and (2) how to use this interpretation to do well on the final task. This approach alleviates the supervision burden of traditional machine learning by focusing on supplying the learner with only human-level task expertise for learning. We evaluate our approach by applying it to the rules of the Freecell solitaire card game. We show that our learning approach can eventually use natural language instructions to learn the target concept and play the game legally. Furthermore, we show that the learned semantic interpreter also generalizes to previously unseen instructions.",,"Decision functions; Domain expertise; Human teachers; Internal representation; Learning approach; Learning process; Natural languages; Process of learning; Artificial intelligence; Learning algorithms; Semantics; Learning systems",2-s2.0-84865492589
"Huynh T.N., Mooney R.J.","Online max-margin weight learning for Markov logic networks",2011,"Proceedings of the 11th SIAM International Conference on Data Mining, SDM 2011",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052396679&partnerID=40&md5=6a5ee8594bb96fb60fd9f4cf7ae0085d","Most of the existing weight-learning algorithms for Markov Logic Networks (MLNs) use batch training which becomes computationally expensive and even infeasible for very large datasets since the training examples may not fit in main memory. To overcome this problem, previous work has used online learning algorithms to learn weights for MLNs. However, this prior work has only applied existing online algorithms, and there is no comprehensive study of online weight learning for MLNs. In this paper, we derive a new online algorithm for structured prediction using the primal-dual framework, apply it to learn weights for MLNs, and compare against existing online algorithms on three large, real-world datasets. The experimental results show that our new algorithm generally achieves better accuracy than existing methods, especially on noisy datasets. Copyright © SIAM.","Online learning; Statistical relational learning; Structured prediction","Markov logic networks; On-line algorithms; Online learning; Online learning algorithms; Real-world datasets; Statistical relational learning; Structured prediction; Training example; Data mining; Learning algorithms; Markov processes; Space division multiple access; E-learning",2-s2.0-80052396679
"Romero Zaldivar V.A., Burgos D., Pardo A.","Meta-rule based recommender systems for educational applications",2011,"Educational Recommender Systems and Technologies: Practices and Challenges",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898095474&doi=10.4018%2f978-1-61350-489-5.ch009&partnerID=40&md5=e85e70c9c3dfff96da85dbbc021f614f","Recommendation Systems are central in current applications to help the user find relevant information spread in large amounts of data. Most Recommendation Systems are more effective when huge amounts of user data are available. Educational applications are not popular enough to generate large amount of data. In this context, rule-based Recommendation Systems seem a better solution. Rules can offer specific recommendations with even no usage information. However, large rule-sets are hard to maintain, reengineer, and adapt to user preferences. Meta-rules can generalize a rule-set which provides bases for adaptation. In this chapter, the authors present the benefits of meta-rules, implemented as part of Meta-Mender, a meta-rule based Recommendation System. This is an effective solution to provide a personalized recommendation to the learner, and constitutes a new approach to Recommendation Systems. © 2012, IGI Global.",,,2-s2.0-84898095474
"Siddharthan A., Nenkova A., McKeown K.","Information status distinctions and referring expressions: An empirical study of references to people in news summaries",2011,"Computational Linguistics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255182061&partnerID=40&md5=8be7a7322a8818c789cd1de1dc80c264","Although there has been much theoretical work on using various information status distinctions to explain the form of references in written text, there have been few studies that attempt to automatically learn these distinctions for generating references in the context of computerregenerated text. In this article, we present a model for generating references to people in news summaries that incorporates insights from both theory and a corpus analysis of human written summaries. In particular, our model captures how two properties of a person referred to in the summary-familiarity to the reader and global salience in the news story-affect the content and form of the initial reference to that person in a summary. We demonstrate that these two distinctions can be learned from a typical input for multi-document summarization and that they can be used to make regeneration decisions that improve the quality of extractive summaries. © 2011 Association for Computational Linguistics.",,,2-s2.0-82255182061
"Martens L.","Data management in mass spectrometry-based proteomics",2011,"Methods in Molecular Biology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960116643&doi=10.1007%2f978-1-61779-068-3_21&partnerID=40&md5=b4fa01cf61c972f10297bdd2b1f04c0a","Mass spectrometry-based proteomics has made the transition from the analysis of a single or only a few proteins per experiment to a high-throughput platform for discovery/validation that can analyze several hundreds to thousands of proteins in a single experiment. This increase in analytical capability hinged on four main components: (1) innovative methodologies, (2) improved instruments, (3) ready availability of comprehensive protein sequence databases, and (4) development of sophisticated software algorithms to match fragmentation mass spectra against these databases. But as the throughput of the approach increased, so did the necessity to manage and automate the data processing workflow. Indeed, modern instruments generate tens of thousands of fragmentation spectra per hour, providing a substantial bioinformatics challenge. This chapter provides insight into the specifics of this challenge, by looking at a typical workflow, the data types and user roles involved, and a broad overview of available software solutions. Finally, the increasingly important link between a local data management system and the global, centralized dissemination of proteomics data is discussed. © 2011 Springer Science+Business Media, LLC.","Data management; Database; Mass spectrometry; Proteomics","article; data base; information dissemination; Internet; mass spectrometry; methodology; organization and management; protein database; proteomics; Database Management Systems; Databases, Protein; Information Dissemination; Internet; Mass Spectrometry; Proteomics",2-s2.0-79960116643
"Ferraro G., Wanner L.","Towards the derivation of verbal content relations from patent claims using deep syntactic structures",2011,"Knowledge-Based Systems",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051470495&doi=10.1016%2fj.knosys.2011.05.014&partnerID=40&md5=00440c13acd59ce86f6a622787bf131e","Research on the extraction of content relations from text corpora is a high-priority topic in natural language processing. This is not surprising since content relations form the backbone of any ontology, and ontologies are increasingly made use of in knowledge-based applications. However, so far most of the works focus on the detection of a restricted number of prominent verbal relations, including in particular is-a, has-part and cause. Our application, which aims to provide comprehensive, easy-to-understand content representations of complex functional objects described in patent claims, faces the need to derive a large number of content relations that cannot be limited a priori. To cope with this problem, we take advantage of the fact that deep syntactic dependency structures of sentences capture all relevant content relations - although without any abstraction. We implement thus a three-step strategy. First, we parse the claims to retrieve the deep syntactic dependency structures from which we then derive the content relations. Second, we generalize the obtained relations by clustering them according to semantic criteria, with the goal to unite all sufficiently similar relations. Finally, we identify a suitable name for each generalized relation. To keep the scope of the article within reasonable limits and to allow for a comparison with state-of-the-art techniques, we focus on verbal relations. © 2011 Elsevier B.V. All rights reserved.","Cluster labeling; Deep dependency parsing; Dependency relation; Relation clustering; Specialized discourse","Cluster labeling; Dependency parsing; Dependency relation; Relation clustering; Specialized discourse; Computational linguistics; Knowledge based systems; Natural language processing systems; Ontology; Patents and inventions; Semantics; Syntactics",2-s2.0-80051470495
"Xiao T., Zhu J., Zhu M.","Language modeling for syntax-based machine translation using tree substitution grammars: A case study on Chinese-english translation",2011,"ACM Transactions on Asian Language Information Processing",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855220723&doi=10.1145%2f2025384.2025386&partnerID=40&md5=a9c22a1a2a10c1d15eb2b484b7133d01","The poor grammatical output of Machine Translation (MT) systems appeals syntax-based approaches within language modeling. However, previous studies showed that syntax-based language modeling using (Context- Free) Treebank Grammars was not very helpful in improving BLEU scores for Chinese-English machine translation. In this article we further study this issue in the context of Chinese-English syntax-based Statistical Machine Translation (SMT) where Synchronous Tree Substitution Grammars (STSGs) are utilized to model the translation process. In particular, we develop a Tree Substitution Grammar-based language model for syntax-based MT, and present three methods to efficiently integrate the proposed language model into MT decoding. In addition, we design a simple and effective method to adapt syntax-based language models for MT tasks. We demonstrate that the proposed methods are able to benefit a state-of-the-art syntax-based MT system. On the NIST Chinese-English MT evaluation corpora, we finally achieve an improvement of 0.6 BLEU points over the baseline. © 2011 ACM.","Machine translation; Syntax-based language model; Tree substitution grammar","Language model; Language modeling; Machine translation systems; Machine translations; MT evaluations; Statistical machine translation; Syntax-based approach; Translation process; Tree substitution grammar; Treebanks; Computational linguistics; Forestry; Information theory; Syntactics; Translation (languages); Context free languages; Forestry; Information Retrieval; Languages; Translation",2-s2.0-84855220723
"Hu H., Correll M., Kvecher L., Osmond M., Clark J., Bekhash A., Schwab G., Gao D., Gao J., Kubatin V., Shriver C.D., Hooke J.A., Maxwell L.G., Kovatich A.J., Sheldon J.G., Liebman M.N., Mural R.J.","DW4TR: A Data Warehouse for Translational Research",2011,"Journal of Biomedical Informatics",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855951781&doi=10.1016%2fj.jbi.2011.08.003&partnerID=40&md5=412692f87a3f75ef1df541ae643ccbc9","The linkage between the clinical and laboratory research domains is a key issue in translational research. Integration of clinicopathologic data alone is a major task given the number of data elements involved. For a translational research environment, it is critical to make these data usable at the point-of-need. Individual systems have been developed to meet the needs of particular projects though the need for a generalizable system has been recognized. Increased use of Electronic Medical Record data in translational research will demand generalizing the system for integrating clinical data to support the study of a broad range of human diseases. To ultimately satisfy these needs, we have developed a system to support multiple translational research projects. This system, the Data Warehouse for Translational Research (DW4TR), is based on a light-weight, patient-centric modularly-structured clinical data model and a specimen-centric molecular data model. The temporal relationships of the data are also part of the model. The data are accessed through an interface composed of an Aggregated Biomedical-Information Browser (ABB) and an Individual Subject Information Viewer (ISIV) which target general users. The system was developed to support a breast cancer translational research program and has been extended to support a gynecological disease program. Further extensions of the DW4TR are underway. We believe that the DW4TR will play an important role in translational research across multiple disease types. © 2011 Elsevier Inc.","Data Warehouse; Ontology; Patient-centric data model; Translational research; User interface","Breast Cancer; Clinical data; Electronic medical record; Human disease; Individual systems; Light weight; Molecular data; Number of datum; Research domains; Temporal relationships; Translational Research; Data structures; Data warehouses; Medical computing; Models; Ontology; User interfaces; Research; epidermal growth factor receptor 2; estrogen receptor; progesterone receptor; access to information; aggregated biomedical information browser; article; breast cancer; computer interface; computer network; computer program; computer security; data base; Data Warehouse for Translational Research; diagnostic imaging; gynecologic disease; human; immunohistochemistry; individual subject information viewer; medical history; medical information system; molecular model; physical examination; priority journal; risk assessment; translational research; virtual reality; Electronic Health Records; Humans; Medical Informatics Applications; Software; Translational Medical Research; User-Computer Interface",2-s2.0-84855951781
"Blake C.","Text mining",2011,"Annual Review of Information Science and Technology",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877109449&doi=10.1002%2faris.2011.1440450110&partnerID=40&md5=feacb49284f70c5a88a259c1dfe7036f",[No abstract available],,,2-s2.0-84877109449
"Broeksema B., Telea A.","Visual support for porting large code bases",2011,"Proceedings of VISSOFT 2011 - 6th IEEE International Workshop on Visualizing Software for Understanding and Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82055193748&doi=10.1109%2fVISSOF.2011.6069450&partnerID=40&md5=3254993e5a54ab41bf3dcf1129d17261","We present a tool that helps C/C++ developers to estimate the effort and automate software porting. Our tool supports project leaders in planning a porting project by showing where a project must be changed, how many changes are needed, what kinds of changes are needed, and how these interact with the code. For developers, we provide an overview of where a given file must be changed, the structure of that file, and close interaction with typical code editors. To this end, we integrate code querying, program transformation, and software visualization techniques. We illustrate our solution with use-cases on real-world code bases. © 2011 IEEE.",,"Large code basis; Program transformations; Project leaders; Software porting; Software visualization; Tool support; File editors; Visualization; Codes (symbols)",2-s2.0-82055193748
"Mousavi H., Kerr D., Iseli M.","A new framework for textual information mining over parse trees",2011,"Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC 2011",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255178604&doi=10.1109%2fICSC.2011.19&partnerID=40&md5=c92963db69ea5c41e2402f1d1936930d","This paper introduces a new text mining framework using a tree-based Linguistic Query Language, called LQL. The framework generates more than one parse tree for each sentence using a probabilistic parser, and annotates each node of these parse trees with main-parts information which is set of key terms from the node's branch based on the branch's linguistic structure. Using main-parts-annotated parse trees, the system can efficiently answer individual queries as well as mine the text for a given set of queries. The framework can also support grammatical ambiguity through probabilistic rules and linguistic exceptions. © 2011 IEEE.",,"Linguistic structure; Parse trees; Probabilistic rules; Text mining; Textual information; Tree-based; Forestry; Linguistics; Query languages; Query processing; Semantics; Data mining; Algorithms; Computer Programing; Data Processing",2-s2.0-81255178604
[No author name available],"Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC 2011",2011,"Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255205453&partnerID=40&md5=38e8ecb3d20568e12c1f371efad94962","The proceedings contain 93 papers. The topics discussed include: SIM, a semantic instrumentation and monitoring solution for large scale reasoning systems; optimizing a semantic comparator Using CUDA-enabled graphics hardware; an improved profile-based CF scheme with privacy; predicting missing provenance using semantic associations in reservoir engineering; preserving privacy in context-aware systems; detecting abnormal semantic web data using semantic dependency; YAGO-QA: answering questions by structured knowledge queries; identifying justifications in written dialogs; end-to-end discourse parser evaluation; semantic service retrieval based on natural language querying and semantic similarity; a modular architecture for adaptive ChatBots; and increasing coverage of syntactic subcategorization patterns in FrameNet using verbnet.",,,2-s2.0-81255205453
"Richardson K.D., Bobrow D.G., Condoravdi C., Waldinger R., Das A.","English access to structured data",2011,"Proceedings - 5th IEEE International Conference on Semantic Computing, ICSC 2011",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255178441&doi=10.1109%2fICSC.2011.67&partnerID=40&md5=dd051d335a0afd3f2aa7b0f8bfd98708","We present work on using a domain model to guide text interpretation, in the context of a project that aims to interpret English questions as a sequence of queries to be answered from structured databases. We adapt a broad-coverage and ambiguity-enabled natural language processing (NLP) system to produce domain-specific logical forms, using knowledge of the domain to zero in on the appropriate interpretation. The vocabulary of the logical forms is drawn from a domain theory that constitutes a higher-level abstraction of the contents of a set of related databases. The meanings of the terms are encoded in an axiomatic domain theory. To retrieve information from the databases, the logical forms must be instantiated by values constructed from fields in the database. The axiomatic domain theory is interpreted by the first-order theorem prover SNARK to identify the groundings, and then retrieve the values through procedural attachments semantically linked to the database. SNARK attempts to prove the logical form as a theorem by reasoning over the theory that is linked to the database and returns the exemplars of the proof(s) back to the user as answers to the query. The focus of this paper is more on the language task; however, we discuss the interaction that must occur between linguistic analysis and reasoning for an end-to-end natural language interface to databases. We illustrate the process using examples drawn from an HIV treatment domain, where the underlying databases are records of temporally bound treatments of individual patients. © 2011 IEEE.","Deductive question answering; HIV drug resistance database; Natural language interfaces to databases; Natural language processing; Theorem proving","Axiomatic domain theory; Domain model; Domain specific; Domain theory; Drug resistance; First-order; Higher-level abstraction; Linguistic analysis; Logical forms; Natural language interfaces; NAtural language processing; Question Answering; Structured data; Structured database; Theorem provers; Computational linguistics; Database systems; Patient treatment; Semantics; Theorem proving; Natural language processing systems",2-s2.0-81255178441
"Kop C.","Domain expert centered ontology reuse for conceptual models",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255153844&doi=10.1007%2f978-3-642-25106-1_24&partnerID=40&md5=02de3f4a40daaa1320e7c66ac72df9ca","Generating a project specific domain model for the database of an information system is a critical task. The notions and structure must be well chosen since the application relies on that. With the Semantic Web, many notions and structures of certain domains already exist as a shared knowledge. Hence, it seems that the designer of an information system just has to take an ontology and reuse it for the information system. However, this strategy must be applied carefully since an ontology might have a different focus and purpose with respect to the intended conceptual database model. Nevertheless, it is worth to examine an ontology for elements which can be reused. Research results already exist for this question but these research approaches focus on ontology engineers or conceptual modelers who are able to analyze the ontology. Although these persons are necessary, they are not the only ones who usually must be involved in such a process! What about the domain experts, who do not have the skills to read and understand an ontology? They must be also involved in a good process. The approach presented here, focuses on the domain experts. It will be described how domain experts can be supported and involved in the process. With respect to the ontology language, this approach focuses on OWL. © 2011 Springer-Verlag.",,"Conceptual model; Critical tasks; Database models; Domain experts; Ontology language; Research approach; Research results; Specific domain model; Information systems; Semantic Web; Ontology",2-s2.0-81255153844
"Draken E., Jarada T.N., Kianmehr K., Alhajj R.","Integrating content and structure into a comprehensive framework for XML document similarity represented in 3D space",2011,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455129743&doi=10.1007%2f978-3-642-22913-8_13&partnerID=40&md5=e4072e15a5408ecf0f564736807dda0a","XML is attractive for data exchange between different platforms, and the number of XML documents is rapidly increasing. This raised the need for techniques capable of investigating the similarity between XML documents to help in classifying them for better organized utilization. In fact, the idea of similarity between documents is not new. However, XML documents are more rich and informative than classical documents in the sense that they encapsulate both structure and content; on the other hand, classical documents are characterized only by the content. According, using both the content and structure of XML documents to assign a similarity metric is relatively new. Of the recent research and algorithms proposed in the literature, the majority assign a similarity metric between 0.0 and 1.0 when comparing two XML documents. The similarity measures between multiple XML documents may be arranged in a matrix whereby data mining may be done to cluster closely related documents. In this chapter the authors have presented a novel way to represent XML document similarity in 3D space. Their approach benefits from the characteristics of the XML documents to produce a measure to be used in clustering and classification techniques, information retrieval and searching methods for the case of XML documents. We mainly derive a three dimensional vector per document by considering two dimensions as the document's structural and content, while the third dimension is a combination of both structure and content characteristics of the document. The outcome from our research allows users to intuitively visualize document similarity. © 2011 Springer-Verlag Berlin Heidelberg.","3D space; document similarity; intuitive representation; platform independence; Similarity measures; visualization; XML",,2-s2.0-80455129743
"Gozman A., Munteanu D., Panu A., Alboaie L., Buraga S.","SINUX - Ubuntu spiced up with Semantic Web",2011,"Proceedings - RoEduNet IEEE International Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055058607&doi=10.1109%2fRoEduNet.2011.5993715&partnerID=40&md5=f21750991d28632bdf0288bb22604e71","In this paper we propose SINUX, a Linux distribution especially created for those who want to learn and use the currently existing Semantic Web technologies. It has integrated many important instruments and references to online informations, making it easy for the users to have a proper development environment without any hassle. © 2011 IEEE.","Linux distribution; Semantic Web; Ubuntu","Development environment; Linux distributions; On-line information; Semantic Web technology; Ubuntu; Computer operating systems; Semantics; User interfaces; Semantic Web",2-s2.0-80055058607
"Haitof H., Krivutsenko A., Gerndt M.","Semantic-based document management system",2011,"International Journal of Reasoning-based Intelligent Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857274812&doi=10.1504%2fIJRIS.2011.043538&partnerID=40&md5=991b374197a528a215530869f15b67b3","In this paper, we present the work towards ORSIS, an ontology-based personal document management system which uses a reasoning engine to allow for knowledge discovery and inference. This work also presents the potentials of knowledge representation technologies, their performance and applicability for personal documents management systems. As part of ORSIS, we also have developed simple but powerful query language which allows executing selection request with different joint criteria. Appropriate grammar has been developed and parser/lexer code was implemented. Users have all the necessary interface features to provide complex queries or to build search request by using appropriate dialogs. Copyright © 2011 Inderscience Enterprises Ltd.","Document management systems; Knowledge discovery; Ontologies; Semantic technologies","Complex queries; Document management systems; Management systems; Ontology-based; Personal documents; Reasoning engine; Semantic technologies; Data mining; Information retrieval systems; Knowledge representation; Ontology; Query languages; Semantic Web; Semantics; Information management",2-s2.0-84857274812
"Pereira F., Moutinho F., Gomes L., Ribeiro J., Campos-Rebelo R.","An IOPT-net state-space generator tool",2011,"IEEE International Conference on Industrial Informatics (INDIN)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054978064&doi=10.1109%2fINDIN.2011.6034907&partnerID=40&md5=69f10e159b1fe2952ee0f96869617f9b","This paper presents the IOPT2SS tool, used to automatically generate state-space graphs associated with IOPT (Input-Output Place-Transition) Petri nets models. The new tool accounts with the non autonomous nature of the IOPT Petri net class, where transition firing is constrained by external input events and input signals (expressed in transition guards); on the other hand, transitions can trigger the occurrence of output signal events and place marking can activate output signals. To achieve the performance level necessary for the fast generation of complex state-spaces during reasonable time, the tool employs a compilation strategy, starting with the automatic creation of an optimized C program. When executed, the program will create an hierarchical XML file containing the state-space graph. The output XML file can subsequently be used for model checking and property analysis, applying standard XML query languages. Finally, the output file can be converted to SVG (Scalable Vector Graphics), enabling the graphical visualization of small to medium size state-space graphs. The new tool was implemented using a set of XSL transformations and can be used as a standalone tool or as a building block in higher level frameworks, with easy integration in Web applications and graphical integrated development environments. © 2011 IEEE.",,"Automatic creations; Building blockes; C programs; External input; Generator tool; Graphical visualization; Hierarchical XML; Input signal; Input-output; Integrated development environment; Medium size; Nonautonomous; Output signal; Performance level; Scalable vector graphics; State-space; Transition firings; WEB application; XML files; XML query language; Automatic programming; C (programming language); Information science; Model checking; Petri nets; Query languages; User interfaces; Vector spaces; Visualization; XML; Equipment",2-s2.0-80054978064
"Hakenberg J., Solt I., Tikk D., Nguyên V.H., Tari L., Nguyen Q.L., Baral C., Leser U.","Molecular event extraction from link grammar parse trees in the BIONLP'09 shared task",2011,"Computational Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455189733&doi=10.1111%2fj.1467-8640.2011.00404.x&partnerID=40&md5=43ea38858caf8226430fba519426e2bc","The BioNLP'09 Shared Task deals with extracting information on molecular events, such as gene expression and protein localization, from natural language text. Information in this benchmark are given as tuples including protein names, trigger terms for each event, and possible other participants such as bindings sites. We address all three tasks of BioNLP'09: event detection, event enrichment, and recognition of negation and speculation. Our method for the first two tasks is based on a deep parser; we store the parse tree of each sentence in a relational database scheme. From the training data, we collect the dependencies connecting any two relevant terms of a known tuple, that is, the shortest paths linking these two constituents. We encode all such linkages in a query language to retrieve similar linkages from unseen text. For the third task, we rely on a hierarchy of hand-crafted regular expressions to recognize speculation and negated events. In this paper, we added extensions regarding a post-processing step that handles ambiguous event trigger terms, as well as an extension of the query language to relax linkage constraints. On the BioNLP Shared Task test data, we achieve an overall F1-measure of 32%, 29%, and 30% for the successive Tasks 1, 2, and 3, respectively. © 2011 Wiley Periodicals Inc.","event extraction; parse tree database; sentence parsing; text mining","Event detection; Event extraction; Event trigger; Extracting information; Link grammar; Molecular events; Natural language text; Parse trees; Post processing; Protein localization; Protein names; Regular expressions; Relational Database; Sentence parsing; Shortest path; Test data; Text mining; Training data; Data mining; Forestry; Formal languages; Gene expression; Natural language processing systems; Object oriented programming; Query languages; Trees (mathematics); Binding; Chemical Bonds; Extraction; Forestry; Genes; Mining",2-s2.0-82455189733
"Moschitti A., Quarteroni S.","Linguistic kernels for answer re-ranking in question answering systems",2011,"Information Processing and Management",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052263476&doi=10.1016%2fj.ipm.2010.06.002&partnerID=40&md5=a66fe1f09a5c2a31211e3f8d045e2872","Answer selection is the most complex phase of a question answering (QA) system. To solve this task, typical approaches use unsupervised methods such as computing the similarity between query and answer, optionally exploiting advanced syntactic, semantic or logic representations. In this paper, we study supervised discriminative models that learn to select (rank) answers using examples of question and answer pairs. The pair representation is implicitly provided by kernel combinations applied to each of its members. To reduce the burden of large amounts of manual annotation, we represent question and answer pairs by means of powerful generalization methods, exploiting the application of structural kernels to syntactic/semantic structures. We experiment with support vector machines and string kernels, syntactic and shallow semantic tree kernels applied to part-of-speech tag sequences, syntactic parse trees and predicate argument structures on two datasets which we have compiled and made available. Our results on classification of correct and incorrect pairs show that our best model improves the bag-of-words model by 63% on a TREC dataset. Moreover, such a binary classifier, used as a re-ranker, improves the mean reciprocal rank of our baseline QA system by 13%. These findings demonstrate that our method automatically selects an appropriate representation of question-answer relations. © 2010 Elsevier Ltd. All rights reserved.","Information Retrieval; Kernel methods; Predicate argument structures; Question answering","Argument structures; Bag of words; Best model; Binary classifiers; Data sets; Discriminative models; Kernel methods; Logic representation; Manual annotation; Mean reciprocal ranks; Part Of Speech; QA system; Question Answering; Question answering systems; Re-ranking; Semantic tree; String Kernel; Syntactic parse tree; Unsupervised method; Classification (of information); Information retrieval; Natural language processing systems; Plant extracts; Rhenium compounds; Semantics; Syntactics; Query processing",2-s2.0-80052263476
"Wilkinson D., Thelwall M.","Researching personal information on the public web: Methods and ethics",2011,"Social Science Computer Review",50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-81755188521&doi=10.1177%2f0894439310378979&partnerID=40&md5=b1a12d72efcdd4a861a7015589dcc5d5","There are many personal and social issues that are rarely discussed in public and hence are difficult to study. Recently, however, the huge uptake of blogs, forums, and social network sites has created spaces in which previously private topics are publicly discussed, giving a new opportunity for researchers investigating such topics. This article describes a range of simple techniques to access personal information relevant to social research questions and illustrates them with small case studies. It also discusses ethical considerations, concluding that the default position is almost the reverse of that for traditional social science research: the text authors should not be asked for consent nor informed of the participation of their texts. Normally, however, steps should be taken to ensure that text authors are anonymous in academic publications even when their texts and identities are already public. © SAGE Publications 2011.","personal information; web research ethics; web research methods",,2-s2.0-81755188521
"Peirsman Y., Padó S.","Semantic relations in bilingual lexicons",2011,"ACM Transactions on Speech and Language Processing",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855250465&doi=10.1145%2f2050100.2050102&partnerID=40&md5=c878e16c0923d52c4f79067075a5c38b","Bilingual lexicons, essential to many NLP applications, can be constructed automatically on the basis of parallel or comparable corpora. In this article, wemake two contributions to their induction from comparable corpora. The first one concerns the creation of these lexicons.We show that seed lexicons can be improved by adding a bootstrapping procedure that uses cross-lingual distributional similarity. The second contribution concerns the evaluation of bilingual lexicons. It is generally based on translation lexicons, which corresponds to the implicit assumption that (cross-lingual) synonymy is the semantic relation of primary interest, even though other semantic relations like (cross-lingual) hyponymy or cohyponymymake up a considerable portion of translation pair candidates proposed by distributional methods. We argue that the focus on synonymy is an oversimplification and that many applications can profit from the inclusion of other semantic relations. We study what effect these semantic relations have on two crosslingual tasks: the cross-lingual projection of polarity scores and the cross-lingual modeling of selectional preferences. We find that the presence of non-synonymous semantic relations may negatively affect the former of these tasks, but benefit the latter. © 2011 ACM.","Bilingual lexicons; Multilingual knowledge induction; Selectional preferences; Semantic relations; Sentiment analysis; Vector space semantics","Bilingual lexicons; Multilingual knowledge induction; Selectional preferences; Semantic relations; Sentiment analysis; Vector space semantics; Natural language processing systems; Profitability; Vector spaces; Semantics",2-s2.0-84855250465
"Dohrn H., Riehle D.","Design and implementation of the sweble wikitext parser: Unlocking the structured data of Wikipedia",2011,"WikiSym 2011 Conference Proceedings - 7th Annual International Symposium on Wikis and Open Collaboration",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054792716&doi=10.1145%2f2038558.2038571&partnerID=40&md5=85d558af7a0fa0c227312111fc1f1221","The heart of each wiki, including Wikipedia, is its content. Most machine processing starts and ends with this content. At present, such processing is limited, because most wiki engines today cannot provide a complete and precise representation of the wiki's content. They can only generate HTML. The main reason is the lack of well-defined parsers that can handle the complexity of modern wiki markup. This applies to Media Wiki, the software running Wikipedia, and most other wiki engines. This paper shows why it has been so difficult to develop comprehensive parsers for wiki markup. It presents the design and implementation of a parser for Wikitext, the wiki markup language of MediaWiki. We use parsing expression grammars where most parsers used no grammars or grammars poorly suited to the task. Using this parser it is possible to directly and precisely query the structured data within wikis, including Wikipedia. The parser is available as open source from http://sweble.org. © 2011 ACM.","abstract syntax tree; AST; parsing expression grammar; PEG; Sweble; wiki; wiki parser; Wikipedia; WYSIWYG","Abstract Syntax Trees; AST; Parsing expression grammars; Sweble; wiki; wiki parser; Wikipedia; WYSIWYG; Trees (mathematics)",2-s2.0-80054792716
"Rakthanmanon T., Zhu Q., Keogh E.J.","Searching historical manuscripts for near-duplicate figures",2011,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054788264&doi=10.1145%2f2037342.2037346&partnerID=40&md5=34ca772a342b36b46044bed67ee1849b","In the next decade a majority of all the books ever published will be digitized and online. Naturally, most of the data in historical manuscripts is text, but there is also a large amount devoted to images. This observation is responsible for the dramatic increase in interest in query-by-content systems for historical documents. While querying/indexing systems can be useful, we believe that this domain is finally ready for unsupervised discovery of patterns. With this in mind, we introduce an efficient and scalable technique that can detect approximately repeated occurrences of images both within and between historical texts. We demonstrate that this ability to find repeated shapes allows us to do automatic annotation of manuscripts. We show the utility of our technique on datasets dating back to the fourteenth century. © 2011 ACM.",,"Automatic annotation; Data sets; Historical documents",2-s2.0-80054788264
"Fisher K., Foster N., Walker D., Zhu K.Q.","Forest: A language and toolkit for programming with filestores",2011,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054062487&doi=10.1145%2f2034773.2034814&partnerID=40&md5=3043e86ca8bf44f8acf4c822d0c5f6ea","A filestore is a structured collection of data files housed in a conventional hierarchical file system. Many applications use filestores as a poor-man's database, and the correct execution of these applications requires that the collection of files, directories, and symbolic links stored on disk satisfy a variety of precise invariants. Moreover, all of these structures must have acceptable ownership, permission, and timestamp attributes. Unfortunately, current programming languages do not provide support for documenting assumptions about filestores, detecting errors in them, or safely loading from and storing to them. This paper describes the design, implementation, and semantics of Forest, a new domain-specific language for describing filestores. The language uses a type-based metaphor to specify the expected structure, attributes, and invariants of filestores. Forest generates loading and storing functions that make it easy to connect data on disk to an isomorphic representation in memory that can be manipulated as if it were any other data structure. Forest also generates metadata that describes the degree to which the structures on the disk conform to the specification, making error detection easy. In a nutshell, Forest extends the rigorous discipline of typed programming languages to the untyped world of file systems. We have implemented Forest as an embedded domain-specific language in Haskell. In addition to generating infrastructure for reading, writing and checking file systems, our implementation generates type class instances that make it easy to build generic tools that operate over arbitrary filestores.We illustrate the utility of this infrastructure by building a file system visualizer, a file access checker, a generic query interface, description-directed variants of several standard UNIX shell tools and (circularly) a simple Forest description inference engine. Finally, we formalize a core fragment of Forest in a semantics inspired by classical tree logics and prove round-tripping laws showing that the loading and storing functions behave sensibly. Copyright © 2011 ACM.","Ad hoc data; Bidirectional transformations; Data description languages; Domain-specific languages; File systems; Filestores; Generic programming; Haskell","Ad hoc datum; Bidirectional transformation; Data description languages; Domain specific languages; File systems; Filestores; Generic programming; Haskell; Computer aided software engineering; Computer programming languages; Data structures; Error detection; Graphical user interfaces; Loading; Metadata; Problem oriented languages; Semantics; Standardization; Functional programming",2-s2.0-80054062487
"Thompson P., McNaught J., Montemagni S., Calzolari N., del Gratta R., Lee V., Marchi S., Monachini M., Pezik P., Quochi V., Rupp C.J., Sasaki Y., Venturi G., Rebholz-Schuhmann D., Ananiadou S.","The BioLexicon: A large-scale terminological resource for biomedical text mining",2011,"BMC Bioinformatics",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053915290&doi=10.1186%2f1471-2105-12-397&partnerID=40&md5=736582eab505bc52fa58a10a9e8e60a9","Background: Due to the rapidly expanding body of biomedical literature, biologists require increasingly sophisticated and efficient systems to help them to search for relevant information. Such systems should account for the multiple written variants used to represent biomedical concepts, and allow the user to search for specific pieces of knowledge (or events) involving these concepts, e.g., protein-protein interactions. Such functionality requires access to detailed information about words used in the biomedical literature. Existing databases and ontologies often have a specific focus and are oriented towards human use. Consequently, biological knowledge is dispersed amongst many resources, which often do not attempt to account for the large and frequently changing set of variants that appear in the literature. Additionally, such resources typically do not provide information about how terms relate to each other in texts to describe events.Results: This article provides an overview of the design, construction and evaluation of a large-scale lexical and conceptual resource for the biomedical domain, the BioLexicon. The resource can be exploited by text mining tools at several levels, e.g., part-of-speech tagging, recognition of biomedical entities, and the extraction of events in which they are involved. As such, the BioLexicon must account for real usage of words in biomedical texts. In particular, the BioLexicon gathers together different types of terms from several existing data resources into a single, unified repository, and augments them with new term variants automatically extracted from biomedical literature. Extraction of events is facilitated through the inclusion of biologically pertinent verbs (around which events are typically organized) together with information about typical patterns of grammatical and semantic behaviour, which are acquired from domain-specific texts. In order to foster interoperability, the BioLexicon is modelled using the Lexical Markup Framework, an ISO standard.Conclusions: The BioLexicon contains over 2.2 M lexical entries and over 1.8 M terminological variants, as well as over 3.3 M semantic relations, including over 2 M synonymy relations. Its exploitation can benefit both application developers and users. We demonstrate some such benefits by describing integration of the resource into a number of different tools, and evaluating improvements in performance that this can bring. © 2011 Thompson et al; licensee BioMed Central Ltd.",,"Application developers; Biomedical domain; Biomedical literature; Biomedical text minings; Part of speech tagging; Protein-protein interactions; Semantic relations; Typical patterns; Data mining; Extraction; Natural language processing systems; Proteins; Semantics; Terminology; Tools; Search engines; article; biology; data mining; factual database; human; linguistics; semantics; Computational Biology; Data Mining; Databases, Factual; Humans; Semantics; Vocabulary, Controlled",2-s2.0-80053915290
"Xia H.-L., Zhang Y.-S.","Design and implementation of a web news extraction system",2011,"Proceedings - 2011 8th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2011",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053395361&doi=10.1109%2fFSKD.2011.6019812&partnerID=40&md5=c9955d0e5d514fd0009ab0033aebd185","With the widespread use of Internet and the development of information technology, there is a tremendous amount of news information resource. The ability to quickly obtain useful resource from the huge news information is a crucial problem at present. Based on the analysis of the structure of the news portal page, this paper combines the technology of regular expressions and HTML-Parser, introduces a general method of news and information automatically extracted, and realizes an efficient general news information extraction system. The system can not only extract the headlines, time released, text content rightly, but also can extract the news information relevant or similar to the subject. © 2011 IEEE.","Content Page; Index Page; Information Extraction; Regular Expressions","Content Page; Extraction systems; General method; Index pages; Information Extraction; News information; News portal page; Regular expressions; Text content; Fuzzy systems; Information analysis; Information retrieval systems",2-s2.0-80053395361
"Kim J.-J., Rebholz-Schuhmann D.","Improving the extraction of complex regulatory events from scientific text by using ontology-based inference",2011,"Journal of Biomedical Semantics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869405581&doi=10.1186%2f2041-1480-2-S5-S3&partnerID=40&md5=442de80b52da1f536231bfd48b1ff98f","Background: The extraction of complex events from biomedical text is a challenging task and requires in-depth semantic analysis. Previous approaches associate lexical and syntactic resources with ontologies for the semantic analysis, but fall short in testing the benefits from the use of domain knowledge. Results: We developed a system that deduces implicit events from explicitly expressed events by using inference rules that encode domain knowledge. We evaluated the system with the inference module on three tasks: First, when tested against a corpus with manually annotated events, the inference module of our system contributes 53.2% of correct extractions, but does not cause any incorrect results. Second, the system overall reproduces 33.1% of the transcription regulatory events contained in RegulonDB (up to 85.0% precision) and the inference module is required for 93.8% of the reproduced events. Third, we applied the system with minimum adaptations to the identification of cell activity regulation events, confirming that the inference improves the performance of the system also on this task. Conclusions: Our research shows that the inference based on domain knowledge plays a significant role in extracting complex events from text. This approach has great potential in recognizing the complex concepts of such biomedical ontologies as Gene Ontology in the literature. © 2011 Kim and Rebholz-Schuhmann; licensee BioMed Central Ltd.",,,2-s2.0-84869405581
"Bender E.M., Flickinger D., Oepen S., Zhang Y.","Parser evaluation over local and non-local deep dependencies in a large corpus",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053229660&partnerID=40&md5=fe66a86534facacb71c442d8f160ba38","In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-the-art parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies. © 2011 Association for Computational Linguistics.",,"Linguistic phenomena; Naturally occurring; Nonlocal; Wikipedia; Computational linguistics; Natural language processing systems",2-s2.0-80053229660
"Artzi Y., Zettlemoyer L.","Bootstrapping semantic parsers from conversations",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053260124&partnerID=40&md5=3528a6fae9dd5dec5de20857a6cefb1d","Conversations provide rich opportunities for interactive, continuous learning. When something goes wrong, a system can ask for clarification, rewording, or otherwise redirect the interaction to achieve its goals. In this paper, we present an approach for using conversational interactions of this type to induce semantic parsers. We demonstrate learning without any explicit annotation of the meanings of user utterances. Instead, we model meaning with latent variables, and introduce a loss function to measure how well potential meanings match the conversation. This loss drives the overall learning approach, which induces a weighted CCG grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system. Experiments on DARPA Communicator conversational logs demonstrate effective learning, despite requiring no explicit meaning annotations. © 2011 Association for Computational Linguistics.",,"Bootstrapping semantics; Continuous learning; Conversational interaction; Dialog systems; Effective learning; Latent variable; Learning approach; Loss functions; Semantic analysis; Computational linguistics; Semantics; User interfaces; Natural language processing systems",2-s2.0-80053260124
"Li P., Wang Y., Gao W., Jiang J.","Generating aspect-oriented multi-document summarization with event-aspect model",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053275670&partnerID=40&md5=020f2ca0c67d58cd7b6e890f3b1e8c76","In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method. © 2011 Association for Computational Linguistics.",,"Aspect-oriented; Automatic Generation; Baseline methods; Dependency trees; Integer Linear Programming; Key feature; Multi-document summarization; Multiple documents; Quantitative evaluation; Random walk models; Sentence compression; Sentence selection; Computational linguistics; Integer programming; Learning algorithms; Plant extracts; Trees (mathematics); Natural language processing systems",2-s2.0-80053275670
"Kwiatkowski T., Zettlemoyer L., Goldwater S., Steedman M.","Lexical generalization in CCG grammar induction for semantic parsing",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053270439&partnerID=40&md5=cde8eba06c30f62d3433b3a971da1ffd","We consider the problem of learning factored probabilistic CCG grammars for semantic parsing from data containing sentences paired with logical-form meaning representations. Traditional CCG lexicons list lexical items that pair words and phrases with syntactic and semantic content. Such lexicons can be inefficient when words appear repeatedly with closely related lexical content. In this paper, we introduce factored lexicons, which include both lexemes to model word meaning and templates to model systematic variation in word usage. We also present an algorithm for learning factored CCG lexicons, along with a probabilistic parse-selection model. Evaluations on benchmark datasets demonstrate that the approach learns highly accurate parsers, whose generalization performance benefits greatly from the lexical factoring. © 2011 Association for Computational Linguistics.",,"Benchmark datasets; Generalization performance; Grammar induction; Lexical content; Lexical items; Semantic content; Semantic parsing; Systematic variation; Word meaning; Computational linguistics; Natural language processing systems; Semantics; Formal languages",2-s2.0-80053270439
"Do Q.X., Chan Y.S., Roth D.","Minimally supervised event causality identification",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053242014&partnerID=40&md5=80cd30d76705e5bb46d89034da6853c7","This paper develops a minimally supervised approach, based on focused distributional similarity methods and discourse connectives, for identifying of causality relations between events in context. While it has been shown that distributional similarity can help identifying causality, we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events. We show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality. © 2011 Association for Computational Linguistics.",,"Distributional similarities; Computational linguistics; Natural language processing systems",2-s2.0-80053242014
"Dinesh N., Joshi A., Lee I.","Computing logical form on regulatory texts",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053280317&partnerID=40&md5=4ee9d20f875dc4a7df87e3e37903458d","The computation of logical form has been proposed as an intermediate step in the translation of sentences to logic. Logical form encodes the resolution of scope ambiguities. In this paper, we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form, called abstract syntax trees (ASTs). The main step in computing ASTs is to order scope-taking operators. A learning model for ranking is adapted for tius ordering. We design features by studying the problem of comparing the scope of one operator to another. The scope comparisons are used to compute ASTs, with an F-score of 90.6% on the set of ordering decisons. © 2011 Association for Computational Linguistics.",,"Abstract Syntax Trees; Design features; F-score; Learning models; Logical forms; Computational linguistics; Trees (mathematics); Natural language processing systems",2-s2.0-80053280317
"Ritter A., Cherry C., Dolan W.B.","Data-driven response generation in social media",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",68,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053292690&partnerID=40&md5=d21fde27a61b296a8d34ddacbffbd04f","We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response. © 2011 Association for Computational Linguistics.",,"Data-driven; Data-driven approach; Human evaluation; Human response; Phrase-based statistical machine translation; Response generation; Social media; Computational linguistics; Information retrieval; Natural language processing systems; Speech transmission; Translation (languages)",2-s2.0-80053292690
"Monz C.","Machine learning for query formulation in question answering",2011,"Natural Language Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855614764&doi=10.1017%2fS1351324910000276&partnerID=40&md5=4e686ef608e48bb1c69e416afae7de2c","Research on question answering dates back to the 1960s but has more recently been revisited as part of TREC's evaluation campaigns, where question answering is addressed as a subarea of information retrieval that focuses on specific answers to a user's information need. Whereas document retrieval systems aim to return the documents that are most relevant to a user's query, question answering systems aim to return actual answers to a users question. Despite this difference, question answering systems rely on information retrieval components to identify documents that contain an answer to a user's question. The computationally more expensive answer extraction methods are then applied only to this subset of documents that are likely to contain an answer. As information retrieval methods are used to filter the documents in the collection, the performance of this component is critical as documents that are not retrieved are not analyzed by the answer extraction component. The formulation of queries that are used for retrieving those documents has a strong impact on the effectiveness of the retrieval component. In this paper, we focus on predicting the importance of terms from the original question. We use model tree machine learning techniques in order to assign weights to query terms according to their usefulness for identifying documents that contain an answer. Term weights are learned by inspecting a large number of query formulation variations and their respective accuracy in identifying documents containing an answer. Several linguistic features are used for building the models, including part-of-speech tags, degree of connectivity in the dependency parse tree of the question, and ontological information. All of these features are extracted automatically by using several natural language processing tools. Incorporating the learned weights into a state-of-the-art retrieval system results in statistically significant improvements in identifying answer-bearing documents. © 2010 Cambridge University Press.",,"Answer extraction; Degree of connectivity; Information need; Linguistic features; NAtural language processing; Parse trees; Part-of-speech tags; Query formulation; Query terms; Question Answering; Question answering systems; Retrieval systems; Term weight; Use-model; Computational linguistics; Forestry; Information retrieval systems; Learning algorithms; Learning systems; Natural language processing systems; Query processing; Search engines; Information retrieval; Algorithms; Computation; Data Processing; Information Retrieval; Programing Languages; Research",2-s2.0-84855614764
"Bobillo F., Straccia U.","Fuzzy ontology representation using OWL 2",2011,"International Journal of Approximate Reasoning",143,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052414638&doi=10.1016%2fj.ijar.2011.05.003&partnerID=40&md5=6b1b6f64cbe72d8b1b7d82561d065a43","The need to deal with vague information in Semantic Web languages is rising in importance and, thus, calls for a standard way to represent such information. We may address this issue by either extending current Semantic Web languages to cope with vagueness, or by providing a procedure to represent such information within current standard languages and tools. In this work, we follow the latter approach, by identifying the syntactic differences that a fuzzy ontology language has to cope with, and by proposing a concrete methodology to represent fuzzy ontologies using OWL 2 annotation properties. We also report on some prototypical implementations: a plug-in to edit fuzzy ontologies using OWL 2 annotations and some parsers that translate fuzzy ontologies represented using our methodology into the languages supported by some reasoners. © 2011 Elsevier Inc. All rights reserved.","Fuzzy description logics; Fuzzy languages for the semantic web; Fuzzy ontologies; Fuzzy OWL 2","Fuzzy description logic; Fuzzy language; Fuzzy ontology; Fuzzy OWL 2; Plug-ins; Prototypical implementation; Semantic web languages; Data description; Formal languages; Semantic Web; Semantics; User interfaces; Ontology",2-s2.0-80052414638
"Naderi N., Kappler T., Baker C.J.O., Witte R.","Organismtagger: Detection, normalization and grounding of organism entities in biomedical documents",2011,"Bioinformatics",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053437050&doi=10.1093%2fbioinformatics%2fbtr452&partnerID=40&md5=a0cb9020561e809f53685dd5a8f32647","Motivation: Semantic tagging of organism mentions in full-text articles is an important part of literature mining and semantic enrichment solutions. Tagged organism mentions also play a pivotal role in disambiguating other entities in a text, such as proteins. A high-precision organism tagging system must be able to detect the numerous forms of organism mentions, including common names as well as the traditional taxonomic groups: genus, species and strains. In addition, such a system must resolve abbreviations and acronyms, assign the scientific name and if possible link the detected mention to the NCBI Taxonomy database for further semantic queries and literature navigation.Results: We present the OrganismTagger, a hybrid rule-based/machine learning system to extract organism mentions from the literature. It includes tools for automatically generating lexical and ontological resources from a copy of the NCBI Taxonomy database, thereby facilitating system updates by end users. Its novel ontology-based resources can also be reused in other semantic mining and linked data tasks. Each detected organism mention is normalized to a canonical name through the resolution of acronyms and abbreviations and subsequently grounded with an NCBI Taxonomy database ID. In particular, our system combines a novel machine-learning approach with rule-based and lexical methods for detecting strain mentions in documents. On our manually annotated OT corpus, the OrganismTagger achieves a precision of 95%, a recall of 94% and a grounding accuracy of 97.5%. On the manually annotated corpus of Linnaeus-100, the results show a precision of 99%, recall of 97% and grounding accuracy of 97.4%. © The Author 2011. Published by Oxford University Press. All rights reserved.",,"algorithm; article; artificial intelligence; classification; data mining; human; medical information system; methodology; natural language processing; nomenclature; publication; semantics; Algorithms; Artificial Intelligence; Classification; Data Mining; Humans; Natural Language Processing; Publications; Semantics; Terminology as Topic; Unified Medical Language System",2-s2.0-80053437050
"Deveaud R., Boudin F., Bellot P.","LIA at INEX 2010 book track",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052705461&doi=10.1007%2f978-3-642-23577-1_10&partnerID=40&md5=d2d1f1dcaf1ec9e37441719852064461","In this paper we describe our participation and present our contributions in the INEX 2010 Book Track. Digitized books are now a common source of information on the Web, however OCR sometimes introduces errors that can penalize Information Retrieval. We propose a method for correcting hyphenations in the books and we analyse its impact on the Best Books for Reference task. The observed improvement is around 1%. This year we also experimented different query expansion techniques. The first one consists of selecting informative words from a Wikipedia page related to the topic. The second one uses a dependency parser to enrich the query with the detected phrases using a Markov Random Field model. We show that there is a significant improvement over the state-of-the-art when using a large weighted list of Wikipedia words, meanwhile hyphenation correction has an impact on their distribution over the book corpus. © 2011 Springer-Verlag.",,"Common source; Dependency parser; Markov Random Field model; Query expansion techniques; Wikipedia; Image segmentation; User interfaces; XML; Information retrieval",2-s2.0-80052705461
"Srikrishna D., Coram M.A.","Using noun phrases for navigating biomedical literature on pubmed: How many updates are we losing track of?",2011,"PLoS ONE",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052829301&doi=10.1371%2fjournal.pone.0024920&partnerID=40&md5=64bc6486cd9bc1471e8466a6d0c1975e","Author-supplied citations are a fraction of the related literature for a paper. The ""related citations"" on PubMed is typically dozens or hundreds of results long, and does not offer hints why these results are related. Using noun phrases derived from the sentences of the paper, we show it is possible to more transparently navigate to PubMed updates through search terms that can associate a paper with its citations. The algorithm to generate these search terms involved automatically extracting noun phrases from the paper using natural language processing tools, and ranking them by the number of occurrences in the paper compared to the number of occurrences on the web. We define search queries having at least one instance of overlap between the author-supplied citations of the paper and the top 20 search results as citation validated (CV). When the overlapping citations were written by same authors as the paper itself, we define it as CV-S and different authors is defined as CV-D. For a systematic sample of 883 papers on PubMed Central, at least one of the search terms for 86% of the papers is CV-D versus 65% for the top 20 PubMed ""related citations."" We hypothesize these quantities computed for the 20 million papers on PubMed to differ within 5% of these percentages. Averaged across all 883 papers, 5 search terms are CV-D, and 10 search terms are CV-S, and 6 unique citations validate these searches. Potentially related literature uncovered by citation-validated searches (either CV-S or CV-D) are on the order of ten per paper - many more if the remaining searches that are not citation-validated are taken into account. The significance and relationship of each search result to the paper can only be vetted and explained by a researcher with knowledge of or interest in that paper. © 2011 Srikrishna, Coram.",,"algorithm; article; information retrieval; medical literature; Medline; natural language processing; nomenclature; search engine; documentation; MeSH heading; methodology; Abstracting and Indexing as Topic; Information Storage and Retrieval; Medical Subject Headings; Natural Language Processing; PubMed; Subject Headings",2-s2.0-80052829301
"Yang T., Zhang Z.","A method of natural language understanding on SPARQL ontology query",2011,"Proceedings of the World Congress on Intelligent Control and Automation (WCICA)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052402741&doi=10.1109%2fWCICA.2011.5970542&partnerID=40&md5=48e6bb7f6a25b5e41679713119228ae6","In this paper, the development of a SPARQL ontology query based on Natural Language Understanding is presented. To obtain ontology knowledge conveniently, Stanford Parser for user's natural language inquiring is utilized, and query triple according to the grammar is constructed. The method greatly reduces the number of combinations compared with the key word method. Combined with user dictionary, the terms of query triple can be more accurately mapped to the ontology entities. Meanwhile scores calculation is not only considered the similarity of words' form and semantic, but also considered the ambiguity of concept, for returning to the specific concept as far as possible. Experimental results are presented to demonstrate the performance and validity of method. © 2011 IEEE.","natural language understanding; ontology query; ontology triple; query triple; SPARQL","Natural language understanding; Natural languages; Ontology query; query triple; Similarity of words; SPARQL; Stanford; Intelligent control; Query processing; Semantics; Ontology",2-s2.0-80052402741
"Yuan Y.","Extracting spatial relations from document for geographic information retrieval",2011,"Proceedings - 2011 19th International Conference on Geoinformatics, Geoinformatics 2011",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052372801&doi=10.1109%2fGeoInformatics.2011.5980797&partnerID=40&md5=50518c74604604091fd7713d572d77ec","Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on ""debris flow took place in Hunan last year"", the documents selected in this way may only contain the words ""debris flow"" and ""Hunan"" rather than refer to ""debris"" flow actually occurred in ""Hunan"". Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. First, we analyze the characters of spatial relation expressions in natural language and there are two types of spatial relations: topology and direction. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree. Therefore, we construct a shortest path dependency kernel for SVM to complete the task. The experiment results show that our dependency tree kernel achieves significant improvement than previous method. © 2011 IEEE.","Dependency tree; Geographic information retrieval; Kernel function; Spatial relations; Support vector machine","Dependency trees; Geographic information retrieval; Kernel function; Spatial relations; Support vector; Debris; Geographic information systems; Graph theory; Natural language processing systems; Plant extracts; Support vector machines; User interfaces; Vector spaces; Information retrieval",2-s2.0-80052372801
"Parhi M., Acharya B.M., Puthal B.","An efficient technique to discover sensor web registry services for WSN with QoS based Multi-layered SOA framework",2011,"International Conference on Recent Trends in Information Technology, ICRTIT 2011",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052210169&doi=10.1109%2fICRTIT.2011.5972385&partnerID=40&md5=b1896ad5a5e64bc8cbe325cefcf569b8","Recently Wireless Sensor Network Services have potential usage in a wide range of application domain. However due to the solid and mature advancement of SWE standard, there is still one challenge is left that has to be addressed within this context i.e. discovery of Sensor Web Registry services throughout heterogeneous environment which raises several concerns like performance, reliability, and robustness. Many approaches and frameworks have been proposed to discover the sensor web registry services. Some of the approaches assume that the requests are placed in SOAP compatible formats while others focus on GUI based parametric query processing. We have formulated an approach that uses the Natural Language Query Processing which is a convenient and easy method of sensor data access in comparison to SQL or XML based Query Language like XQuery and XPath. We propose an architecture based on Multi-layered SOA Framework that organizes the method of sensor web registry service discovery in an efficient and structured manner by adding some new layers like Request Parser & Query Generator(RPQ), Service Verifier and Certifier (SVC) and Service Rank Calculator(SRC). We describe a typical weather sensor web service discovery where RPQ facilitates the processing of plain text request query to a most appropriate weather sensor web service and also an algorithm with implementation for a complete cycle of suitable sensor web registry service discovery according to requester's functional and QoS requirements. © 2011 IEEE.","Natural Language Processing; QoS; Request Parser & Query Generator (RPQ); Sensor Web Enablement Standard (SWE); Sensor Web Registry; Service Rank Calculator (SRC); Service Verifier and Certifier (SVC); Wireless Sensor Network (WSN)","NAtural language processing; Sensor Web Enablement Standard (SWE); Sensor Web Registry; Service Rank Calculator (SRC); Service Verifier and Certifier (SVC); Wireless sensor; Computational linguistics; Information technology; Mathematical instruments; Natural language processing systems; Quality of service; Query languages; Query processing; Sensors; Wireless sensor networks; Web services",2-s2.0-80052210169
"Fisher K., Foster N., Walker D., Zhu K.Q.","Forest: A language and toolkit for programming with filestores",2011,"ACM SIGPLAN Notices",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053362080&doi=10.1145%2f2034574.2034814&partnerID=40&md5=b0980bbfb77bc0dea5732dcf9578f1ad","A filestore is a structured collection of data files housed in a conventional hierarchical file system. Many applications use filestores as a poor-man's database, and the correct execution of these applications requires that the collection of files, directories, and symbolic links stored on disk satisfy a variety of precise invariants. Moreover, all of these structures must have acceptable ownership, permission, and timestamp attributes. Unfortunately, current programming languages do not provide support for documenting assumptions about filestores, detecting errors in them, or safely loading from and storing to them. This paper describes the design, implementation, and semantics of Forest, a new domain-specific language for describing filestores. The language uses a type-based metaphor to specify the expected structure, attributes, and invariants of filestores. Forest generates loading and storing functions that make it easy to connect data on disk to an isomorphic representation in memory that can be manipulated as if it were any other data structure. Forest also generates metadata that describes the degree to which the structures on the disk conform to the specification, making error detection easy. In a nutshell, Forest extends the rigorous discipline of typed programming languages to the untyped world of file systems. We have implemented Forest as an embedded domain-specific language in Haskell. In addition to generating infrastructure for reading, writing and checking file systems, our implementation generates type class instances that make it easy to build generic tools that operate over arbitrary filestores.We illustrate the utility of this infrastructure by building a file system visualizer, a file access checker, a generic query interface, description-directed variants of several standard UNIX shell tools and (circularly) a simple Forest description inference engine. Finally, we formalize a core fragment of Forest in a semantics inspired by classical tree logics and prove round-tripping laws showing that the loading and storing functions behave sensibly. Copyright © 2011 ACM.","Ad hoc data; Bidirectional transformations; Data description languages; Domain-specific languages; File systems; Filestores; Generic programming; Haskell","Ad hoc datum; Bidirectional transformation; Data description languages; Domain specific languages; File systems; Filestores; Generic programming; Haskell; Building codes; Computer aided software engineering; Data structures; Error detection; Graphical user interfaces; Loading; Metadata; Semantics; Problem oriented languages",2-s2.0-80053362080
"Gregory I.N., Hardie A.","Visual GISting: Bringing together corpus linguistics and geographical information systems",2011,"Literary and Linguistic Computing",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051980174&doi=10.1093%2fllc%2ffqr022&partnerID=40&md5=f6dd1d6344698aa060007f661cd54611","Corpus linguistics and Geographical Information Systems (GIS) are approaches exploiting computer-based methodologies in the study of, respectively, language and language usage, and spatial patterns in geographical databases. We present an approach that uses corpus methods to bridge the gap between the textual content of a corpus (and, thus, the typically textual concerns of many branches of the humanities) and the geo-referenced database at the heart of a GIS. Using part-of-speech tagging to extract instances of proper nouns from a corpus, and a gazetteer to limit these instances to those representing place-names, a database of the places mentioned in a corpus can be created, visualized, and analysed using GIS technology. It is also possible to visualize the meanings associated with particular place-names, by building GIS databases on the collocation of place-names with particular semantic categories in their immediate context. In this way, we can create maps that visualize the geographical distribution of mentions of concepts such as war, government, or money in a particular data set. The approach cannot be entirely automated and some manual intervention is required. Nevertheless, the method is clearly valuable for the interpretation of spatial phenomena in text corpora. © The Author 2011. Published by Oxford University Press on behalf of ALLC. All rights reserved.",,,2-s2.0-80051980174
"Can D., Saraçlar M.","Lattice indexing for spoken term detection",2011,"IEEE Transactions on Audio, Speech and Language Processing",67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052042597&doi=10.1109%2fTASL.2011.2134087&partnerID=40&md5=a639a354e08653185541c0a74d2cb677","This paper considers the problem of constructing an efficient inverted index for the spoken term detection (STD) task. More specifically, we construct a deterministic weighted finite-state transducer storing soft-hits in the form of (utterance ID, start time, end time, posterior score) quadruplets. We propose a generalized factor transducer structure which retains the time information necessary for performing STD. The required information is embedded into the path weights of the factor transducer without disrupting the inherent optimality. We also describe how to index all substrings seen in a collection of raw automatic speech recognition lattices using the proposed structure. Our STD indexing/search implementation is built upon the OpenFst Library and is designed to scale well to large problems. Experiments on Turkish and English data sets corroborate our claims. © 2011 IEEE.","Factor automata; lattice indexing; speech retrieval (SR); spoken term detection (STD); weighted finite-state transducers","Factor automata; lattice indexing; speech retrieval (SR); spoken term detection (STD); Weighted finite-state transducers; Indexing (of information); Transducers; Speech recognition",2-s2.0-80052042597
"Waldinger R., Bobrow D.G., Condoravdi C., Das A., Richardson K.","Accessing structured health information through English queries and automatic deduction",2011,"AAAI Spring Symposium - Technical Report",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051516712&partnerID=40&md5=1c0284fd98795b4b4388af8ed6e0080f","While much health data is available online, patients who are not technically astute may be unable to access it because they may not know the relevant resources, they may be reluctant to confront an unfamiliar interface, and they may not know how to compose an answer from information provided by multiple heterogeneous resources. We describe ongoing research in using natural English text queries and automated deduction to obtain answers based on multiple structured data sources in a specific subject domain. Each English query is transformed using natural language technology into an unambiguous logical form; this is submitted to a theorem prover that operates over an axiomatic theory of the subject domain. Symbols in the theory are linked to relations in external databases known to the system. An answer is obtained from the proof, along with an English language explanation of how the answer was obtained. Answers need not be present explicitly in any of the databases, but rather may be deduced or computed from the information they provide. Although English is highly ambiguous, the natural language technology is informed by subject domain knowledge, so that readings of the query that are syntactically plausible but semantically impossible are discarded. When a question is still ambiguous, the system can interrogate the patient to determine what meaning was intended. Additional queries can clarify earlier ones or ask questions referring to previously computed answers. We describe a prototype system, Quadri, which answers questions about HIV treatment using the Stanford HIV Drug Resistance Database and other resources. Natural language processing is provided by PARC's Bridge, and the deductive mechanism is SRI's SNARK theorem prover. We discuss some of the problems that must be faced to make this approach work, and some of our solutions.",,"Automated deduction; Axiomatic theory; Domain knowledge; Drug resistance; English languages; Health data; Health informations; Heterogeneous resources; Know-how; Logical forms; NAtural language processing; Natural languages; Prototype system; Stanford; Structured data; Text query; Theorem provers; Artificial intelligence; Computational linguistics; Database systems; Health; Natural language processing systems; Query processing",2-s2.0-80051516712
"Wang H., Liu R., Theodoratos D., Wu X.","Efficient storage and temporal query evaluation in hierarchical data archiving systems",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961187831&doi=10.1007%2f978-3-642-22351-8_7&partnerID=40&md5=c8ddaabc89ac95c7db0d0c41b89e3c3e","Data archiving has been commonly used in many fields for data backup and analysis purposes. Although comprehensive application software, new computing and storage technologies, and the Internet have made it easier to create, collect and store all types of data, the meaningful storing, accessing, and managing of database archives in a cost-effective way remains extremely challenging. In this paper, we focus on hierarchical data archiving that has been popularly used in the scientific field and web data management. First, we propose a novel compaction scheme for archiving hierarchical data. By compacting both data and timestamps, our scheme substantially reduces not only the amount of needed storage, but also the incremental archiving time. Second, we design a temporal query language to support data retrieval from the compact data archives. Third, as compaction on data and timestamps may bring significant overhead to query evaluation, we investigate how to optimize such overhead by exploiting the characteristics of the queries and of the archived hierarchical data. Finally, we conduct an extensive experimentation to demonstrate the effectiveness and efficiency of both our efficient storage and query optimization techniques. © 2011 Springer-Verlag Berlin Heidelberg.",,"Application softwares; Data archives; Data archiving; Data backups; Data retrieval; Hierarchical data; Query evaluation; Query optimization; Scientific fields; Storage technology; Temporal queries; Time stamps; Web data management; Compaction; Hierarchical systems; Information management; Management information systems; Optimization; Query languages; User interfaces; Search engines",2-s2.0-79961187831
"Ahmad M., Aboulnaga A., Babu S., Munagala K.","Interaction-aware scheduling of report-generation workloads",2011,"VLDB Journal",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960451994&doi=10.1007%2fs00778-011-0217-y&partnerID=40&md5=6ec0859d3327ab3752a81607ebc7f69f","The typical workload in a database system consists of a mix of multiple queries of different types that run concurrently. Interactions among the different queries in a query mix can have a significant impact on database performance. Hence, optimizing database performance requires reasoning about query mixes rather than considering queries individually. Current database systems lack the ability to do such reasoning. We propose a new approach based on planning experiments and statistical modeling to capture the impact of query interactions. Our approach requires no prior assumptions about the internal workings of the database system or the nature and cause of query interactions, making it portable across systems. To demonstrate the potential of modeling and exploiting query interactions, we have developed a novel interaction-aware query scheduler for report-generation workloads. Our scheduler, called QShuffler, uses two query scheduling algorithms that leverage models of query interactions. The first algorithm is optimized for workloads where queries are submitted in large batches. The second algorithm targets workloads where queries arrive continuously, and scheduling decisions have to be made online. We report an experimental evaluation of QShuffler using TPC-H workloads running on IBM DB2. The evaluation shows that QShuffler, by modeling and exploiting query interactions, can consistently outperform (up to 4x) query schedulers in current database systems. © 2011 Springer-Verlag.","Business intelligence; Experiment-driven performance modeling; Query interactions; Report generation; Scheduling; Workload management","Database performance; Experimental evaluation; Multiple queries; Performance Modeling; Planning experiments; Query interactions; Report generation; Scheduling decisions; Significant impacts; Statistical modeling; TPC-H workload; Workload management; Experiments; Optimization; Scheduling algorithms; Query languages",2-s2.0-79960451994
"Jeliazkova N., Kochev N.","AMBIT-SMARTS: Efficient searching of chemical structures and fragments",2011,"Molecular Informatics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052090509&doi=10.1002%2fminf.201100028&partnerID=40&md5=772197591a35ee092d60b5d041814ffd","We present new developments in the AMBIT open source software package for efficient searching of chemical structures and structural fragments. AMBIT-SMARTS is a Java based software built on top of The Chemistry Development Kit. The AMBIT-SMARTS parser implements the entire SMARTS language specification with several syntax extensions that enable support for custom modifications introduced by third party software packages such as OpenEye, MOE and OpenBabel. The goal of yet another open-source SMARTS parser implementation is to achieve better performance and compatibility with multiple existing flavours of the SMARTS language, as well as to provide utilities for running efficient SMARTS queries in large structural databases. We describe a combination of approaches towards lowering the computational cost and improving the response time of substructure queries. An exhaustive comparison of the AMBIT algorithm with several subgraph isomorphism implementations is performed. To demonstrate the performance of the entire system from an end-user point of view, response time statistics for Web service substructure search queries against a database of 4.5M structures are also reported. The package has wide applicability in the implementation of various chemoinformatics tasks. It has already been used in several projects dealing with descriptor calculation and predictive algorithms, database queries, web applications and web services. Copyright © 2011 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.","Benchmark; Database; Screening; SMARTS; Subgraph isomorphism; Substructure searching","algorithm; article; chemical structure; chemoinformatics; computer program; data base; information science; priority journal; quantitative structure activity relation; screening",2-s2.0-80052090509
"Valliyammai C., Thamarai Selvi S.","A hierarchical mobile agent monitoring mechanism with XML-based mobile agents",2011,"Communications in Computer and Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960351372&doi=10.1007%2f978-3-642-22543-7_8&partnerID=40&md5=47477e5c5b7372261739b52a9cd6f9d0","Mobility management is a necessity in highly dynamic and large-scale Mobile Agent (MA) networks, especially in a multi-region environment, in order to control and communicate with agents after launching. In this paper, we have proposed a hierarchical mobile agent monitoring mechanism using Mobile Monitor Agents (MMA's). The mobile agent system is implemented in XML. This XML based system provides a system independent, standardized, effective and customized solution for application development. Distributing MMA dynamically solves the scalability issue of Centralized monitoring mechanism, and still has the advantages of Hierarchical monitoring mechanism to decrease the information processing bottleneck issue at centralized server. The MMA takes care of a certain number of agent servers and it reduces the traffic in the centralized server by bypassing queries from the agent servers under its control. XML helps to decrease the amount of data transferred and the processing part with the Java parsers is not transferred in network. There is less transfer of data because only data representation is migrated. © 2011 Springer-Verlag.","Agent Server; Mobile Agents; Mobile Monitor Agent","Agent monitoring; Agent servers; Application development; Centralized server; Customized solutions; Data representations; Hierarchical mobile; Mobile agent system; Mobile Monitor Agent; Mobility management; Scalability issue; Data processing; Semantic Web; Semantics; User interfaces; XML; Mobile agents",2-s2.0-79960351372
"Ferrara E., Baumgartner R.","Design of automatically adaptable web wrappers",2011,"ICAART 2011 - Proceedings of the 3rd International Conference on Agents and Artificial Intelligence",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960126787&partnerID=40&md5=1ae7054d9392155d5745ef15934723a5","Nowadays, the huge amount of information distributed through the Web motivates studying techniques to be adopted in order to extract relevant data in an efficient and reliable way. Both academia and enterprises developed several approaches of Web data extraction, for example using techniques of artificial intelligence or machine learning. Some commonly adopted procedures, namely wrappers, ensure a high degree of precision of information extracted from Web pages, and, at the same time, have to prove robustness in order not to compromise quality and reliability of data themselves. In this paper we focus on some experimental aspects related to the robustness of the data extraction process and the possibility of automatically adapting wrappers. We discuss the implementation of algorithms for finding similarities between two different version of a Web page, in order to handle modifications, avoiding the failure of data extraction tasks and ensuring reliability of information extracted. Our purpose is to evaluate performances, advantages and draw-backs of our novel system of automatic wrapper adaptation.","Data mining; Information extraction; Semantic web","Amount of information; Automatic wrappers; Data extraction; Degree of precision; Experimental aspects; Information Extraction; Reliability of information; Web data extraction; Web page; Web wrappers; Artificial intelligence; Semantic Web; Semantics; User interfaces; World Wide Web; Data mining",2-s2.0-79960126787
"Sinkovics A.","Functional extensions to the boost metaprogram library",2011,"Electronic Notes in Theoretical Computer Science",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960211621&doi=10.1016%2fj.entcs.2011.06.006&partnerID=40&md5=5362d572111576b9fb4bef8e39753b7a","More and more C++ applications use template metaprograms directly or indirectly by using libraries based on that. Given the complexity of template metaprogramming, developers need supporting libraries. The most widely used one is the Boost template metaprogramming library. It implements commonly used compile time algorithms and meta-data structures in an extensible and reusable way. Despite the wellknown commonality of template metaprogramming and the functional programming paradigm, boost::mpl lacks a few important features directly supporting the functional style. In this paper we evaluate how and in what degree boost::mpl supports functional programming and present new elements it can be improved with. © 2011 Elsevier B.V. All rights reserved.","boost::mpl; C++; functional programming; template metaprogramming","boost::mpl; C++; Compile-time algorithms; Functional extension; Metaprograms; Template metaprogramming; Computer applications; Data structures; Libraries; Functional programming",2-s2.0-79960211621
"Chandra S., Torlak E., Barman S., Bodik R.","Angelic debugging",2011,"Proceedings - International Conference on Software Engineering",63,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959882115&doi=10.1145%2f1985793.1985811&partnerID=40&md5=0fb25ab2bc31d52c70e931ea06195b29","Software ships with known bugs because it is expensive to pinpoint and fix the bug exposed by a failing test. To reduce the cost of bug identification, we locate expressions that are likely causes of bugs and thus candidates for repair. Our symbolic method approximates an ideal approach to fixing bugs mechanically, which is to search the space of all edits to the program for one that repairs the failing test without breaking any passing test. We approximate the expensive ideal of exploring syntactic edits by instead computing the set of values whose substitution for the expression corrects the execution. We observe that an expression is a repair candidate if it can be replaced with a value that fixes a failing test and in each passing test, its value can be changed to another value without breaking the test. The latter condition makes the expression flexible in that it permits multiple values. The key observation is that the repair of a flexible expression is less likely to break a passing test. The method is called angelic debugging because the values are computed by angelically nondeterministic statements. We implemented the method on top of the Java PathFinder model checker. Our experiments with this technique show promise of its applicability in speeding up program debugging. © 2011 ACM.","angelic non-determinism; debugging; symbolic execution; tests","Java PathFinder; Model checker; Non-determinism; Symbolic execution; Symbolic methods; tests; Model checking; Repair; Software engineering; Software testing; Testing; Program debugging",2-s2.0-79959882115
"Al-Bahadili H., Al-Saab S.","Development of a novel compressed index-query web search engine model",2011,"International Journal of Information Technology and Web Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857616301&doi=10.4018%2fjitwe.2011070103&partnerID=40&md5=f0f5d48732187d5556b43debc517cdff","In this paper, the authors present a description of a new Web search engine model, the compressed index-query (CIQ) Web search engine model. This model incorporates two bit-level compression layers implemented at the back-end processor (server) side, one layer resides after the indexer acting as a second compression layer to generate a double compressed index (index compressor), and the second layer resides after the query parser for query compression (query compressor) to enable bit-level compressed index-query search. The data compression algorithm used in this model is the Hamming codes-based data compression (HCDC) algorithm, which is an asymmetric, lossless, bit-level algorithm permits CIQ search. The different components of the new Web model are implemented in a prototype CIQ test tool (CIQTT), which is used as a test bench to validate the accuracy and integrity of the retrieved data and evaluate the performance of the proposed model. The test results demonstrate that the proposed CIQ model reduces disk space requirements and searching time by more than 24%, and attains a 100% agreement when compared with an uncompressed model. Copyright © 2011, IGI Global.","Bit-Level Compression; Full-Text Compressed Self-Index; Hamming Codes-Based Data Compression (HCDC) Algorithm; Indexes; Query Optimization; Web Search Engine","Bit-level compression; Compression layers; Data compression algorithms; Disk space; Indexes; Lossless; Query optimization; Searching time; Second layer; Self-index; Test benches; Test tools; Web models; Algorithms; Information retrieval; Search engines; World Wide Web; Data compression",2-s2.0-84857616301
"Pérez-Castillo R., De Guzmán I.G.-R., Piattini M., Weber B., Places Á.S.","An empirical comparison of static and dynamic business process mining",2011,"Proceedings of the ACM Symposium on Applied Computing",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959314832&doi=10.1145%2f1982185.1982249&partnerID=40&md5=aed91413020c6c60d3a4bf386d103d92","Legacy information systems age over time as a consequence of the uncontrolled maintenance and need to be modernized. Process mining allows the discovery of business processes embedded in legacy information systems, which is necessary to preserve the legacy business knowledge, and align them with the new, modernized information systems. There are two main approaches to address the mining of business processes from legacy information systems: (i) the static approach that only considers legacy source code's elements from a syntactical viewpoint; and (ii) the dynamic approach, which also considers information derived by system execution. Unfortunately, there is a lack of empirical evidence facilitating the selection of one of them. This paper provides a formal comparison of the static and dynamic approach through a case study. This study shows that the static approach provides better performance, while the dynamic approach discovers more accurate business processes. © 2011 ACM.","business process mining; case study; software modernization","Business knowledge; Business Process; Empirical comparison; Empirical evidence; Legacy information systems; Process mining; software modernization; Source codes; Static and dynamic; Static and dynamic approach; Static approach; Data mining; Embedded systems; Information systems; Legacy systems",2-s2.0-79959314832
"Tendulkar D.M., Phalak C.","Proactive performance testing using SQL performance assurance services (SQL-PASS)",2011,"International Conference and Workshop on Emerging Trends in Technology 2011, ICWET 2011 - Conference Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958734871&doi=10.1145%2f1980022.1980138&partnerID=40&md5=a6e7cda4e08a38b9807ffef968d69f90","The information systems are becoming more and more complex and it is very common these days where database sizes are in hundreds of GBs. Handling of such large data volumes is creating challenges for assuring the performance of end-user applications. Applications involving database transactions are worst hit as there is no clue about the time required to fetch the relevant data from the huge database. The tools available in the market and the existing methodologies are suitable for the production environment but not effective in the development environment. This creates a gap between database application development and its deployment in the production environment. Therefore assuring the performance against high volume is an indisputable problem faced by the application developer and tester. In this paper, we discuss the work done in industry to tackle the SQL performance problem arising because of large data volumes and present SQL-PASS (SQL Performance Assurance Services), a web based service developed by TCS innovation Lab - Performance engineering, which helps to validate the SQL performance against high volumes without using actual data. We will also discuss how this service has been used in the development project to assure the performance. Copyright © 2011 ACM.","Database emulation; Database statistics; Explain plan; Extrapolation of database statistics; Query execution time forecasting; Query optimization; SQL performance","Database emulation; Database statistics; Explain plan; Query execution time; Query optimization; SQL performance; Innovation; User interfaces; Database systems",2-s2.0-79958734871
"Jimeno-Yepes A., Mclnnes B.T., Aronson A.R.","Collocation analysis for UMLS knowledge-based word sense disambiguation",2011,"BMC Bioinformatics",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958167843&doi=10.1186%2f1471-2105-12-S3-S4&partnerID=40&md5=efa71127d81ddd981a6711d238e5f6a8","Background: The effectiveness of knowledge-based word sense disambiguation (WSD) approaches depends in part on the information available in the reference knowledge resource. Off the shelf, these resources are not optimized for WSD and might lack terms to model the context properly. In addition, they might include noisy terms which contribute to false positives in the disambiguation results.Methods: We analyzed some collocation types which could improve the performance of knowledge-based disambiguation methods. Collocations are obtained by extracting candidate collocations from MEDLINE and then assigning them to one of the senses of an ambiguous word. We performed this assignment either using semantic group profiles or a knowledge-based disambiguation method. In addition to collocations, we used second-order features from a previously implemented approach.Specifically, we measured the effect of these collocations in two knowledge-based WSD methods. The first method, AEC, uses the knowledge from the UMLS to collect examples from MEDLINE which are used to train a Naïve Bayes approach. The second method, MRD, builds a profile for each candidate sense based on the UMLS and compares the profile to the context of the ambiguous word.We have used two WSD test sets which contain disambiguation cases which are mapped to UMLS concepts. The first one, the NLM WSD set, was developed manually by several domain experts and contains words with high frequency occurrence in MEDLINE. The second one, the MSH WSD set, was developed automatically using the MeSH indexing in MEDLINE. It contains a larger set of words and covers a larger number of UMLS semantic types.Results: The results indicate an improvement after the use of collocations, although the approaches have different performance depending on the data set. In the NLM WSD set, the improvement is larger for the MRD disambiguation method using second-order features. Assignment of collocations to a candidate sense based on UMLS semantic group profiles is more effective in the AEC method.In the MSH WSD set, the increment in performance is modest for all the methods. Collocations combined with the MRD disambiguation method have the best performance. The MRD disambiguation method and second-order features provide an insignificant change in performance. The AEC disambiguation method gives a modest improvement in performance. Assignment of collocations to a candidate sense based on knowledge-based methods has better performance.Conclusions: Collocations improve the performance of knowledge-based disambiguation methods, although results vary depending on the test set and method used. Generally, the AEC method is sensitive to query drift. Using AEC, just a few selected terms provide a large improvement in disambiguation performance. The MRD method handles noisy terms better but requires a larger set of terms to improve performance.",,"Disambiguation method; False positive; Group profiles; Improve performance; Knowledge resource; Knowledge-based methods; Second-order features; Word-sense disambiguation; Semantics; Knowledge based systems; algorithm; article; Bayes theorem; knowledge base; medical informatics; medical information system; Medline; MeSH heading; methodology; natural language processing; nomenclature; semantics; Algorithms; Bayes Theorem; Knowledge Bases; Medical Informatics; Medical Subject Headings; MEDLINE; Natural Language Processing; Semantics; Terminology as Topic; Unified Medical Language System",2-s2.0-79958167843
"Heidenreich F., Johannes J., Karol S., Seifert M., Thiele M., Wende C., Wilke C.","Integrating OCL and textual modelling languages",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957644161&doi=10.1007%2f978-3-642-21210-9_34&partnerID=40&md5=2059b75b5b2ad99f5beac581e363c0b7","In the past years, many OCL tools achieved a transition of OCL from a language meant to constrain UML models to a universal constraint language applied to various modelling and metamodelling languages. However, OCL users still experience a discrepancy between the now highly extensible parsing and evaluation backend of OCL tools and the lack of appropriate frontend tooling like advanced OCL editors that adapt to the different application scenarios. We argue that this has to be addressed both at a technical and methodological level. Therefore, this paper provides an overview of the technical foundations to provide an integrated OCL tooling frontend and backend for arbitrary textual modelling languages and contributes a stepwise process for such an integration. We distinguish two kinds of integration: external definition of OCL constraints and embedded definition of OCL constraints. Due to the textual notation of OCL the second kind provides particularly deep integration with textual modelling languages. We apply our approach in two case studies and discuss the benefits and limitations of the approach in general and both integration kinds in particular. © 2011 Springer-Verlag Berlin Heidelberg.",,"Application scenario; Constraint language; Modelling and metamodelling; Modelling language; Textual notation; UML Model; Integration; Software engineering; Unified Modeling Language; Models",2-s2.0-79957644161
"Chou C.-P., Jea K.-F., Liao H.-H.","A syntactic approach to twig-query matching on XML streams",2011,"Journal of Systems and Software",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953711119&doi=10.1016%2fj.jss.2011.01.033&partnerID=40&md5=a44fe930799896c71b04d191fa38a989","Query matching on XML streams is challenging work for querying efficiency when the amount of queried stream data is huge and the data can be streamed in continuously. In this paper, the method Syntactic Twig-Query Matching (STQM) is proposed to process queries on an XML stream and return the query results continuously and immediately. STQM matches twig queries on the XML stream in a syntactic manner by using a lexical analyzer and a parser, both of which are built from our lexical-rules and grammar-rules generators according to the user's queries and document schema, respectively. For query matching, the lexical analyzer scans the incoming XML stream and the parser recognizes XML structures for retrieving every twig-query result from the XML stream. Moreover, STQM obtains query results without a post-phase for excluding false positives, which are common in many streaming query methods. Through the experimental results, we found that STQM matches the twig query efficiently and also has good scalability both in the queried data size and the branch degree of the twig query. The proposed method takes less execution time than that of a sequence-based approach, which is widely accepted as a proper solution to the XML stream query. © 2011 Elsevier Inc. All rights reserved.","Stream query; Syntactic pattern recognition; Twig query processing; XML","Data size; Execution time; False positive; Lexical analyzers; Query matching; Query methods; Query results; Stream data; Stream query; Syntactic approach; Syntactic pattern recognition; Twig queries; Twig query processing; XML stream; Hydraulics; Pattern recognition; Query processing; Syntactics; XML",2-s2.0-79953711119
"Nguyen T.-T., Truong N.-T., Nguyen V.-H.","Verifying java object invariants at runtime",2011,"International Journal of Software Engineering and Knowledge Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052325900&doi=10.1142%2fS0218194011005281&partnerID=40&md5=4d157e2bcb938c388d507576e30a79b8","An object invariant consisting of a set of properties that must hold for all instances of a class at any time is usually used in object-oriented design. However, verifying object invariants at runtime is always a challenging task in software verification. This paper proposes a method for verifying invariants of Java objects at runtime using AOP. Suppose that a software application is designed using UML models and its constraints are specified in OCL expressions, the software is then implemented, by default, using the UML design. They propose to construct verifiable aspects which are automatically generated from OCL constraints. These aspects can be woven into Java code to check whether object invariants are violated at runtime. Benefiting from AOP in separation of crosscutting concerns and weaving mechanisms, generated aspects can do the verification task whenever values of objects' attributes are changed. A Verification Aspect Generator (VAG) tool has been developed allowing the automatic generation of verifying aspects from the UML/OCL constraints. © 2011 World Scientific Publishing Company.","AOP; AspectJ; OCL invariant; runtime; Software verification","AOP; Aspect-J; OCL invariant; Runtimes; Software verification; Unified Modeling Language; Weaving; Verification",2-s2.0-80052325900
"D'Ulizia A., Ferri F., Grifoni P.","A survey of grammatical inference methods for natural language learning",2011,"Artificial Intelligence Review",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650768553&doi=10.1007%2fs10462-010-9199-1&partnerID=40&md5=b2b8c99791c60c6b0b88387405d1df15","The high complexity of natural language and the huge amount of human and temporal resources necessary for producing the grammars lead several researchers in the area of Natural Language Processing to investigate various solutions for automating grammar generation and updating processes. Many algorithms for Context-Free Grammar inference have been developed in the literature. This paper provides a survey of the methodologies for inferring context-free grammars from examples, developed by researchers in the last decade. After introducing some preliminary definitions and notations concerning learning and inductive inference, some of the most relevant existing grammatical inference methods for Natural Language are described and classified according to the kind of presentation (if text or informant) and the type of information (if supervised, unsupervised, or semi-supervised). Moreover, the state of the art of the strategies for evaluation and comparison of different grammar inference methods is presented. The goal of the paper is to provide a reader with introduction to major concepts and current approaches in Natural Language Learning research. © 2011 Springer Science+Business Media B.V.","Context free grammar; Grammatical inference; Natural language","Grammar inference; Grammatical inference; Grammatical inferences; Inductive inference; Natural language learning; NAtural language processing; Natural languages; Semi-supervised; State of the art; Computational linguistics; Context free grammars; Natural language processing systems; Research; Surveys; Inference engines",2-s2.0-78650768553
"Surdeanu M., Ciaramita M., Zaragoza H.","Learning to rank answers to non-factoid questions from web collections",2011,"Computational Linguistics",66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958717927&doi=10.1162%2fCOLI_a_00051&partnerID=40&md5=1cbdfe96e2c4f21e454a4d7c2b3dda84","This work investigates the use of linguistically motivated features to improve search, in particular for ranking answers to non-factoid questions. We show that it is possible to exploit existing large collections of question-answer pairs (from online social Question Answering sites) to extract such features and train ranking models which combine them effectively.We investigate a wide range of feature types, some exploiting natural language processing such as coarse word sense disambiguation, named-entity identification, syntactic parsing, and semantic role labeling. Our experiments demonstrate that linguistic features, in combination, yield considerable improvements in accuracy. Depending on the system settings we measure relative improvements of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling evidence to date that complex linguistic features such as word senses and semantic roles can have a significant impact on large-scale information retrieval tasks. © 2011 Association for Computational Linguistics.",,,2-s2.0-79958717927
"Saigaonkar S., Rao M., Mantha S.S.","Publish subscribe system based on ontology and XML filtering",2011,"ICCRD2011 - 2011 3rd International Conference on Computer Research and Development",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957560394&doi=10.1109%2fICCRD.2011.5763993&partnerID=40&md5=f5ab9974a36546bfd702889ab7a5c99d","Publish subscribe systems have received considerable attention during recent years. XML based publish-subscribe systems are on tremendous rise because of the properties of XML. But in order to accomplish the task filtering mechanism is required. Many filtering mechanisms exists such as XFilter, YFilter, AFilter etc, but most of these mechanisms do not filter the document for semantic matched information. M-Filter performs semantic matching of XML documents. But value-based predicates cannot be filtered by M-Filter. This paper proposes a technique which does semantic matching of XML data and also handles equality based value-based predicates. User Profiles are specified as XPath queries. A query node is checked in OWL classes, if a node is found it is returned with the matching results as well as with the semantically related data. Thus multiple queries are formed from a single subscription and these queries are then converted into twig patterns. On the other hand, XML document is parsed by DOM parser and a tree is built. Matching of twig nodes and the tree nodes takes place and only the matched information is displayed or delivered to the user. © 2011 IEEE.","Ontology; Publish-Subscribe; Semantics; XML Filtering","Filtering mechanism; Multiple queries; Publish-Subscribe; Publish-subscribe systems; Query nodes; Semantic matching; Tree nodes; Twig pattern; User profile; Value-based predicates; XML data; XML Filtering; XPath queries; Distributed computer systems; Information retrieval systems; Matched filters; Ontology; Plant extracts; Publishing; Semantics; XML",2-s2.0-79957560394
"Nyein S.S.","Mining contents in web page using cosine similarity",2011,"ICCRD2011 - 2011 3rd International Conference on Computer Research and Development",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957570699&doi=10.1109%2fICCRD.2011.5764177&partnerID=40&md5=597914fa8e98b8277624d0def6148dcf","Web pages typically contain a large amount of information that is not part of the main contents of the pages, e.g.; banner ads, navigation bars, copy right and privacy notices, advertisements which are not related to the main content (relevant information). In this paper, an algorithm is proposed that extract the main content from the web documents. The algorithm based on Content Structure Tree (CST). Firstly, the proposed system use HTML Parser to construct DOM (Document Object Model) tree from which construct Content Structure Tree (CST) which can easily separate the main content blocks from the other blocks. The proposed system then introduce cosine similarity measure to evaluate which parts of the CST tree represent the less important and which parts represent the more important of the page. The proposed system can define the ranking of the documents using similarity values and also extracts the top ranked documents as more relevant to the query. © 2011 IEEE.","Cosine Similarity; CST tree; DOM tree","Amount of information; Content structure; Cosine similarity; CST tree; Document object model; DOM tree; System use; Web document; Web page; Algorithms; User interfaces; World Wide Web; XML; Trees (mathematics)",2-s2.0-79957570699
"Shiming Z., Zhen Q., Xiang X.H.","A Deep Web education resources sharing solution different from China e-Learning Technology Standard",2011,"International Journal of Networking and Virtual Organisations",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956112378&doi=10.1504%2fIJNVO.2011.040004&partnerID=40&md5=5adb8e03018f405b5915656d074d12ee","In the paper, a solution based on Deep Web integration technology is designed to realise the Deep Web education resources sharing and solve the defects of China e-Learning Technology Standard. The solution supports four modules: knowledge discovery, integrated query, knowledge base and system monitor. The former two modules function based on the integration with the later two modules. The design has been applied to an inter-library search system. Four libraries are joined in the system. Good feedback has been received. The project has been awarded the 'Shanghai Science and Technology Progress Third Level Award'. The solution is still preliminary, and there are many aspects to be improved. The future research is supported by Innovation Program of Shanghai Municipal Education Commission. Copyright © 2011 Inderscience Enterprises Ltd.","China; China E-Learning Technology Standard; Deep Web integrated technology; Education resources sharing solution; Environment change monitor; HTML form parser kit; HTTP client programming tool kit; Information isolated island; Integrated query process; Knowledge discovery","China; China E-Learning Technology Standard; Deep Web integrated technology; Education resources sharing solution; Environment change monitor; Information isolated island; Integrated query process; Knowledge discovery; E-learning; Education; HTTP; Integration; Knowledge based systems; Standards; User interfaces; World Wide Web; Technology",2-s2.0-79956112378
"Zhu S., Wang Y., Ji C., Tao H.","TGG based automatic transformation between SBML and other biological modeling languages",2011,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959582404&partnerID=40&md5=22626285889a5b3a9e15e7aa70e166d3","XSLT based transformation, between SBML and other biological modeling languages, cannot describe comprehensive context-sensitive semantic correspondences among the inner elements of biological modeling objects; cannot guarantee the determinacy and syntactic correctness of transformation results; and also cannot meet industrial transformation requirements. Toward these problems, a triple graph grammar (TGG) based transformation method is presented, which utilizes graph grammars to define SBML schema and other biological modeling languages, and utilizes TGG to construct transformation between them. On this basis, a transformation algorithm is presented, which has polynomial time complexity and can guarantee determinacy and syntactic correctness. Compared with the traditional transformation between SBML and other biological modeling languages, the method in this paper has the following characteristics: 1) It utilizes context-sensitive grammar and has strong description capability; 2) It imposes graph-based approach to simplify transformation definition process; 3) It only needs static analysis of transformation rules at the design time without exploring dynamic analysis, because validation must be achieved if transformation rules satisfy some constraints; 4) It only requires to change direction of transformation rules to implement bi-directional transformation, without modifying any element; and 5) It supports incremental change propagation, since it preserves the correspondence information between source and target objects. Finally, correctness and effectiveness of this method are verified through an example of transformation between Petri net and SBML.","Graph grammar; Model transformation; SBML; TGG; XML","Automatic transformations; Bi-directional; Biological modeling; Context-sensitive; Design time; Graph grammar; Graph-based; Incremental change propagation; Industrial transformations; Model transformation; Polynomial time complexity; SBML; Semantic correspondence; Target object; TGG; Transformation algorithm; Transformation methods; Transformation rules; Triple graph grammars; Context sensitive grammars; Dynamic analysis; Petri nets; Polynomial approximation; Semantics; Syntactics; Context sensitive languages",2-s2.0-79959582404
"Lutteroth C., Draheim D., Weber G.","A type system for reflective program generators",2011,"Science of Computer Programming",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952616846&doi=10.1016%2fj.scico.2010.12.002&partnerID=40&md5=ce10053d03ef421cd7252b352a68793d","We describe a type system for a generative mechanism that generalizes the concept of generic types by combining it with a controlled form of reflection. This mechanism makes many code generation tasks possible for which generic types alone would be insufficient. The power of code generation features are carefully balanced with their safety, which enables us to perform static type checks on generator code. This leads to a generalized notion of type safety for generators. © 2008 Elsevier B.V. All rights reserved.","Generic programming; Model-based generation; Reflection; Type safety","Code Generation; Generative mechanism; Generic programming; Generic types; Model-based generation; Type safety; Type systems; Automatic programming; Network components; Java programming language",2-s2.0-79952616846
"Kilpeläinen P.","Checking determinism of XML Schema content models in optimal time",2011,"Information Systems",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951552066&doi=10.1016%2fj.is.2010.10.001&partnerID=40&md5=359472390f8476a67113d96a1e2ce6dc","We consider the determinism checking of XML Schema content models, as required by the W3C Recommendation. We argue that currently applied solutions have flaws and make processors vulnerable to exponential resource needs by pathological schemas, and we help to eliminate this potential vulnerability of XML Schema based systems. XML Schema content models are essentially regular expressions extended with numeric occurrence indicators. A previously published polynomial-time solution to check the determinism of such expressions is improved to run in linear time, and the improved algorithm is implemented and evaluated experimentally. When compared to the corresponding method of a popular production-quality XML Schema processor, the new implementation runs orders of magnitude faster. Enhancing the solution to take further extensions of XML Schema into account without compromising its linear scalability is also discussed. © 2010 Elsevier B.V. All rights reserved.","Java; Numeric occurrence indicator; One-unambiguity; Regular expression; Unique particle attribution; Weak determinism","Java; Numeric occurrence indicator; One-unambiguity; Regular expression; Unique particle attribution; Weak determinism; Model checking; Polynomials; XML",2-s2.0-79951552066
"Stoter J., Visser T., van Oosterom P., Quak W., Bakker N.","A semantic-rich multi-scale information model for topography",2011,"International Journal of Geographical Information Science",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960379887&doi=10.1080%2f13658816.2010.490218&partnerID=40&md5=d18ec973466931301b3f3232a28bf173","National mapping agencies maintain topographic data sets at different scales. Keeping the data sets consistent, for example by means of automated update propagation, requires formal knowledge on how the different data sets relate to each other. This article presents a multi-scale information model that, first, integrates the data states at the different scales and, second, formalises semantics on scale transitions. This is expressed using the Unified Modelling Language (UML) class diagrams, complemented with Object Constraint Language (OCL). Based on a requirement analysis using the needs of the Netherlands' Kadaster as case study, this article examines several modelling alternatives and selects the optimal modelling approach for a multi-scale information model for topography. The model is evaluated through a prototype database implementation. The results show that UML/OCL provides an appropriate formalism to model rich semantics on both multi-scale data content and scale transitions, which can be used for guarding consistency based on automated generalisation of updates. Further research is required to express generalisation specifications that are currently not formalised and that are only available in software code or as cartographers' knowledge. © 2011 Taylor & Francis.","Generalisation; Knowledge formalisation; Multi-scale; Multi-scale semantics; Spatial data modelling","cartography; data set; knowledge; mapping; numerical model; software; spatial data; topography",2-s2.0-79960379887
"Huang J., Wang H., Jia Y., Fuxman A.","Link-based hidden attribute discovery for objects on Web",2011,"ACM International Conference Proceeding Series",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953878885&doi=10.1145%2f1951365.1951421&partnerID=40&md5=3734c3cce63e90eb1a45fe093128d45a","Information extraction from the Web is of growing importance. Objects on the Web are often associated with many attributes that describe the objects. It is essential to extract these attributes and map them to their corresponding objects. However, much attribute information about an object is hidden in the dynamic user interaction and is not on the Web page that describes the object. Existing information extraction approaches focus on getting information from the object Web page only, which means a lot of attribute information is lost. In this paper, we study the dynamic user interaction on exploratory search Websites and propose a novel link-based approach to discover attributes and map them to objects. We build an exploratory search model for exploratory Web sites, and we propose algorithms for identifying, clustering, and relationship mining of related Web pages based on the model. Using the unsupervised method in our approach, we are able to discover hidden attributes not explicitly shown on object Web pages. We test our approach on two online shopping Websites. We achieve high precision and recall: For entirely crawled Web sites the precision and recall are 98% and 97% respectively. For randomly crawled (sampled) Web sites the precision and recall are 98% and 80% respectively.","Attribute discovery; Attribute labeling; Exploratory search; Information extraction; Query link","Attribute discovery; Attribute labeling; Exploratory search; Information extraction; Query link; Clustering algorithms; Database systems; Electronic commerce; Information analysis; Technology; Websites",2-s2.0-79953878885
"Saha D., Narula V.","Gramin: A system for incremental learning of programming language grammars",2011,"Proceedings of the 4th India Software Engineering Conference 2011, ISEC'11",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953892813&doi=10.1145%2f1953355.1953380&partnerID=40&md5=e636b7193333fc3aad5cabc3100363fc","Major software vendors now offer software maintenance as service. Bug detection and removal are major tasks in the maintenance phase of the software life cycle. Vendors often rely on in-house built analysis tools for software fault localization. Programming language grammar is an absolute requirement for building such tools, especially for platforms which are built by third parties. However, the grammar may not be publicly available. Grammars obtained from language reference manuals are often not complete due to language evolution. Manual fixes of such a grammar is a cumbersome task, and requires special expertise. Grammar inference techniques have been traditionally used to infer grammar from the strings of a language, in the context of computational linguistic, but have not been successful in learning industry standard complex programming language grammars. In this paper we present a programming language grammar inference system, called Gramin, which is used to infer grammar from sample programs. Gramin employs various optimizations to make grammar inference practical in the domain of programming languages. We demonstrate the effectiveness of the Gramin system by inferring a programming language grammar used in industry, and not available in public domain.","Grammar induction; Grammar inference; Grammar learning; Incremental learning; Programming language grammar inference","Grammar induction; Grammar inference; Grammar learning; Incremental learning; Programming language grammar inference; Ada (programming language); Computational grammars; Computational linguistics; Computer programming languages; Maintainability; Software engineering; Computer software maintenance",2-s2.0-79953892813
"Zhang M.-H.","Quantitative structural information for inferring context free grammars with an extended Cocke-Younger-Kasami algorithm",2011,"Pattern Recognition Letters",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79851515863&doi=10.1016%2fj.patrec.2010.12.017&partnerID=40&md5=726982f93a42f8fca1f07876ac88dfc4","In this paper, we propose an approach to quantitative structural information for inferring context free grammars. First, we construct derivative capacity of nonterminal symbols in context free grammar, concomitant indicator and embedded dimensional number of strings in samples set, which are called as quantitative structural information; then, we rewrite Cocke-Younger-Kasami (CYK) algorithm for parsing in the form of the derivative set; third, we present the construction of new production rule and the descriptive procedure for inferring with an extended CYK algorithm by the quantitative structural information. Finally, we discuss the extended CYK algorithm for inferring context free grammars. © 2011 Elsevier B.V. All rights reserved.","Chomsky normal form; Context free grammars; CYK algorithm; Grammatical inference; Machine learning; Quantitative structural information","Chomsky normal form; Context-free; CYK algorithm; Grammatical inferences; Machine learning; Structural information; Formal languages; Inference engines; Learning algorithms; Learning systems; Context free grammars",2-s2.0-79851515863
"Hoffman D.M., Ly-Gagnon D., Strooper P., Wang H.-Y.","Grammar-based test generation with YouGen",2011,"Software - Practice and Experience",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952478666&doi=10.1002%2fspe.1017&partnerID=40&md5=581e7c29283835ef6d205d31acf59b93","Grammars are traditionally used to recognize or parse sentences in a language, but they can also be used to generate sentences. In grammar-based test generation (GBTG), context-free grammars are used to generate sentences that are interpreted as test cases. A generator reads a grammar G and generates L(G), the language accepted by the grammar. Often L(G) is so large that it is not practical to execute all of the generated cases. Therefore, GBTG tools support 'tags': extra-grammatical annotations which restrict the generation. Since its introduction in the early 1970s, GBTG has become well established: proven on industrial projects and widely published in academic venues. Despite the demonstrated effectiveness, the tool support is uneven; some tools target specific domains, e.g. compiler testing, while others are proprietary. The tools can be difficult to use and the precise meaning of the tags are sometimes unclear. As a result, while many testing practitioners and researchers are aware of GBTG, few have detailed knowledge or experience. We present YouGen, a new GBTG tool supporting many of the tags provided by previous tools. In addition, YouGen incorporates covering-array tags, which support a generalized form of pairwise testing. These tags add considerable power to GBTG tools and have been available only in limited form in previous GBTG tools. We provide semantics for the YouGen tags using parse trees and a new construct, generation trees. We illustrate YouGen with both simple examples and a number of industrial case studies. © 2010 John Wiley & Sons, Ltd.","automated testing; covering array; firewall testing; grammar-based test generation; RSS; XML","Automated testing; covering array; firewall testing; grammar-based test generation; Industrial case study; Industrial projects; Pairwise testing; Parse trees; Test case; Test generations; Tool support; Tool supporting; Semantics; Software testing; Testing; XML; Equipment",2-s2.0-79952478666
"Romaniuk J., Suszczańska N., Szmal P.","Semantic analyzer in the thetos-3 system",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953230083&doi=10.1007%2f978-3-642-20095-3_22&partnerID=40&md5=e95d52d27693167225207c1fb668477a","The paper describes elements of the model of Polish language semantics, which have been developed for the system Thetos; Thetos translates Polish texts into a dialect of the Polish Sign Language. Translation takes place after the stage of semantic analysis, first of all after the predicate-argument structure of sentence is determined. Among other things, we discuss issues connected to semantic interpretation of syntactic groups (SGs), in that - semantic classification of noun SGs and verb SGs, and problems of deducing semantic relations from syntactic ones. We also discuss in brief problems of introducing lexical and non-lexical facial expressions into sign language utterances being generated. © 2011 Springer-Verlag.","NLP; Polsem analyzer; Polsyn analyzer; semantic processing; Thetos system","NLP; Polsem analyzer; Polsyn analyzer; semantic processing; Thetos system; Natural language processing systems; Semantics; Syntactics; Translation (languages)",2-s2.0-79953230083
"Syeed M.M.M., Aaltonen T., Hammouda I., Systä T.","Tool assisted analysis of open source projects: A multi-faceted challenge",2011,"International Journal of Open Source Software and Processes",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860521721&doi=10.4018%2fjossp.2011040103&partnerID=40&md5=c4c2e47178f2bd32ee7303e9ed0386d6","Open Source Software (OSS) is currently a widely adopted approach to developing and distributing software. OSS code adoption requires an understanding of the structure of the code base. For a deeper understanding of the maintenance, bug fixing and development activities, the structure of the developer community also needs to be understood, especially the relations between the code and community structures. This, in turn, is essential for the development and maintenance of software containing OSS code. This paper proposes a method and support tool for exploring the relations of the code base and community structures of OSS projects. The method and proposed tool, Binoculars, rely on generic and reusable query operations, formal definitions of which are given in the paper. The authors demonstrate the applicability of Binoculars with two examples. The authors analyze a well-known and active open source project, FFMpeg, and the open source version of the IaaS cloud computing project Eucalyptus. © 2011, IGI Global.","Computer Science; Open Source Software (OSS); Reverse Engineering; Social Network Analysis; Software Repositories; Tool Support","Bug-fixing; Community structures; Computing projects; Development activity; Formal definition; Open source projects; Open Source Software; Open sources; Query operations; Social Network Analysis; Software repositories; Support tool; Tool support; Binoculars; Computer science; Computer software; Computer software maintenance; Reverse engineering; Social networking (online); Social sciences; Open systems",2-s2.0-84860521721
"Tatar S., Cicekli I.","Automatic rule learning exploiting morphological features for named entity recognition in Turkish",2011,"Journal of Information Science",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954467274&doi=10.1177%2f0165551511398573&partnerID=40&md5=fe436a153996765f6141cf02cb3b3748","Named entity recognition (NER) is one of the basic tasks in automatic extraction of information from natural language texts. In this paper, we describe an automatic rule learning method that exploits different features of the input text to identify the named entities located in the natural language texts. Moreover, we explore the use of morphological features for extracting named entities from Turkish texts. We believe that the developed system can also be used for other agglutinative languages. The paper also provides a comprehensive overview of the field by reviewing the NER research literature. We conducted our experiments on the TurkIE dataset, a corpus of articles collected from different Turkish newspapers. Our method achieved an average F-score of 91.08% on the dataset. The results of the comparative experiments demonstrate that the developed technique is successfully applicable to the task of automatic NER and exploiting morphological features can significantly improve the NER from Turkish, an agglutinative language. © The Author(s) 2011.","automatic rule learning; morphological features; named entity recognition; Turkish","Agglutinative language; Automatic extraction; Comparative experiments; Data sets; F-score; morphological features; Named entities; named entity recognition; Natural language text; Rule learning; Turkish; Turkish texts; Turkishs; Experiments; Natural language processing systems; Query languages; Feature extraction",2-s2.0-79954467274
"Louis A.L., Engelbrecht A.P.","Unsupervised discovery of relations for analysis of textual data",2011,"Digital Investigation",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953859692&doi=10.1016%2fj.diin.2010.08.004&partnerID=40&md5=a225549e7a24cd0202fd00b0533bef08","This paper addresses the problem of analysing textual data for evidence discovery. A novel framework in which to perform evidence discovery is proposed in order to reduce the quantity of data to be analysed, aid the analysts' exploration of the data and enhance the intelligibility of the presentation of the data. The framework combines information extraction techniques with visual exploration techniques to provide a novel approach to performing evidence discovery, in the form of an evidence discovery system. By utilising unrestricted, unsupervised information extraction techniques, the investigator does not require input queries or keywords for searching, thus enabling the investigator to analyse portions of the data that may not have been identified by keyword searches. A preliminary study was performed to assess the usefulness of a text mining approach to evidence discovery from a text corpus in comparison with a traditional information retrieval approach. It was concluded that the novel approach to text analysis for evidence discovery presented in this paper is a viable and promising approach for consideration in digital forensics. The preliminary experiment showed that the results obtained from the evidence discovery system are sensible and useful. © 2010 Published by Elsevier Ltd. All rights reserved.","Evidence discovery; Information extraction; Relation discovery; Text analysis; Text mining","Evidence discovery; Information extraction; Relation discovery; Text analysis; Text mining; Information retrieval; Natural language processing systems; Data mining",2-s2.0-79953859692
"Gardent C., Gottesman B., Perez-Beltrachini L.","Using regular tree grammars to enhance sentence realisation",2011,"Natural Language Engineering",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960371208&doi=10.1017%2fS1351324911000076&partnerID=40&md5=94572c38acddcf4d840080018134a1ed","Feature-based regular tree grammars (FRTG) can be used to generate the derivation trees of a feature-based tree adjoining grammar (FTAG). We make use of this fact to specify and implement both an FTAG-based sentence realiser and a benchmark generator for this realiser. We argue furthermore that the FRTG encoding enables us to improve on other proposals based on a grammar of TAG derivation trees in several ways. It preserves the compositional semantics that can be encoded in feature-based TAGs; it increases efficiency and restricts overgeneration; and it provides a uniform resource for generation, benchmark construction and parsing. © 2011 Cambridge University Press.",,"Compositional semantics; Feature-based; Regular tree; Tree adjoining grammars; Semantics; Plant extracts",2-s2.0-79960371208
"Karimi S., Scholer F., Turpin A.","Machine transliteration survey",2011,"ACM Computing Surveys",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955674313&doi=10.1145%2f1922649.1922654&partnerID=40&md5=c34a6637e6462d3a6a4e2a2b924e2854","Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. The development of algorithms specifically for machine transliteration began over a decade ago based on the phonetics of source and target languages, followed by approaches using statistical and language-specific methods. In this survey, we review the key methodologies introduced in the transliteration literature. The approaches are categorized based on the resources and algorithms used, and the effectiveness is compared. © 2011 ACM.","Automatic translation; Machine learning; Machine transliteration; Natural language processing; Transliteration evaluation","Automatic translation; Machine learning; Machine transliteration; Natural language processing; Transliteration evaluation; Computational linguistics; Learning algorithms; Learning systems; Natural language processing systems; Surveys; Translation (languages)",2-s2.0-79955674313
"Frunza O., Inkpen D., Tran T.","A machine learning approach for identifying disease-treatment relations in short texts",2011,"IEEE Transactions on Knowledge and Data Engineering",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955523019&doi=10.1109%2fTKDE.2010.152&partnerID=40&md5=57688efa83c9ac0a5a356a9c47d86615","The Machine Learning (ML) field has gained its momentum in almost any domain of research and just recently has become a reliable tool in the medical domain. The empirical domain of automatic learning is used in tasks such as medical decision support, medical imaging, protein-protein interaction, extraction of medical knowledge, and for overall patient management care. ML is envisioned as a tool by which computer-based systems can be integrated in the healthcare field in order to get a better, more efficient medical care. This paper describes a ML-based methodology for building an application that is capable of identifying and disseminating healthcare information. It extracts sentences from published medical papers that mention diseases and treatments, and identifies semantic relations that exist between diseases and treatments. Our evaluation results for these tasks show that the proposed methodology obtains reliable outcomes that could be integrated in an application to be used in the medical care domain. The potential value of this paper stands in the ML settings that we propose and in the fact that we outperform previous results on the same data set. © 2011 IEEE.","Healthcare; machine learning; natural language processing.","Automatic-learning; Computer-based system; Data sets; Evaluation results; Machine-learning; Medical care; Medical decisions; Medical domains; Medical knowledge; Medical papers; NAtural language processing; Patient management; Potential values; Protein-protein interactions; Semantic relations; Computational linguistics; Decision support systems; Diseases; Health care; Learning algorithms; Learning systems; Medical computing; Natural language processing systems; Semantics; Medical imaging",2-s2.0-79955523019
"Bobrow D.G., Condoravdi C., Richardson K., Waldinger R., Das A.","Deducing answers to English questions from structured data",2011,"International Conference on Intelligent User Interfaces, Proceedings IUI",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952742263&doi=10.1145%2f1943403.1943450&partnerID=40&md5=d43d4c05ddb2c4b413c4ab2b17aa8528","We describe ongoing research using natural English text queries as an intelligent interface for inferring answers from structured data in a specific domain. Users can express queries whose answers need to be deduced from data in different databases, without knowing the structures of those databases nor even the existence of the sources used. Users can pose queries incrementally, elaborating on an initial query, and ask follow-up questions based on answers to earlier queries. Inference in an axiomatic theory of the subject domain links the natural form in which the question is posed to the way relevant data is represented in a database, and composes information obtained from several databases into an answer to a complex question. We describe the status of a prototype system, called Quadri, for answering questions about HIV treatment, using the Stanford HIV Drug Resistance Database [8] and European resources. We discuss some of the problems that need to be solved to make this approach work, and some of our solutions. © 2011 ACM.","Deductive question answering; Domain-specific queries; Interoperability; Logical form; Natural-language database access; Theorem proving","Axiomatic theory; Complex questions; Domain-specific queries; Drug resistance; Intelligent interface; Logical forms; Natural-language database access; Prototype system; Question Answering; Stanford; Structured data; Text query; Interoperability; Problem solving; Theorem proving; User interfaces; Query languages",2-s2.0-79952742263
"Béchet F.","Named Entity Recognition",2011,"Spoken Language Understanding: Systems for Extracting Semantic Information from Speech",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886663784&doi=10.1002%2f9781119992691.ch10&partnerID=40&md5=95199fcfd15d77b361def8595473d7ef",[No abstract available],"Benchmark Data Sets, Applications; Challenges Using Speech Input; Comparative Methods for NER from Speech; Evaluation Metrics; Message Understanding Conferences (MUC); Named Entity Recognition; New Trends in NER from Speech; Task Description; Word Error Rate (WER)",,2-s2.0-84886663784
"Tur G., Hakkani-Tür D.","Human/Human Conversation Understanding",2011,"Spoken Language Understanding: Systems for Extracting Semantic Information from Speech",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886654848&doi=10.1002%2f9781119992691.ch9&partnerID=40&md5=049adb9fb4f2227c3116c55190ec2bde",[No abstract available],"Action Item and Decision Detection; Argument Diagramming; Dialogue Act Segmentation and Tagging; Hot Spot Detection; Human/Human Conversation Understanding; Human/Human Conversation Understanding Tasks; Modeling Dominance; Speaker Role Detection; Subjectivity, Sentiment, and Opinion Detection",,2-s2.0-84886654848
"Zdraveski V., Jovanovik M., Stojanov R., Trajanov D.","HDL IP cores search engine based on semantic web technologies",2011,"Communications in Computer and Information Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952321345&doi=10.1007%2f978-3-642-19325-5_31&partnerID=40&md5=dcebcc397c737192aa87b5531ab8f864","A few years ago, the System on Chip idea grew largely and 'flooded' the market of embedded systems. Many System on Chip designers started to write their own HDL components and made them available on the Internet. The idea of searching for a couple of pre-written cores and building your own System on Chip only by connecting them seemed time saving. We've developed a system that enables a semantic description of VHDL IP components, allows search of specific components based on the unambiguous semantic description and works with prebuilt VHDL IP cores. We present an application built around the system and focus on the benefits the application user gains during the process of System on Chip design. © 2011 Springer-Verlag.","Components; Composition; HDL; Search; Semantic Web; System on Chip; VHDL","Components; Composition; HDL; Search; System on Chip; VHDL; Computer hardware description languages; Distributed computer systems; Embedded systems; Information technology; Innovation; Internet protocols; Search engines; Semantics; World Wide Web; Semantic Web",2-s2.0-79952321345
"Andrade D., Matsuzaki T., Tsujii J.","Effective use of dependency structure for bilingual lexicon creation",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952276189&doi=10.1007%2f978-3-642-19437-5_7&partnerID=40&md5=a6ab17f04ef66267b441a71ee2c6161a","Existing dictionaries may be effectively enlarged by finding the translations of single words, using comparable corpora. The idea is based on the assumption that similar words have similar contexts across multiple languages. However, previous research suggests the use of a simple bag-of-words model to capture the lexical context, or assumes that sufficient context information can be captured by the successor and predecessor of the dependency tree. While the latter may be sufficient for a close language-pair, we observed that the method is insufficient if the languages differ significantly, as is the case for Japanese and English. Given a query word, our proposed method uses a statistical model to extract relevant words, which tend to co-occur in the same sentence; additionally our proposed method uses three statistical models to extract relevant predecessors, successors and siblings in the dependency tree. We then combine the information gained from the four statistical models, and compare this lexical-dependency information across English and Japanese to identify likely translation candidates. Experiments based on openly accessible comparable corpora verify that our proposed method can increase Top 1 accuracy statistically significantly by around 13 percent points to 53%, and Top 20 accuracy to 91%. © 2011 Springer-Verlag.",,"Bag of words; Bilingual lexicons; Comparable corpora; Context information; Dependency structures; Dependency trees; Lexical contexts; Multiple languages; Query words; Statistical models; Computational linguistics; Text processing; Word processing; Translation (languages)",2-s2.0-79952276189
"Tai C.-H., Sam V., Gibrat J.-F., Garnier J., Munson P.J., Lee B.","Protein domain assignment from the recurrence of locally similar structures",2011,"Proteins: Structure, Function and Bioinformatics",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551474759&doi=10.1002%2fprot.22923&partnerID=40&md5=6a43e37a3768371660a03657f558a5ac","Domains are basic units of protein structure and essential for exploring protein fold space and structure evolution. With the structural genomics initiative, the number of protein structures in the Protein Databank (PDB) is increasing dramatically and domain assignments need to be done automatically. Most existing structural domain assignment programs define domains using the compactness of the domains and/or the number and strength of intra-domain versus inter-domain contacts. Here we present a different approach based on the recurrence of locally similar structural pieces (LSSPs) found by one-against-all structure comparisons with a dataset of 6373 protein chains from the PDB. Residues of the query protein are clustered using LSSPs via three different procedures to define domains. This approach gives results that are comparable to several existing programs that use geometrical and other structural information explicitly. Remarkably, most of the proteins that contribute the LSSPs defining a domain do not themselves contain the domain of interest. This study shows that domains can be defined by a collection of relatively small locally similar structural pieces containing, on average, four secondary structure elements. In addition, it indicates that domains are indeed made of recurrent small structural pieces that are used to build protein structures of many different folds as suggested by recent studies. © 2010 Wiley-Liss, Inc.","Clustering; Domain parsing; LSSP; Pairwise correlation method; Secondary structures; Singular value decomposition; Structure comparisons; Symmetric matrix factorization","algorithm; amino acid sequence; article; cluster analysis; correlation analysis; priority journal; protein domain; protein structure; Protein Conformation; Proteins",2-s2.0-79551474759
"Mittal S., Mittal A.","Versatile question answering systems: Seeing in synthesis",2011,"International Journal of Intelligent Information and Database Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952542587&doi=10.1504%2fIJIIDS.2011.038968&partnerID=40&md5=4feb61d9f1a6461bfdff9d98b3566dd9","Recent advances in massive information storage and growth of internet have led to increased necessity of tools such as search engines and QA systems to get meaningful answers from the vast amount of information. The complex questions appearing in real-life generally require multiple techniques or sub-division of question at various levels. Several practical systems have been developed to explore this. This paper reviews state-of-art QA systems which provide enhanced and versatile functionality at various levels in their architecture and then makes the following contributions: © 2011 Inderscience Enterprises Ltd.","QA design parameters; QAS; Question answering system; Survey; Versatile QA systems","Amount of information; Complex questions; Design parameters; Information storage; Practical systems; QA system; QAS; Question answering system; Question answering systems; Natural language processing systems; Surveys; Search engines",2-s2.0-79952542587
"Porkoláb Z., Sinkovics Á.","Domain-specific language integration with compile-time parser generator library",2011,"ACM SIGPLAN Notices",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951735500&doi=10.1145%2f1942788.1868315&partnerID=40&md5=8ab2d74a33d5abf3b4ce74f25f2f26ae","Smooth integration of domain-specific languages into a general purpose host language requires absorbing of domain code written in arbitrary syntax. The integration should cause minimal syntactical and semantic overhead and introduce minimal dependency on external tools. In this paper we discuss a DSL integration technique for the C++ programming language. The solution is based on compile-time parsing of the DSL code. The parser generator is a C++ template metaprogram reimplementation of a runtime Haskell parser generator library. The full parsing phase is executed when the host program is compiled. The library uses only standard C++ language features, thus our solution is highly portable. As a demonstration of the power of this approach, we present a highly efficient and type-safe version of printf and the way it can be constructed using our library. Despite the well known syntactical difficulties of C++ template metaprograms, building embedded languages using our library leads to self-documenting C++ source code. Copyright © 2010 ACM.","C++ template metaprogram; DSL integration; Haskell; Parser generator","C++ language; C++ templates; Compile time; Domain specific languages; Embedded Languages; External tools; General purpose; Haskell; Highly-portable; Integration techniques; Library use; Metaprograms; Parser generator; Parser generators; Runtimes; Source codes; Computer aided software engineering; Graphical user interfaces; Problem oriented languages; Semantics; Integration",2-s2.0-79951735500
"Goulding R., Voell M., Marden E., Levy E.D.","Expansion of the Canadian research exemption for biotechnology research tools",2011,"Biotechnology Law Report",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952147080&doi=10.1089%2fblr.2011.9983&partnerID=40&md5=c1b73e06d10e0780b6aa07402f29236f",[No abstract available],,,2-s2.0-79952147080
"Lu J., Meng X., Ling T.W.","Indexing and querying XML using extended Dewey labeling scheme",2011,"Data and Knowledge Engineering",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649771393&doi=10.1016%2fj.datak.2010.08.001&partnerID=40&md5=84ecd219a81d3bbb8b8cb200acf85a1c","Finding all the occurrences of a tree pattern in an XML database is a core operation for efficient evaluation of XML queries. The Dewey labeling scheme is commonly used to label an XML document to facilitate XML query processing by recording information on the path of an element. In order to improve the efficiency of XML tree pattern matching, we introduce a novel labeling scheme, called extended Dewey, which effectively extends the existing Dewey labeling scheme to combine the types and identifiers of elements in a label, and to avoid the scan of labels for internal query nodes to accelerate query processing (in I/O cost). Based on extended Dewey, we propose a series of holistic XML tree pattern matching algorithms. We first present TJFast to answer an XML twig pattern query. To efficiently answer a generalized XML tree pattern, we then propose GTJFast, an optimization that exploits the non-output nodes. In addition, we propose TJFastTL and GTJFastTL based on the tag + level data partition scheme to further reduce I/O costs by level pruning. Finally, we report our comprehensive experimental results to show that our set of XML tree pattern matching algorithms are superior to existing approaches in terms of the number of elements scanned, the size of intermediate results and query performance. © 2010 Elsevier B.V. All rights reserved.","Performance; XML database; XML query processing","Commonly used; Data partition; Intermediate results; Labeling scheme; Performance; Query nodes; Query performance; Recording information; Tree pattern; XML database; XML queries; XML query processing; XML trees; XML twig pattern; Algorithms; Pattern matching; Query languages; Query processing; Trees (mathematics); XML",2-s2.0-78649771393
"Almendros-Jiménez J.M., Caballero R., García-Ruiz Y., Sáenz-Pérez F.","XQuery in the functional-logic language toy",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867501395&partnerID=40&md5=c5aae6e53ee8fe32dcfbb108737709e2","This paper presents an encoding of the XML query language XQuery in the functional-logic language TOY. The encoding is based on the definition of for-let-where-return constructors by means of TOY functions, and uses the recently proposed XPath implementation for this language as a basis. XQuery expressions can be executed in TOY obtaining sequences of XML elements as answers. Our setting exploits the non-deterministic nature of TOY by retrieving the elements of the XML tree once at a time when necessary. We show that one of the advantages of using a rewriting-based language for implementing XQuery is that it can be used for optimizing XQuery expressions by query rewriting. With this aim, XQuery expressions are converted into higher order patterns that can be analyzed and modified by TOY functions. © Springer-Verlag Berlin Heidelberg 2011.","Functional-logic programming; Higher-order patterns; Non-deterministic functions; XQuery","Computer programming languages; Encoding (symbols); Functional programming; Logic programming; Query languages; Relational database systems; XML; Deterministic functions; Functional logic languages; Functional logic programming; Higher-order; Query rewritings; XML query language; XQuery; XQuery expressions; Computer circuits",2-s2.0-84867501395
"Choi Y.S.","Tree pattern expression for extracting information from syntactically parsed text corpora",2011,"Data Mining and Knowledge Discovery",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651373156&doi=10.1007%2fs10618-010-0184-8&partnerID=40&md5=cdb3c9009d502658475b77efb66d779b","With the public availability of a number of syntactically parsed text corpora, it has been increasingly important to efficiently extract desired information from such corpora. Many conventional works extract a desired text part by matching the parse tree of each sentence to a query that is represented as a structural form of relational predicates expressing a common structural pattern of desired text parts. However, although those works can be useful for limited types of simple queries, they are not very efficient in general because query formulations are sometimes very complicated for complex patterns of desired text parts and query matching tasks are likely to be exponentially time-consuming when considering a variety of complex sentential structures in a text corpus. In order to overcome such inadequacy, we present a novel tree pattern expression (TPE) that can represent various structural patterns intuitively and reduce pattern-matching complexity significantly. This paper first proposes TPE and its pattern-matching algorithm, and then theoretically analyzes the complexity of the proposed pattern-matching algorithm. It also illustrates a TPE-based information extraction system, which is applied to real text mining in a bio-text corpus. It finally shows some experimental results with some discussions in comparison with other systems. © 2010 The Author(s).","Information extraction; Tree pattern; Tree pattern-matching algorithm","Complex pattern; Extracting information; Information Extraction; Information extraction systems; Parse trees; Pattern-matching algorithm; Query formulation; Query matching; Structural form; Structural pattern; Text corpora; Text mining; Tree pattern; Algorithms; Data mining; Information retrieval systems; Software agents; Trees (mathematics)",2-s2.0-78651373156
"Munusamy M., Arasu G.T., Palanisamy V., Selvarajan S.","Object-net approach for data extraction from database",2011,"European Journal of Scientific Research",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953891852&partnerID=40&md5=54fd5d99e0d9faeda998825829ba67c1","This paper attempts to present an object net method for data extraction from database using natural English language. It is proposed to model to autonomously undertake the analysis and synthesis processes of meaning the elementary meanings of English sentence and mapping it as per the actual database structure from which user requires extracting the data, and writing the SQL query for the input English sentence or the requirement which is written in regular English. It is different from existing approach on database query language like SQL, PLSQL, Oracle, Sybase, because any of the database engine can able understand only its own query language but if we try to enter our requirement interims of English sentence the database engine will not process our sentence and it will give error message. The proposed here an algorithm understands the English language and writs the equivalent corresponding database query language and interacts with the database engine with its language to retrieve the data and then show it to user. This object - net approach disambiguates original English text with high precision of 96% of the verbs and 97% of nouns for data extraction from database and writes the SQL query. © EuroJournals Publishing, Inc. 2011.","And domain; Context; Database; NLPNatural language processing; Object; Parse tree; SQL- structured query language; WSD- Word sense disambiguation",,2-s2.0-79953891852
"Peshterliev S., Koychev I.","Semantic retrieval approach to factoid question answering for Bulgarian",2011,"Advances in Intelligent and Soft Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976643154&partnerID=40&md5=396ae7af1a7c38bed2c7a2f3cebcec81","With the abundance of information available today, we need efficient tools to explore it. Search engines attempt to retrieve the most relevant documents for a given query, but still require users to look for the exact answer. Question Answering (Q&A) systems go one step further by trying to answer users' questions posed in natural language. In this paper we describe a semantic approach to Q&A retrieval for Bulgarian language. We investigate how the usage of named entity recognition, question answer type detection and dependency parsing can improve the retrieval of answer-bearing structures compared to the bag-of-words model. Moreover, we evaluate nine different dependency parsing algorithms for Bulgarian, and a named entity recognizer trained with data automatically extracted from Wikipedia. © Springer-Verlag Berlin Heidelberg 2011.","Information retrieval; Question answering; Semantic annotation","Computational linguistics; Information retrieval; Query processing; Search engines; Semantic Web; Semantics; Bag-of-words models; Dependency parsing; Named entity recognition; Question Answering; Relevant documents; Semantic annotations; Semantic approach; Semantic retrieval; Natural language processing systems",2-s2.0-84976643154
"Ghosh A., Bhaskar P., Pal S., Bandyopadhyay S.","Rule based plagiarism detection using information retrieval",2011,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032416&partnerID=40&md5=48690ff28b7018f4a0e8ae45dc491212","This paper reports about the development of a Plagiarism detection system as a part of the Plagiarism detection task in PAN 2011. The external plagiarism detection problem has been solved with the help of Nutch, an open source Information Retrieval (IR) system. The system contains three phases - knowledge preparation, candidate retrieval and plagiarism detection. From the source documents, knowledge base has been prepared for developing the Nutch index and the queries have been formed from the suspicious documents for submission to the Nutch IR system. The retrieved candidate source sentences are assigned similarity scores by Nutch. Dissimilarity score is assigned for each candidate sentence and the suspicious sentence. Each candidate source sentence is ranked based on these two scores. The top ranked candidate sentence is selected for each suspicious sentence.","Dissimilarity score; Information retrieval system; Plagiarism detection; Similarity score","Information retrieval; Information retrieval systems; Intellectual property; Knowledge based systems; Open systems; Dissimilarity score; Ir systems; Knowledge base; Open source information; Plagiarism detection; Rule based; Similarity scores; Three phasis; Search engines",2-s2.0-84922032416
"Verberne S.","Retrieval-based question answering for machine reading evaluation",2011,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922041531&partnerID=40&md5=b0f50f4f33c1b31db98c38f7d9ebf1d4","The Question Answering for Machine Reading (QA4MRE) task was set up as a reading comprehension test consisting of 120 multiple-choice questions per- taining to twelve target texts (the test documents) grouped in three different topics. Since this is the first year that we participate in the task, we decided to follow a relatively knowledge-poor approach that is mainly based on Information Retrieval (IR) techniques. We participated in the English task only. In its most basic version, our system takes the question as a query and uses a standard word-based ranking model to retrieve the most relevant fragments from the test document. Then it matches each of the multiple-choice answer candidates against the set of retrieved text fragments to select the answer with the highest summed similarity (again measured using a standard ranking model). We investigated two forms of information expansion to improve over this baseline: (1) statistical expansion of the test document with sentences from the topical background corpus, and (2) expansion of the question with automatically gathered facts from the background corpus. Our best-performing experimental setting reaches an overall c@1 score of 0.37. We found that statistical expansion of the test documents gives very different results for the three topics but overall it gives an improvement over the document-only baseline.We could not gain any improvements from question-to-facts expansion. More experiments are needed to find a good implementation of fact expansion. In the near future, we will follow up on the current work with more experiments related to expansion of questions and documents for the purpose of question answering.",,"Testing; First year; Knowledge-poor; Multiple choice; Multiple choice questions; Question Answering; Ranking model; Reading comprehension tests; Text fragments; Expansion",2-s2.0-84922041531
"D'hondt E., Verberne S., Alink W., Cornacchia R.","Combining document representations for prior-art retrieval",2011,"CEUR Workshop Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922051493&partnerID=40&md5=a8a408cf6a9984ad57f2643cac9c0c17","In this paper we report on our participation in the CLEF-IP 2011 prior art retrieval task. We investigated whether adding syntactic information in the form of dependency triples to a bag-of-words representation could lead to improvements in patent retrieval. In our experiments, we investigated this effect on the title, abstract and first 400 words of the description section. The experiments were conducted in the Spinque framework with which we tried to optimize for the combinations of text representation and document sections. We found that adding triples did not improve overall MAP scores, compared to the baseline bag-of-words approach but does result in slightly higher set recall scores. In future work we will extend our experiments to use all the text sections of the patent documents and fine-tune the mixture weights.","CLEF-IP track; Dependency triples; Patent retrieval; Prior art search","Abstracting; CLEF-IP track; Dependency triples; Document Representation; Patent retrieval; Prior art retrievals; Prior art search; Syntactic information; Text representation; Patents and inventions",2-s2.0-84922051493
"Zhang J., Qi Y., Hou D., Li M.","A reference programming model for building context-aware application",2011,"IEICE Transactions on Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650958563&doi=10.1587%2ftransinfE94.D.114&partnerID=40&md5=fbedd91b51526a2e359446a4f12f8bb7","Context-aware applications are a key aspect of pervasive computing. The core issue of context-aware application development is how to make the application behave suitably according to the changing context without coupling such context dependencies in the program. Several programming paradigms and languages have been proposed to facilitate the development, but they are either lack of sufficient flexibility or somewhat complex for programming and deploying. A reference programming model is proposed in this paper to make up inadequacy of those approaches. In the model, virtual tables constructed by system and maintained by space manager connect knowledge of both developer and space manager while separating dependency between context and application logic from base program. Hierarchy and architecture of the model are presented, and implementation suggestions are also discussed. Validation and evaluation show that the programming model is lightweight and easy to be implemented and deployed. Moreover, the model brings better flexibility for developing context-aware applications. Copyright © 2011 The Institute of Electronics, Information and Communication Engineers.","Context-aware; Language extension; Pervasive computing; Programming model; Table-driven","Computer applications; Management; Managers; Context-Aware; Language extensions; Pervasive computing; Programming models; Table-driven; Ubiquitous computing",2-s2.0-78650958563
"Lämmel R., Zaytsev V.","Recovering grammar relationships for the Java Language Specification",2011,"Software Quality Journal",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952789092&doi=10.1007%2fs11219-010-9116-5&partnerID=40&md5=15a3e6148b09eb3326f0fa0f6b66c85d","Grammar convergence is a method that helps in discovering relationships between different grammars of the same language or different language versions. The key element of the method is the operational, transformation-based representation of those relationships. Given input grammars for convergence, they are transformed until they are structurally equal. The transformations are composed from primitive operators; properties of these operators and the composed chains provide quantitative and qualitative insight into the relationships between the grammars at hand. We describe a refined method for grammar convergence, and we use it in a major study, where we recover the relationships between all the grammars that occur in the different versions of the Java Language Specification (JLS). The relationships are represented as grammar transformation chains that capture all accidental or intended differences between the JLS grammars. This method is mechanized and driven by nominal and structural differences between pairs of grammars that are subject to asymmetric, binary convergence steps. We present the underlying operator suite for grammar transformation in detail, and we illustrate the suite with many examples of transformations on the JLS grammars. We also describe the extraction effort, which was needed to make the JLS grammars amenable to automated processing. We include substantial metadata about the convergence process for the JLS so that the effort becomes reproducible and transparent. © 2010 Springer Science+Business Media, LLC.","Grammar convergence; Grammar extraction; Grammar recovery; Grammar transformation; Language documentation",,2-s2.0-79952789092
"Aktolga E., Allan J., Smith D.A.","Passage reranking for question answering using syntactic structures and answer types",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055038371&partnerID=40&md5=69c15cdb425adca6e3d1888b5dfb501f","Passage Retrieval is a crucial step in question answering systems, one that has been well researched in the past. Due to the vocabulary mismatch problem and independence assumption of bag-of-words retrieval models, correct passages are often ranked lower than other incorrect passages in the retrieved list. Whereas in previous work, passages are reranked only on the basis of syntactic structures of questions and answers, our method achieves a better ranking by aligning the syntactic structures based on the question’s answer type and detected named entities in the candidate passage. We compare our technique with strong retrieval and reranking baselines. Experimental results using the TREC QA 1999-2003 datasets show that our method significantly outperforms the baselines over all ranks in terms of the MRR measure. © Springer-Verlag Berlin Heidelberg 2011.","Dependency parsing; Named entities; Passage retrieval; Question answering; Reranking","Information retrieval; Natural language processing systems; Dependency parsing; Named entities; Passage retrieval; Question Answering; Re-ranking; Syntactics",2-s2.0-80055038371
"Pallotta V., Delmonte R., Vrieling L., Walker D.","Interaction Mining: The new frontier of Call Center Analytics",2011,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891121346&partnerID=40&md5=092c2e4c4a5fc07d7d68742d23a39855","In this paper, we present our solution for pragmatic analysis of call center conversations in order to provide useful insights for enhancing Call Center Analytics to a level that will enable new metrics and key performance indicators (KPIs) beyond the standard approach. These metrics rely on understanding the dynamics of conversations by highlighting the way participants discuss about topics. By doing that we can detect situations that are simply impossible to detect with standard approaches such as controversial topics, customer-oriented behaviors and also predict customer ratings.",,"Benchmarking; Call centers; Controversial topics; Interaction-mining; Key performance indicators; Information filtering",2-s2.0-84891121346
"Moiseev R., Hayashi S., Saeki M.","Using hierarchical transformation to generate assertion code from OCL constraints",2011,"IEICE Transactions on Information and Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952173856&doi=10.1587%2ftransinf.E94.D.612&partnerID=40&md5=335ffc663125d261e55ebd6d3b32ef49","Object Constraint Language (OCL) is frequently applied in software development for stipulating formal constraints on software models. Its platform-independent characteristic allows for wide usage during the design phase. However, application in platform-specific processes, such as coding, is less obvious because it requires usage of bespoke tools for that platform. In this paper we propose an approach to generate assertion code for OCL constraints for multiple platform specific languages, using a unified framework based on structural similarities of programming languages. We have succeeded in automating the process of assertion code generation for four different languages using our tool. To show effectiveness of our approach in terms of development effort, an experiment was carried out and summarised. © 2011 The Institute of Electronics, Information and Communication Engineers.","Assertion code; Constraints; OCL; Programming languages","Computer programming languages; Formal methods; Object oriented programming; Software design; Software engineering; Assertion code; Constraints; Formal constraints; Multiple platforms; Object Constraint Language; Platform independent; Specific languages; Structural similarity; Codes (symbols)",2-s2.0-79952173856
"Steedman M.","Taking Scope: The Natural Semantics of Quantifiers",2011,"Taking Scope: The Natural Semantics of Quantifiers",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895612774&partnerID=40&md5=657adf128b036458bbd5411cd30ee4db","A novel view of the syntax and semantics of quantifier scope that argues for a ""combinatory"" theory of natural language syntax. © 2012 The MIT Press. All rights reserved.",,,2-s2.0-84895612774
"Yao L., Divoli A., Mayzus I., Evans J.A., Rzhetsky A.","Benchmarking ontologies: Bigger or better?",2011,"PLoS Computational Biology",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551510490&doi=10.1371%2fjournal.pcbi.1001055&partnerID=40&md5=4f53d7841c6513955568a51b99f9a148","A scientific ontology is a formal representation of knowledge within a domain, typically including central concepts, their properties, and relations. With the rise of computers and high-throughput data collection, ontologies have become essential to data mining and sharing across communities in the biomedical sciences. Powerful approaches exist for testing the internal consistency of an ontology, but not for assessing the fidelity of its domain representation. We introduce a family of metrics that describe the breadth and depth with which an ontology represents its knowledge domain. We then test these metrics using (1) four of the most common medical ontologies with respect to a corpus of medical documents and (2) seven of the most popular English thesauri with respect to three corpora that sample language from medicine, news, and novels. Here we show that our approach captures the quality of ontological representation and guides efforts to narrow the breach between ontology and collective discourse within a domain. Our results also demonstrate key features of medical ontologies, English thesauri, and discourse from different domains. Medical ontologies have a small intersection, as do English thesauri. Moreover, dialects characteristic of distinct domains vary strikingly as many of the same words are used quite differently in medicine, news, and novels. As ontologies are intended to mirror the state of knowledge, our methods to tighten the fit between ontology and domain will increase their relevance for new areas of biomedical science and improve the accuracy and power of inferences computed across them. © 2011 Yao et al.",,"accuracy; article; biomedicine; book; data base; data mining; information processing; internal consistency; knowledge; metaphysics; nomenclature; ontology; taxonomy; information retrieval; Information Storage and Retrieval",2-s2.0-79551510490
"Mihalcea R., Radev D.","Graph-based natural language processing and information retrieval",2011,"Graph-Based Natural Language Processing and Information Retrieval",55,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924673764&doi=10.1017%2fCBO9780511976247&partnerID=40&md5=740dd9b35ae9c225f5eb9bf871c6155c","Graph theory and the fields of natural language processing and information retrieval are well-studied disciplines. Traditionally, these areas have been perceived as distinct, with different algorithms, different applications and different potential end-users. However, recent research has shown that these disciplines are intimately connected, with a large variety of natural language processing and information retrieval applications finding efficient solutions within graph-theoretical frameworks. This book extensively covers the use of graph-based algorithms for natural language processing and information retrieval. It brings together topics as diverse as lexical semantics, text summarization, text mining, ontology construction, text classification and information retrieval, which are connected by the common underlying theme of the use of graph-theoretical methods for text and information processing tasks. Readers will come away with a firm understanding of the major methods and applications in natural language processing and information retrieval that rely on graph-based representations and algorithms. © Rada Mihalcea and Dragomir Radev 2011.",,"Classification (of information); Data mining; Graph theory; Graphic methods; Information retrieval; Ontology; Semantics; Text processing; Graph-based algorithms; Graph-based representations; Ontology construction; Retrieval applications; Text classification; Text summarization; Theoretical framework; Theoretical methods; Natural language processing systems",2-s2.0-84924673764
"McEnery T., Hardie A.","Corpus linguistics: Method, theory and practice",2011,"Corpus Linguistics: Method, Theory and Practice",104,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924445539&doi=10.1017%2fCBO9780511981395&partnerID=40&md5=3283221cd83dc37615ce13ac99981989","Corpus linguistics is the study of language data on a large scale – the computer-aided analysis of very extensive collections of transcribed utterances or written texts. This textbook outlines the basic methods of corpus linguistics, explains how the discipline of corpus linguistics developed and surveys the major approaches to the use of corpus data. It uses a broad range of examples to show how corpus data has led to methodological and theoretical innovation in linguistics in general. Clear and detailed explanations lay out the key issues of method and theory in contemporary corpus linguistics. A structured and coherent narrative links the historical development of the field to current topics in 'mainstream' linguistics. Practical tasks and questions for discussion at the end of each chapter encourage students to test their understanding of what they have read and an extensive glossary provides easy access to definitions of technical terms used in the text. © Tony McEnery and Andrew Hardie 2012.",,,2-s2.0-84924445539
"Porkoláb Z., Sinkovics A.","Domain-specific language integration with compile-time parser generator library",2010,"GPCE'10 - Proceedings of the 2010 Conference on Generative Programming and Component Engineering",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650093323&doi=10.1145%2f1868294.1868315&partnerID=40&md5=02c964b45f6ac91ee458240870efb2b7","Smooth integration of domain-specific languages into a general purpose host language requires absorbing of domain code written in arbitrary syntax. The integration should cause minimal syntactical and semantic overhead and introduce minimal dependency on external tools. In this paper we discuss a DSL integration technique for the C++ programming language. The solution is based on compile-time parsing of the DSL code. The parser generator is a C++ template metaprogram reimplementation of a runtime Haskell parser generator library. The full parsing phase is executed when the host program is compiled. The library uses only standard C++ language features, thus our solution is highly portable. As a demonstration of the power of this approach, we present a highly efficient and type-safe version of printf and the way it can be constructed using our library. Despite the well known syntactical difficulties of C++ template metaprograms, building embedded languages using our library leads to self-documenting C++ source code. © 2010 ACM.","C++ template metaprogram; DSL integration; Haskell; Parser generator","C++ language; C++ templates; Compile time; Domain specific languages; Embedded Languages; External tools; General purpose; Haskell; Highly-portable; Integration techniques; Library use; Metaprograms; Parser generators; Runtimes; Source codes; Computer aided software engineering; Graphical user interfaces; Problem oriented languages; Semantics; Integration",2-s2.0-78650093323
"Hasan M., Stroulia E., Barbosa D., Alalfi M.","Analyzing natural-language artifacts of the software process",2010,"IEEE International Conference on Software Maintenance, ICSM",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650130735&doi=10.1109%2fICSM.2010.5609680&partnerID=40&md5=6970f7cf3e71d665d9813cd5d3a76df5","Software teams, as they communicate throughout the life-cycle of their projects, generate a substantial stream of textual data. Through emails and chats, developers discuss the requirements of their software system, they negotiate the distribution of tasks among them, and they make decisions about the system design, and the internal structure and functionalities of its code modules. The software research community has long recognized the importance and potential usefulness of such textual information. In this paper, we discuss our recent work on systematically analyzing several textual streams collected through our WikiDev2.0 tool. We use two different text-analysis methods to examine five different sources of textual data. We report on our experience using our method on analyzing the communications of a nine-member team over four months. © 2010 IEEE.",,"Code modules; Internal structure; Research communities; Software process; Software systems; Software teams; System design; Text-analysis methods; Textual data; Textual information; Systems analysis; Computer software maintenance",2-s2.0-78650130735
"Nguyen D.T., Nguyen V.B., Tran P.N.","Semantic representation of WHO-questions on IndriBasedQA system",2010,"ICSTE 2010 - 2010 2nd International Conference on Software Technology and Engineering, Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649996494&doi=10.1109%2fICSTE.2010.5608828&partnerID=40&md5=8c5ba27e936037e7eb88a16d4ccd6e83","In our previous studies on building Question Answering system, we propose a strategy in which some basic forms of What-questions are processed. Our approach includes following steps: analyzing syntactic structure of question by a parser, transforming the syntactic structure into its semantic model, transforming the semantic model into a query based on a search engine or knowledge base, and post-processing results obtained from searching to build answers. In this paper, we introduce our method in transforming syntactic structures of Who-question forms into their semantic representations. Due to the limitation of this paper, we do not introduce the processing of semantic models in our system. © 2010 IEEE.","Indri query language; Question answering; Semantic model","Indri query language; Knowledge base; Post processing; Query-based; Question Answering; Question answering systems; Semantic model; Semantic representation; Syntactic structure; Knowledge based systems; Natural language processing systems; Query languages; Search engines; Syntactics; Semantics",2-s2.0-78649996494
"Kim J.-D., Yamamoto Y., Yamaguchi A., Nakao M., Oouchida K., Chun H.-W., Takagi T.","Natural language query processing for life science knowledge: Position paper",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649890051&doi=10.1007%2f978-3-642-15470-6_17&partnerID=40&md5=3488171bd6791c568c130d267a4ed004","As the progress of life science, human knowledge about life science keeps increased and diversified. To effectively access the diverse and complex knowledge base, people need to cope with complex representation of desired knowledge pieces. As a potential solution to the problem, we propose a natural language query processing system which can be used to build a human-friendly question-answering system. In this position paper, we present the background, an initial design, and a preliminary investigation of the natural language query processing. © 2010 Springer-Verlag.",,"Human knowledge; Human-friendly; Initial design; Knowledge base; Life-sciences; Natural language queries; Position papers; Potential solutions; Question answering systems; Artificial intelligence; Knowledge based systems; Query processing; Knowledge representation",2-s2.0-78649890051
[No author name available],"Methods and Tools of Parallel Programming Multicomputers - Second Russia-Taiwan Symposium, MTPP 2010, Revised Selected Papers",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649863281&partnerID=40&md5=80ff611720ab180322cfc76561a6c875","The proceedings contain 33 papers. The topics discussed include: on numerical solution of integral equations for three-dimensional diffraction problems; discrete models of physicochemical processes and their parallel implementation; a fast general parser for automatic code generation; a compound scheduling strategy for irregular array redistribution in cluster based parallel system; metacluster system for managing the HPC integrated environment; a message forward tool for integration of clusters of clusters based on MPI architecture; dynamic resource provisioning for interactive workflow applications on cloud computing platform; a Xen-based paravirtualization system toward efficient high performance computing environments; a scalable multi-attribute range query approach on cluster-based hybrid overlays; a dynamic file maintenance scheme with Bayesian network for data grids; and using molecular dynamics simulation and parallel computing technique of the deposition of diamond-like carbon thin films.",,,2-s2.0-78649863281
"Feng D., Shanahan J.G., Murray N., Zajac R.","Learning query parser for local web search",2010,"Proceedings - 2010 IEEE 4th International Conference on Semantic Computing, ICSC 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952047014&doi=10.1109%2fICSC.2010.97&partnerID=40&md5=a132b7d6cfd4687ba4adfde9bfd8e7de","Parsing unstructured local web queries is often tackled using simple syntactic rules that tend to be limited and brittle. Here we present a data-driven approach to learning a query parser for local-search (geographical) queries. The learnt model uses class-level ngram language model-based features; these ngram language models, harvested from structured queries logs, insulate the model from surface-level tokens. The proposed approach is compared with a finite state model. Experiments show significant improvements for parsing geographical web queries using these learnt models. © 2010 IEEE.","Finite state machine; Query analysis; Statistical approach; Structured query logs","Data-driven approach; Finite state machines; Finite-state models; N-gram language models; Query analysis; Statistical approach; Structured queries; Syntactic rules; Web searches; Computational linguistics; Contour followers; Semantics; Web services; Query processing",2-s2.0-79952047014
"Guo Y., Shao Z., Hua N.","A cognitive interactionist sentence parser with simple recurrent networks",2010,"Information Sciences",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956616297&doi=10.1016%2fj.ins.2010.08.008&partnerID=40&md5=bfe552faa789387fdfe45abc9b331fc0","Sentence parsing has a long history in the research fields of machine learning and natural language processing. The state-of-the-art technologies used to tackle this task include those based on statistical language learning. In the meantime, human sentence parsing has attracted massive research efforts for decades in the field of cognitive psychology. A range of behaviouristic experiments verify that the interactionist approach is a sensible and effective way to simulate the human parsing mechanism. This paper proposes a novel and effective sentence parser, the Cognitive Interactionist Parser (CIParser), which incorporates the cognitive interactionist approach with semantic information and simple recurrent networks to extend and enrich the technologies for sentence parsing. Considering the parsing efficiency, CIParser processes the semantic information of nouns and verbs in current stage. The performance of the Cognitive Interactionist Parser is evaluated using elaborately designed experiments using the noted SUSANNE Corpus. The experimental results demonstrate that the Cognitive Interactionist Parser surpasses two state-of-the-art statistical parsers in two classical measures, Precision and Recall, of Information Retrieval (IR). © 2010 Elsevier Inc. All rights reserved.","Cognitive; Interactionist; Semantics; Sentence parser; Simple recurrent networks","Cognitive; Cognitive psychology; Designed experiments; Interactionist; Language learning; Machine-learning; NAtural language processing; Nouns and verbs; Precision and recall; Research efforts; Research fields; Semantic information; Sentence parser; Sentence parsing; Simple recurrent networks; State-of-the-art technology; Statistical parser; Two-state; Computational linguistics; Industrial research; Learning algorithms; Natural language processing systems; Experiments",2-s2.0-77956616297
"Parhi M., Acharya B.M., Puthal B.","An effective mechanism to discover sensor web registry services for wireless sensor network under x-SOA approach",2010,"Proceedings of the 2nd International Conference on Trendz in Information Sciences and Computing, TISC-2010",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952798696&doi=10.1109%2fTISC.2010.5714638&partnerID=40&md5=506e284e898d6899db04ff58632b298a","The application of WSN is emerging as a new trend in different sphere of modern society. However due to the advancement of SWE, discovering sensor web registry services throughout heterogeneous environments is becoming a challenging task and raises several concerns like performance, reliability, and robustness. Many approaches and frameworks have been proposed to discover the sensor web registry services. Some of the approaches assume that the requests are placed in SOAP compatible formats while others focus on GUI based parametric query processing. We have formulated an approach that uses the Natural Language Query Processing which is a convenient and easy method of sensor data access in comparison to SQL or XML based Query Language like XQuery and XPath. We also propose an architecture based on x-SOA that organizes the method of sensor web registry service discovery in an efficient and structured manner using an intermediary, requester friendly layer called the Request Parser & Query Generator (RPQ) between the service provider and service requester via a service registry. We describe how RPQ facilitates the processing of plain text request query to a most appropriate sensor web service and also an algorithm with implementation for a complete cycle of sensor web registry service discovery. ©2010 IEEE.","Natural language processing; Request Parser & Query Generator (RPQ) x-SOA; Sensor Web Enablement Standard (SWE); Sensor web registry; Wireless Sensor Network (WSN)","Effective mechanisms; Heterogeneous environments; NAtural language processing; Natural language queries; Plain text; Sensor data; Sensor web; Sensor Web Enablement Standard (SWE); Sensor web registry; Service discovery; Service provider; Service registry; Service requesters; Computational linguistics; Information science; Natural language processing systems; Query languages; Query processing; Sensors; Spheres; Wireless sensor networks; Web services",2-s2.0-79952798696
"Ghodke S., Bird S.","Fast query for large treebanks",2010,"NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053274769&partnerID=40&md5=593a814220e189000aad579a291fbcf2","A variety of query systems have been developed for interrogating parsed corpora, or tree-banks. With the arrival of efficient, wide-coverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks. © 2010 Association for Computational Linguistics.",,"Curation; Engineering tasks; Fast query; Parsed corpora; Query systems; Query types; Scale-up; Treebanks; Very large database; XML database; Computational linguistics; Information retrieval; Forestry; Forestry; Information Retrieval; Languages",2-s2.0-80053274769
"Annamaa A., Breslav A., Kabanov J., Vene V.","An interactive tool for analyzing embedded SQL queries",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650735634&doi=10.1007%2f978-3-642-17164-2_10&partnerID=40&md5=0d61c47fc3a6ddc113952fab6a8890a9","It is conventional for Java developers to access database engines through standard JDBC API which requires passing SQL queries in plain Java strings. This is referred to as embedding of SQL queries into Java. The strings are not checked at compile time, and errors in the queries (e.g. syntax errors or misspelled names) are usually detected only by testing. In this paper we describe a tool which statically analyzes SQL queries embedded into Java programs. It combines a sound syntactic analyzer with a testing facility which generates small tests to detect errors in individual queries and runs them on an actual database engine. The tool is implemented as a plug-in for Eclipse IDE and allows for interactive use in real-life projects. © 2010 Springer-Verlag.",,"Compile time; Database engine; Interactive tool; Java developers; Java program; Plug-ins; SQL query; Syntactic analyzers; Syntax errors; Testing facility; Computer software; Computer systems programming; Errors; Java programming language; Syntactics; Query languages",2-s2.0-78650735634
"Al-Bahadili H., Al-Saab S., Naoum R., Hussain S.M.","A web search engine model based on index-query bit-level compression",2010,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751654793&doi=10.1145%2f1874590.1874597&partnerID=40&md5=e85be5dc1ce6462eee46739a47cf46f2","In this paper, we propose a new web search engine model based on index-query bit-level compression. The model incorporates two bit-level compression layers both implemented at the backend processor (server) side, one layer resides after the indexer acting as a second compression layer to generate a double compressed index, and the second layer be located after the query parser for query compression to enable bit-level compressed index-query search. This contributes to reducing the size of the index file as well as reducing disk I/O overheads, and consequently yielding higher retrieval rate and performance. The data compression scheme used in this model is the adaptive character wordlength (ACW(n,s)) scheme, which is an asymmetric, lossless, bit-level scheme that permits compressed index-query search. Results investigating the performance of the ACW(n,s) scheme is presented and discussed. © 2010 ACM.","ACW(n,s) scheme; bit-level data compression; full-text compressed self-index; index files; query optimization; web search engine","ACW(n,s) scheme; Index files; Query optimization; Self-index; Web search engines; Information retrieval; Optimization; Search engines; Semantic Web; Semantics; Web services; Data compression",2-s2.0-78751654793
"McCann D., Cappellari P., Roantree M.","Exploiting schema information for efficient streaming queries",2010,"19th International Conference on Software Engineering and Data Engineering 2010, SEDE 2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883614429&partnerID=40&md5=083f8b8ffb4ccc661b60d5dce4d13cac","There is a growing interest in efficiently processing concurrent queries on streaming sensor data. Much of the research focuses on processing concurrent queries for efficient filtering of XML streams or on translating raw sensor data into a standard format such as XML. Here we present liveSensor, an approach to efficiently processing concurrent XPath queries on raw streaming sensor data by exploiting schema information. We leverage on schema information to create a compact structure to represent both queries and query answers. A mapping description between the input stream and the schema allow us to generate parsers to process queries directly on raw sensor streams, bypassing XML conversion and parsing processes.",,"Compact structures; Input streams; Parsing process; Research focus; Schema information; Sensor data; Standard format; Xpath queries; Computer hardware description languages; Sensors; XML; Software engineering",2-s2.0-84883614429
"Bhaskar P., Bandyopadhyay S.","A query focused multi document automatic summarization",2010,"PACLIC 24 - Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863875222&partnerID=40&md5=652ec882746d706294f8068fc99040f7","The present paper describes the development of a query focused multi-document automatic summarization. A graph is constructed, where the nodes are sentences of the documents and edge scores reflect the correlation measure between the nodes. The system clusters similar texts having related topical features from the graph using edge scores. Next, query dependent weights for each sentence are added to the edge score of the sentence and accumulated with the corresponding cluster score. Top ranked sentence of each cluster is identified and compressed using a dependency parser. The compressed sentences are included in the output summary. The inter-document cluster is revisited in order until the length of the summary is less than the maximum limit. The summarizer has been tested on the standard TAC 2008 test data sets of the Update Summarization Track. Evaluation of the summarizer yielded accuracy scores of 0.10317 (ROUGE-2) and 0.13998 (ROUGE-SU-4).","Cluster based approach; Multi Document Summarizer; Parsed and Compressed Sentences; Query Focused; ROUGE Evaluation","Cluster based approach; Multi-document; Parsed and Compressed Sentences; Query Focused; ROUGE Evaluation; Natural language processing systems; Information management",2-s2.0-84863875222
"Sun Y., Leigh J., Johnson A., Lee S.","Articulate: A semi-automated model for translating natural language queries into meaningful visualizations",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956271024&doi=10.1007%2f978-3-642-13544-6_18&partnerID=40&md5=a248ac29e419cb169d0eac04d2d0b5db","While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This paper presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface to allow users to verbally describe and then manipulate what they want to see. We use natural language processing and machine learning methods to translate the imprecise sentences into explicit expressions, and then apply a heuristic graph generation algorithm to create a suitable visualization. The goal is to relieve the user of the burden of having to learn a complex user-interface in order to craft a visualization. © 2010 Springer-Verlag Berlin Heidelberg.","Automatic visualization; Conversational interface; Natural language processing; Visual analytics","Analytic models; Automatic visualization; Complex data; Conversational interface; Explicit expressions; Graph generation; Machine learning methods; Natural language processing; Natural language queries; Semi-automated; Visual analytics; Visualization tools; Computational linguistics; Data visualization; Heuristic methods; Learning algorithms; Learning systems; Natural language processing systems; User interfaces; Visualization; Translation (languages)",2-s2.0-79956271024
"Granicz A.","Rapid prototyping of DSLs with F#",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650874897&doi=10.1007%2f978-3-642-17685-2_1&partnerID=40&md5=8d6ece6944de8ce37a2b5e90dc152fd1","In these lecture notes we present the F# implementation of a small programming language we call Simply. We give the parser implementation using active patterns, F#'s unique feature for extensible pattern matching, which as we demonstrate provide an elegant and type-safe mechanism to embed parsers as an alternative approach to parser generators. We also build an evaluator, and extend the core Simply language with Logo-like primitives and build a graphical shell environment around it. As a warm-up, we give a rudimentary survey of some notable F# features, including sequence expressions and active patterns. For a treatment of units of measure, used briefly in the Simply shell environment, the reader is encouraged to study [AK-09] and [AK-CEFP-09]. © 2010 Springer-Verlag.",,"Alternative approach; Extensible patterns; Lecture Notes; Parser generators; Programming language; Unique features; Pattern matching; Rapid prototyping; Functional programming",2-s2.0-78650874897
"Kent C.K., Salim N.","Web based cross language plagiarism detection",2010,"Proceedings - 2nd International Conference on Computational Intelligence, Modelling and Simulation, CIMSim 2010",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952094777&doi=10.1109%2fCIMSiM.2010.10&partnerID=40&md5=b691e0ad0ad790b6081ae4150c18f5de","As the Internet help us cross language and cultural border by providing different types of translation tools, cross language plagiarism, also known as translation plagiarism are bound to arise. In this paper, we propose a new approach in detecting cross language plagiarism. In order to limit certain scale of our proposed system, we are consider Bahasa Melayu as an input language of the submitted query document and English as a target language of similar, possibly plagiarised documents. Input documents are translated into English using Google Translate API before undergo pre-processing phase (stemming and removal of stop words). Tokenized documents are sent to the Google AJAX Search API to detect similar documents throughout the World Wide Web. Only top ten sources retrieved by the Google Search API are considered as the candidate of source documents. We integrate the use of Stanford Parser and WordNet to determine the similarity level between the suspected documents with those candidate source documents. After that, a detailed similarity analysis is performed and a report of results is produced. © 2010 IEEE.","Cross language; Plagiarism; Semantic similarity; Stanford Parser; WordNet","Cross language; Plagiarism; Semantic similarity; Stanford; Wordnet; Artificial intelligence; Ontology; Semantics; World Wide Web; Translation (languages)",2-s2.0-79952094777
"Saigaonkar S., Rao M.","XML filtering system based on ontology",2010,"Proceedings of the 1st Amrita ACM-W Celebration of Women in Computing in India, A2CWiC'10",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649420755&doi=10.1145%2f1858378.1858429&partnerID=40&md5=5c2baf290ff28921dac0ec2396abe5d9","In recent years, publish subscribe systems based on XML document filtering has been on tremendous rise. Many filtering mechanisms exist such as XFilter, YFilter, AFilter etc. But most of these mechanisms do not filter the document for semantic matched information. M-Filter performs semantic matching of XML documents. But value-based predicates cannot be filtered by M-Filter. This paper proposes a new technique which does semantic matching of XML data and also handles value-based predicates. User Profiles are specified as XPath queries. A query node is checked in OWL classes, if a node is found it is returned with its semantically related data. These queries are then converted into twig patterns. On the other hand, XML document is parsed by DOM parser and a tree is built. Matching of twig nodes and the tree nodes takes place and only the matched information is displayed to the user. © 2010 ACM.","ontology; publish-subscribe; XML filtering","Based on XML documents; Filtering mechanism; publish-subscribe; Publish-subscribe systems; Query nodes; Semantic matching; Tree nodes; Twig pattern; User profile; Value-based predicates; XML data; XML filtering; XPath queries; Matched filters; Ontology; Publishing; XML; Information retrieval systems",2-s2.0-78649420755
"Jakubíček M., Kilgarriff A., McCarthy D., Rychlý P.","Fast syntactic searching in very large corpora for many languages",2010,"PACLIC 24 - Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863863426&partnerID=40&md5=5dcccdd1dc8a284e0f78170e2a63017f","For many linguistic investigations, the first step is to find examples. In the 21st century, they should all be found, not invented. Thus linguists need flexible tools for finding even quite rare phenomena. To support linguists well, they need to be fast even where corpora are very large and queries are complex. We present extensions to the CQL 'Corpus Query Language' for intuitive creation of syntactically rich queries, and demonstrate that they can be computed quickly within our tool even on multi-billion word corpora.","Corpus search; CQL; Large corpora; Syntactic search","Corpus search; CQL; Flexible tool; Large corpora; Rich query; Linguistics; Query languages; Syntactics; Software agents",2-s2.0-84863863426
"Brekle J.","Abstracting queries",2010,"INFORMATIK 2010 - Service Science - Neue Perspektiven fur die Informatik, Beitrage der 40. Jahrestagung der Gesellschaft fur Informatik e.V. (GI)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874281557&partnerID=40&md5=79c1e137b95880cfa59e2c5919e10785",[No abstract available],,,2-s2.0-84874281557
"Dayarathna M., Tsukahara Y., Sugiura K.","TelescopeVisualizer: A real-time internet information visualizer with a flexible user interface",2010,"Asian Internet Engineering Conference, AINTEC 2010",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858324791&doi=10.1145%2f1930286.1930289&partnerID=40&md5=15249aeb9f4923cd905c78e33c392ddc","Visualizing the Internet activities has been an interesting and challenging research issue. Many successful efforts were concentrated around visualizing a single aspect of the Internet. However such fixed application user interfaces cannot cope with visualizing the diversity of network information and ever changing nature of the Internet. We present TelescopeVisualizer, a scalable real-time Internet information visualizer that allows for querying and visualizing multiple aspects of the Internet interactions. TelescopeVisualizer's XAML based flexible user interface avoids tight coupling of visualization application with the rest of the system making it possible to visualize Internet information in realtime. It allows users to upload custom query layouts, post queries and change the visualizations as they require. Using a query traffic generator application, we demonstrate scalability of TelescopeVisualizer system in different hardware platforms. © 2010 ACM.","declarative graphical user interfaces; information visualization; internet; network visualization; query languages","Flexible user interfaces; Hardware platform; Information visualization; Internet activity; Internet information; Network information; network visualization; Real time; Real-time internet; Research issues; Tight coupling; Traffic generators; Visualization application; Visualizers; Graphical user interfaces; Information systems; Query languages; Telescopes; Visualization; Internet",2-s2.0-84858324791
"Passonneau R.J., Epstein S.L., Ligorio T., Gordon J.B., Bhutada P.","Learning about voice search for spoken dialogue systems",2010,"NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051485549&partnerID=40&md5=cc8ff557080ef2b719d9dabb9700455e","In a Wizard-of-Oz experiment with multiple wizard subjects, each wizard viewed automated speech recognition (ASR) results for utterances whose interpretation is critical to task success: requests for books by title from a library database. To avoid non-understandings, the wizard directly queried the application database with the ASR hypothesis (voice search). To learn how to avoid misunderstandings, we investigated how wizards dealt with uncertainty in voice search results. Wizards were quite successful at selecting the correct title from query results that included a match. The most successful wizard could also tell when the query results did not contain the requested title. Our learned models of the best wizard's behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores. © 2010 Association for Computational Linguistics.",,"Acoustic model; Automated speech recognition; Query results; Search results; Spoken dialogue system; Wizard of Oz; Feature extraction; Speech processing; Computational linguistics",2-s2.0-80051485549
"Liu S., Seneff S., Glass J.","A collective data generation method for speech language models",2010,"2010 IEEE Workshop on Spoken Language Technology, SLT 2010 - Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951789564&doi=10.1109%2fSLT.2010.5700855&partnerID=40&md5=cd661603be972b8eaa592130a448eb22","Recently we began using Amazon Mechanical Turk (AMT), an Internet marketplace, to deploy our spoken dialogue systems to large audiences for user testing and data collection purposes. This crowdsourcing method of collecting data contrasts with the time- and labor- intensive developer annotation methods. In this paper, we compare these data in various combinations with traditionally-collected corpora for training our speech recognizer's language model. Our results show that AMT text queries are effective for initial language model training for spoken dialogue systems, and that crowdsourced speech collection within the context of a spoken dialogue framework provides significant improvement. ©2010 IEEE.","Amazon mechanical Turk; Crowdsourcing; Language models","Annotation methods; Crowdsourcing; Data collection; Data generation; Language model; Language models; Mechanical turks; Speech recognizer; Spoken dialogue; Spoken dialogue system; Text query; User testing; Computational linguistics; Speech processing; Speech recognition; Data acquisition",2-s2.0-79951789564
"Mishra A., Mishra N., Agrawal A.","Context-aware restricted geographical domain question answering system",2010,"Proceedings - 2010 International Conference on Computational Intelligence and Communication Networks, CICN 2010",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952082384&doi=10.1109%2fCICN.2010.108&partnerID=40&md5=a8e3c559830168e53aef18d9ca20e205","With the rapid use of the internet into users' daily lives, it is imperative to capture location-specific information from local database to satisfy the user's local information needs [context]. This is especially true in the increasingly local search environments. There are a number of local Web search engines enabling users to find information which is location specific Web content. In this aspect web search engine gives sets of relevant documents related to user query. Thereby user has to scan all searched documents and read line by line to get specific answer. So there is great need to automate the overall process, so that user gets answer in the form of compact text along with map for visualization purpose. This aims at developing geographic domain question answering system [QAS] embedded with mapping abilities where our system will allow users to specify a location addition to the keywords they are searching for as a query. Our system will then return results of the specified location so as to make geographical information retrieval truly context-aware. The testing results show satisfactory performance of the system. © 2010 IEEE.","Automatic question answering; Geographic information retrieval; Knowledge discovery; Ontology; Question semantic representation","Context-Aware; Daily lives; Geographic information retrieval; Geographical information; Knowledge Discovery; Local information; Local search; Question Answering; Question answering systems; Question semantic representation; Relevant documents; Specific information; Testing results; User query; Web content; Web search engines; Artificial intelligence; Information retrieval; Knowledge representation; Natural language processing systems; Ontology; Semantics; Telecommunication networks; Visualization; World Wide Web; Search engines",2-s2.0-79952082384
"Martínez-Morales A.A., Vidal M.-E.","Directed byper-graphs for RDF documents",2010,"Revista Tecnica de la Facultad de Ingenieria Universidad del Zulia",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551470657&partnerID=40&md5=d1d99939a42fc36e536c2f6de9f33fc3","Resource Descrlption Framework (ROF) is a W3C proposal to express metadata about resourc s in the Web. The RDF data model has been formalizecl using several graph-based representations; each one offers different expressive power amIsupport for the tasks of query answering nd semantic reasoning. In this paper, we propose a directed hyper-graph formal model to represent and manage RDF documenls efficiently. We have developed algorithms that exploit the propertie of the proposed representation. and conducted an experimental study lo analyze the space and time savings of our solution With respect to the labeled directed graph representation. Our study has been performed over synthetic ami real-world RDF do 'umenls. and we could observe that our approach reduces the space required to store an RDF documenl and sp eds up the task ofquery answering. overcoming the labeled dir cted graph represent tion.","Dala model; Directed hyper-graphs; Rcsource description framework (rdf)","Dala model; Directed graphs; Directed hyper-graphs; Experimental studies; Expressive power; Formal model; Graph-based representations; Query answering; Rcsource description framework (rdf); RDF data; Real-world; Semantic reasoning; Space and time; Metadata; Semantics; Semantic Web",2-s2.0-79551470657
"Ligorio T., Epstein S.L., Passonneau R.J.","Wizards' dialogue strategies to handle noisy speech recognition",2010,"2010 IEEE Workshop on Spoken Language Technology, SLT 2010 - Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951801799&doi=10.1109%2fSLT.2010.5700871&partnerID=40&md5=967c15d3876444774f0352f8fa313f07","This paper reports on a novel approach to the design and implementation of a spoken dialogue system. A human subject, or wizard, is presented with input of the sort intended for the dialogue system, and selects from among a set of pre-defined actions. The wizard has access to hypotheses generated by noisy automated speech recognition and queries a database with them using partial matching. During the ambitious study reported here, different wizards exhibited different behaviors, elicited different degrees of caller affinity for the system, and achieved different degrees of accuracy on retrieval of the requested items. Our data illustrates that wizards did not trust automated speech recognition hypotheses when they could not lead to a correct database match, and instead asked informed questions. The wealth of data and the richness of the interactions are a valuable resource with which to model expert wizard behavior. ©2010 IEEE.","Corpus resources; Spoken dialogue systems; Wizard of Oz study","Automated speech recognition; Corpus resources; Dialogue strategy; Dialogue systems; Human subjects; Noisy speech recognition; Partial matching; Spoken dialogue system; Spoken dialogue systems; Wizard of Oz study; Query languages; Speech processing; Speech recognition",2-s2.0-79951801799
"Pandit S., Honavar V.","Ontology-guided extraction of complex nested relationships",2010,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751545879&doi=10.1109%2fICTAI.2010.98&partnerID=40&md5=ea355c791913cd2d7d41a9150530fde3","Many applications call for methods to enable automatic extraction of structured information from unstructured natural language text. Due to inherent challenges of natural language processing, most of the existing methods for information extraction from text tend to be domain specific. We explore a modular ontology-based approach to information extraction that decouples domain-specific knowledge from the rules used for information extraction.We describe a framework for extraction of a subset of complex nested relationships (e.g., Joe reports that Jim is a reliable employee). The extracted relationships are output in the form of sets of RDF (resource description framework) triples, which can be queried using query languages for RDF and mined for knowledge acquisition. © 2010 IEEE.",,"Automatic extraction; Domain specific; Domain-specific knowledge; Existing method; Information Extraction; Modular ontologies; NAtural language processing; Nested relationships; Resource description framework; Structured information; Unstructured natural language; Artificial intelligence; Computational linguistics; Information analysis; Knowledge acquisition; Natural language processing systems; Query languages; Semantic Web; Ontology",2-s2.0-78751545879
"Pralayankar P., Devi S.L.","Anaphora resolution algorithm for Sanskrit",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651089182&doi=10.1007%2f978-3-642-17528-2_16&partnerID=40&md5=ab325581f93e0b0561d7c51ae7308e37","This paper presents an algorithm, which identifies different types of pronominal and its antecedents in Sanskrit, an Indo-European language. The computational grammar implemented here uses very familiar concepts such as clause, subject, object etc., which are identified with the help of morphological information and concepts such as precede and follow. It is well known that natural languages contain anaphoric expressions, gaps and elliptical constructions of various kinds and that understanding of natural languages involves assignment of interpretations to these elements. Therefore, it is only to be expected that natural language understanding systems must have the necessary mechanism to resolve the same. The method we adopt here for resolving the anaphors is by exploiting the morphological richness of the language. The system is giving encouraging results when tested with a small corpus. © 2010 Springer-Verlag Berlin Heidelberg.","Anaphora resolution; Finite State Automata; Rule based Technique; Sanskrit","Anaphora resolution; Anaphors; European languages; Finite state automata; Morphological information; Natural language understanding; Natural languages; Rule-based techniques; Sanskrit; Algorithms; Computational linguistics; Finite automata; Linguistics; Query languages; Computational grammars",2-s2.0-78651089182
[No author name available],"International Conference on High Performance Computing Systems 2010, HPCS 2010",2010,"International Conference on High Performance Computing Systems 2010, HPCS 2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878132944&partnerID=40&md5=b97b80bb25d243c120bbf93c6024b6d6","The proceedings contain 16 papers. The topics discussed include: set-associative load/store caches; locality-conscious superpaging for improved TLB behavior of stencil computations; design of a least distance protocol in MANET using search techniques; object oriented combinatory parser; query performance analysis of database objects on hard disks and flash SSDs; DSP specific, optimization strategy for the implementation of Viterbi decoder; practical implementation issues of optimum adaptive algorithms with individualized and time varying convergence factors; a new method for warehousing assessment and prioritize activities; high performance computing with graphics cards to accelerate processing density functional theory; current trends and the future of software-managed on-chip memories in modern processors; and a computational framework for modeling time-frequency distributions.",,,2-s2.0-84878132944
"Kim J., Mooney R.J.","Generative alignment and semantic parsing for learning from ambiguous supervision",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053395899&partnerID=40&md5=651ec0bab65df3532ac608d2575d9c40","We present a probabilistic generative model for learning semantic parsers from ambiguous supervision. Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations. It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators.",,"Generative model; Language generators; Learning semantics; Logical forms; Natural languages; RoboCup; Semantic parsing; Alignment; Computational linguistics; Context free grammars; Semantics",2-s2.0-80053395899
"Clarke J., Goldwasser D., Chang M.-W., Roth D.","Driving semantic parsing from the world's response",2010,"CoNLL 2010 - Fourteenth Conference on Computational Natural Language Learning, Proceedings of the Conference",78,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053261123&partnerID=40&md5=a495f74d49ce9278504af993b790f7e8","Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. © 2010 Association for Computational Linguistics.",,"Annotated training data; Binary feedback; Complex structure; Feedback signal; Learning paradigms; Logical forms; Semantic parsing; Syntactic patterns; Learning algorithms; Semantics",2-s2.0-80053261123
[No author name available],"EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",2010,"EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053230059&partnerID=40&md5=87be738d2a28669c206e81897c64a989","The proceedings contain 125 papers. The topics discussed include: on dual decomposition and linear programming relaxations for natural language processing; turbo parsers: dependency parsing by approximate variational inference; jointly modeling aspects and opinions with a MaxEnt-LDA hybrid; handling noisy queries in cross language FAQ retrieval; soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions; a hybrid morpheme-word representation for machine translation of morphologically rich languages; joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks; crouching Dirichlet, hidden Markov model: unsupervised POS tagging with context local tag generation; storing the web in memory: space efficient language models with constant time retrieval; automatic discovery of manner relations and its applications; and exploiting conversation structure in unsupervised topic segmentation for emails.",,,2-s2.0-80053230059
"Pitler E., Bergsma S., Lin D., Church K.","Using web-scale N-grams to improve base NP parsing performance",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053408984&partnerID=40&md5=4bf7ed518322f3e4f4096ced0071a7f8","We use web-scale N-grams in a base NP parser that correctly analyzes 95.4% of the base NPs in natural text. Web-scale data improves performance. That is, there is no data like more data. Performance scales log-linearly with the number of parameters in the model (the number of unique N-grams). The web-scale N-grams are particularly helpful in harder cases, such as NPs that contain conjunctions.",,"N-grams; Computational linguistics; User interfaces",2-s2.0-80053408984
"Ponvert E., Baldridge J., Erk K.","Simple unsupervised identification of low-level constituents",2010,"Proceedings - 2010 IEEE 4th International Conference on Semantic Computing, ICSC 2010",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952066740&doi=10.1109%2fICSC.2010.20&partnerID=40&md5=8b858ea68510bfbb76f12de9aebc4e7c","We present an approach to unsupervised partial parsing: the identification of low-level constituents (which we dub clumps) in unannotated text. We begin by showing that CCLParser [1], an unsupervised parsing model, is particularly adept at identifying clumps, and that, surprisingly, building a simple right-branching structure above its clumps actually outperforms the full parser itself. This indicates that much of the CCLParser's performance comes from good local predictions. Based on this observation, we define a simple bigram model that is competitive with CCLParser for clumping, which further illustrates how important this level of representation is for unsupervised parsing. © 2010 IEEE.",,"Branching structures; Local prediction; Partial parsing; Unannotated texts; Semantics",2-s2.0-79952066740
"Shi S., Zhang H., Yuan X., Wen J.-R.","Corpus-based semantic class mining: Distributional vs. pattern-based approaches",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053427864&partnerID=40&md5=0761aef39b483549cb3c505fc9952d7c","Main approaches to corpus-based semantic class mining include distributional similarity (DS) and pattern-based (PB). In this paper, we perform an empirical comparison of them, based on a publicly available dataset containing 500 million web pages, using various categories of queries. We further propose a frequency based rule to select appropriate approaches for different types of terms.",,"Data sets; Distributional similarities; Empirical comparison; Semantic class; Computational linguistics; User interfaces; Semantics",2-s2.0-80053427864
"Hernault H., Bollegala D., Ishizuka M.","A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension",2010,"EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952272600&partnerID=40&md5=bd3518f19d245bb027a02e60b9867d9b","Several recent discourse parsers have employed fully-supervised machine learning approaches. These methods require human annotators to beforehand create an extensive training corpus, which is a time-consuming and costly process. On the other hand, un-labeled data is abundant and cheap to collect. In this paper, we propose a novel semi-supervised method for discourse relation classification based on the analysis of co-occurring features in unlabeled data, which is then taken into account for extending the feature vectors given to a classifier. Our experimental results on the RST Discourse Tree-bank corpus and Penn Discourse Treebank indicate that the proposed method brings a significant improvement in classification accuracy and macro-average F-score when small training datasets are used. For instance, with training sets of c.a. 1000 labeled instances, the proposed method brings improvements in accuracy and macro-average F-score up to 50% compared to a baseline classifier. We believe that the proposed method is a first step towards detecting low-occurrence relations, which is useful for domains with a lack of annotated data. © 2010 Association for Computational Linguistics.",,"Classification accuracy; F-score; Feature vectors; Machine-learning; Semi-supervised; Semi-supervised method; Small training; Training corpus; Training sets; Treebanks; Unlabeled data; Computational linguistics; Learning algorithms; Natural language processing systems; Classification (of information)",2-s2.0-79952272600
"Hoffman D., Wang H.-Y., Chang M., Ly-Gagnon D., Sobotkiewicz L., Strooper P.","Two case studies in grammar-based test generation",2010,"Journal of Systems and Software",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049327129&doi=10.1016%2fj.jss.2010.07.048&partnerID=40&md5=31759e4e89b9e6329410c756f146579e","Grammar-based test generation (GBTG) has seen extensive study and practical use since the 1970s. GBTG was introduced to generate source code for testing compilers from context-free grammars specifying language syntax. More recently, GBTG has been applied to many other testing problems, including the generation of eXtensible Markup Language (XML) documents and the generation of packets for testing communications protocols. Recent research has shown how to integrate covering-array techniques such as pairwise testing into GBTG tools. While the integration offers considerable power to the tester, there are few practical demonstrations in the literature. We present two case studies showing how to use grammars and covering arrays for automated software testing. The first case study exposes HTML injection vulnerabilities in an RSS feed parser. The second case study determines the effectiveness of network firewalls when faced with TCP flag attacks. The case studies illustrate the use of covering arrays in a GBTG context, the use of visualization to understand large test logs, and the issues and tradeoffs in the design of fully automated GBTG test suites. © 2010 Elsevier Inc.","Automated testing; Covering array; eXtended Markup Language (XML); Grammar-based test generation; Really Simple Syndication (RSS); TCP","Automated testing; Covering arrays; eXtended Markup Language (XML); Really simple syndications; TCP; Test generations; Automation; Computer software selection and evaluation; Computer system firewalls; Hypertext systems; Markup languages; Research; RSS; Software testing; Testing; Transmission control protocol; Visualization; XML; Context free languages",2-s2.0-78049327129
"De Raedt L., Kimmig A., Gutmann B., Kersting K., Costa V.S., Toivonen H.","Probabilistic inductive querying using problog",2010,"Inductive Databases and Constraint-Based Data Mining",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955777896&doi=10.1007%2f978-1-4419-7738-0_10&partnerID=40&md5=a3ce17778b8b7022326e60f815015906","We study how probabilistic reasoning and inductive querying can be combined within ProbLog, a recent probabilistic extension of Prolog. ProbLog can be regarded as a database system that supports both probabilistic and inductive reasoning through a variety of querying mechanisms. After a short introduction to ProbLog, we provide a survey of the different types of inductive queries that ProbLog supports, and show how it can be applied to the mining of large biological networks. © 2010 Springer Science+Business Media, LLC.",,,2-s2.0-79955777896
"Subhashini R., Senthil Kumar V.J.","Shallow NLP techniques for noun phrase extraction",2010,"Proceedings of the 2nd International Conference on Trendz in Information Sciences and Computing, TISC-2010",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952793127&doi=10.1109%2fTISC.2010.5714612&partnerID=40&md5=da366726266f3ced3784a64db5bef225","The field of Information Retrieval plays an important role in searching on the Internet. Most of the information retrieval systems are limited to the query processing based on keywords. In information retrieval system the matching of the query against a set of text record is the core of the system. Retrieval of the relevant natural language text document is of more challenge. Today's most search engines are based on keyword based (bag of words) techniques, which results in some disadvantages. For text retrieval key phrases can help to narrow the search results or rank retrieved documents. We exploit shallow NLP techniques to support a range of NL queries and snippets over an existing keyword-based search. This paper describes a simple system for choosing noun phrases from a document as key phrases. The noun phrase extractor is made up of three modules: tokenization; part-of-speech tagging; noun phrase identification using Chunking. A preliminary evaluation was conducted to test this technique with the standard IR benchmark collections such as classic test collections and then with the web snippets collection from the search engines results. The experimental results have been encouraging. ©2010 IEEE.","Chunking; Information retrieval; NLP; Noun phrases","Bag of words; Chunking; Key-phrase; Keyword-based search; Natural language text documents; NLP; Noun phrase; Noun phrase extractors; Noun phrases; Part of speech tagging; Retrieved documents; Search results; Shallow NLP; Simple system; Test Collection; Text retrieval; Tokenization; Information retrieval systems; Information science; Natural language processing systems; Search engines; Information retrieval",2-s2.0-79952793127
"Da Silva P.C., Dos Santos M.M., Times V.C.","XLPath: A XML linking path language",2010,"Proceedings of the IADIS International Conference WWW/Internet 2010",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860706752&partnerID=40&md5=9d06e78fbc88740f004fa59d2ea0b9bf","XLink are XML elements used for describing relationships and semantic information contained in valid XML documents. Thus, the structure of XLink can be navigated and their components (arcs, locators, resources and title) can be retrieved through a path language, like XPath. However, it was not specified to retrieve information from specific elements of Xlink, which can contain semantic information. This paper proposes XLPath, a path language turned to navigation on XLink, which allows performing queries on the elements that compose the links between XML documents. Aiming to demonstrate the use of XLPath, a processor prototype for this language was developed, which was evaluated in a case study based on XBRL documents, since XBRL is a technology that makes extensive use of XLink. © 2010 IADIS.","Navigational language; XLink; XLPath; XML","Navigational language; Semantic information; XLink; XLPath; Navigation; Semantics; XML",2-s2.0-84860706752
"Zaman A.N.K., Brown C.G.","Latent semantic indexing and large dataset: Study of term-weighting schemes",2010,"2010 5th International Conference on Digital Information Management, ICDIM 2010",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650941967&doi=10.1109%2fICDIM.2010.5664669&partnerID=40&md5=367777c4da0cd3c98769951df9774a74","The primary purpose of an information retrieval (IR) system is to retrieve all the relevant documents, which are relevant to the user query. Latent Semantic Indexing/Analysis (LSI/LSA) based ad hoc document retrieval task investigates the performance of retrieval systems that search a static set of documents using new questions. Performance of LSI has been tested by others for several smaller datasets (e.g. MED, CISI abstracts) however, LSI has not been tested for a large dataset. So, we decided to test LSI for a very large dataset. We used TREC-8 LA Times dataset for our experimentation. We applied three different term weighting schemes and our own stop word list to judge the performance. Recall-precision graph and Coefficient of Variation (CV) were used to evaluate the retrieval performance of LSI based retrieval system. We found tf-idf term weighting scheme performs better than log-entropy and raw term frequency weighting schemes when the test collection became very large. ©2010 IEEE.","Coefficient of variation; Latent semantic indexing; Recallprecision; Retrieval performance; Term-weighting","Coefficient of variation; Latent Semantic Indexing; Recallprecision; Retrieval performance; Term-weighting; Indexing (of information); Information management; Semantics; Statistical tests; Text processing; Information retrieval",2-s2.0-78650941967
"Rusiñol M., Lladós J.","Symbol spotting in digital libraries: Focused retrieval over graphic-rich document collections",2010,"Symbol Spotting in Digital Libraries: Focused Retrieval over Graphic-rich Document Collections",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891384825&doi=10.1007%2f978-1-84996-208-7&partnerID=40&md5=2fc19898da1c1091f96537c779aaf66b","The specific problem of symbol recognition in graphical documents requires additional techniques to those developed for character recognition. The most well-known obstacle is the so-called Sayre paradox: Correct recognition requires good segmentation, yet improvement in segmentation is achieved using information provided by the recognition process. This dilemma can be avoided by techniques that identify sets of regions containing useful information. Such symbol-spotting methods allow the detection of symbols in maps or technical drawings without having to fully segment or fully recognize the entire content. This unique text/reference provides a complete, integrated and large-scale solution to the challenge of designing a robust symbol-spotting method for collections of graphic-rich documents. The book examines a number of features and descriptors, from basic photometric descriptors commonly used in computer vision techniques to those specific to graphical shapes, presenting a methodology which can be used in a wide variety of applications. Additionally, readers are supplied with an insight into the problem of performance evaluation of spotting methods. Some very basic knowledge of pattern recognition, document image analysis and graphics recognition is assumed. Topics and features: Includes a Foreword by Professor Karl Tombre, Director of the INRIA Nancy - Grand Est research center and Editor-in-Chief of Springer's International Journal on Document Analysis and Recognition Introduces the problems of symbol spotting and focused retrieval, outlining an architecture for solving these problems Examines techniques in computer vision for recognizing objects in scenes, and their application to the specific problem of spotting graphical symbols in documents Describes a method to determine probable locations of symbols in technical drawings which makes use of vectorial signatures as symbol descriptors Presents a spotting method which uses a prototype-based search as the basis for the focused retrieval task Explores an indexing method that retrieves locations of interest where a query symbol is likely to be found Investigates techniques for evaluating the performance of symbol-spotting systems, defined in terms of recognition abilities, location accuracy and scal ility Researchers and practitioners in the field of graphics recognition, interested in the problem of symbol spotting and focused retrieval applications in the context of digital libraries, will find this an invaluable text on the subject. Dr. Mar'al Rusiñol is a Research Associate at the Computer Vision Center and at the Computer Sciences Department of the Universitat Autònoma de Barcelona, Spain. Dr. Josep Llad's is Director of the Computer Vision Center and Associate Professor at the Computer Sciences Department of the Universitat Autònoma de Barcelona, Spain. © Springer-Verlag London Limited 2010.",,,2-s2.0-84891384825
"Osis J., Šlihte A.","Transforming textual use cases to a computation independent model",2010,"Proceedings of the 2nd International Workshop on Model-Driven Architecture and Modelling Theory-Driven Development, MDA and MTDD 2010, in Conjunction with ENASE 2010",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650924035&partnerID=40&md5=e047a174ac961e2599c1ae76317b98b7","This paper analyzes the specific text analysis tasks of transforming textual use cases of a business system to a Computation Independent Model (CIM) for Model Driven Architecture (MDA). By applying language processing methods to textual use case analysis and using Topological Functioning Model (TFM) as CIM a workable solution can be developed. Implementing a TFM Tool is considered to enable fetching a TFM from use cases, editing and verifying the TFM, and transforming TFM to Unified Modeling Language (UML). Solution's compatibility to MDA standards is also considered, thus increasing the completeness of MDA and providing a formal method to automatically acquire a CIM from description of a business system in form of textual use cases.",,"Business systems; Case analysis; Computation independent model; Language processing; Model driven architectures; Text analysis; Architecture; Formal methods; Software design; Topology; Unified Modeling Language; Software architecture",2-s2.0-78650924035
"Zhang J., Gu Y., Liu W., Hu W., Zhao T., Mu X., Marx J., Frost F., Tjoe J.","Automatic patient search for breast cancer clinical trials using free-text medical reports",2010,"IHI'10 - Proceedings of the 1st ACM International Health Informatics Symposium",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650943724&doi=10.1145%2f1882992.1883052&partnerID=40&md5=af86a1dfd88b9fbb688ed3fcb7f033da","The purpose of this work is to develop algorithms to automatically identify qualified patients for breast cancer clinical trials from free-text medical reports. Specifically, we developed an algorithm, called subtree match, that achieves this by finding structural patterns in free-text patient report sentences that are consistent with given trial criteria. Experimental results indicate that this technique is effective and performs better than several competing techniques. Our work is useful in two respects. First, it can potentially increase the efficiency and reduce the cost of the patient enrollment process. Second, it can be extended/adapted to the clinical trials of other diseases. © 2010 ACM.","breast cancer; clinical trials; patient search; subtree","Breast Cancer; Clinical trial; patient search; Structural pattern; Subtrees; Algorithms; Information science; Experiments",2-s2.0-78650943724
"Spitkovsky V.I., Jurafsky D., Alshawi H.","Profiting from mark-up: Hyper-text annotations for guided parsing",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859941983&partnerID=40&md5=a44ecf27bd8440c12da7b1320d155585","We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-theart by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus - nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP. © 2010 Association for Computational Linguistics.",,"Conversion procedures; Data sets; Dependency model; Dependency parsing; Grammar induction; HTML tags; Linguistic analysis; Orders of magnitude; Phrase boundary; Syntactic structure; Wall Street Journal; Computational linguistics",2-s2.0-84859941983
"Liu H., Blouin C., Keselj V.","Biological event extraction using subgraph matching",2010,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874225719&partnerID=40&md5=520c7239a12ecaa211b07115a37f75a4","An important task in biological information extraction is to identify descriptions of biological relations and events involving genes or proteins. We propose a graph-based approach to automatically learn rules for detecting biological events in the literature. The detection is performed by searching for isomorphism between event rules and the dependency graphs of complete sentences. When applying our approach to the datasets of the Task 1 of the BioNLP shared task, we achieved an 37.28% F-score in detecting biological events across 9 event types.",,"Biological information; Dependency graphs; Event extraction; Event Types; F-score; Graph-based; Subgraph matching; Semantics; Information analysis",2-s2.0-84874225719
"Brito M., Vale L., Carvalho P., Henriques J.","A sensor Middleware for integration of heterogeneous medical devices",2010,"2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650837961&doi=10.1109%2fIEMBS.2010.5626267&partnerID=40&md5=2348a4661dfc4b87b32a6eef40607c97","In this paper, the architecture of a modular, service-oriented, Sensor Middleware for data acquisition and processing is presented. The described solution was developed with the purpose of solving two increasingly relevant problems in the context of modern pHealth systems: i) to aggregate a number of heterogeneous, off-the-shelf, devices from which clinical measurements can be acquired and ii) to provide access and integration with an 802.15.4 network of wearable sensors. The modular nature of the Middleware provides the means to easily integrate pre-processing algorithms into processing pipelines, as well as new drivers for adding support for new sensor devices or communication technologies. Tests performed with both real and artificially generated data streams show that the presented solution is suitable for use both in a Windows PC or a Windows Mobile PDA with minimal overhead. © 2010 IEEE.",,"Clinical measurements; Communication technologies; Data stream; Medical Devices; Pre-processing algorithms; Sensor device; Service Oriented; Wearable sensors; Windows mobiles; Biomedical engineering; Data handling; Middleware; Sensors; Service oriented architecture (SOA); Pipeline processing systems",2-s2.0-78650837961
"Mora G., Farkas R.","Species taxonomy for gene normalization",2010,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874235988&partnerID=40&md5=4ffb38a6d0ca65e5fac3ea3b31ecf624","Background: The task of gene normalization is to assign a unique identifier from a database to the gene mentions. Using these identifiers a great deal of information can be gathered from external databases such as interactions, pathways, sequences and protein structures. Normalizing gene mentions in articles is a difficult task as the inter-species ambiguity of the gene mentions in biomedical publications is high. The experiences gained from the BioCreative II Gene Normalization Task indicate that the biggest challenge in gene normalization is the recognition of the species that a specific gene mention belongs to. In biomedical scientific articles the authors often use taxonomical entities besides concrete species mentions as references to different group of organisms. Species taxonomies are hierarchical systems (trees) of living creatures and therefore provide a classification of species. Here we investigate the added value of the utilization of taxonomic entity mentions in the inter-species gene normalization task. Results: We present a method which marks those words mentioning all taxonomic entities (genus, family, etc.) and applies filtering heuristics to select the taxonomic entities referring to species mentioned in the document. These entities are then treated as species mentions together with standard species annotations and we employ them in gene normalization. Conclusion: After experiments were carried out on the BioCreative III Gene Normalization Task's data-set to investigate the contribution of the additional species mentions to the gene disambiguation task, we found that our approach improves the performance of the inter-species gene mention disambiguator, both in terms of precision and recall.",,"Added values; As species; External database; Living creatures; Precision and recall; Protein structures; Scientific articles; Unique identifiers; Hierarchical systems; Information retrieval systems; Semantics; Taxonomies; Genes",2-s2.0-84874235988
"Doush I.A., Alkhateeb F., Al Maghayreh E.","Towards meaningful mathematical expressions in e-learning",2010,"ACM International Conference Proceeding Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751652164&doi=10.1145%2f1874590.1874612&partnerID=40&md5=1561a8cc9509283afb468f79a4268cec","This paper presents a new framework for adding semantic into the mathematical expressions in the context of e-learning. The proposed system converts presentation MathML into content MathML with RDFa annotations. The objective is to add meaning into the mathematical contents, and to create a framework to facilitate the searching of mathematical contents on the web. The proposed approach relies on two principles. The first principle is the automatic semantic addition by converting presentation MathML into content MathML. The second feature of the proposed system is the ability to search for the mathematical expression and find where it is located exactly in the e-learning web page. This is accomplished by having RDFa annotations embedded in the resulted MathML. © 2010 ACM.","e-learning; MathML; semantic web","Content mathml; First-principles; Mathematical expressions; MathML; Web page; E-learning; Semantics; Web services; Semantic Web",2-s2.0-78751652164
"Chih-Chun C., Ming-Shien C., Ping-Yu H.","Effective storage structure for multi-version XML documents",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651109016&doi=10.1007%2f978-3-642-17569-5_25&partnerID=40&md5=baa85f68b446b8ee9a31e6a2a57dcd66","Office applications such as OpenOffice.org and Microsoft office are widely used to do everything you expect from your needs. Because of more and more requirements of information exchange and retrieve, XML becomes a standard in doing this way. With the adoption of using XML in both office application groups, the abilities for efficient storing historical office documents are become a growing issue. This paper introduces an efficient way to process multi-version XML documents. It is not only effective storage space need but also keeping the integral of original documents. It minimizes the change of data values or structures transmutation of historical XML documents. The purpose is to well-managed electronic documents for enterprises and all the messages were involved in should be preserved. © 2010 Springer-Verlag Berlin Heidelberg.","Historical document; OpenOffice.org; Storage structure; XML","Data values; Electronic document; Historical documents; Information exchanges; Microsoft Office; Multi-version; Office applications; Openoffice.org; Storage spaces; Storage structures; Information technology; Security of data; Word processing; XML",2-s2.0-78651109016
"Bergsma S., Pitler E., Lin D.","Creating robust supervised classifiers via web-scale N-gram data",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053268186&partnerID=40&md5=19dc76d3e8364afc02dd6bf8f85c28c3","In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. © 2010 Association for Computational Linguistics.",,"Data sets; Labeled training data; N-grams; Noun compounds; Part Of Speech; Robust performance; Spelling correction; Supervised classifiers; Computational linguistics",2-s2.0-80053268186
"Kwiatkowski T., Zettlemoyer L., Goldwater S., Steedman M.","Inducing probabilistic CCG grammars from logical form with higher-order unification",2010,"EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",106,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053253268&partnerID=40&md5=52ed34cbe18b497f0f50ced4c75fb5b6","This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. © 2010 Association for Computational Linguistics.",,"Benchmark data; General method; Higher-order unification; Hypothesis space; Logical forms; Logical representations; Natural languages; Online learning algorithms; Training data; Computational linguistics; Learning algorithms; Natural language processing systems",2-s2.0-80053253268
"Wu F., Weld D.S.","Open information extraction using Wikipedia",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",215,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455197425&partnerID=40&md5=a3e7a54dddec30f7e37d2dde1a344fc6","Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunner's precision and recall. The key to WOE's performance is a novel form of self-supervised learning for open extractors - using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE's extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher. © 2010 Association for Computational Linguistics.",,"Attribute values; Information Extraction; Precision and recall; Semantic relations; Training data; Wikipedia; Computational linguistics; Semantics; Supervised learning; Websites",2-s2.0-84455197425
"Miller J.A., Han J., Hybinette M.","Using domain specific language for modeling and simulation: Scalation as a case study",2010,"Proceedings - Winter Simulation Conference",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951640392&doi=10.1109%2fWSC.2010.5679113&partnerID=40&md5=124753c803443b0ede358cecfb4a936e","Progress in programming paradigms and languages has over time influenced the way that simulation programs are written. Modern object-oriented, functional programming languages are expressive enough to define embedded Domain Specific Languages (DSLs). The Scala programming language is used to implement ScalaTion that supports several popular simulation modeling paradigms. As a case study, ScalaTion is used to consider how language features of object-oriented, functional programming languages and Scala in particular can be used to write simulation programs that are clear, concise and intuitive to simulation modelers. The dichotomy between ""model specification"" and ""simulation program"" is also considered both historically and in light of the potential narrowing of the gap afforded by embedded DSLs. ©2010 IEEE.",,"Domain specific languages; Embedded domain specific languages; Functional programming languages; Language features; Model specifications; Modeling and simulation; Object oriented; Programming language; Programming paradigms; Simulation modelers; Simulation modeling; Simulation program; Computer simulation; Functional programming; Object oriented programming",2-s2.0-79951640392
"Volk M., Göhring A., Marek T.","Combining parallel treebanks and geo-tagging",2010,"ACL 2010 - LAW 2010: 4th Linguistic Annotation Workshop, Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880388685&partnerID=40&md5=3f30e1830a0cea50c542b898465fbc81","This paper describes a new kind of semantic annotation in parallel treebanks. We build French-German parallel treebanks of mountaineering reports, a text genre that abounds with geographical names which we classify and ground with reference to a large gazetteer of Swiss toponyms. We discuss the challenges in obtaining a high recall and precision in automatic grounding, and sketch how we represent the grounding information in our treebank. © 2010 Association for Computational Linguistics.",,"Geo-tagging; Recall and precision; Semantic annotations; Text genre; Treebanks; Data mining; Linguistics; Semantics; Forestry",2-s2.0-84880388685
"Hunkeler U., Scotton P.","A code generator for distributing sensor data models",2010,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885893336&doi=10.1007%2f978-3-642-11528-8_10&partnerID=40&md5=81b51a85b9598ed4eec417fcf2135b61","As wireless sensor networks mature, it becomes clear that the raw data collected by this technology can only be used in a meaningful way if it can be analyzed automatically. Describing the behavior of the data with a model, and then looking at the parameters of the model, or detecting differences between the model and the real data, is how experimental data is typically used in other fields. The work presented here aims at facilitating the use of sensor data models to describe the expected behavior of the sensor observations. The processing of such models can be pushed into the wireless sensor network to eliminate redundant information as early in the data collection chain as possible, thus minimizing both bandwidth requirements and energy consumption. © 2009 Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering.",,"Bandwidth requirement; Code generators; Data collection; Experimental data; Redundant informations; Sensor data; Energy utilization; Models; Sensors; Wireless sensor networks",2-s2.0-84885893336
"Lim B., Kim J.","Page flip contents caching method using user request on digital media server",2010,"2010 International Congress on Ultra Modern Telecommunications and Control Systems and Workshops, ICUMT 2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951500129&doi=10.1109%2fICUMT.2010.5676649&partnerID=40&md5=2e2ae03a0c40580c7a2fc89dd84c9d1b","In server-client model in Network, there is no guarantee that a user request will be responded to in a timely manner, especially via a mobile device. A mobile network cannot guarantee high speed; also, the web server response time varies infinitely depending on which commercial net is being used. Moreover, a small screen size can limit the user from seeing the full list of contents or information, so it is difficult for the user to pinpoint the desired location. Previous research has attempted to reduce the latency between a client request and the server response and has discussed straight-forward methods like using the cache control technique. However, these methods, in some cases, cause an increase in latency. In this paper, we propose a breakthrough method more effective than the above mentioned technique regarding the methodologies between the server and client. Suggestions are made based on the optimal combination of new cache control research using page flip based on user view and adjusting the cache buffer size depending on the user variable request. There is the limitation of displaying all the information and contents on the screen. We will provide methods to make full use of the limited representation of the display based on page flip contents list caching. ©2010 IEEE.","Caching; DLNA; Prefetching; Server-client; UPnP","Buffer sizes; Cache control; Caching; Caching methods; Client request; Digital media; DLNA; Mobile networks; Optimal combination; Prefetching; Response time; Server-client model; Small screens; Straight-forward method; UPnP; User view; Web servers; Digital storage; Mobile devices; Control systems",2-s2.0-79951500129
"Feng Y., Hong Y., Yan Z., Yao J., Zhu Q.","A novel method for bilingual web page acquisition from search engine web records",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053416567&partnerID=40&md5=314e16549fdd2103d4e2564a26fd06e8","A new approach has been developed for acquiring bilingual web pages from the result pages of search engines, which is composed of two challenging tasks. The first task is to detect web records embedded in the result pages automatically via a clustering method of a sample page. Identifying these useful records through the clustering method allows the generation of highly effective features for the next task which is high-quality bilingual web page acquisition. The task of high-quality bilingual web page acquisition is a classification problem. One advantage of our approach is that it is search engine and domain independent. The test is based on 2516 records extracted from six search engines automatically and annotated manually, which gets a high precision of 81.3% and a recall of 94.93%. The experimental results indicate that our approach is very effective.",,"Clustering methods; High precision; High quality; Web page; Web record; Classification (of information); Cluster analysis; Computational linguistics; Mergers and acquisitions; Search engines; World Wide Web",2-s2.0-80053416567
"Chiarcos C., Eckart K., Ritz J.","Creating and exploiting a resource of parallel parses",2010,"ACL 2010 - LAW 2010: 4th Linguistic Annotation Workshop, Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880383640&partnerID=40&md5=85633649c3c096b0626cdd262402045a","This paper describes the creation of a resource of German sentences with multiple automatically created alternative syntactic analyses (parses) for the same text, and how qualitative and quantitative investigations of this resource can be performed using ANNIS, a tool for corpus querying and visualization. Using the example of PP attachment, we show how parsing can benefit from the use of such a resource. © 2010 Association for Computational Linguistics.",,"German sentences; Quantitative investigation; Syntactic analysis; Linguistics; Data mining",2-s2.0-84880383640
"Kim J.-J., Rebholz-Schuhmann D.","Improving the extraction of complex regulatory events from scientific text by using ontology-based inference",2010,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874265316&partnerID=40&md5=3b29e735aab8df5e6c745419915743f5","Background: The extraction of complex events from biomedical text is a challenging task and requires in-depth semantic analysis. Previous approaches associate lexical and syntactic resources with ontologies for the semantic analysis, but fall short in testing the benefits from the use of domain knowledge. Results: We developed a system that deduces implicit events from explicitly expressed events by using inference rules that encode domain knowledge. We evaluated the system with the inference module on three tasks: First, when tested against a corpus with manually annotated events, the inference module of our system contributes 53.2% of correct extractions, but does not cause any incorrect results. Second, the system overall reproduces 33.1% of the transcription regulatory events contained in Regulon DB (up to 85.0% precision) and the inference module is required for 93.8% of the reproduced events. Third, we applied the system with minimum adaptations to the identification of cell activity regulation events, confirming that the inference improves the performance of the system also on this task. Conclusions: Our research shows that the inference based on domain knowledge plays a significant role in extracting complex events from text. This approach has great potential in recognizing the complex concepts of such biomedical ontologies as Gene Ontology in the literature.",,"Biomedical ontologies; Biomedical text; Cell activity; Complex events; Domain knowledge; Gene ontology; Inference rules; Ontology-based; Scientific texts; Semantic analysis; Extraction; Natural language processing systems; Semantics",2-s2.0-84874265316
"Sassano M., Kurohashi S.","Using smaller constituents rather than sentences in active learning for Japanese dependency parsing",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859057566&partnerID=40&md5=6c9caced8207f8831ef256f649162e6e","We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning. © 2010 Association for Computational Linguistics.",,"Active Learning; Active learning methods; Dependency parsing; Dependency relation; Learning curves; Passive learning; Computational linguistics; Content based retrieval; Context free grammars",2-s2.0-84859057566
"Assawamekin N., Sunetnanta T., Pluempitiwiriyawej C.","Ontology-based multiperspective requirements traceability framework",2010,"Knowledge and Information Systems",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649499993&doi=10.1007%2fs10115-009-0259-2&partnerID=40&md5=8e26a191aa7bba749f3b010536274fe9","Large-scaled software development inevitably involves a group of stakeholders, each of whom may express their requirements differently in their own terminology and representation depending on their perspectives or perceptions of their shared problems. In view of that, the heterogeneity must be well handled and resolved in tracing and managing changes of such requirements. This paper presents our multiperspective requirements traceability (MUPRET) framework which deploys ontology as a knowledge management mechanism to intervene mutual ""understanding"" without restricting the freedom in expressing requirements differently. Ontology matching is applied as a reasoning mechanism in automatically generating traceability relationships. The relationships are identified by deriving semantic analogy of ontology concepts representing requirements elements. The precision and recall of traceability relationships generated by the framework are verified by comparing with a set of traceability relationships manually identified by users as a proof-of-concept of this framework. © 2009 Springer-Verlag London Limited.","Knowledge management; Multiperspective software development; Ontology; Requirements traceability",,2-s2.0-78649499993
"Gupta P., Ramirez G., Lie D.Y.C., Dallas T., Banister R.E., Dentino A.","MEMS based sensing and algorithm development for fall detection and gait analysis",2010,"Progress in Biomedical Optics and Imaging - Proceedings of SPIE",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958168729&doi=10.1117%2f12.841963&partnerID=40&md5=b092d04aa0a95016bc62509d5fbdf23a","Falls by the elderly are highly detrimental to health, frequently resulting in injury, high medical costs, and even death. Using a MEMS-based sensing system, algorithms are being developed for detecting falls and monitoring the gait of elderly and disabled persons. In this study, wireless sensors utilize Zigbee protocols were incorporated into planar shoe insoles and a waist mounted device. The insole contains four sensors to measure pressure applied by the foot. A MEMS based tri-axial accelerometer is embedded in the insert and a second one is utilized by the waist mounted device. The primary fall detection algorithm is derived from the waist accelerometer. The differential acceleration is calculated from samples received in 1.5s time intervals. This differential acceleration provides the quantification via an energy index. From this index one may ascertain different gait and identify fall events. Once a pre-determined index threshold is exceeded, the algorithm will classify an event as a fall or a stumble. The secondary algorithm is derived from frequency analysis techniques. The analysis consists of wavelet transforms conducted on the waist accelerometer data. The insole pressure data is then used to underline discrepancies in the transforms, providing more accurate data for classifying gait and/or detecting falls. The range of the transform amplitude in the fourth iteration of a Daubechies-6 transform was found sufficient to detect and classify fall events. © 2010 SPIE.","Fall detection; Gait classification; MEMS; Tri-axial accelerometer; Wireless sensors","Accelerometer data; Algorithm development; Energy indexes; Fall detection; Frequency Analysis; Gait classification; Pressure data; Sensing systems; Time interval; Triaxial accelerometer; Wireless sensor; Wireless sensors; Zig-Bee; Accelerometers; Algorithms; Handicapped persons; Microfluidics; Microsystems; Sensors; Wavelet transforms; Wireless sensor networks; Pattern recognition",2-s2.0-79958168729
"Umansky-Pesin S., Reichart R., Rappoport A.","A multi-domainWeb-based algorithm for POS tagging of unknown words",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053404364&partnerID=40&md5=67d8d95d63d01bfe14f90b5d5a0f471d","We present a web-based algorithm for the task of POS tagging of unknown words (words appearing only a small number of times in the training data of a supervised POS tagger). When a sentence s containing an unknown word u is to be tagged by a trained POS tagger, our algorithm collects from the web contexts that are partially similar to the context of u in s, which are then used to compute new tag assignment probabilities for u. Our algorithm enables fast multi-domain unknown word tagging, since, unlike previous work, it does not require a corpus from the new domain. We integrate our algorithm into the MXPOST POS tagger (Ratnaparkhi, 1996) and experiment with three languages (English, German and Chinese) in seven in-domain and domain adaptation scenarios. Our algorithm provides an error reduction of up to 15.63% (English), 18.09% (German) and 13.57% (Chinese) over the original tagger.",,"Domain adaptation; Error reduction; Multi domains; PoS taggers; PoS tagging; Training data; Computational linguistics; User interfaces; Algorithms",2-s2.0-80053404364
"Cohen S.B., Smith N.A.","Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859976571&partnerID=40&md5=86b01c3ff9488bdea82c246ea525af10","We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by ""Viterbi training."" We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. © 2010 Association for Computational Linguistics.",,"Assignment of; Hardness result; Log likelihood; Model parameters; NP-hard; Probabilistic context free grammars; Viterbi; Computational linguistics; Competition",2-s2.0-84859976571
"QasemiZadeh B., Buitelaar P., Monaghan F.","Developing a dataset for technology structure mining",2010,"Proceedings - 2010 IEEE 4th International Conference on Semantic Computing, ICSC 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952062900&doi=10.1109%2fICSC.2010.73&partnerID=40&md5=98323a613a756a1b752612b85b1b7341","This paper describes steps that have been taken to construct a development dataset for the task of Technology Structure Mining. We have defined the proposed task as the process of mapping a scientific corpus into a labeled digraph named a Technology Structure Graph as described in the paper. The generated graph expresses the domain semantics in terms of interdependencies between pairs of technologies that are named (introduced) in the target scientific corpus. The dataset comprises a set of sentences extracted from the ACL Anthology Corpus. Each sentence is annotated with at least two technologies in the domain of Human Language Technology and the interdependence between them. The annotations - technology mark-up and their interdependencies - are expressed at two layers: lexical and termino-conceptual. Lexical representation of technologies comprises varying lexicalizations of a technology. However, at the termino-conceptual layer all these lexical variations refer to the same concept. We have adopted the same approach for representing Semantic Relations; at the lexical layer a semantic relation is a predicate i.e. defined based on the sentence surface structure; however at the termino-conceptual layer semantic relations are classified into conceptual relations either taxonomic or non-taxonomic. Morover, the contexts that interdependencies are extracted from are classified into five groups based on the linguistic criteria and syntactic structure that are identified by the human annotators. The dataset initially comprises of 482 sentences. We hope this effort results in a benchmark that can be used for the technology structure mining task as defined in the paper. © 2010 IEEE.","NLP; Technology structure mining; Text mining","Conceptual relations; Data sets; Domain semantics; Human language technologies; Lexical layers; NLP; Semantic relations; Structure mining; Syntactic structure; Text mining; Two layers; Natural language processing systems; Semantics; Surface structure; Technology",2-s2.0-79952062900
"Mairesse F., Gašić M., Jurčíček F., Keizer S., Thomson B., Yu K., Young S.","Phrase-based statistical language generation using graphical models and active learning",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859994385&partnerID=40&md5=9296fbda1fe67c4c8250b8ccec36d9b9","Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data. © 2010 Association for Computational Linguistics.",,"Active Learning; Data sets; Decision process; Dynamic Bayesian networks; Gold standards; GraphicaL model; Human evaluation; Information presentation; Language generation; Language generators; Statistical models; Bayesian networks; Computational linguistics; Speech recognition",2-s2.0-84859994385
"Rogin F., Drechsler R.","Debugging at the electronic system level",2010,"Debugging at the Electronic System Level",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891443291&doi=10.1007%2f978-90-481-9255-7&partnerID=40&md5=a570431eae8cb1746117822c27567a4b","Debugging becomes more and more the bottleneck to chip design productivity, especially while developing modern complex integrated circuits and systems at the Electronic System Level (ESL). Today, debugging is still an unsystematic and lengthy process. Here, a simple reporting of a failure is not enough, anymore. Rather, it becomes more and more important not only to find many errors early during development but also to provide efficient methods for their isolation. In Debugging at the Electronic System Level the state-of-the-art of modeling and verification of ESL designs is reviewed. There, a particular focus is taken onto SystemC. Then, a reasoning hierarchy is introduced. The hierarchy combines well-known debugging techniques with whole new techniques to improve the verification efficiency at ESL. The proposed systematic debugging approach is supported amongst others by static code analysis, debug patterns, dynamic program slicing, design visualization, property generation, and automatic failure isolation. All techniques were empirically evaluated using real-world industrial designs. Summarized, the introduced approach enables a systematic search for errors in ESL designs. Here, the debugging techniques improve and accelerate error detection, observation, and isolation as well as design understanding. © Springer Science+Business Media B.V. 2010.",,,2-s2.0-84891443291
"Zheng G., Bouguettaya A.","Web service mining: Application to discoveries of biological pathways",2010,"Web Service Mining: Application to Discoveries of Biological Pathways",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891475765&doi=10.1007%2f978-1-4419-6539-4&partnerID=40&md5=39155b4de6f7e9968d52db9f1847d86f","Web Service Mining: Application to Discoveries of Biological Pathways presents the major issues and solutions to mining services on the Web. This book focuses specifically on a reference framework for Web service mining that is inspired by molecular recognition and the drug discovery process; known as a molecular-based approach. Web Service Mining: Application to Discoveries of Biological Pathways applies the service mining framework and techniques back to biological processes for the discovery of biological pathways. It links various processes that are involved in an interaction network, as well as provides performance benchmarks for assessing web service mining techniques and algorithms. About this book: Presents in-depth analysis of issues related to service mining on the Web. Includes a novel application of service modeling and mining methodologies to the discovery of biological pathways. Web Service and Mining: Application to Discoveries of Biological Pathways is designed for researchers and practitioners working in the web service oriented computing industry. This book is also suitable for advanced-level students in computer science and biology as a secondary text or reference book. © Springer Science+Business Media, LLC 2010. All rights reserved.",,,2-s2.0-84891475765
"Taira R.K.","Natural language processing of medical reports",2010,"Medical Imaging Informatics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894960651&doi=10.1007%2f978-1-4419-0385-3_6&partnerID=40&md5=f620fa67e8762e208828d843f8a0a7f4","A significant amount of information regarding the observations, assessments, and recommendations related to a patient's case is documented within free-text medical reports. The ability to structure and standardize clinical patient data has been a grand goal of medical informatics since the inception of the field - especially if this structuring can be (automatically) achieved at the patient bedside and within the modus operandi of current medical practice. A computational infrastructure that transforms the process of clinical data collection from an uncontrolled to highly controlled operation (i.e., precise, completely specified, standard representation) can facilitate medical knowledge acquisition and its application to improve healthcare. Medical natural language processing (NLP) systems attempt to interpret free-text to facilitate a clinical, research, or teaching task. An NLP system performs translates a source language (e.g., free-text) to a target surrogate, computer-understandable representation (e.g., first-order logic), which in turn can support the operations of a driving application. NLP is really then a transformation from a representational form that is not very useful from the perspective of a computer (a sequence of characters) to a form that is useful (a logic-based representation of the text meaning). In general, the accuracy and speed of translation is heavily dependent on the end application. This chapter presents work related to natural language processing of clinical reports, covering issues related to representation, computation, and evaluation. We first summarize a number of typical clinical applications. We then present a high-level formalization of the medical NLP problem in order to provide structure as to how various aspects of NLP fit and complement one another. Examples of approaches that target various forms of representations and degrees of potential accuracy are discussed. Individual NLP subtasks are subsequently discussed. We conclude this chapter with evaluation methods and a discussion of the directions expected in the processing of clinical medical reports. Throughout, we describe applications illustrating the many open issues revolving around medical natural language processing. © Springer Science + Business Media, LLC 2010.",,,2-s2.0-84894960651
"Huffmire T., Irvine C., Nguyen T.D., Levin T., Kastner R., Sherwood T.","Handbook of FPGA design security",2010,"Handbook of FPGA Design Security",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889954492&doi=10.1007%2f978-90-481-9157-4&partnerID=40&md5=25ccd846cf16bbd99992bb631314f7e0","The purpose of Handbook of FPGA Design Security is to provide a practical approach to managing security in FPGA designs for researchers and practitioners in the electronic design automation (EDA) and FPGA communities, including corporations, industrial and government research labs, and academics. Handbook of FPGA Design Security combines theoretical underpinnings with a practical design approach and worked examples for combating real world threats. To address the spectrum of lifecycle and operational threats against FPGA systems, a holistic view of FPGA security is presented, from formal top level specification to low level policy enforcement mechanisms. This perspective integrates recent advances in the fields of computer security theory, languages, compilers, and hardware. The net effect is a diverse set of static and runtime techniques that, working in cooperation, facilitate the composition of robust, dependable, and trustworthy systems using commodity components. © Springer Science+Business Media B.V. 2010.",,,2-s2.0-84889954492
"Wendt K., Ehrlich M., Schüffny R.","GMPath - A path language for navigation, information query and modification of data graphs",2010,"Proceedings of the 6th International Workshop on Artificial Neural Networks and Intelligent Information Processing - Workshop, ANNIIP 2010, in Conjunction with ICINCO 2010",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649360995&partnerID=40&md5=943173801001978f2246a073540fa1c8","This paper presents a newly developed path language named GMPath intended to ease the navigation, information query and modification of general, directed model graphs for the FACETS Stage 2 Large Scale Reconfigurable Neural Hardware Simulator. Furthermore it introduces the reader to the relevant aspects of the FACETS system and its software framework accordingly.",,"Data graph; Information query; Neural hardware; Re-configurable; Software frameworks; Data processing; Navigation; Reconfigurable hardware; Neural networks",2-s2.0-78649360995
"Boitet C., Blanchon H., Seligman M., Bellynck V.","MT on and for the web",2010,"Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649261429&doi=10.1109%2fNLPKE.2010.5587865&partnerID=40&md5=6ad997fe01b206ccf7ecc07e076d2ff1","A Systran MT server became available on the minitel network in 1984, and on Internet in 1994. Since then we have come to a better understanding of the nature of MT systems by separately analyzing their linguistic, computational, and operational architectures. Also, thanks to the CxAxQ metatheorem, the systems' inherent limits have been clarified, and design choices can now be made in an informed manner according to the translation situations. MT evaluation has also matured: tools based on reference translations are useful for measuring progress; those based on subjective judgments for estimating future usage quality; and task-related objective measures (such as post-editing distances) for measuring operational quality. Moreover, the same technological advances that have led to ""Web 2.0"" have brought several futuristic predictions to fruition. Free Web MT services have democratized assimilation MT beyond belief. Speech translation research has given rise to usable systems for restricted tasks running on PDAs or on mobile phones connected to servers. New man-machine interface techniques have made interactive disambiguation usable in large-coverage multimodal MT. Increases in computing power have made statistical methods workable, and have led to the possibility of building low-linguistic-quality but still useful MT systems by machine learning from aligned bilingual corpora (SMT, EBMT). In parallel, progress has been made in developing interlingua-based MT systems, using hybrid methods. Unfortunately, many misconceptions about MT have spread among the public, and even among MT researchers, because of ignorance of the past and present of MT R&D. A compensating factor is the willingness of end users to freely contribute to building essential parts of the linguistic knowledge needed to construct MT systems, whether corpus-related or lexical. Finally, some developments we anticipated fifteen years ago have not yet materialized, such as online writing tools equipped with interactive disambiguation, and as a corollary the possibility of transforming source documents into self-explaining documents (SEDs) and of producing corresponding SEDs fully automatically in several target languages. These visions should now be realized, thanks to the evolution of Web programming and multilingual NLP techniques, leading towards a true Semantic Web, ""Web 3.0"", which will support ubilingual (ubiquitous multilingual) computing. ©2010 IEEE.","Computational architecture; Interactive disambiguation; Linguistic architecture; MT; Operational architecture; Self-explaining documents; Semantic web MT; Speech MT; Task-related evaluation","Computational architecture; Interactive disambiguation; MT; Operational architecture; Self-explaining documents; Task-related evaluation; Computational linguistics; Knowledge engineering; Learning algorithms; Natural language processing systems; Network architecture; Servers; Software agents; Telecommunication equipment; Translation (languages); Ubiquitous computing; Semantic Web",2-s2.0-78649261429
"Sonntag D., Reithinger N., Herzog G., Becker T.","A discourse and dialogue infrastructure for industrial dissemination",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549252405&doi=10.1007%2f978-3-642-16202-2_12&partnerID=40&md5=1dbc8bd031965a463642c1196e06ec82","We think that modern speech dialogue systems need a prior usability analysis to identify the requirements for industrial applications. In addition, work from the area of the Semantic Web should be integrated. These requirements can then be met by multimodal semantic processing, semantic navigation, interactive semantic mediation, user adaptation/personalisation, interactive service composition, and semantic output representation which we will explain in this paper.We will also describe the discourse and dialogue infrastructure these components develop and provide two examples of disseminated industrial prototypes. © 2010 Springer-Verlag.",,"Interactive services; Multi-modal; Semantic mediation; Semantic navigation; Semantic processing; Speech dialogue systems; Usability analysis; Industrial applications; Industry; Speech processing; Semantic Web",2-s2.0-78549252405
"Temprado-Battad B., Sarasa-Cabezuelo A., Sierra J.-L.","Modular specifications of XML processing tasks with attribute grammars defined on multiple syntactic views",2010,"Proceedings - 21st International Workshop on Database and Expert Systems Applications, DEXA 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449285753&doi=10.1109%2fDEXA.2010.72&partnerID=40&md5=964333a4ac926b739ba65741ce6f47bf","This paper describes an approach to the modular specification of XML processing tasks. The approach is language-oriented: it considers the programs that process a type of XML documents to be processors (e.g., compilers or interpreters) for the markup language used in these documents. In addition, it encourages the declarative specification of these processors through attribute grammars. In doing so, the syntax of the language must be characterized with a context-free grammar, which is equivalent to the schema of the markup language. In turn, the processing of documents must be characterized with semantic attributes and semantic equations added to this context-free grammar. To manage the difficulty of specifying complex tasks, it is possible to split the specification into several attribute grammars, each dealing with a specific aspect of the processing. In addition, each of these attribute grammars can have a different underlying context-free grammar, which conforms to a syntactic view specially tailored for the processing aspect addressed. To make these specifications executable, this paper proposes a processing engine based on GLR parsing and on a demand-driven attribute evaluation method. Also, examples are given of using this approach with a simple XML-based language for coding arithmetic formulas. © 2010 IEEE.","Attribute grammar; GLR parsing method; Syntax-directed translation; XML processing","Attribute evaluation; Attribute grammars; Complex task; GLR parsing; Modular specifications; Processing engine; Semantic attribute; Syntax-directed translation; XML processing; XML-based languages; Context free grammars; Context sensitive grammars; Expert systems; Hypertext systems; Markup languages; Object oriented programming; Problem solving; Program compilers; Program interpreters; Specifications; Syntactics; Translation (languages); XML; Context free languages",2-s2.0-78449285753
"Wilke C., Thiele M., Wende C.","Extending variability for OCL interpretation",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349272824&doi=10.1007%2f978-3-642-16145-2_25&partnerID=40&md5=1f6c9994799a15148eefad031b7b8d48","In recent years, OCL advanced from a language used to constrain UML models to a constraint language that is applied to various modelling languages. This includes Domain Specific Languages (DSLs) and meta-modelling languages like MOF or Ecore. Consequently, it is rather common to provide variability for OCL parsers to work with different modelling languages. A second variability dimension relates to the technical space that models are realised in. Current OCL interpreters do not support such variability as their implementation is typically bound to a specific technical space like Java, Ecore, or a specific model repository. In this paper we propose a generic adaptation architecture for OCL that hides models and model instances behind well-defined interfaces. We present how the implementation of such an architecture for DresdenOCL enables reuse of the same OCL interpreter for various technical spaces and evaluate our approach in three case studies. © 2010 Springer-Verlag.","Adaptation; Constraint Interpretation; MDSD; Modelling; OCL; OCL Infrastructure; OCL Tool; Technological Spaces; Variability","Adaptation; Constraint Interpretation; MDSD; Modelling; OCL; OCL Infrastructure; OCL Tool; Technological Spaces; Variability; Unified Modeling Language; Models",2-s2.0-78349272824
"Maggi F.","A recognizer of rational trace languages",2010,"Proceedings - 10th IEEE International Conference on Computer and Information Technology, CIT-2010, 7th IEEE International Conference on Embedded Software and Systems, ICESS-2010, ScalCom-2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249274946&doi=10.1109%2fCIT.2010.77&partnerID=40&md5=1ebef8aeae1db3e7eae42cc086b52aa9","The relevance of instruction parallelization and optimal event scheduling is currently increasing. In particular, because of the high amount of computational power available today, the industrial interest on automatic code parallelization is raising notably. In the last years, several contributions have arisen in these fields, exploiting the theory of traces that provides a powerful mathematical formalism that can be effectively used to model and study concurrent executions of events. However, there is a quite large amount of open problems that need to be further investigated in this area. In this paper, we present a one-pass recognition algorithm to solve the membership problem for rational trace languages, that is the problem of deciding whether or not a certain string belongs (i.e., is member of) a trace, or a trace language. Solving this problem is fundamental for designing efficient parsers. Our solution is detailed through the formal specification of the Buffer Machine, a non-deterministic, finite-state automaton with multiple buffers that can solve the membership problem in polynomial time. © 2010 IEEE.",,"Automatic codes; Computational power; Concurrent execution; Event scheduling; Finite-state automata; Formal Specification; Mathematical formalism; Membership problem; One-pass; Open problems; Parallelizations; Polynomial-time; Recognition algorithm; Theory of traces; Embedded software; Embedded systems; Information technology; Polynomial approximation; Query languages; Problem solving",2-s2.0-78249274946
"Han L., Suzek T.O., Wang Y., Bryant S.H.","The Text-mining based PubChem Bioassay neighboring analysis",2010,"BMC Bioinformatics",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049479811&doi=10.1186%2f1471-2105-11-549&partnerID=40&md5=11acde67f76a2524e7ac470d918194ff","Background: In recent years, the number of High Throughput Screening (HTS) assays deposited in PubChem has grown quickly. As a result, the volume of both the structured information (i.e. molecular structure, bioactivities) and the unstructured information (such as descriptions of bioassay experiments), has been increasing exponentially. As a result, it has become even more demanding and challenging to efficiently assemble the bioactivity data by mining the huge amount of information to identify and interpret the relationships among the diversified bioassay experiments. In this work, we propose a text-mining based approach for bioassay neighboring analysis from the unstructured text descriptions contained in the PubChem BioAssay database.Results: The neighboring analysis is achieved by evaluating the cosine scores of each bioassay pair and fraction of overlaps among the human-curated neighbors. Our results from the cosine score distribution analysis and assay neighbor clustering analysis on all PubChem bioassays suggest that strong correlations among the bioassays can be identified from their conceptual relevance. A comparison with other existing assay neighboring methods suggests that the text-mining based bioassay neighboring approach provides meaningful linkages among the PubChem bioassays, and complements the existing methods by identifying additional relationships among the bioassay entries.Conclusions: The text-mining based bioassay neighboring analysis is efficient for correlating bioassays and studying different aspects of a biological process, which are otherwise difficult to achieve by existing neighboring procedures due to the lack of specific annotations and structured information. It is suggested that the text-mining based bioassay neighboring analysis can be used as a standalone or as a complementary tool for the PubChem bioassay neighboring process to enable efficient integration of assay results and generate hypotheses for the discovery of bioactivities of the tested reagents. © 2010 Han et al; licensee BioMed Central Ltd.",,"Amount of information; Biological process; Clustering analysis; Complementary tools; Distribution analysis; High-throughput screening; Structured information; Unstructured texts; Bioactivity; Experiments; Bioassay; article; bioassay; data mining; factual database; high throughput screening; methodology; Biological Assay; Data Mining; Databases, Factual; High-Throughput Screening Assays",2-s2.0-78049479811
"Bernard G., Rosset S., Galibert O., Adda G., Bilinski E.","The LIMSI participation in the QAst 2009 track: Experimenting on answer scoring",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049324228&doi=10.1007%2f978-3-642-15754-7_33&partnerID=40&md5=0976b574fbd1871992bcf3537cb645b1","We present in this paper the three LIMSI question-answering systems on speech transcripts which participated to the QAst 2009 evaluation. These systems are based on a complete and multi-level analysis of both queries and documents. These systems use an automatically generated research descriptor. A score based on those descriptors is used to select documents and snippets. Three different methods are tried to extract and score candidate answers, and we present in particular a tree transformation based ranking method. We participated to all the tasks and submitted 30 runs (for 24 sub-tasks). The evaluation results for manual transcripts range from 27% to 36% for accuracy depending on the task and from 20% to 29% for automatic transcripts. © 2010 Springer-Verlag Berlin Heidelberg.",,"Automatically generated; Descriptors; Evaluation results; Multi-level analysis; Question answering systems; Ranking methods; Speech transcripts; Subtasks; Tree transformation; Artificial intelligence; Linguistics; Information retrieval",2-s2.0-78049324228
"Kaufmann E., Bernstein A.","Evaluating the usability of natural language query languages and interfaces to Semantic Web knowledge bases",2010,"Journal of Web Semantics",72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649323291&doi=10.1016%2fj.websem.2010.06.001&partnerID=40&md5=c5d8f7a2fe6a77769a788ae19c15427e","The need to make the contents of the Semantic Web accessible to end-users becomes increasingly pressing as the amount of information stored in ontology-based knowledge bases steadily increases. Natural language interfaces (NLIs) provide a familiar and convenient means of query access to Semantic Web data for casual end-users. While several studies have shown that NLIs can achieve high retrieval performance as well as domain independence, this paper focuses on usability and investigates if NLIs and natural language query languages are useful from an end-user's point of view. To that end, we introduce four interfaces each allowing a different query language and present a usability study benchmarking these interfaces. The results of the study reveal a clear preference for full natural language query sentences with a limited set of sentence beginnings over keywords or formal query languages. NLIs to ontology-based knowledge bases can, therefore, be considered to be useful for casual or occasional end-users. As such, the overarching contribution is one step towards the theoretical vision of the Semantic Web becoming reality. © 2010 Elsevier B.V. All rights reserved.","Natural language interfaces; Query languages; Usability study","Amount of information; End-users; Knowledge basis; Natural language interfaces; Natural language queries; One step; Ontology-based; Retrieval performance; Usability studies; Ontology; Semantic Web; Usability engineering; Query languages",2-s2.0-78649323291
"Chen L., Yao N.","Publishing Linked Data from relational databases using traditional views",2010,"Proceedings - 2010 3rd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2010",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958538183&doi=10.1109%2fICCSIT.2010.5563576&partnerID=40&md5=eb2206ef913c1e04c8da731081107c46","Many achieves have focused on how to publish Linked Data from the relational databases. However, with current RDB2RDF tools, they showed an unsatisfactory performance because of their inherent gaps between RDF Schema (RDFS) and RDB schema. In this paper, we give a light-weight Linked Data publication mechanism based on the traditional views, which make users retrieve the data from the relational databases as RDF datasets and pose SPARQL queries on the traditional web server. We also give a method which can transform the SPARQL queries into the SQL queries based on Jena ARQ parser. The experiment shows that our approach performs efficiently compared to other RDB2RDF tools. © 2010 IEEE.","Linked data; Query rewriting; RDF; SPARQL; View","Linked datum; Query rewritings; RDF; SPARQL; View; Computer science; Data communication systems; Information technology; Database systems",2-s2.0-77958538183
"Imamura M., Takayama Y., Akiyoshi M., Komoda N.","Acquisition of term knowledge from operating manuals for information equipment by using the structure of headline sentences",2010,"Electronics and Communications in Japan",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349244809&doi=10.1002%2fecj.10314&partnerID=40&md5=46d408344c6f07efff5cc3022dd5e5d5","This paper proposes a method for automatically extracting term knowledge such as case relations and IS-A relations between words in the headline sentences of the operating manuals for information equipment. The proposed method acquires term knowledge by the following iterative processing: case relation extraction using correspondence relations between the surface cases and the deep cases; case and IS-A relation extraction using compound word structures; and IS-A relation extraction using correspondence between the case structures in the hierarchical headline sentences. The distinctive feature of our method is that it extracts new case relations and IS-A relations by comparison and matching of the case relations extracted from the super and sub headline sentences, using the headline hierarchy. We confirmed that the proposed method could achieve approximately 90% recall and precision for the extraction of case relations and IS-A relations from the operating manuals of a car navigation system and a mobile phone. © 2010 Wiley Periodicals, Inc.","case frame; document structure; linguistic knowledge acquisition; operating manual","Car navigation systems; case frame; Case relations; Case structures; Distinctive features; document structure; Iterative processing; linguistic knowledge acquisition; operating manual; Recall and precision; Relation extraction; Word structures; Knowledge acquisition; Linguistics; Telecommunication equipment; Telephone systems; Mergers and acquisitions",2-s2.0-78349244809
"Cohen S.B., Smith N.A.","Covariance in unsupervised learning of probabilistic grammars",2010,"Journal of Machine Learning Research",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551472048&partnerID=40&md5=824cf1be1526adb019b1c1c4b7d86d40","Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text. Their symbolic component is amenable to inspection by humans, while their probabilistic component helps resolve ambiguity. They also permit the use of well-understood, generalpurpose learning algorithms. There has been an increased interest in using probabilistic grammars in the Bayesian setting. To date, most of the literature has focused on using a Dirichlet prior. The Dirichlet prior has several limitations, including that it cannot directly model covariance between the probabilistic grammar's parameters. Yet, various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties. In this paper, we suggest an alternative to the Dirichlet prior, a family of logistic normal distributions. We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction, demonstrating performance improvements with our priors on a set of six treebanks in different natural languages. Our covariance framework permits soft parameter tying within grammars and across grammars for text in different languages, and we show empirical gains in a novel learning setting using bilingual, non-parallel data. © 2010 Evangelos Theodorou, Jonas Buchli and Stefan Schaal.","Bayesian inference; Dependency grammar induction; Logistic normal distribution; Variational inference","Bayesian; Bayesian inference; Dependency grammar; Directly model; Dirichlet prior; Inference algorithm; Learning settings; Linguistic properties; Natural language text; Natural languages; Parallel data; Performance improvements; Probabilistic grammars; Sequential data; Treebanks; Variational inference; Bayesian networks; Computational grammars; Inference engines; Learning algorithms; Normal distribution",2-s2.0-79551472048
"Van Tan V., Yoo D.-S., Yi M.-J.","Designing and developing a modern distributed data acquisition and monitoring system",2010,"Journal of Research and Practice in Information Technology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052768409&partnerID=40&md5=57da38b04369f8b7f70cb5e9936f4f91","This paper introduces new control system aspects used for designing and implementing a distributed data acquisition and monitoring software system. These aspects are proposed according to OPC technologies and XML as standards for interfaces, functionalities, and architectures. The proposed system allows us to easily aggregate existing OPC Data Access (DA) servers and new OPC XML-DA servers into a unified and flexible system that can support complex data exchange among these OPC servers. To guarantee security of remote invocations for the proposed system including the authentication of clients, the encryption of messages, the access control, and the security level aspects are discussed. These security aspects provide more security solutions for technical-level readers. The comparison and discussion are made to indicate that the proposed system has a good design and an acceptable performance. Copyright© 2010, Australian Computer Society Inc.","Complex data; Control system aspects; Monitoring system; OPC; Security; XML","Complex data; Data access; Distributed data acquisition; Flexible system; Monitoring system; OPC; OPC server; Security; Security level; Security solutions; Software systems; Access control; Control systems; XML; Monitoring",2-s2.0-80052768409
"Cao D., Bai D.","Design and implementation for SQL parser based on ANTLR",2010,"ICCET 2010 - 2010 International Conference on Computer Engineering and Technology, Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958050989&doi=10.1109%2fICCET.2010.5485593&partnerID=40&md5=4d81f9ae7d4be680694e867d64498e1d","To solve the close relationship between the data query and interface design of management information system, the paper implements a SQL parser based on ANTLR. The SQL parser is composed of lexical analysis, syntax analysis and semantic checking. The SQL parser is successfully integrated with management information system and the query is implemented by the SQL parser to search the data from database. It solves the problem that the relationship between the data query and interface design of management information system is very close, and improves the reliability and reusability of the software. © 2010 IEEE.","ANTLR; Lexical analysis; SQL parser; Syntax analysis","ANTLR; Data query; Interface designs; Lexical analysis; SQL parser; Syntax analysis; Computer software reusability; Design; Information systems; Management information systems; Reusability; Software reliability; Syntactics; Knowledge management",2-s2.0-77958050989
"Xu Y., Wang L., Yan S.","The study on natural language interface of relational databases",2010,"2010 2nd Conference on Environmental Science and Information Application Technology, ESIAT 2010",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957999568&doi=10.1109%2fESIAT.2010.5568401&partnerID=40&md5=f4b343352d867bf5dc50345af139b84a","This paper proposes a NLIDBs system model and design framework based on Ontology. It uses WordNet as basic lexicon, and defines domain lexicon beyond that. This paper also defines relational database E-R model and domain business module by using OWL Ontology, which increases the accuracy of query sentences. Meanwhile, it's a highly reliable and portable approach to creating a WordNet-based natural language interface to relational databases. ©2010 IEEE.","Natural language; OWL; Relational databases; SQL","Design frameworks; Natural language interfaces; Natural languages; OWL; OWL ontologies; Relational Database; SQL; System models; Wordnet; Database systems; Environmental engineering; Semantic Web; Ontology",2-s2.0-77957999568
"Misek J., Zavoral F.","High-level web data abstraction using language integrated query",2010,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957287186&doi=10.1007%2f978-3-642-15211-5_2&partnerID=40&md5=fe2267f63d72b309d1506187afb9cc8d","Web pages containing huge amount of information are designed for human readers; it makes their automatic computer processing difficult. Moreover web pages live their content is changing. Once a page is downloaded and processed, few seconds after that its content can be different. Many scraping frameworks and extraction mechanisms have been proposed and implemented; their common task is to download and extract required data. Nevertheless, the complexity of development of such application is enormous since the nature of data does not conform to common programming paradigms. Moreover, the changing content of the web pages often implies repetitive extracting of the whole data set. This paper describes the LinqToWeb framework for web data extraction. It is designed in an innovative way that allows defining strongly typed object model transparently reflecting data on the living web. This mechanism provides access to raw web data in a completely object oriented way using modern techniques of Language Integrated Query (LINQ). Using this framework development of web-based applications such as data semantization tools is more efficient, type-safe, and the resulting product is easily maintainable and extendable. © 2010 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-77957287186
"Li J., Brew C.","Class-based approach to disambiguating Levin verbs",2010,"Natural Language Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650056138&doi=10.1017%2fS1351324910000136&partnerID=40&md5=9b073209bcd677879090134c22fe2b00","Lapata and Brew (Computational Linguistics, vol. 30, 2004, pp. 295-313) (hereafter LB04) obtain from untagged texts a statistical prior model that is able to generate class preferences for ambiguous Lewin (English Verb Classes and Alternations: A Preliminary Investigation, 1993, University of Chicago Press) verbs (hereafter Levin). They also show that their informative priors, incorporated into a Naive Bayes classifier deduced from hand-tagged data (HTD), can aid in verb class disambiguation. We re-analyse LB04's prior model and show that a single factor (the joint probability of class and frame) determines the predominant class for a particular verb in a particular frame. This means that the prior model cannot be sensitive to fine-grained lexical distinctions between different individual verbs falling in the same class. We replicate LB04's supervised disambiguation experiments on large-scale data, using deep parsers rather than the shallow parser of LB04. In addition, we introduce a method for training our classifier without using HTD. This relies on knowledge of Levin class memberships to move information from unambiguous to ambiguous instances of each class. We regard this system as unsupervised because it does not rely on human annotation of individual verb instances. Although our unsupervised verb class disambiguator does not match the performance of the ones that make use of HTD, it consistently outperforms the random baseline model. Our experiments also demonstrate that the informative priors derived from untagged texts help improve the performance of the classifier trained on untagged data. Copyright © 2010 Cambridge University Press.",,"Baseline models; Class-based; Human annotations; Informative Priors; Joint probability; Naive Bayes classifiers; Tagged data; University of Chicago; Computational linguistics; Classifiers",2-s2.0-78650056138
"Kats L.C.L., Kalleberg K.T., Visser E.","Domain-specific languages for composable editor plugins",2010,"Electronic Notes in Theoretical Computer Science",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956408206&doi=10.1016%2fj.entcs.2010.08.038&partnerID=40&md5=336309b330c3de6f0c15865e8b9a55a4","Modern IDEs increase developer productivity by incorporating many different kinds of editor services. These can be purely syntactic, such as syntax highlighting, code folding, and an outline for navigation; or they can be based on the language semantics, such as in-line type error reporting and resolving identifier declarations. Building all these services from scratch requires both the extensive knowledge of the sometimes complicated and highly interdependent APIs and extension mechanisms of an IDE framework, and an in-depth understanding of the structure and semantics of the targeted language. This paper describes Spoofax/IMP, a meta-tooling suite that provides high-level domain-specific languages for describing editor services, relieving editor developers from much of the framework-specific programming. Editor services are defined as composable modules of rules coupled to a modular SDF grammar. The composability provided by the SGLR parser and the declaratively defined services allows embedded languages and language extensions to be easily formulated as additional rules extending an existing language definition. The service definitions are used to generate Eclipse editor plugins. We discuss two examples: an editor plugin for WebDSL, a domain-specific language for web applications, and the embedding of WebDSL in Stratego, used for expressing the (static) semantic rules of WebDSL. © 2010 Elsevier B.V. All rights reserved.","Domain specific language; editor plugin; integrated development environment","Composability; Domain specific languages; editor plugin; Embedded Languages; High-level domain; In-depth understanding; In-line; Integrated development environment; Language extensions; Language semantics; Plug-ins; Semantic rules; Service definition; Stratego; Type errors; WEB application; Graphical user interfaces; Object oriented programming; Problem oriented languages; Query languages; Semantic Web; Semantics; Syntactics; Web services; XML; Linguistics",2-s2.0-77956408206
"Baars A., Doaitse Swierstra S., Viera M.","Typed transformations of typed grammars: The left corner transform",2010,"Electronic Notes in Theoretical Computer Science",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956395985&doi=10.1016%2fj.entcs.2010.08.031&partnerID=40&md5=223aae2e418d1b6e30604030e85010d0","One of the questions which comes up when using embedded domain specific languages is to what extent we can analyze and transform embedded programs, as normally done in more conventional compilers. Special problems arise when the host language is strongly typed, and this host type system is used to type the embedded language. In this paper we describe how we can use a library, which was designed for constructing transformations of typed abstract syntax, in the removal of left recursion from a typed grammar description. The algorithm we describe is the Left-Corner Transform, which is small enough to be fully explained, involved enough to be interesting, and complete enough to serve as a tutorial on how to proceed in similar cases. The described transformation has been successfully used in constructing a compositional and efficient alternative to the standard Haskell read function. © 2010 Elsevier B.V. All rights reserved.","GADT; Left-Corner Transform; Meta Programming; Type Systems; Typed Abstract Syntax; Typed Transformations","Abstract syntax; GADT; Left-Corner Transform; Meta Programming; Type systems; Typed Transformations; Abstracting; Linguistics; Query languages; Syntactics",2-s2.0-77956395985
"Engelen L., Van Den Brand M.","Integrating textual and graphical modelling languages",2010,"Electronic Notes in Theoretical Computer Science",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956417903&doi=10.1016%2fj.entcs.2010.08.035&partnerID=40&md5=344d40db8f5a339b96e50d9248d6821a","Graphical diagrams are the main modelling constructs offered by the popular modelling language UML. Because textual representations of models also have their benefits, we investigated the integration of textual and graphical modelling languages, by comparing two approaches. One approach uses grammarware and the other uses modelware. As a case study, we implemented two versions of a textual alternative for Activity Diagrams, which is an example of a surface language. This paper describes our surface language, the two approaches, and the two implementations that follow these approaches. © 2010 Elsevier B.V. All rights reserved.","Grammarware; modelware; surface language","Activity diagram; Grammarware; Graphical modelling; Modelling language; modelware; Textual representation; Integration; Query languages; Linguistics",2-s2.0-77956417903
"Jain S., Pareek J.","Automatic identification of Granularity level of learning document",2010,"2010 International Conference on Technology for Education, T4E 2010",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956502137&doi=10.1109%2fT4E.2010.5550057&partnerID=40&md5=d42d87c8413954d89f033d859d1d5af4","With energetic development of the Internet, especially the web page interaction technology, online learning object repositories have become more and more realistic and popular in the past ten years. These repositories enable sharing and reuse of learning materials by different users. This information background is needed for querying services to perform accurate queries for learning object retrieval. The granularity of learning objects is one among several metadata attributes that has a crucial impact on the ability to adapt, aggregate, and arrange content suiting the needs and preferences of the learner. This paper showcases our work on redefining the Granularity level, more suited towards personalization of learning document and its automatic extraction using Natural Language Processing approach and with the help of Domain ontology. © 2010 IEEE.","Aggregation level; Granularity level; Learning object metadata","Aggregation level; Automatic extraction; Automatic identification; Domain ontologies; Energetic development; Granularity levels; Interaction technology; Learning materials; Learning object metadata; Learning objects; NAtural language processing; Online learning objects; Personalizations; Querying services; Web page; Automation; Computational linguistics; Electronic data interchange; Natural language processing systems; Ontology; Metadata",2-s2.0-77956502137
"Hammond T.","nature.com OpenSearch: A case study in OpenSearch and SRU integration",2010,"D-Lib Magazine",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956420307&doi=10.1045%2fjuly2010-hammond&partnerID=40&md5=4dc64ef30ba44ff3048b2be14a109994","This paper provides a case study of OpenSearch and SRU integration on the nature.com science publisher platform. These two complementary search methodologies are implemented on top of a common base service and provide alternate interfaces into the underlying search engine. Specific points addressed include query strings, response formats, and service control and discovery. Current applications and future work directions are also discussed. © 2010 Tony Hammond.",,,2-s2.0-77956420307
"Del Rosal E., De La Puente A.O., Marin D.P.","PNEPs for shallow parsing: NEPs extended for parsing applied to shallow parsing",2010,"ICAART 2010 - 2nd International Conference on Agents and Artificial Intelligence, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956372979&partnerID=40&md5=7f05ffea630e6e7e623eaf09580a139a","PNEPs (Parsing Networks of Evolutionary Processors) extend NEPs with context free (instead of substituting) rules, leftmost derivation, bad terminals check and indexes to rebuild the derivation tree. It is possible to build a PNEP from any context free grammar without additional constraints, able to generate all the different derivations for ambiguous grammars with a temporal performance bound by the depth of the derivation tree. One of the main difficulties encountered by parsing techniques when building complete parsing trees for natural languages is the spatial and temporal performance of the analysis. Shallow parsing tries to overcome these difficulties. The goal of shallow parsing is to analyze the main components of the sentences (for example, noun groups, verb groups, etc.) rather than complete sentences. The current paper is mainly focused on testing the suitability of PNEPs to shallow parsing.","Natural computing; Natural language processing; Nets of evolutionary processors; Shallow parsing","Context-free; Evolutionary processors; Main component; Natural Computing; NAtural language processing; Natural languages; Performance bounds; Shallow parsing; Artificial intelligence; Computational linguistics; Context free grammars; Natural language processing systems; Query languages",2-s2.0-77956372979
"Becerra-Bonache L., Bensch S., Jiménez-López M.D.","The linguistic relevance of lindenmayer systems",2010,"ICAART 2010 - 2nd International Conference on Agents and Artificial Intelligence, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956389952&partnerID=40&md5=2eec9f531370aa3b11dd36af4f38fdc2","In this paper, we investigate the linguistic relevance of Lindenmayer Systems (L Systems). L systems were introduced in the late sixties by Aristid Lindemayer as a mathematical theory of biological development. Thus they can be considered as one of the first bio-inspired models in the theory of formal languages. Two main properties in L systems are 1) the idea of parallelism in the rewriting process and 2) their expressiveness to describe non-context free structures that can be found in natural languages. Therefore, the linguistic relevance of this formalism is clearly based on three main features: bio-inspiration, parallelism and generation of non-context free languages. Despite these interesting properties, L systems have not been investigated from a linguistic point of view. With this paper we point out the interest of applying these bio-inspired systems to the description and processing of natural language.","Bio-inspired models; Lindenmayer Systems; Natural language; Non-context freeness; Parallelism","Bio-inspired models; Lindenmayer system; Natural languages; Non-context freeness; Parallelism; Artificial intelligence; Context free languages; Linguistics; Query languages; Natural language processing systems",2-s2.0-77956389952
"Pembe F.C., Güngör T.","A tree learning approach to web document sectional hierarchy extraction",2010,"ICAART 2010 - 2nd International Conference on Agents and Artificial Intelligence, Proceedings",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956293213&partnerID=40&md5=fdabb2f3f64ff137564028d6f8d82fef","There is an increasing availability of documents in electronic form due to the widespread use of the Internet. Hypertext Markup Language (HTML) which is mostly concerned with the presentation of documents is still the most commonly used format on the Web, despite the appearance of semantically richer markup languages such as XML. Effective processing of Web documents has several uses such as the display of content on small-screen devices and summarization. In this paper, we investigate the problem of identifying the sectional hierarchy of a given HTML document together with the headings in the document. We propose and evaluate a learning approach suitable to tree representation based on Support Vector Machines.","Document structure; Hypertext Markup Language; Machine learning; World Wide Web","Commonly used; Document structure; Electronic forms; HTML documents; Hypertext Markup Language; Learning approach; Machine-learning; Small-screen devices; Tree representation; Web document; Artificial intelligence; Display devices; HTML; Hypertext systems; Learning systems; Linguistics; Query languages; World Wide Web; XML; Markup languages",2-s2.0-77956293213
"Yousef M., Hashem A., Saad H., Gamal A., Galal O., Hussain K.F.","A scripting language for digital content creation applications",2010,"ACM SIGGRAPH 2010 Posters, SIGGRAPH '10",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956196673&doi=10.1145%2f1836845.1836956&partnerID=40&md5=535b15bca0c52c042bc68a178296721d","Digital Content Creation (DCC) Applications (e.g. Blender, Autodesk 3ds Max) have long been used for the creation and editing of digital content. Due to current advancement in the field, the need for controlled automated work forced these applications to add support for small programming languages that gave power to artists without diving into many details. With time these languages developed into more mature languages and were used for more complex tasks (driving physics simulations, controlling particle systems, or even game engines). For long, these languages have been interpreted, embedded within the applications, lagging the UIs or incomparable with real programming languages (regarding Completeness, Expressiveness, Extensibility and Abstractions). Two approaches were used to implement those languages. Either build them from scratch (like MaxScript), or use an existing popular language and write a set of extensions to it and embed it (like Blender and Python). In practice, both those solutions suffer, the first method produces languages lacking being real, competitive languages and generally very inefficient, the second method has problems arising from not being dedicated in first place for that kind of applications so, they lack expressiveness facilities (like dedicated constructs) that support that particular domain, also it's very hard to optimize these languages for specific DCC situations. © ACM 2010.",,"3ds max; Autodesk; Complex task; Digital content creation; Digital contents; Game Engine; MaxScript; Particle systems; Physics simulation; Programming language; Real programming languages; Scripting languages; Blending; Interactive computer graphics; Query languages; Linguistics",2-s2.0-77956196673
"Stivala A.D., Stuckey P.J., Wirth A.I.","Fast and accurate protein substructure searching with simulated annealing and GPUs",2010,"BMC Bioinformatics",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956481825&doi=10.1186%2f1471-2105-11-446&partnerID=40&md5=62c18a087b0b317c5454dd57f4a73561","Background: Searching a database of protein structures for matches to a query structure, or occurrences of a structural motif, is an important task in structural biology and bioinformatics. While there are many existing methods for structural similarity searching, faster and more accurate approaches are still required, and few current methods are capable of substructure (motif) searching.Results: We developed an improved heuristic for tableau-based protein structure and substructure searching using simulated annealing, that is as fast or faster and comparable in accuracy, with some widely used existing methods. Furthermore, we created a parallel implementation on a modern graphics processing unit (GPU).Conclusions: The GPU implementation achieves up to 34 times speedup over the CPU implementation of tableau-based structure search with simulated annealing, making it one of the fastest available methods. To the best of our knowledge, this is the first application of a GPU to the protein structural search problem. © 2010 Stivala et al; licensee BioMed Central Ltd.",,"GPU implementation; Graphics Processing Unit; Parallel implementations; Protein structures; Query structures; Structural biology; Structural motifs; Structural similarity; Bioinformatics; Computer graphics; Computer graphics equipment; Program processors; Proteins; Query processing; Simulated annealing; Heuristic methods; protein; article; chemistry; computer graphics; computer simulation; protein database; protein tertiary structure; Computer Graphics; Computer Simulation; Databases, Protein; Protein Structure, Tertiary; Proteins",2-s2.0-77956481825
"Chen L., Tokuda N., Nagai A.","A dialogue tutoring system exploiting extracting-rule-embedded-template structure and the expanding powers of buggy rules",2010,"ICETC 2010 - 2010 2nd International Conference on Education Technology and Computer",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956076597&doi=10.1109%2fICETC.2010.5529668&partnerID=40&md5=d2ae04022c4d2358c30ab6f50598aee1","This paper develops a new tutorial dialogue system exploiting the extracting-rule-based extracting-rule-em beddedtemplate structure and the expanding powers of buggy rules. The system is effective not only in simplitying, thus reducing otherwise time-consuming authoring task of template generation ([1, 2]) but also in improving the system performance. The extracting-rule-embedded-template architecture introduced leads to system simplification and performance improvement firstly because it allows many different templates to be integrated into, or equivalently to be extracted from, a single extracting-rule-embedded-template by applying extracting rules assigned to some of its transitional nodes and secondly because the buggy rules introduced are capable of identifying and hence generating bugs from learners' erroneous responses automatically. The NLP (natural language processing) technology plays a key role in the system development because a parser is used to examine the syntactical structure of the learner's free format response while its semantic structure is examined by matching the respective response to the semantically equivalent paths of template databases prepared. The extracting rule-based and buggy rule-based extractingrule-em bedded-template structure is expected to play an important role in many applications including both written and spoken tutoring systems, voice-enabling call center or voice portal systems or in fact any system focusing more enhanced human computer interfaces, implementing more natural human computer interactions between the system and humans. © 2010 IEEE.","Extracting-rule- embedded-template; Natural language processing; Online dialogue tutoring","Call centers; Dialogue systems; Extracting rules; Extracting-rule- embedded-template; Human computer interfaces; Natural language processing; Online dialogue tutoring; Performance improvements; Rule based; Semantic structures; Syntactical structures; System development; Template database; Template generation; Template structures; Tutoring system; Voice portal; Computational linguistics; Human computer interaction; Interfaces (computer); Natural language processing systems; Query languages; Speech processing; Learning systems",2-s2.0-77956076597
"Paiva S., Ramos-Cabrer M., Gil-Solla A.","Semantic query validation in guided-based systems: Assuring the construction of queries that make sense",2010,"Proceedings - 11th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD2010",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956047459&doi=10.1109%2fSNPD.2010.11&partnerID=40&md5=a624dc972b76e78b2d5d7ad5d16a6581","Semantic search importance is growing each day as millions of people around the world need to obtain objective and concrete information. Guided-based systems are an alternative to natural language systems regarding the query construction method. However, current guided-based systems perform query validation only at a syntactical level. In this paper we address the semantic validation of queries in order to avoid redundancy and incoherency, therefore preventing the formulation of queries that do not make sense. © 2010 IEEE.","Query construction; Semantic guided-based systems; Semantic search; Semantic validation rules","Incoherency; Natural language systems; Query construction; Semantic query; Semantic search; Artificial intelligence; Semantics; Software engineering; Query processing",2-s2.0-77956047459
"Sporleder C.","Natural Language Processing for Cultural Heritage Domains",2010,"Linguistics and Language Compass",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956484716&doi=10.1111%2fj.1749-818X.2010.00230.x&partnerID=40&md5=49e509b7f337e9054d54318fdd05996d","Museums, archives, libraries and other cultural heritage institutes maintain large collections of artefacts, which are valuable knowledge sources for both experts and interested lay persons. Recently, more and more cultural heritage institutes have started to digitise their collections, for instance to make them accessible via web portals. However, while digitisation is a necessary first step towards improved information access, to fully unlock the knowledge contained in these collections, users have to be able to easily browse, search and query these collections. This requires cleaning, linking and enriching the data, a process that is often too time-consuming to be performed manually. Information technology can help with (partially) automating this task. Because data processing and enrichment typically involve the textual metadata level, natural language processing has a key role to play in this endeavour. At the same time, cultural heritage domains pose significant challenges for language technology and call for the development of very robust and flexible solutions. Consequently, cultural heritage data can also serve as a good test-bed for the development of robust natural language processing tools. © 2010 The Author. Language and Linguistics Compass © 2010 Blackwell Publishing Ltd.",,,2-s2.0-77956484716
"Xia F., Dou Y., Zhou D., Li X.","Fine-grained parallel RNA secondary structure prediction using SCFGs on FPGA",2010,"Parallel Computing",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955428322&doi=10.1016%2fj.parco.2010.05.005&partnerID=40&md5=caa2b56722513aa834925edf261c7382","In the field of RNA secondary structure prediction, the CYK (Coche-Younger-Kasami) algorithm is one of the most popular methods using a SCFG (stochastic context-free grammar) model. Accelerating SCFGs for large models and large RNA database searching becomes a challenging task in computational bioinformatics because the parallel efficiency of general purpose computer systems is limited by the O (L3) computational complexity and by complicated data dependences. Furthermore, large scale parallel computers are too expensive to be easily accessible to many research institutes. Recently, FPGA chips have emerged as one promising application accelerator to accelerate the CYK algorithm by exploiting a fine-grained custom design. We propose a systolic-like array structure including one master PE and multiple slave PEs for the fine-grained hardware implementation on FPGA to accelerate the CYK/inside algorithm with Query-Dependent Banding (QDB) heuristics. We partition the tasks by columns and assign them to PEs for load balance. We exploit data reuse schemes to reduce the need to load matrices from external memory. The experimental results show a speedup factor of more than 14x over the Infernal-1.0 with QDB optimization for the alignment of a single long RNA sequence to a large CM model with thousands of states running on a PC platform with Intel Dual-core 2.5 GHz CPU. The computational power of our accelerator is comparable to that of a PC cluster consisting of 16 Intel-Xeon 2.0 GHz Quad CPUs for large-scale database alignment applications (cmsearch) with multiple input sequences, but the power consumption is only about 10% of that of the cluster. © 2010 Elsevier B.V.","Bioinformatics; FPGA; Hardware accelerator; Parallel CYK algorithm; RNA; SCFG model; Secondary structure prediction","FPGA; Hardware accelerator; Hardware accelerators; Parallel CYK algorithm; SCFG model; Secondary structure prediction; Algorithms; Alignment; Bioinformatics; Computational complexity; Computer hardware; Forecasting; General purpose computers; Hardware; Program processors; RNA; Search engines; Stochastic models; Structural design",2-s2.0-77955428322
"Xin R.S., Dantressangle P., Lightstone S., McLaren W., Schormann S., Schwenger M.","Meet DB2: Automated database migration evaluation",2010,"Proceedings of the VLDB Endowment",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865568099&partnerID=40&md5=51862ab01492cece3fac20d8a397edd1","Commercial databases compete for market share, which is composed of not only net-new sales to those purchasing a database for the first time, but also competitive ""win-backs"" and migrations. Database migration, or the act of moving both application code and its underlying database platform from one database to another, presents a serious administrative and application development challenge fraught with large manual costs. Migration is typically a high cost effort due to incompatibilities between database platforms. Incompatibilities are caused most often by product specific extensions to language support, procedural logic, DDL, and administrative interfaces. The migration evaluation is the first step in any competitive database migration process. Historically this has been a manual process, with the high costs and subjective results. This has led us to reexamine traditional practices and explore an automatic, innovative solution. We have designed and implemented the Migration Evaluation and Enablement Tool for DB2 for Linux Unix and Windows, or MEET DB2, a tool for automatically evaluating database migration projects. Encapsulated in a simple one-click interface, MEET DB2 is able to provide detailed evaluation of migration complexity based on its deep analysis on the source database. In this paper, we present MEET DB2, and discuss many aspects of our design, and report measurements from real-world use cases. In particular, we show a novel way to use XML and XQuery in this domain for better extensibility and interoperability. We have evaluated MEET DB2 on 18 source code samples, covering nearly 1 million lines of code. The utility has provided benefits in several dimensions including: dramatically reduced time for evaluation, consistency, improved accuracy over human analysis, improved reporting, reduced skill requirements for migration analysis, and clear analytics for product planning. © 2010 VLDB Endowment.",,"Application codes; Application development; Database platforms; High costs; Human analysis; Innovative solutions; Lines of code; Manual process; Market share; Migration complexity; Migration process; Product planning; Skill requirements; Source codes; Competition; Computer operating systems; Costs; Database systems",2-s2.0-84865568099
"Fine G.A., Ellis B.","The Global Grapevine: Why Rumors of Terrorism, Immigration, and Trade Matter",2010,"The Global Grapevine: Why Rumors of Terrorism, Immigration, and Trade Matter",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958037074&doi=10.1093%2facprof%3aoso%2f9780199736317.001.0001&partnerID=40&md5=7284b02e58a77a4c84ae053b7ec94c73","Now that increased internationalism has challenged the traditional worldviews of many Americans, concerns and fears abound concerning the potential danger posed by contact with foreigners. During the period when rapid change occurs, this new relationship with the rest of the world is initially explored through rumors and legends. Some of these stories are fantastic; many of them are inaccurate; but all of them reflect Americans' first hesitant steps to understand their new place on the globe. This book calls for a close and fair reading of several cycles of rumors on their own terms: as a culture's first efforts to express difficult and painful opinions about the transformation it feels itself undergoing. This book surveys the ways in which the impact of Islamist terrorism and increased Latino immigration have been seen through a filter of stereotype and conspiracy theory. It also presents ways in which tourism and the dangers of international trade also expose Americans' attitudes toward foreigners. Finally, it shows how Americans, in turn, are the targets of similar rumors abroad, as illustrated by widespread claims of organ trafficking. Rumors can't simply be dismissed as trivial or ignorant, the book concludes, but as our best source of what Americans define as the real practical issues facing the nation as it enters a world increasingly made smaller by trade and communication. © 2010 by Oxford University Press, Inc. All rights reserved.","Conspiracy; Immigration; International trade; Internationalism; Legend; Rumor; Stereotype; Terrorism; Tourism",,2-s2.0-79958037074
"Güler F.M., Birturk A.","Natural intelligence - Commonsense question answering with conceptual graphs",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955860248&doi=10.1007%2f978-3-642-14197-3_12&partnerID=40&md5=ed060898297891be278cf7aaa31bd41d","Natural Intelligence (NI) is a question answering system based on Combinatory Categorial Grammar (CCG) as the theory of grammar and Conceptual Graphs (CG) for knowledge representation and reasoning. CCG is a lexicalized theory of grammar and very suitable for semantic analysis. Conceptual Graphs is a special kind of semantic network which can express full first-order logic. It aims to address the problem of commonsense reasoning in question answering, by using the state of the art tools such as C&C tools, Cogitant and Open Cyc. C&C tools are used for parsing natural language, Cogitant is used for Conceptual Graph operations, and Open Cyc is used for upper ontology and commonsense handling. © 2010 Springer-Verlag Berlin Heidelberg.","Combinatory Categorial Grammar; Commonsense Reasoning; Conceptual Graphs; Open Cyc","Combinatory categorial grammar; Commonsense reasoning; Conceptual graph; First order logic; Knowledge representation and reasoning; Natural intelligence; Natural languages; Open Cyc; Question Answering; Question answering systems; Semantic analysis; Semantic network; State of the art; Upper ontology; Graphic methods; Knowledge representation; Ontology; Query languages; Semantics; Multi agent systems",2-s2.0-77955860248
"Körner S.J., Landhäußer M.","Semantic enriching of natural language texts with automatic thematic role annotation",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955461025&doi=10.1007%2f978-3-642-13881-2_9&partnerID=40&md5=9351b8b7b57b5e2004267f5d4d106cd9","This paper proposes an approach which utilizes natural language processing (NLP) and ontology knowledge to automatically denote the implicit semantics of textual requirements. Requirements documents include the syntax of natural language but not the semantics. Semantics are usually interpreted by the human user. In earlier work Gelhausen and Tichy showed that SalEMX automatically creates UML domain models from (semantically) annotated textual specifications [1]. This manual annotation process is very time consuming and can only be carried out by annotation experts. We automate semantic annotation so that SalEMX can be completely automated. With our approach, the analyst receives the domain model of a requirements specification in a very fast and easy manner. Using these concepts is the first step into farther automation of requirements engineering and software development. © 2010 Springer-Verlag.",,"Domain model; Human users; Implicit semantics; Manual annotation; Natural language processing; Natural language text; Natural languages; Requirements document; Requirements specifications; Semantic annotations; Software development; Computational linguistics; Concentration (process); Information systems; Ontology; Query languages; Semantics; Software design; Specifications; Natural language processing systems",2-s2.0-77955461025
"Chang H.","Conceptual modeling of online entertainment programming guide for natural language interface",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955450357&doi=10.1007%2f978-3-642-13881-2_19&partnerID=40&md5=e6e52b1873b1c248b0e5980838648609","This paper describes a new novel approach to the conceptual modeling of text-based electronic programming guide (EPG) for broadcast TV programs by using a large text corpus constructed from the EPG metadata source. Two empirical experiments are carried out to evaluate the EPG-specific language models created using the new algorithm in context of natural language (NL) based information retrieval systems. The experimental results show the effectiveness of the algorithm for developing low-complexity concept models with high coverage for the user's language models associated with both typed and spoken queries when interacting with a NL based EPG search interface. © 2010 Springer-Verlag.","electronic programming guide; linguistic properties of entertainment language; metadata harvesting; Natural language modeling","Broadcast TV; Concept model; Conceptual modeling; Language model; Linguistic properties; Low-complexity; Metadata harvesting; Natural language interfaces; Natural languages; Online entertainment; Search interfaces; Specific languages; Text corpora; Computational linguistics; Computer programming; Harvesting; Information retrieval; Information retrieval systems; Information systems; Metadata; Natural language processing systems",2-s2.0-77955450357
"Kuznetsov V., Riley D., Afaq A., Sekhri V., Guo Y., Lueking L.","The CMS DBS query language",2010,"Journal of Physics: Conference Series",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955378047&doi=10.1088%2f1742-6596%2f219%2f4%2f042043&partnerID=40&md5=d72e6b9df7ec8fc77645be2b79cdc6ce","The CMS experiment has implemented a flexible and powerful system enabling users to find data within the CMS physics data catalog. The Dataset Bookkeeping Service (DBS) comprises a database and the services used to store and access metadata related to CMS physics data. To this, we have added a generalized query system in addition to the existing web and programmatic interfaces to the DBS. This query system is based on a query language that hides the complexity of the underlying database structure by discovering the join conditions between database tables. This provides a way of querying the system that is simple and straightforward for CMS data managers and physicists to use without requiring knowledge of the database tables or keys. The DBS Query Language uses the ANTLR tool to build the input query parser and tokenizer, followed by a query builder that uses a graph representation of the DBS schema to construct the SQL query sent to underlying database. We will describe the design of the query system, provide details of the language components and overview of how this component fits into the overall data discovery system architecture. © 2010 IOP Publishing Ltd.",,"Data discovery; Data sets; Database structures; Database tables; Graph representation; Language component; Physics data; Powerful systems; Query systems; SQL query; Tokenizer; High energy physics; Linguistics; Metadata; Nuclear physics; Query languages",2-s2.0-77955378047
"Wu A., Paquet J., Mokhov S.A.","Object-oriented intensional programming: Intensional Java/Lucid classes",2010,"8th ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2010",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955323717&doi=10.1109%2fSERA.2010.29&partnerID=40&md5=cd2c02148689503ece6271de5d29873f","This article introduces Object-Oriented Intensional Programming (OO-IP), a new hybrid language between Object-Oriented and Intensional Programming Languages in the sense of the latest evolutions of Lucid. This new hybrid language - called JOOIP for Java Object Oriented Intensional Programming-combines the essential characteristics of Lucid and Java, and introduces the notion of object streams which makes it is possible that each element in a Lucid stream to be an object with embedded intensional properties. Interestingly, this hybrid language also brings to Java objects the power to explicitly express and manipulate the notion of context, creating the novel concept of intensional object, i.e. objects whose evaluation is context-dependent, which are demonstrated to be translatable into standard objects. © 2010 IEEE.","General intensional programming system (GIPSY); Hybrid programming languages; Intensional programming; Java; Object-oriented programming","Context dependent; Essential characteristic; General intensional programming systems; Hybrid languages; Hybrid programming; Intensional programming; Intensional programming languages; Java objects; Novel concept; Object oriented; Object streams; Computer hardware description languages; Computer software; Engineering research; Java programming language; Linguistics; Query languages; Object oriented programming",2-s2.0-77955323717
"Tomai E., Forbus K.","Using narrative functions as a heuristic for relevance in story understanding",2010,"Proceedings of the Intelligent Narrative Technologies III Workshop, INT3 '10",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955116749&doi=10.1145%2f1822309.1822318&partnerID=40&md5=13ef2dd86d4b10d43cfb326e7ecaf41a","Story understanding requires a degree of knowledge and expressiveness beyond the current state of natural language understanding. We present an approach that addresses these needs, using a large-scale knowledge base, simplified English grammar and a combination of compositional frame semantics and abductive reasoning. This in turn raises a significant challenge disambiguating complex semantic structures, which requires a pragmatics of narrative for constraint and guidance. We present a theory of narrative functions that serve as a heuristic for relevance in narrative, and provide evidence that this heuristic is effective for disambiguation that leads to consistent understanding. © 2010 ACM.","Abductive reasoning; Knowledge representation; Narrative; Natural language understanding; Semantics; Story","Abductive reasoning; Complex semantic structures; Knowledge base; Narrative; Natural language understanding; Story understanding; Knowledge based systems; Linguistics; Query languages; Semantics; Speech recognition; Knowledge representation",2-s2.0-77955116749
"Bamman D., Babeu A., Crane G.","Transferring structural markup across translations using multilingual alignment and projection",2010,"Proceedings of the ACM International Conference on Digital Libraries",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955122218&doi=10.1145%2f1816123.1816126&partnerID=40&md5=3c3f1a03c3e195cee0cb703d92db4d28","We present here a method for automatically projecting structural information across translations, including canonical citation structure (such as chapters and sections), speaker information, quotations, markup for people and places, and any other element in TEI-compliant XML that delimits spans of text that are linguistically symmetrical in two languages. We evaluate this technique on two datasets, one containing perfectly transcribed texts and one containing error-ful OCR, and achieve an accuracy rate of 88.2% projecting 13,023 XML tags from source documents to their transcribed translations, with an 83.6% accuracy rate when projecting to texts containing uncorrected OCR. This approach has the potential to allow a highly granular multilingual digital library to be bootstrapped by applying the knowledge contained in a small, heavily curated collection to a much larger but unstructured one. © 2010 ACM.","Annotation projection; Knowledge transfer; Multilingual alignment","Accuracy rate; Annotation projection; Data sets; Knowledge transfer; Structural information; XML tags; Alignment; Data processing; Knowledge management; Linguistics; Markup languages; Transcription; Translation (languages); XML; Digital libraries",2-s2.0-77955122218
"Aiordǎchioaie A., Baumann P.","PetaScope: An open-source implementation of the OGC WCS Geo service standards suite",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955042674&doi=10.1007%2f978-3-642-13818-8_13&partnerID=40&md5=3159d676d15603f8c33cb45736cef7ce","A growing number of scientific data archives set up Web interfaces for researchers and sometimes for the general public. While these services often deploy sophisticated search facilities, these are constrained to metadata level where conventional SQL/XML technology can be leveraged; no comparable retrieval support is available on original observation or simulation data. For raster data in the earth sciences, this is overcome by the 2008 Open GeoSpatial Consortium (OGC) Web Coverage Processing Service (WCPS) standard. It defines a retrieval language on multi-dimensional raster data for ad-hoc navigation, extraction, aggregation, and analysis. WCPS is part of the Web Coverage Service (WCS) suite which additionally contains a simple retrieval service and a upload and update service for raster data. In this contribution we present PetaScope, an open-source implementation of the complete WCS suite. Most of the suite's functionality is available already, and a first code version has been released. An online showcase is being built, and the system will soon undergo real-life evaluation on mass data. After briefly introducing the WCPS language concept we discuss the architecture of the service stack based on examples publicly available on the demo website. © 2010 Springer-Verlag Berlin Heidelberg.",,"First code; General publics; Life evaluation; Mass data; Open geospatial consortium; Open source implementation; Raster data; Retrieval languages; Scientific data; Search facilities; Service stack; Service standards; Simulation data; Web interface; Management information systems; Metadata; Linguistics",2-s2.0-77955042674
"Glaab E., Garibaldi J.M., Krasnogor N.","Vrmlgen: An R package for 3D data visualization on the web",2010,"Journal of Statistical Software",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958120648&partnerID=40&md5=e0a584092ddcbd1bc8d875a878027251","The 3-dimensional representation and inspection of complex data is a frequently used strategy in many data analysis domains. Existing data mining software often lacks functionality that would enable users to explore 3D data interactively, especially if one wishes to make dynamic graphical representations directly viewable on the web. In this paper we present vrmlgen, a software package for the statistical programming language R to create 3D data visualizations in web formats like the Virtual Reality Markup Language (VRML) and LiveGraphics3D. vrmlgen can be used to generate 3D charts and bar plots, scatter plots with density estimation contour surfaces, and visualizations of height maps, 3D object models and parametric functions. For greater flexibility, the user can also access low-level plotting methods through a unified interface and freely group different function calls together to create new higher-level plotting methods. Additionally, we present a web tool allowing users to visualize 3D data online and test some of vrmlgen's features without the need to install any software on their computer.","3D; Chart; Graph; Plot; Visualization; VRML",,2-s2.0-77958120648
"Xi Q., Walker D.","A context-free markup language for semi-structured text",2010,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954730878&doi=10.1145%2f1806596.1806622&partnerID=40&md5=cae1c37f61a3e2fa1133973a8800ad5d","An ad hoc data format is any nonstandard, semi-structured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system. © 2010 ACM.","ad hoc data; ANNE; domain-specific languages; PADS; tool generation","Context-free; Core elements; Data format; Data source; Domain specific languages; Grammar extraction; Parse trees; Relevance logic; Semantic theories; Semi-structured data formats; Semi-structured text; Syntactic structure; System behaviors; System use; Tabular data; Text data; Tool generation; Unannotated texts; User annotations; C (programming language); Data processing; Linguistics; Markup languages; Metadata; Problem oriented languages; Query languages; Semantics; Trees (mathematics); Wavelet transforms; XML; Context free languages",2-s2.0-77954730878
"Schaefer C.A., Pankratius V., Tichy W.F.","Engineering parallel applications with tunable architectures",2010,"Proceedings - International Conference on Software Engineering",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954722310&doi=10.1145%2f1806799.1806859&partnerID=40&md5=5fa9d164c4e6f585359783135cc231ce","Current multicore computers differ in many hardware characteristics. Software developers thus hand-tune their parallel programs for a specific platform to achieve the best performance; this is tedious and leads to non-portable code. Although the software architecture also requires adaptation to achieve best performance, it is rarely modified because of the additional implementation effort. The Tunable Architectures approach proposed in this paper automates the architecture adaptation of parallel programs and uses an auto-tuner to find the best-performing software architecture for a particular machine. We introduce a new architecture description language based on parallel patterns and a framework to express architecture variants in a generic way. Several case studies demonstrate significant performance improvements due to architecture tuning and show the applicability of our approach to industrial applications. Software developers are exposed to less parallel programming complexity, thus making the approach attractive for experts as well as inexperienced parallel programmers. © 2010 ACM.","D.1.3 [Programming Techniques]: Concurrent Programming - Parallel programming; D.2.11 [Software Engineering]: Software Architectures - Patterns; Design; Languages; Performance","Architecture adaptation; Architecture description languages; Architecture tuning; Concurrent programming-parallel programming; D.2.11 [Software Engineering]: Software Architectures - Patterns; Design languages; Express architecture; Hardware characteristics; Multi core; Parallel application; Parallel patterns; Parallel program; Performance improvements; Portable code; Programming technique; Software developer; Computer software; Engineering; Industrial applications; Linguistics; Parallel architectures; Parallel programming; Query languages; Software architecture",2-s2.0-77954722310
"Sabic A., El-Zayat M.","Building E-University recommendation system",2010,"ICIME 2010 - 2010 2nd IEEE International Conference on Information Management and Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954675330&doi=10.1109%2fICIME.2010.5477605&partnerID=40&md5=59508ec6fcbc912c4370e92fa77491f4","Recommendation System (RS) helps users (learners, educators etc.) choose quality and important content and activities in E-University Environment. This paper briefly analyses related work in application of recommendation models in E-Learning environments, enhances existing recommendation model and suggests possible usage in EUniversity System (EUS). Our RS is based upon main concepts of FlexRecs recommendation engine, using multiple data sources and working with multiple subsystems of EUS. The architecture of RS consists of four main parts: Workflow Manager, Query Parser, Recommendation Plan Generator and Recommendation Generator. For the presentation of recommendation results, intuitive graphical user interface is designed, and also a Suggestion Preference Module (SPM), which provides users the ability to define specific parameters for the current used workflow. © 2010 IEEE.","E-University system; Hybrid and flexible recommendations; Suggestion Preference Module","E-learning environment; Multiple data sources; Multiple subsystems; Recommendation engine; Recommendation systems; University environment; University system; Graphical user interfaces; Rapid solidification; Information management",2-s2.0-77954675330
"Kop C.","Can queries help to validate database design?",2010,"2nd International Conference on Advances in Databases, Knowledge, and Data Applications, DBKDA 2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954527712&doi=10.1109%2fDBKDA.2010.24&partnerID=40&md5=8892b2ecbf4dd67fd74928090535fcd1","The design of a conceptual database schema is a critical task. The more methods a conceptual database designer has in order to communicate with the end user, the better it is for the quality of the conceptual schema. This paper focuses on the question: Can queries be used for checking missing concepts in a conceptual database schema? The usefulness of queries for schema checking will be presented in this paper. © 2010 IEEE.","Conceptual models; Evaluation; Queries","Conceptual model; Conceptual schemas; Critical tasks; Database design; Database schemas; End users; Database systems; Design; Human computer interaction; Multi agent systems",2-s2.0-77954527712
"Hajjar M., Al Hajjar A.E.S., Zreik K., Gallinari P.","An improved structured and progressive electronic dictionary for the Arabic language: iSPEDAL",2010,"5th International Conference on Internet and Web Applications and Services, ICIW 2010",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954481685&doi=10.1109%2fICIW.2010.80&partnerID=40&md5=6eba97de31dc353ed03ba76c392eb022","In this article, we propose an improved structured and progressive electronic dictionary for the Arabic language (iSPEDAL) which can be presented in the form of a relational database or in the form of an XML document which can be easily exploitable using suitable query languages. Indeed, many Arabic dictionaries are found but are not structured and not directly exploitable since they are in flat textual files form. iSPEDAL doesn't contain any duplicated data (roots, prefixes, suffixes, the infixes, the patterns and the derived words). Moreover, for a given word, it provides links to its root, to their associated affixes, and to its patterns. iSPEDAL is supplied automatically from one or several traditional textual dictionaries and is enriched permanently with any Arabic textual corpus using system that we built. This system is composed of a Parser, a Selector, a Classifier, an Extractor, a Comparator, an Analyzer, and a Validator. The Parser allows the transformation of a textual source (dictionary or textual corpus) into a set of words. The Selector determines if a word is new or already exists in iSPEDAL. The Classifier allows to classify a given word and to add it to iSPEDAL as a root or as a derived word. The Extractor uses the Arabic extraction method to deduce the root of all words arriving to this component without their root or any indication about their root. The Comparator permits to avoid duplication of roots, affixes or patterns in iSPEDAL. The Analyzer allows the extraction of the affixes and the pattern from a derived word and of its root. The Validator can validate the information (word, root, patterns, and affixes) before adding to iSPEDAL database. This dictionary can be used to evaluate the information extraction methods from an Arabic document, given that; the vocabulary of the Arabic language is essentially built from the roots. © 2010 IEEE.","Arabic language; Corpus; Dictionary; Information extraction; Root","Arabic document; Arabic languages; Electronic dictionaries; Extraction method; Information Extraction; Information extraction methods; Relational Database; Classifiers; Comparators (optical); Information analysis; Internet; Linguistics; World Wide Web; Query languages",2-s2.0-77954481685
"Choubey N.S., Kharat M.U.","PDA simulator for CFG induction using genetic algorithm",2010,"UKSim2010 - UKSim 12th International Conference on Computer Modelling and Simulation",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954501925&doi=10.1109%2fUKSIM.2010.25&partnerID=40&md5=c79ecb951ad971166cddc6f0a7032f4b","Grammar Induction (or Grammar Inference or Language Learning) is the process of learning of a grammar from training data of the positive and negative strings of the language. Genetic algorithms are amongst the techniques which provide successful result for the grammar induction. The paper describes a Pushdown Automata (PDA) simulator used to parse the training data with the grammar induced by the Genetic Algorithm process. The grammar is induced by using an extended approach of stochastic mutation scheme based on Adaptive Genetic. The algorithm produces successive generations of individuals, computing their ""fitness value"" at each step and selecting the best of them when the termination condition is reached. The paper deals with the issues in implementation of the algorithm, chromosome representation and evaluation, selection and replacement strategy, and the genetic operators for crossover and mutation. The model has been implemented, and the results obtained for the set of four languages are presented. © 2010 IEEE.","Automata; Context free grammar; Evolutionary computation; Genetic algorithm; Grammar induction; Simulator","Crossover and mutation; Evolutionary computations; Fitness values; Genetic operators; Grammar induction; Grammar inference; Language learning; Pushdown automata; Replacement strategy; Termination condition; Training data; Adaptive algorithms; Computation theory; Context free grammars; Genetic algorithms; Linguistics; Mathematical operators; Query languages; Robots; Simulators; Translation (languages); Formal languages",2-s2.0-77954501925
"Klyuev V., Oleshchuk V.","A novel approach to improve the accuracy of web retrieval",2010,"2010 5th International Conference on Future Information Technology, FutureTech 2010 - Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954421544&doi=10.1109%2fFUTURETECH.2010.5482671&partnerID=40&md5=5dc1d1da0206ecb77051a708fe8ad331","General purpose search engines utilize a very simple view on text documents: They consider them as bags of words. It results that after indexing, the semantics of documents is lost. In this paper, we introduce a novel approach to improve the accuracy of Web retrieval. We utilize the WordNet and WordNet SenseRelate All Words Software as main tools to preserve the semantics of the sentences of documents and user queries. Nouns and verbs in the WordNet are organized in the tree hierarchies. The word meanings are presented by numbers that reference to the nodes on the semantic tree. The meaning of each word in the sentence is calculated when the sentence is analyzed. The goal is to put each noun and verb of the sentence on the right place on the tree. Taking this information into account, it is possible to solve the ambiguity problem for the query keywords and create the indicative summaries taking into account query words, and semantically related hypernyms and synonyms. ©2010 IEEE.","Disambiguation; Semantic search; WordNet","General-purpose search engines; Hypernyms; Nouns and verbs; Query words; Semantic search; Semantic tree; Text document; User query; Web retrieval; Word meaning; Wordnet; Information technology; Ontology; Search engines; Semantic Web; Semantics",2-s2.0-77954421544
"Natarajan K., Stein D., Jain S., Elhadad N.","An analysis of clinical queries in an electronic health record search utility",2010,"International Journal of Medical Informatics",44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952892152&doi=10.1016%2fj.ijmedinf.2010.03.004&partnerID=40&md5=502c09c1f69ded7a350be9e268dfdda0","Purpose: While search engines have become nearly ubiquitous on the Web, electronic health records (EHRs) generally lack search functionality; furthermore, there is no knowledge on how and what healthcare providers search while using an EHR-based search utility. In this study, we sought to understand user needs as captured by their search queries. Methods: This post-implementation study analyzed user search log files for 6 months from an EHR-based, free-text search utility at our large academic institution. The search logs were de-identified and then analyzed in two steps. First, two investigators classified all the unique queries as navigational, transactional, or informational searches. Second, three physician reviewers categorized a random sample of 357 informational searches into high-level semantic types derived from the Unified Medical Language System (UMLS). The reviewers were given overlapping data sets, such that two physicians reviewed each query. Results: We analyzed 2207 queries performed by 436 unique users over a 6-month period. Of the 2207 queries, 980 were unique queries. Users of the search utility included clinicians, researchers and administrative staff. Across the whole user population, approximately 14.5% of the user searches were navigational searches and 85.1% were informational. Within informational searches, we found that users predominantly searched for laboratory results and specific diseases. Conclusions: A variety of user types, ranging from clinicians to administrative staff, took advantage of the EHR-based search utility. Though these users' search behavior differed, they predominantly performed informational searches related to laboratory results and specific diseases. Additionally, a number of queries were part of words, implying the need for a free-text module to be included in any future concept-based search algorithm. © 2010 Elsevier Ireland Ltd. All rights reserved.","Computerized; Information storage and retrieval; Medical informatics; Medical informatics applications; Medical record systems","Academic institutions; Administrative staff; Concept based search; Electronic health record; Free-text search; Health care providers; High level semantics; Information storage and retrieval; Log file; Medical informatics; Medical informatics applications; Medical record systems; Overlapping data; Post-implementation; Random sample; Search behavior; Search functionality; Search logs; Search queries; Unified medical language systems; User need; User type; High level languages; Learning algorithms; Navigation; Records management; Search engines; Information science; article; clinical decision making; electronic medical record; information retrieval; information storage; Internet; medical informatics; medical information system; online system; patient information; priority journal; search engine; data mining; electronic medical record; United States; utilization; Data Mining; Medical Records Systems, Computerized; New York; Search Engine; Data Mining; Medical Records Systems, Computerized; New York; Search Engine",2-s2.0-77952892152
"Garcia M., Izmaylova A., Schupp S.","Extending scala with database query capability",2010,"Journal of Object Technology",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956309887&partnerID=40&md5=20a17cdac99d9ba89757ced348bb354e","The integration of database and programming languages is difficult due to the different data models and type systems prevalent in each field. We present a solution where the developer may express queries encompassing program and database data. The notation used for queries is based on comprehensions, a declarative style that does not impose any specific execution strategy. In our approach, the type safety of languageintegrated queries is analyzed at compile-time, followed by a translation that optimizes for database evaluation. We show the translation total and semantics preserving, and introduce a language-independent classification. According to this classification, our approach compares favorably with Microsoft's LINQ, today's best-known representative. We provide an implementation in terms of Scala compiler plugins, accepting two notations for queries: LINQ and the native Scala syntax for comprehensions. The prototype relies on Ferry, a query language that already supports comprehensions yet targets SQL:1999. The reported techniques pave the way for further progress in bridging the programming and the database worlds. ©JOT 2010.","Compiler plugins; Language-integrated query; Scala programming language; Typedirected program transformation",,2-s2.0-77956309887
"Bravenboer M., Dolstra E., Visser E.","Preventing injection attacks with syntax embeddings",2010,"Science of Computer Programming",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950862952&doi=10.1016%2fj.scico.2009.05.004&partnerID=40&md5=9d085a97d0bc8825ce1b2786279f1ffb","Software written in one language often needs to construct sentences in another language, such as SQL queries, XML output, or shell command invocations. This is almost always done using unhygienic string manipulation, the concatenation of constants and client-supplied strings. A client can then supply specially crafted input that causes the constructed sentence to be interpreted in an unintended way, leading to an injection attack. We describe a more natural style of programming that yields code that is impervious to injections by construction. Our approach embeds the grammars of the guest languages (e.g. SQL) into that of the host language (e.g. Java) and automatically generates code that maps the embedded language to constructs in the host language that reconstruct the embedded sentences, adding escaping functions where appropriate. This approach is generic, meaning that it can be applied with relative ease to any combination of context-free host and guest languages. © 2009 Elsevier B.V. All rights reserved.","Concrete object syntax; Injection attacks; Program generation; Program transformation; Security; Syntax embedding","Concrete object syntax; Context-free; Embedded Languages; Embeddings; Program generation; Program transformations; Shell command; SQL query; Automatic programming; Context free languages; Linguistics; Network security; Query languages; Syntactics; Java programming language",2-s2.0-77950862952
"Culpepper R., Felleisen M.","Debugging hygienic macros",2010,"Science of Computer Programming",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950857765&doi=10.1016%2fj.scico.2009.06.001&partnerID=40&md5=7c93f0d71a40c35f409eded01a67829f","Over the past two decades, Scheme macros have evolved into a powerful API for the compiler front end. Like Lisp macros, their predecessors, Scheme macros expand source programs into a small core language; unlike Lisp systems, Scheme macro expanders preserve lexical scoping, and advanced Scheme macro systems handle other important properties such as source location. Using such macros, Scheme programmers now routinely develop the ultimate abstraction: embedded domain-specific programming languages. Unfortunately, a typical Scheme programming environment provides little support for macro development. This lack makes it difficult for programmers to debug their macros and for novices to study the behavior of macros. In response, we have developed a stepping debugger specialized to the concerns of macro expansion. This debugger presents the macro expansion process as a linear rewriting sequence of annotated terms; it graphically illustrates the binding structure of the program as expansion reveals it; and it adapts to the programmer's level of abstraction, hiding details of syntactic forms that the programmer considers built-in. © 2009 Elsevier B.V. All rights reserved.","Debugging; Macros; Scheme; Syntactic abstraction","Annotated terms; Debuggers; Embedded domains; Expansion process; Front end; Level of abstraction; Lexical scoping; Macro systems; Macro-developments; Programming environment; Source location; Syntactic abstraction; Abstracting; Application programming interfaces (API); Computer systems programming; Linguistics; LISP (programming language); Program compilers; Program debugging; Query languages; Syntactics; Macros",2-s2.0-77950857765
"Clark A., Fox C., Lappin S.","The Handbook of Computational Linguistics and Natural Language Processing",2010,"The Handbook of Computational Linguistics and Natural Language Processing",47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862231955&doi=10.1002%2f9781444324044&partnerID=40&md5=cbbc98265a19a89621a0762fb3062b02","This comprehensive reference work provides an overview of the concepts, methodologies, and applications in computational linguistics and natural language processing (NLP). Features contributions by the top researchers in the field, reflecting the work that is driving the discipline forward. Includes an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has produced. Presents the major developments in an accessible way, explaining the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologies. Serves as an invaluable state-of-the-art reference source for computational linguists and software engineers developing NLP applications in industrial research and development labs of software companies. © 2010 Blackwell Publishing Ltd.",,,2-s2.0-84862231955
"Crespi Reghizzi S., Mandrioli D.","Operator precedence and the visibly pushdown property",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953732590&doi=10.1007%2f978-3-642-13089-2_18&partnerID=40&md5=3b5fc751e6b24bc55c8629264e351cd8","Operator precedence languages, designated as Floyd's Languages (FL) to honor their inventor, are a classical deterministic context-free family. FLs are known to be a boolean family, and have been recently shown to strictly include the Visibly Pushdown Languages (VPDL); the latter are FLs characterized by operator precedence relations determined by the alphabet partition. In this paper we give the non-obvious proves that FLs have the same closure properties that motivated the introduction of VPDLs, namely under reversal, concatenation and Kleene's star. Thus, rather surprisingly, the historical FL family turns out to be the largest known deterministic context-free family that includes the VPDL and has the same closure properties needed for applications to model checking and for defining mark-up languages such as HTML. As a corollary, an extended regular expression of precedence-compatible FLs is a FL and a deterministic parser for it can be algorithmically obtained. © 2010 Springer-Verlag Berlin Heidelberg.",,"Closure property; Context-free; Extended regular expressions; Precedence relations; Visibly pushdown languages; Automata theory; Linguistics; Model checking; Query languages; Translation (languages); Context free languages",2-s2.0-77953732590
"Yamamoto E., Taura T., Ohashi S., Yamamoto M.","Thesaurus for natural-language-based conceptual design",2010,"Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference 2009, DETC2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953797244&doi=10.1115%2fDETC2009-86943&partnerID=40&md5=45519cd018bb2c24bf17cbe5f01fa823","Conceptual design is a process wherein new functions are created through engineering design. In conceptual design, we use natural language since it plays an important role in the expression and operation of a function. Moreover, natural language is used in our day-to-day thinking processes and is expected to keep a fine interface with the designer. However, it is at a disadvantage with regard to the expression of a function, since physical phenomena, which are the essence of a function, are better expressed in the form of mathematical equations than natural languages. In this study, we attempt to develop a method for using natural language for operating a function by harnessing its advantages and overcoming its disadvantage. We focus on the vital process in conceptual design, that is, the function dividing process wherein the required function is decomposed into sub functions that satisfy the required function. We construct a thesaurus by semiautomatic extraction of the hierarchical structures of words from a document by using natural language processing. We show that the constructed thesaurus can be useful in supporting the function dividing process. Copyright © 2009 by ASME.",,"Engineering design; Hierarchical structures; Mathematical equations; NAtural language processing; Natural languages; Physical phenomena; Thinking process; Computational linguistics; Conceptual design; Engineering; Multi agent systems; Natural language processing systems; Query languages; Thesauri; Functions",2-s2.0-77953797244
"Mithun S.","Exploiting rhetorical relations in blog summarization",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953759985&doi=10.1007%2f978-3-642-13059-5_53&partnerID=40&md5=75943e739c04397a3053b77c95e4e9eb","With the goal of developing an efficient query-based opinion summarization approach, we have targeted to resolve Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have utilized rhetorical relations of texts with the help of text schema and Rhetorical Structure Theory (RST). © 2010 Springer-Verlag Berlin Heidelberg.",,"Incoherency; Rhetorical relations; Rhetorical structure theory; Text processing; Artificial intelligence",2-s2.0-77953759985
"Zhang S.-W., Li Y.-J., Xia L., Pan Q.","PPLook: An automated data mining tool for protein-protein interaction",2010,"BMC Bioinformatics",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954850017&doi=10.1186%2f1471-2105-11-326&partnerID=40&md5=d57e62330f14429864156d40859b6e9d","Background: Extracting and visualizing of protein-protein interaction (PPI) from text literatures are a meaningful topic in protein science. It assists the identification of interactions among proteins. There is a lack of tools to extract PPI, visualize and classify the results.Results: We developed a PPI search system, termed PPLook, which automatically extracts and visualizes protein-protein interaction (PPI) from text. Given a query protein name, PPLook can search a dataset for other proteins interacting with it by using a keywords dictionary pattern-matching algorithm, and display the topological parameters, such as the number of nodes, edges, and connected components. The visualization component of PPLook enables us to view the interaction relationship among the proteins in a three-dimensional space based on the OpenGL graphics interface technology. PPLook can also provide the functions of selecting protein semantic class, counting the number of semantic class proteins which interact with query protein, counting the literature number of articles appearing the interaction relationship about the query protein. Moreover, PPLook provides heterogeneous search and a user-friendly graphical interface.Conclusions: PPLook is an effective tool for biologists and biosystem developers who need to access PPI information from the literature. PPLook is freely available for non-commercial users at http://meta.usc.edu/softs/PPLook. © 2010 Zhang et al; licensee BioMed Central Ltd.",,"Automated data mining; Connected component; Heterogeneous searches; Interaction relationship; Pattern-matching algorithm; Protein-protein interactions; Three dimensional space; Topological parameters; Application programming interfaces (API); Semantics; Three dimensional computer graphics; Tools; Proteins; protein; algorithm; article; computer interface; computer program; data mining; Internet; metabolism; methodology; Algorithms; Data Mining; Internet; Proteins; Software; User-Computer Interface",2-s2.0-77954850017
"Nguyen D.T., Pham T.N., Phan Q.T.","A semantic model for building the Vietnamese language query processing framework in e-library searching application",2010,"ICMLC 2010 - The 2nd International Conference on Machine Learning and Computing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953032263&doi=10.1109%2fICMLC.2010.17&partnerID=40&md5=bfb7b5340f2516ae554edc72b954c590","This paper aims to establish a semantic model which is the most important and central component of our Vietnamese Language Query Processing (VLQP) framework. The VLQP framework is architecture of 2-tiers. This framework includes a restricted parser for analyzing Vietnamese query from users based on a class of the predefined syntactic rules and a transformer for transforming syntactic structure of query to its semantic representation. In this framework, the semantic model is an original feature we have addressed. This semantic model contributes to the syntax analysis and representation of Vietnamese query forms involving to application domain. We also propose transforming rules to transform syntactic structures to their semantic representation. © 2010 IEEE.","Document retrieval; Natural language processing; Question answering; Search","Application domains; Central component; Document Retrieval; IS architecture; NAtural language processing; Question Answering; Semantic Model; Semantic representation; Syntactic rules; Syntactic structure; Syntax analysis; Computational linguistics; Information retrieval; Learning algorithms; Learning systems; Natural language processing systems; Query processing; Syntactics; Semantics",2-s2.0-77953032263
"Marcal P.V., Fong J.T., Yamagata N.","Artificial Intelligence (AI) tools for data acquisition and probability risk analysis of nuclear piping failure databases",2010,"American Society of Mechanical Engineers, Pressure Vessels and Piping Division (Publication) PVP",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953014642&doi=10.1115%2fPVP2009-77871&partnerID=40&md5=01b7bfd5c8715ff5116701567d5452cd","Over the last thirty years, much research has been done on the development and application of in-service inspection (ISI) and failure event databases for pressure vessels and piping, as reported in two recent symposia: (1) ASME 2007 PVP Symposium (in honor of the late Dr. Spencer Bush), San Antonio, Texas, on ""Engineering Safety, Applied Mechanics, and Nondestructive Evaluation (NDE)."" (2) ASME 2008 PVP Symposium, Chicago, Illinois, on ""Failure Prevention via Robust Design and Continuous NDE Monitoring."" The two symposia concluded that those databases, if properly documented and maintained on a worldwide basis, could hold the key to the continued safe and profitable operation of numerous aging nuclear power or petro-chemical processing plants. During the 2008 symposium, four uncertainty categories associated with causing uncertainty in fatigue life estimates were identified, namely, (1) Uncertainty-1 in failure event databases, (2) Uncertainty-2 in NDE databases, (3) Uncertainty-3 in material property databases, and (4) Uncertainty-M in crack-growth and damage modeling. In this paper, which is one of a series of four to address all those four uncertainty categories, we introduce an automatic natural language abstracting and processing (ANLAP) tool to address Uncertainty-1. Three examples are presented and discussed.","Aging structures; ANLAP; Artificial intelligence; Computational linguistics; Conceptual dependency; Crack propagation; Database software; Dataplot; Design of experiments; Failure event database; Fatigue; Flaw detection; In-service inspection; Information extraction; Life extension; Material property database; Mathematical modeling; Natural language processing; NDE database; Nondestructive evaluation; Nuclear power plants; Nuclear safety; Petro-chemical plants; Python; Risk-informed analysis; Risk-informed engineering economics; SQL; Statistical data analysis; Uncertainty analysis","Aging structures; ANLAP; Database software; Dataplot; Engineering economics; Failure events; Flaw detection; In-service inspection; Life extension; Life extensions; Material property; Mathematical modeling; NAtural language processing; NDE database; Non destructive evaluation; Nuclear safety; Statistical data analysis; Artificial intelligence; Boilers; Caissons; Chemical detection; Chemical industry; Chemical plants; Chemicals; Computational linguistics; Computer simulation; Computer software; Concrete bridges; Crack detection; Crack propagation; Data flow analysis; Data handling; Database systems; Design of experiments; Failure analysis; Fatigue damage; High level languages; Information analysis; Materials properties; Natural language processing systems; Nuclear energy; Nuclear power plants; Pressure vessels; Probability density function; Profitability; Quality assurance; Risk analysis; Risk assessment; Safety factor; Service vessels; Uncertainty analysis",2-s2.0-77953014642
"Li P., HeGuangyu T., Li B., Wei H.","Interface design of on-line Optimal Power Flow (OPF) based on xml and IEEE format",2010,"Asia-Pacific Power and Energy Engineering Conference, APPEEC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870008970&doi=10.1109%2fAPPEEC.2010.5448917&partnerID=40&md5=aa768ac4dae5dbbe6f94d4671c31d276","This paper presents a new design method for the online OPF interface based on XML (eXtensible Markup Language) and the IEEE common data format. It has potential benefits in the readable, usable and uniform standard. The online application of Shanghai Power System shows the validity and applicability of the proposed design method. ©2010 IEEE.","Ieee common data format; Interface design; On-line optimal power flow (opf); XML","Data format; Design method; Interface designs; New design; On-line applications; On-line optimal power flow (opf); Optimal power flows; Potential benefits; Power systems; XML (extensible markup language); Acoustic generators; Design; Electric load flow; Electric power systems; Markup languages; Optimization; Query languages; XML; Hypertext systems",2-s2.0-84870008970
"Miłkowski M.","Developing an open-source, rule-based proofreading tool",2010,"Software - Practice and Experience",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954130800&doi=10.1002%2fspe.971&partnerID=40&md5=f5708770ae0179b267650aa0e72ab7f2","In this paper, we show how an open-source, language-independent proofreading tool has been built. Many languages lack contextual proofreading tools; for many others, only partial solutions are available. Using existing, largely language-independent tools and collaborative processes it is possible to develop a practical style and grammar checker and to fight the digital divide in countries where commercial linguistic application software is unavailable or too expensive for average users. The described solution depends on relatively easily available language resources and does not require a fully formalized grammar nor a deep parser, yet it can detect many frequent context-dependent spelling mistakes, as well as grammatical, punctuation, usage, and stylistic errors. Copyright © 2010 John Wiley & Sons, Ltd.","Computational linguistics; Controlled vocabulary; Grammar checker; Pattern matching; Proofreading","Collaborative process; Context dependent; Controlled vocabulary; Digital divide; Grammar checker; Grammar checkers; Language resources; Linguistic applications; Open-source; Rule based; Computational linguistics; Economic analysis; Pattern matching; Vocabulary control; Query languages",2-s2.0-77954130800
"Xi Q., Walker D.","A context-free markup language for semi-structured text",2010,"ACM SIGPLAN Notices",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957582338&doi=10.1145%2f1809028.1806622&partnerID=40&md5=7a2c7cb4ae52cff6c6b684f0ae55c5b9","Format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description [19], which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system. Copyright © 2010 ACM.","Ad Hoc Data; ANNE; Domain-specific Languages; PADS; Tool Generation","Ad hoc datum; ANNE; Domain specific languages; PADS; Tool Generation; Context free grammars; Data processing; Markup languages; Metadata; Problem oriented languages; Query languages; Trees (mathematics); XML; Context free languages",2-s2.0-77957582338
"Wu R., Hisada M.","Static and dynamic analysis for web security in industry applications",2010,"International Journal of Electronic Security and Digital Forensics",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954440022&doi=10.1504%2fIJESDF.2010.033782&partnerID=40&md5=57e0de30ca560cd0603241e297183a75","To apply our analysis work in industry security applications, we are investigating semantic metadata and structural syntax analysis. This paper explains how our approaches achieve the goal in terms of static and dynamic analysis by using industry scenarios. To better explain the framework and roadmap, we describe our approaches by using macro and micro views individually. Macro view oversees syntax structure and identification, while micro view envisions metadata messaging and parser automaton. The coherence of macro and micro views forms web security framework in tracking and validation. Our research applies the security service in industry fraud detection. It demonstrates metadata messaging for tracking, and HIPA code generation for validation. This bridges the gap between static and dynamic analysis. This also builds up the foundation of web security governance.","Abstract syntax; Dynamic analysis; Static analysis; Tracking; Vulnerability; Web security","Abstract syntax; Code Generation; Fraud detection; Industry applications; Roadmap; Security application; Security services; Semantic metadata; Static and dynamic analysis; Syntax analysis; Syntax structure; WEB security; Abstracting; Electric load shedding; Industry; Metadata; Security of data; Static analysis; Syntactics; Dynamic analysis",2-s2.0-77954440022
"Verberne S., Boves L., Oostdijk N., Coppen P.-A.","What is not in the bag of words for why-QA?",2010,"Computational Linguistics",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952914178&doi=10.1162%2fcoli.2010.09-032-R1-08-034&partnerID=40&md5=eaada4ee98e41cdcf6fb122d2e2e0723","While developing an approach to why-QA, we extended a passage retrieval system that uses off-the-shelf retrieval technology with a re-ranking step incorporating structural information. We get significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10. The 23% improvement that we reach in terms of MRR is comparable to the improvement reached on different QA tasks by other researchers in the field, although our re-ranking approach is based on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and document structure. © 2010 Association for Computational Linguistics.",,,2-s2.0-77952914178
"Meredith P.O., Jin D., Chen F., Roşu G.","Efficient monitoring of parametric context-free patterns",2010,"Automated Software Engineering",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952094141&doi=10.1007%2fs10515-010-0063-y&partnerID=40&md5=e1675e7f6c1e68ffba180c498d5edf8b","Recent developments in runtime verification and monitoring show that parametric regular and temporal logic specifications can be efficiently monitored against large programs. However, these logics reduce to ordinary finite automata, limiting their expressivity. For example, neither can specify structured properties that refer to the call stack of the program. While context-free grammars (CFGs) are expressive and well-understood, existing techniques for monitoring CFGs generate large runtime overhead in real-life applications. This paper demonstrates that monitoring parametric CFGs is practical (with overhead on the order of 12% or lower in most cases). We present a monitor synthesis algorithm for CFGs based on an LR(1) parsing algorithm, modified to account for good prefix matching. In addition, a logic-independent mechanism is introduced to support matching against the suffixes of execution traces. © 2010 Springer Science+Business Media, LLC.","Context-free languages; Monitoring; Runtime verification","Context-free; Efficient monitoring; Execution trace; Large programs; Parsing algorithm; Prefix matching; Real-life applications; Run-time verification; Runtime overheads; Synthesis algorithms; Temporal logic specifications; Automata theory; Electric reactors; Linguistics; Monitoring; Synthesis (chemical); Temporal logic; Context free languages",2-s2.0-77952094141
"Hunter A., Liu W.","A survey of formalisms for representing and reasoning with scientific knowledge",2010,"Knowledge Engineering Review",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958545535&doi=10.1017%2fS0269888910000019&partnerID=40&md5=ec5ed2c810d2b158adb0a78974353157","With the rapid growth in the quantity and complexity of scientific knowledge available for scientists, and allied professionals, the problems associated with harnessing this knowledge are well recognized. Some of these problems are a result of the uncertainties and inconsistencies that arise in this knowledge. Other problems arise from heterogeneous and informal formats for this knowledge. To address these problems, developments in the application of knowledge representation and reasoning technologies can allow scientific knowledge to be captured in logic-based formalisms. Using such formalisms, we can undertake reasoning with the uncertainty and inconsistency to allow automated techniques to be used for querying and combining of scientific knowledge. Furthermore, by harnessing background knowledge, the querying and combining tasks can be carried out more intelligently. In this paper, we review some of the significant proposals for formalisms for representing and reasoning with scientific knowledge. © 2010 Cambridge University Press.",,"Automated techniques; Background knowledge; Knowledge representation and reasoning; Rapid growth; Scientific knowledge; Automata theory; Knowledge representation",2-s2.0-77958545535
"Litoriya R., Ranjan A.","Implementation of relational algebra interpreter using another query language",2010,"DSDE 2010 - International Conference on Data Storage and Data Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952619067&doi=10.1109%2fDSDE.2010.33&partnerID=40&md5=17a085d9e22bec7b9409f42e05eb38e5","Relational database systems have succeeded commercially because of their straightforwardness and well-built theoretical foundation. This paper ""Implementation of Relational Algebra Interpreter using another query language"" has been designed, implemented and tested in such a way so that queries written in relational algebra can be compiled into SQL and executed on a relational database system. The implementation takes a relational algebra statement as input, does syntactic and lexical parsing on it. In the event of an error in the syntax of the expression it will forward the error to user. If the syntax is correct the relational algebra expression is converted into a SQL statement and executed on an RDBMS. This application can serve as a basis learning Relational Algebra for students as well as for different class of users, as they will be given immediate feedbacks about their queries. © 2010 IEEE.","Interpreter; Parser; Relational algebra; Structured query language","Relational algebra; Structured Query Language; Theoretical foundations; Algebra; Data storage equipment; Linguistics; Query languages; Syntactics; Relational database systems",2-s2.0-77952619067
"Maeda K.","Toward a bottom-up parsing virtual machine",2010,"2010 2nd International Conference on Computer Engineering and Applications, ICCEA 2010",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952607906&doi=10.1109%2fICCEA.2010.83&partnerID=40&md5=bdad790bfd62a43f26434e625f93eb0e","This paper proposes a virtual machine specialized for bottom-up parsing. All existing parser generators are tightly coupled to their target programming languages. In this paper's approach, generated parsers are written in machine code for a virtual machine. They are loosely coupled to any programming languages. If we port a parser from one programming language to another, we do not need to modify the parser generator. All we have to do is to read the specifications of the virtual machine and customize it. The virtual machine is under development, but the proposed approach will have the advantage of platform independence for parser development. © 2010 IEEE.",,"Machine codes; Parser generators; Platform independence; Programming language; Target programming language; Tightly-coupled; Virtual machines; Linguistics; Query languages; Computer applications",2-s2.0-77952607906
"Li H., Shi Y.","A WordNet-based natural language interface to relational databases",2010,"2010 The 2nd International Conference on Computer and Automation Engineering, ICCAE 2010",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952624604&doi=10.1109%2fICCAE.2010.5451893&partnerID=40&md5=1b1576f6cf68252447c00a45bc57b12f","This paper proposes an approach to creating an WordNet-based natural language interface to relational databases which allows end-users to access and query information in database with a natural language. In order to strengthen the reliability and the user-friendliness of the system, our approach integrates WordNet as the base lexicon and ontology as the knowledge base of the semantic interpreter. The user inputs will be first translated into syntax tree by syntax analysis, then it will be translated into an intermediate representation language DRS by the ontology-based semantic interpreter. The SQL generator transfers DRS expressions into SQL sentences by a template technique. The RDMS executes SQL sentences and returns the final result. This paper presents the framework based on ontology techniques to implement a portable NLIDBs that makes it easier to migrate from one domain to another. The prototype system and our experiments show that our approach has generated good results with excellent performance on common queries. ©2010 IEEE.","DRS; NLIDBs; Ontology; OWL; RDMS; SQL; WordNet","End-users; Excellent performance; Intermediate representations; Knowledge base; Natural language interfaces; Natural languages; NLIDBs; Ontology technique; Ontology-based; Prototype system; Query information; Relational Database; Syntax analysis; Syntax tree; Template technique; User input; Wordnet; Database systems; Knowledge based systems; Linguistics; Semantic Web; Semantics; Syntactics; Translation (languages); Ontology",2-s2.0-77952624604
"Ma Z., Su Y., Ma Y.","The study of chinese dictionary mechanism based on the suboptimal search tree",2010,"2010 The 2nd International Conference on Computer and Automation Engineering, ICCAE 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952593888&doi=10.1109%2fICCAE.2010.5451862&partnerID=40&md5=791ad8249cb707293831ea4548cf1886","The speed of dictionary query affects not only the speed of segmentation, but also the wide use of the segmentation system in the mass calculation. According to the different occurrence frequency of words in the text, the dictionary mechanism of the suboptimal search tree is designed so that the comparison times is reduced in the process of segmentation and the speed of segmentation is improved. Finally, the contrast experiment of the largest reverse segmentation algorithm shows that the efficiency of segmentation is improved. ©2010 IEEE.","Binary-seek-by-word; Chinese word segmentation; Dictionary mechanism; Suboptimal search tree","Chinese word segmentation; Contrast experiment; Dictionary mechanism; Search trees; Segmentation algorithms; Segmentation system",2-s2.0-77952593888
"Gao S., Li S., Xu W., Guo J.","Cross-document coreference resolution based on automatic text summary",2010,"3rd International Conference on Knowledge Discovery and Data Mining, WKDD 2010",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952123065&doi=10.1109%2fWKDD.2010.56&partnerID=40&md5=16eb15cc71cbaf0ce21ff81de444296e","Cross-document coreference resolution plays an import part in the filed of natural language processing (NLP). It captures the ability of gathering documents for information about a certain entity. Most previous algorithms identify the underlying entity of a given document depending on the original text, which is unreliable if the original text contains multiple parts of different themes. In this paper, we propose a cross-document coreference resolution algorithm based on automatic text summary instead of the original text. In our approach, we extract query-specific and informative-indicative summary from the original text by using Hobbs algorithm and measure the similarity between two summaries. This automatic text summary-based cross-document coreference resolution (ATSCDCR) system is effective in disambiguating different entities of the same mention name and identifying the same entity of different mention names. The results from our experiments show that the macro average of ATSCDCR system is up to 73.16% and the micro average of ATSCDCR system is 67.34 %. © 2010 IEEE.","Automatic text summary; Cross-document coreference resolution; Hobbs algorithm; Named entity type recognition","Automatic text summary; Coreference resolution; Multiple parts; Named entities; Natural language processing; Algorithms; Character recognition; Computational linguistics; Data flow analysis; Data mining; Natural language processing systems; Security of data",2-s2.0-77952123065
[No author name available],"Software Language Engineering - Second International Conference, SLE 2009, Revised Selected Papers",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951637220&partnerID=40&md5=1adb159e99b238e4e68f92495876443a","The proceedings contain 23 papers. The topics discussed include: language evolution in practice: the history of GMF; a novel approach to semi-automated evolution of DSML model transformation; composing feature models; multi-view composition language for software product line requirements; yet another language extension scheme; model transformation languages relying on models as ADTs; towards dynamic evolution of domain specific languages; scalaQL: language-integrated database queries for scala; integration of data validation and user interface concerns in a DSL for web applications; ontological metamodeling with explicit instantiation; verifiable parse table composition for deterministic parsing; natural and flexible error recovery for generated parsers; graphical template language for transformation synthesis; and a role-based approach towards modular language engineering.",,,2-s2.0-77951637220
"De Jonge M., Nilsson-Nyman E., Kats L.C.L., Visser E.","Natural and flexible error recovery for generated parsers",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951546373&doi=10.1007%2f978-3-642-12107-4_16&partnerID=40&md5=0d9e8a4a72eb9d2c4c70969f24fbca91","Parser generators are an indispensable tool for rapid language development. However, they often fall short of the finesse of a hand-crafted parser, built with the language semantics in mind. One area where generated parsers have provided unsatisfactory results is that of error recovery. Good error recovery is both natural, giving recovery suggestions in line with the intention of the programmer; and flexible, allowing it to be adapted according to language insights and language changes. This paper describes a novel approach to error recovery, taking into account not only the context-free grammar, but also indentation usage. We base our approach on an extension of the SGLR parser that supports fine-grained error recovery rules and can be used to parse complex, composed languages. We take a divide-and-conquer approach to error recovery: using indentation, erroneous regions of code are identified. These regions constrain the search space for applying recovery rules, improving performance and ensuring recovery suggestions local to the error. As a last resort, erroneous regions can be discarded. Our approach also integrates bridge parsing to provide more accurate suggestions for indentation-sensitive language constructs such as scopes. We evaluate our approach by comparison with the JDT Java parser used in Eclipse. © 2010 Springer-Verlag.",,"Divide-and-conquer approach; Error recovery; Improving performance; In-line; Indispensable tools; Language constructs; Language development; Language semantics; Parser generators; Search spaces; Computer software; Linguistics; Query languages; Recovery",2-s2.0-77951546373
"Schwerdfeger A., Van Wyk E.","Verifiable parse table composition for deterministic parsing",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951584575&doi=10.1007%2f978-3-642-12107-4_15&partnerID=40&md5=337aeb96d06218c6de1e080ecaed6296","One obstacle to the implementation of modular extensions to programming languages lies in the problem of parsing extended languages. Specifically, the parse tables at the heart of traditional LALR(1) parsers are so monolithic and tightly constructed that, in the general case, it is impossible to extend them without regenerating them from the source grammar. Current extensible frameworks employ a variety of solutions, ranging from a full regeneration to using pluggable binary modules for each different extension. But recompilation is time-consuming, while the pluggable modules in many cases cannot support the addition of more than one extension, or use backtracking or non-deterministic parsing techniques. We present here a middle-ground approach that allows an extension, if it meets certain restrictions, to be compiled into a parse table fragment. The host language parse table and fragments from multiple extensions can then always be efficiently composed to produce a conflict-free parse table for the extended language. This allows for the distribution of deterministic parsers for extensible languages in a pre-compiled format, eliminating the need for the ""source code"" grammar to be distributed. In practice, we have found these restrictions to be reasonable and admit many useful language extensions. © 2010 Springer-Verlag.",,"Conflict free; Deterministic parsing; Extended languages; Extensible framework; Language extensions; Modular extension; Parse table; Pluggable modules; Programming language; Recompilation; Source codes; Computer software; Linguistics; Query languages; Formal languages",2-s2.0-77951584575
"Hemel Z., Visser E.","PIL: A platform independent language for retargetable DSLs",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951581593&doi=10.1007%2f978-3-642-12107-4_17&partnerID=40&md5=4763ae7d46a199a1debc5809c14dc559","Intermediate languages are used in compiler construction to simplify retargeting compilers to multiple machine architectures. In the implementation of domain-specific languages (DSLs), compilers typically generate high-level source code, rather than low-level machine instructions. DSL compilers target a software platform, i.e. a programming language with a set of libraries, deployable on one or more operating systems. DSLs enable targeting multiple software platforms if its abstractions are platform independent. While transformations from DSL to each targeted platform are often conceptually very similar, there is little reuse between transformations due to syntactic and API differences of the target platforms, making supporting multiple platforms expensive. In this paper, we discuss the design and implementation of PIL, a Platform Independent Language, an intermediate language providing a layer of abstraction between DSL and target platform code, abstracting from syntactic and API differences between platforms, thereby removing the need for platform-specific transformations. We discuss the use of PIL in an implemementation of WebDSL, a DSL for building web applications. © 2010 Springer-Verlag.",,"Compiler construction; Domain specific languages; Intermediate languages; Machine instructions; Multiple machine; Multiple platforms; Operating systems; Platform independent; Programming language; Retargetable; Software platforms; Source codes; WEB application; Abstracting; Application programming interfaces (API); Computer operating systems; DSL; Embedded systems; Modems; Problem oriented languages; Program compilers; Query languages; Syntactics; Targets; Telecommunication lines; XML; Linguistics",2-s2.0-77951581593
"Renggli L., Denker M., Nierstrasz O.","Language boxes: Bending the host language with modular language changes",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951543322&doi=10.1007%2f978-3-642-12107-4_20&partnerID=40&md5=cfc6e244b945660139db02955ed66d63","As domain-specific modeling begins to attract widespread acceptance, pressure is increasing for the development of new domain-specific languages. Unfortunately these DSLs typically conflict with the grammar of the host language, making it difficult to compose hybrid code except at the level of strings; few mechanisms (if any) exist to control the scope of usage of multiple DSLs; and, most seriously, existing host language tools are typically unaware of the DSL extensions, thus hampering the development process. Language boxes address these issues by offering a simple, modular mechanism to encapsulate (i) compositional changes to the host language, (ii) transformations to address various concerns such as compilation and syntax highlighting, and (iii) scoping rules to control visibility of fine-grained language extensions. We describe the design and implementation of language boxes, and show with the help of several examples how modular extensions can be introduced to a host language and environment. © 2010 Springer-Verlag.",,"Compositional changes; Development process; Domain specific languages; Domain specific modeling; Language extensions; Language tools; Modular extension; Modular language; Scoping; Computer software; Problem oriented languages; Query languages; Linguistics",2-s2.0-77951543322
"Bauer T., Erwig M.","Declarative scripting in Haskell",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951585098&doi=10.1007%2f978-3-642-12107-4_21&partnerID=40&md5=ddf68bad6088553501a8d4502942dd2a","We present a domain-specific language embedded within the Haskell programming language to build scripts in a declarative and type-safe manner. We can categorize script components into various orthogonal dimensions, or concerns, such as IO interaction, configuration, or error handling. In particular, we provide special support for two dimensions that are often neglected in scripting languages, namely creating deadlines for computations and tagging and tracing of computations. Arbitrary computations may be annotated with a textual tag explaining its purpose. Upon failure a detailed context for that error is automatically produced. The deadline combinator allows one to set a timeout on an operation. If it fails to complete within that amount of time, the computation is aborted. Moreover, this combinator works with the tag combinator so as to produce a contextual trace. © 2010 Springer-Verlag.",,"Domain specific languages; Error handling; Haskell; Haskell programming language; Scripting languages; Two-dimension; Computer aided software engineering; Computer software; Linguistics; Problem oriented languages; Query languages",2-s2.0-77951585098
"Dai Z., Ni N., Zhu J.","A 1 cycle-per-byte XML parsing accelerator",2010,"ACM/SIGDA International Symposium on Field Programmable Gate Arrays - FPGA",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951575054&doi=10.1145%2f1723112.1723148&partnerID=40&md5=53e5288fb07655eb1824eded068f7db9","Extensible Markup Language (XML) is playing an increasing important role in web services and database systems. However, the task of XML parsing is often the bottleneck, and as a result, the target of acceleration using custom hardware or multicore CPUs. In this paper, we detail the design of the first complete field programmable gate array (FPGA) accelerator capable of XML well-formed checking, schema validation, and tree construction at a throughput of 1 cycle per byte (CPB). This is a significant advancement from 40 CPB, the best previous reported commercial result. We demonstrate our design on a Xilinx Virtex-5 board, which successfully saturates a 1 Gbps Ethernet link. Copyright 2010 ACM.","BART; Bloom filter; DOM; Ethernet; Schema validation; String comparison; Tree construction; XML parsing","BART; Bloom filters; Schema validation; String comparison; Tree construction; XML parsing; Biological materials; Blooms (metal); Breakwaters; Ethernet; Field programmable gate arrays (FPGA); Hypertext systems; Logic gates; Markup languages; Program processors; Query languages; XML",2-s2.0-77951575054
"Wang H., Li J., Wang H.","Processing XPath over F&B-Index",2010,"Jisuanji Yanjiu yu Fazhan/Computer Research and Development",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953790441&partnerID=40&md5=d29e5046a49bd111971dcba5778e219d","XML is widely used as a standard of information exchange and representation. Queries on XML can retrieve a subset of XML data nodes satisfying certain constraints. Queries on large XML data are usually processed in two steps: 1. An index of XML nodes is created; 2. Queries are processed on that index without accessing XML data. XML index provides high efficiency for XML query processing. Particularly, F&B-index is the smallest index that supports twig query processing. However, few researches are proposed on how to efficiently create F&B-Index and how to process queries based on F&B-Index. Proposed in this paper is a new labeling scheme called prime sequence. This labeling scheme helps not only on creating an F&B-Index but also on efficient query processing. With prime sequence labeling, an F&B-index can be created by parsing the XML document only once with a SAX parser. Further, region encoding and CCPI on F&B-Index can be created during the creation of F&B-Index. Thus, TwigStack algorithm and related-path-join algorithm can be used to process queries on created F&B-Index. Also proposed is an efficient algorithm named division match over F&B-Index. The algorithm can efficiently judge relationship between two nodes based on a property of prime sequence labeling. Experiments show that prime sequence labeling provides high efficiency on creating F&B-Index and high efficiency on query processing on different datasets.","CCPI; F&B-index; Index; Prime sequence labeling; TwigStack; XML","B-index; Data sets; Efficient algorithm; High efficiency; Information exchanges; Join algorithm; Labeling scheme; Sequence Labeling; Twig queries; XML data; XML indices; XML query processing; Algorithms; Labeling; Markup languages; Query processing; XML",2-s2.0-77953790441
"Al-Shawakfa E., Al-Badarneh A., Shatnawi S., Al-RabaB'Ah K., Bani-lsmail B.","A comparison study of some arabic root finding algorithms",2010,"Journal of the American Society for Information Science and Technology",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951168035&doi=10.1002%2fasi.21301&partnerID=40&md5=e229f218d53640c72634caf1997292b2","Arabie hasacomplex structure, which makes it difficult to apply natural language processing (NLP). Much research on Arabic NLP (ANLP) does exist; however, it is not as mature as that of other languages. Finding Arabic roots is an important step toward conducting effective research on most of ANLP applications. The authors have studied and compared six root-finding algorithms with success rates of over 90%. All algorithms of this study did not use the same testing corpus and/or benchmarking measures. They unified the testing process by implementing their own algorithm descriptions and building a corpus out of 3823 triliteral roots, applying 73 triliteral patterns, and with 18 affixes, producing around 27.6 million words. They tested the algorithms with the generated corpus and have obtained interesting results; they offer to share the corpus freely for benchmarking and ANLP research. © 2010 ASIS&T.",,"Algorithm description; Comparison study; Natural language processing; Root finding algorithms; Testing process; Benchmarking; Computational linguistics; Natural language processing systems; Query languages; Research; Algorithms",2-s2.0-77951168035
"Xia F., Dou Y., Song J., Lei G.-Q.","Fine-grained parallel RNA secondary structure prediction using SCFGs on FPGA",2010,"Jisuanji Xuebao/Chinese Journal of Computers",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953776909&doi=10.3724%2fSP.J.1016.2010.00797&partnerID=40&md5=43a72a72adedd6ef77a053afd22272f8","In the field of RNA secondary structure prediction, CYK(Coche-Younger-Kasami) algorithm is one of the most popular methods using SCFG (stochastic context-free grammars) model. Accelerating SCFGs for large models and large RNA database searches becomes a challenge task in computational bioinformatics because of the O(L3) computational demands that are required. General purpose computers including parallel SMP multiprocessors or cluster systems exhibit low parallel efficiency. Furthermore, large scaled parallel computers are too expensive to be used easily for many research institutes. FPGA chips provide a new approach to accelerate CYK algorithm by exploiting fine-grained custom design. CYK algorithm shows complicated data dependence, in which the dependence distance is variable, and the dependence direction is also across two dimensions. This paper proposes a systolic-like array structure including one master PE (Processing Element) and multiple slave PEs for fine-grained hardware implementation on FPGA. By columns and assign, tasks are partitioned to PEs for load balance. Data reuse schemes reduce the need to load energy matrices from external memory. The experimental results show a factor of more than 14 speedup over the Infernal-1.0 software for 959-residue RNA sequence and a CM model with 3145 states running on a PC platform with Intel Dual-Core 2.5 GHz CPU. The computational power of the accelerator is comparable to a PC cluster consisting of 20 Intel-Xeon CPUs for RNA secondary structure prediction using SCFGs, but the hardware cost and power consumption is only about 20% and 10% that of the latter respectively.","Bioinformatics; FPGA; Hardware accelerator; Parallel CYK algorithm; RNA; Secondary structure prediction; Stochastic context-free grammars model","FPGA; Hardware accelerators; RNA secondary structure prediction; Secondary structure prediction; Stochastic context free grammar; Algorithms; Bioinformatics; Cluster computing; Computer hardware; Context free grammars; Forecasting; General purpose computers; Hardware; Program processors; RNA; Search engines; Stochastic systems; Stochastic models",2-s2.0-77953776909
"Yang D., Powers D.M.W.","Using grammatical relations to automate thesaurus construction",2010,"Journal of Research and Practice in Information Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649502359&partnerID=40&md5=9d7f97106b48ac374a1088dfb3076450","In this paper we introduce a novel method of automating thesauri using syntactically constrained distributional similarity. With respect to syntactically conditioned co-occurrences, most popular approaches to automatic thesaurus construction simply ignore the salience of grammatical relations and effectively merge them into one united 'context'. We distinguish semantic differences of each syntactic dependency and propose to generate thesauri through word overlapping across major types of grammatical relations. The encouraging results show that our proposal can build automatic thesauri with significantly higher precision than the traditional methods. Copyright © 2010, Australian Computer Society Inc.","Distribution; Similarity; Syntactic dependency","Distribution; Distributional similarities; Grammatical relations; Novel methods; Semantic difference; Similarity; Syntactics; Thesauri",2-s2.0-78649502359
"Androutsopoulos I., Malakasiotis P.","A survey of paraphrasing and textual entailment methods",2010,"Journal of Artificial Intelligence Research",149,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957887924&doi=10.1613%2fjair.2985&partnerID=40&md5=466b61a7de633dfead99bb68771b13bf","Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural language expressions that convey almost the same information. Textual entailment methods, on the other hand, recognize, generate, or extract pairs of natural language expressions, such that a human who reads (and trusts) the first element of a pair would most likely infer that the other element is also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of natural language processing applications, including question answering, summarization, text generation, and machine translation. We summarize key ideas from the two areas by considering in turn recognition, generation, and extraction methods, also pointing to prominent articles and resources. © 2010 AI Access Foundation.",,"Extraction method; Machine translations; Natural language expressions; NAtural language processing; Question Answering; Text generations; Textual entailment; Computational linguistics; Information theory; Speech transmission; Text processing; Natural language processing systems",2-s2.0-77957887924
"Jim T., Mandelbaum Y., Walker D.","Semantics and algorithms for data-dependent grammars",2010,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950871101&doi=10.1145%2f1706299.1706347&partnerID=40&md5=3a40aba9fc5c20899adc79ee7aa6386f","We present the design and theory of a new parsing engine, YAKKER, capable of satisfying the many needs of modern programmers and modern data processing applications. In particular, our new parsing engine handles (1) full scannerless context-free grammars with (2) regular expressions as right-hand sides for defining nonterminals. YAKKER also includes (3) facilities for binding variables to intermediate parse results and (4) using such bindings within arbitrary constraints to control parsing. These facilities allow the kind of data-dependent parsing commonly needed in systems applications, particularly those that operate over binary data. In addition, (5) nonterminals may be parameterized by arbitrary values, which gives the system good modularity and abstraction properties in the presence of data-dependent parsing. Finally, (6) legacy parsing libraries,such as sophisticated libraries for dates and times, may be directly incorporated into parser specifications. We illustrate the importance and utility of this rich collection of features by presenting its use on examples ranging from difficult programming language grammars to web server logs to binary data specification. We also show that our grammars have important compositionality properties and explain why such properties areimportant in modern applications such as automatic grammar induction. In terms of technical contributions, we provide a traditional high-level semantics for our new grammar formalization and show how to compile grammars into non deterministic automata. These automata are stack-based, somewhat like conventional push-down automata,but are also equipped with environments to track data-dependent parsing state. We prove the correctness of our translation of data-dependent grammars into these new automata and then show how to implement the automata efficiently using a variation of Earley's parsing algorithm. Copyright © 2010 ACM.","Ambiguous grammars; Automata; Context-sensitive grammars; Data-dependent grammars; Earley parsing; EBNF; L-attributed grammars; Regular expressions; Regular right-sides; Scannerless parsing; Semantic predicates; Transducers","Arbitrary constraints; Arbitrary values; Attributed grammars; Binary data; Compositionality; Context-sensitive; Data processing applications; Data-dependent grammars; Dates and time; Grammar induction; High level semantics; Modern applications; Nondeterministic automata; Parameterized; Parsing algorithm; Programming language; Push-down automata; Regular expressions; Right-hand sides; Technical contribution; Track data; Web server logs; Computer software; Context sensitive grammars; Libraries; Linguistics; Object oriented programming; Query languages; Robots; Semantics; Specifications; Transducers; Translation (languages); Data processing",2-s2.0-77950871101
"Lehmann J.","Learning OWL class expressions",2010,"Learning OWL Class Expressions",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951676283&partnerID=40&md5=2c69f3b46b79b7c64d21943e08e56fc1","With the advent of the Semantic Web and Semantic Technologies, ontologies have become one of the most prominent paradigms for knowledge representation and reasoning. However, recent progress in the field faces a lack of well structured ontologies with large amounts of instance data due to the fact that engineering such ontologies requires a considerable investment of resources. Nowadays, knowledge bases often provide large volumes of data without sophisticated schemata. Hence, methods for automated schema acquisition and maintenance are sought. Schema acquisition is closely related to solving typical classification problems in machine learning, e.g. the detection of chemical compounds causing cancer. In this work, we investigate both, the underlying machine learning techniques and their application to knowledge acquisition in the Semantic Web. © 2010, Akademische Verlagsgesellschaft AKA GmbH, Heidelberg. All rights reserved.",,,2-s2.0-79951676283
"Wu X., Bryant B.R., Gray J., Mernik M.","Component-based LR parsing",2010,"Computer Languages, Systems and Structures",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349114226&doi=10.1016%2fj.cl.2009.01.002&partnerID=40&md5=1a111186868da3860fb60d4e8f59e82d","A language implementation with proper compositionality enables a compiler developer to divide-and-conquer the complexity of building a large language by constructing a set of smaller languages. Ideally, these small language implementations should be independent of each other such that they can be designed, implemented and debugged individually, and later be reused in different applications (e.g., building domain-specific languages). However, the language composition offered by several existing parser generators resides at the grammar level, which means all the grammar modules need to be composed together and all corresponding ambiguities have to be resolved before generating a single parser for the language. This produces tight coupling between grammar modules, which harms information hiding and affects independent development of language features. To address this problem, we have developed a novel parsing algorithm that we call Component-based LR (CLR) parsing, which provides code-level compositionality for language development by producing a separate parser for each grammar component. In addition to shift and reduce actions, the algorithm extends general LR parsing by introducing switch and return actions to empower the parsing action to jump from one parser to another. Our experimental evaluation demonstrates that CLR increases the comprehensibility, reusability, changeability and independent development ability of the language implementation. Moreover, the loose coupling among parser components enables CLR to describe grammars that contain LR parsing conflicts or require ambiguous token definitions, such as island grammars and embedded languages. © 2009 Elsevier Ltd. All rights reserved.","Component-based software development; LR parsing; Parser generator","Component based; Component-based software development; Compositionality; Development ability; Divide and conquer; Domain specific languages; Embedded Languages; Experimental evaluation; Information hiding; Language development; Language features; Loose couplings; LR parsing; Parser generator; Parser generators; Parsing algorithm; Shift-and; Tight coupling; Computer software; Electric reactors; Formal languages; Linguistics; Query languages; Reusability; Software design; Computational linguistics",2-s2.0-70349114226
"Wu W.-L., Lu R.-Z., Duan J.-Y., Liu H., Gao F., Chen Y.-Q.","Spoken language understanding using weakly supervised learning",2010,"Computer Speech and Language",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349235396&doi=10.1016%2fj.csl.2009.05.002&partnerID=40&md5=74e379f648fb7e56f79401a16a7917c1","In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems. We model the task of spoken language understanding as a two-stage classification problem. Firstly, the topic classifier is used to identify the topic of an input utterance. Secondly, with the restriction of the recognized target topic, the slot classifiers are trained to extract the corresponding slot-value pairs. It is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language. More importantly, it allows that weakly supervised strategies are employed for training the two kinds of classifiers, which could significantly reduce the number of labeled sentences. We investigated active learning and naive self-training for the two kinds of classifiers. Also, we propose a practical method for bootstrapping topic-dependent slot classifiers from a small amount of labeled sentences. Experiments have been conducted in the context of the Chinese public transportation information inquiry domain and the English DARPA Communicator domain. The experimental results show the effectiveness of our proposed SLU framework and demonstrate the possibility to reduce human labeling efforts significantly. © 2009 Elsevier Ltd. All rights reserved.","Active learning; Bootstrapping; Self-training; Spoken dialogue system; Spoken language understanding; Topic classification","Active learning; Bootstrapping; Self-training; Spoken dialogue system; Spoken language understanding; Topic classification; Classifiers; Education; Linguistics; Speech processing; Supervised learning; Speech recognition",2-s2.0-70349235396
"Standley D.M., Yamashita R., Kinjo A.R., Toh H., Nakamura H.","SeSAW: Balancing sequence and structural information in protein functional mapping",2010,"Bioinformatics",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952792362&doi=10.1093%2fbioinformatics%2fbtq116&partnerID=40&md5=47a82f9b9af81bf62f3b212a22f23fa6","Motivation: Functional similarity between proteins is evident at both the sequence and structure levels. SeSAW is a web-based program for identifying functionally or evolutionarily conserved motifs in protein structures by locating sequence and structural similarities, and quantifying these at the level of individual residues. Results can be visualized in 2D, as annotated alignments, or in 3D, as structural superpositions. An example is given for both an experimentally determined query structure and a homology model. Availability and Implementation: The web server is located at http://www.pdbj.org/SeSAW/. Contact: standley@ifrec.osaka-u.ac.jp. © The Author(s) 2010. Published by Oxford University Press.",,"algorithm; animal; article; biochemistry; biology; computer graphics; computer program; genomics; Internet; metabolism; methodology; mouse; protein motif; sequence alignment; sequence analysis; statistical model; Thermus thermophilus; Algorithms; Amino Acid Motifs; Animals; Biochemistry; Computational Biology; Computer Graphics; Genomics; Internet; Mice; Models, Statistical; Sequence Alignment; Sequence Analysis, Protein; Software; Thermus thermophilus",2-s2.0-77952792362
"Hafiz R., Frost R.A.","Lazy combinators for executable specifications of general attribute grammars",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749295472&doi=10.1007%2f978-3-642-11503-5_15&partnerID=40&md5=8cb4a1ae3a649c43fc1c87eed2f74a16","A lazy-evaluation based top-down parsing algorithm has been implemented as a set of higher-order functions (combinators) which support directly-executable specifications of fully general attribute grammars. This approach extends aspects of previous approaches, and allows natural language processors to be constructed as modular and declarative specifications while accommodating ambiguous context-free grammars (including direct and indirect left-recursive rules), augmented with semantic rules with arbitrary attribute dependencies (including dependencies from right). This one-pass syntactic and semantic analysis method has polynomial time and space (w.r.t. the input length) for processing ambiguous input, and helps language developers build and test their models with little concern for the underlying computational methods. © 2010 Springer-Verlag.","Attribute grammars; Lazy evaluation; Natural-language processing; Parser combinators; Top-down parsing","Attribute grammars; Language processing; Lazy evaluation; Parser combinators; Top-down parsing; Context sensitive grammars; Linguistics; Polynomial approximation; Query languages; Semantics; Specifications; Textiles; XML; Context free languages",2-s2.0-77749295472
"Zhong H., Duan B., Su Y., Huang L.","A study on configuration of wind power plant in wind farms based on IEC 61400-25",2010,"Dianli Xitong Zidonghua/Automation of Electric Power Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951456897&partnerID=40&md5=a8b45a42633180ffb730db515e6ea651","Based on the successful application of substation configuration language (SCL) and the characteristics of wind farms, a IEC 61400-25 standard wind power plant configuration tool is designed using extensible markup language (XML), information model in IEC 61400-25 and database. The configuration tool includes modules of configuration interface, XML parser, transformation between tree controls and wind power plant configuration language (WPPCL) document, WPPCL document generation and data storage. A method is proposed for transmitting standard configuration files among involved equipments to implement uniform configuration of wind power plants. ©2010 State Grid Electric Power Research Institute Press.","Configuration tool; Database; IEC 61400-25; Uniform configuration; Wind power plant; WPPCL; XML","Configuration files; Configuration languages; Data storage; Document generation; Extensible markup language; IEC 61400-25; Information models; Substation configurations; Uniform configuration; Uniform configurations; Wind farm; XML parser; Electric utilities; Hypertext systems; Linguistics; Markup languages; Power plants; Query languages; Reactive power; Trees (mathematics); XML; Wind power",2-s2.0-77951456897
"Krahn H., Rumpe B., Völkel S.","MontiCore: A framework for compositional development of domain specific languages",2010,"International Journal on Software Tools for Technology Transfer",79,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955230740&doi=10.1007%2fs10009-010-0142-1&partnerID=40&md5=e48af5d56904fc6c5608cdb386a12788","Domain specific languages (DSLs) are increasingly used today. Coping with complex language definitions, evolving them in a structured way, and ensuring their error freeness are the main challenges of DSL design and implementation. The use of modular language definitions and composition operators are therefore inevitable in the independent development of language components. In this article, we discuss these arising issues by describing a framework for the compositional development of textual DSLs and their supporting tools. We use a redundance-free definition of a readable concrete syntax and a comprehensible abstract syntax as both representations significantly overlap in their structure. For enhancing the usability of the abstract syntax, we added concepts like associations and inheritance to a grammar-based definition in order to build up arbitrary graphs (as known from metamodeling). Two modularity concepts, grammar inheritance and embedding, are discussed. They permit compositional language definition and thus simplify the extension of languages based on already existing ones. We demonstrate that compositional engineering of new languages is a useful concept when project-individual DSLs with appropriate tool support are defined. © 2010 Springer-Verlag.","Composition; Domain specific language; Grammarware","Abstract syntax; Arbitrary graphs; Composition domains; Composition operators; Compositional development; Concrete syntax; Domain specific languages; Language component; Metamodeling; Modular language; Modularity concept; Supporting tool; Tool support; Abstracting; Query languages; Syntactics; Linguistics",2-s2.0-77955230740
"Brabrand C., Giegerich R., Møller A.","Analyzing ambiguity of context-free grammars",2010,"Science of Computer Programming",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73649141265&doi=10.1016%2fj.scico.2009.11.002&partnerID=40&md5=5c9e03392f6c6fd5eb2a97c6ef63745f","It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures. We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this by presenting an ambiguity analysis framework based on conservative language approximations. As a concrete example, we propose a technique based on local regular approximations and grammar unfoldings. We evaluate the analysis using grammars that occur in RNA analysis in bioinformatics, and we demonstrate that it is sufficiently precise and efficient to be practically useful. © 2009 Elsevier B.V. All rights reserved.","Approximation; Biosequence analysis; Context-free languages","Language design; Parser generation; Physical structures; Real-world; RNA analysis; Unfoldings; Bioinformatics; Linguistics; Models; Query languages; RNA; Context free languages",2-s2.0-73649141265
"Meister J.A., Foster J.S., Hicks M.","Serializing C intermediate representations for efficient and portable parsing",2010,"Software - Practice and Experience",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749319300&doi=10.1002%2fspe.954&partnerID=40&md5=0002dde340fa380525099697305c16ec","C static analysis tools often use intermediate representations (IRs) that organize program data in a simple, well-structured manner. However, the C parsers that create IRs are slow, and because they are difficult to write, only a few implementations exist, limiting the languages in which a C static analysis can be written. To solve these problems, we investigate two language-independent, on-disk representations of C IRs: one using XML and the other using an Internet standard binary encoding called eXternal Data Representation (XDR). We benchmark the parsing speeds of both options, finding the XML to be about a factor of 2 slower than parsing C and the XDR over 6 times faster. Furthermore, we show that the XML files are far too large at 19 times the size of C source code, whereas XDR is only 2.2 times the C size. We also demonstrate the portability of our XDR system by presenting a C source code querying tool in Ruby. Our solution and the insights we gained from building it will be useful to analysis authors and other clients of C IRs. Copyright © 2010 John Wiley & Sons, Ltd.","C; Intermediate representations; Parsing; Static analysis; XDR; XML","Binary encodings; C; Data representations; Disk representation; Intermediate representations; Internet Standard; Source codes; XML files; Linguistics; Markup languages; Query languages; Static analysis; XML",2-s2.0-77749319300
"Johnstone A., Mosses P.D., Scott E.","An agile approach to language modelling and development",2010,"Innovations in Systems and Software Engineering",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949283661&doi=10.1007%2fs11334-009-0111-6&partnerID=40&md5=42bee350b8f1356e6921ec873cfa670a","We have developed novel techniques for component-based specification of programming languages. In our approach, the semantics of each fundamental programming construct is specified independently, using an inherently modular framework such that no reformulation is needed when constructs are combined. A language specification consists of an unrestricted context-free grammar for the syntax of programs, together with an analysis of each language construct in terms of fundamental constructs. An open-ended collection of fundamental constructs is currently being developed. When supported by appropriate tools, our techniques allow a more agile approach to the design, modelling, and implementation of programming and domain-specific languages. In particular, our approach encourages language designers to proceed incrementally, using prototype implementations generated from specifications to test tentative designs. The components of our specifications are independent and highly reusable, so initial language specifications can be rapidly produced, and can easily evolve in response to changing design decisions. In this paper, we outline our approach, and relate it to the practices and principles of agile modelling. © Springer-Verlag London Limited 2009.","Agile methods; Programming language models; Semantics; Syntax","Agile approaches; Agile methods; Component based; Design decisions; Domain specific languages; Language constructs; Language modelling; Language specification; Modular framework; Novel techniques; Programming language; Programming language models; Prototype implementations; Tentative design; C (programming language); Computational linguistics; Computer software; Design; Problem oriented languages; Query languages; Semantics; Specifications; Syntactics; Context free languages",2-s2.0-77949283661
"Ober I., Féraud L., Percebois C.","Dealing with variability within a family of domain-specific languages: Comparative analysis of different techniques",2010,"Innovations in Systems and Software Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949279212&doi=10.1007%2fs11334-009-0117-0&partnerID=40&md5=a7455cf53628cc8f7c561491971aaffe","Almost a decade has passed since the OMG has issued the Model Driven Architecture (MDA) initiative. It soon became obvious that raising the level of abstraction in development and reasoning at the model level would help in asking the right questions at the right time. Based on a concrete problem, we discuss four alternative solutions to a multi-language system design problem. These solutions use a traditional approach, a technique based on modeling, a domain-specific approach, and a mix of modeling and domain-specific techniques, respectively. The solutions depend on the problem, but they are representative for the situations we encounter in practice, therefore giving us a good basis for a larger discussion on the appropriateness of using modeling techniques and on the place of MDA in current software engineering practice and design. © Springer-Verlag London Limited 2010.","Automatic unification; Comparative analysis; Domain-specific (modeling) language; Model-based development","Comparative analysis; Domain specific; Domain specific languages; Language model; Level of abstraction; Model driven architectures; Modeling technique; Software engineering practices; System design; Linguistics; Problem oriented languages; Software architecture; Query languages",2-s2.0-77949279212
"Yuan C., Wang X., Ren F.","Exploiting lexical information for function tag labeling",2010,"International Journal of Innovative Computing, Information and Control",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950152716&partnerID=40&md5=4c25e8da40dbc82bbd078d58f3bcb3b8","This paper proposes a novel approach to annotate function tags for unparsed text. What distinguishes our work from previous attempts is that we assign function tags directly basing on lexical information other than on parsed trees, thus our method is general and easily portable to languages in shortage of parsing resources. In order to demonstrate the effectiveness and versatility of our method, we investigate function tag assignment for unparsed Chinese text by applying two statistical models, one is log-linear maximum entropy model, another is maximum margin based support vector machine model. We show that function tag types could be determined via powerful lexical features and effective learning algorithms. Currently, our method achieves the best F-score of 86.4 when tested on the Penn Chinese Treebank data, the highest score ever reported for Chinese text. © 2010 ISSN 1349-4198.","Chinese language processing; Function tags; Machine learning; Penn treebank; Unparsed text","Chinese language; Chinese text; Effective learning; F-score; Function tags; Lexical features; Lexical information; Machine-learning; Maximum entropy models; Maximum margin; Penn Chinese Treebank; Statistical models; Tag types; Treebanks; Learning systems; Linguistics; Query languages; Learning algorithms",2-s2.0-77950152716
"Minock M.","C-Phrase: A system for building robust natural language interfaces to databases",2010,"Data and Knowledge Engineering",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74849109344&doi=10.1016%2fj.datak.2009.10.007&partnerID=40&md5=6917ca572109556ac879f5e43f0c7ad1","This article presents C-Phrase, a natural language interface system that can be configured by normal, non-specialized, web-based technical teams. C-Phrase models queries in an extended version of Codd's tuple calculus and uses synchronous context-free grammars with lambda-expressions to represent semantic grammars. Given an arbitrary relational database, authors rapidly build an NLI using what we term the name-tailor-define protocol. We present a small study demonstrating the effectiveness of this approach for the GEO corpus and we introduce the evaluation metric of willingness that complements the standard metrics of precision and recall. However our true evaluation comes as we open-source C-Phrase. © 2009 Elsevier B.V. All rights reserved.",,"Evaluation metrics; Extended versions; Natural language interfaces; Open-source; Precision and recall; Relational Database; Semantic grammars; Standard metrics; Internet protocols; Linguistics",2-s2.0-74849109344
"Rinderknecht C., Volanschi N.","Theory and practice of unparsed patterns for metacompilation",2010,"Science of Computer Programming",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73749085907&doi=10.1016%2fj.scico.2009.09.011&partnerID=40&md5=becbac83c9dc9b2653eccd2d6542aa71","Several software development tools support the matching of concrete syntax user-supplied patterns against the application source code, allowing the detection of invalid, risky, inefficient or forbidden constructs. When applied to compilers, this approach is called metacompilation. These patterns are traditionally parsed into tree patterns, i.e., fragments of abstract-syntax trees with metavariables, which are then matched against the abstract-syntax tree corresponding to the parsing of the source code. Parsing the patterns requires extending the grammar of the application programming language with metavariables, which can be difficult, especially in the case of legacy tools. Instead, we propose a novel matching algorithm which is independent of the programming language because the patterns are not parsed and, as such, are called unparsed patterns. It is as efficient as the classic pattern matching while being easier to implement. By giving up the possibility of static checks that parsed patterns usually enable, it can be integrated within any existing utility based on abstract-syntax trees at a low cost. We present an in-depth coverage of the practical and theoretical aspects of this new technique by describing a working minimal patch for the GNU C compiler, together with a small standalone prototype punned Matchbox, and by lying out a complete formalisation, including mathematical proofs of key algorithmic properties, like correctness and equivalence to the classic matching. © 2009 Elsevier B.V. All rights reserved.","Code checking; Formal methods; Metacompilation; Pattern matching; Tree pattern","Algorithmic properties; Application programming; C compilers; Code checking; Concrete syntax; Formalisation; Low costs; Matching algorithm; Mathematical proof; Metacompilation; Metavariables; Programming language; Software development tools; Source codes; Syntax tree; Theoretical aspects; Theory and practice; Tree pattern; Abstracting; C (programming language); Computer software; Formal languages; Linguistics; Pattern matching; Program compilers; Syntactics; Formal methods",2-s2.0-73749085907
"Amin M.S., Jamil H.","An efficient web-based wrapper and annotator for tabular data",2010,"International Journal of Software Engineering and Knowledge Engineering",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953253353&doi=10.1142%2fS0218194010004657&partnerID=40&md5=2b9ce6a70fe929b70445d41ff40d2dd5","In the last few years, several works in the literature have addressed the problem of data extraction from web pages. The importance of this problem derives from the fact that, once extracted, data can be handled in a way similar to instances of a traditional database, which in turn can facilitate application of web data integration and various other domain specific problems. In this paper, we propose a novel table extraction technique that works on web pages generated dynamically from a back-end database. The proposed system can automatically discover table structure by relevant pattern mining from web pages in an efficient way, and can generate regular expression for the extraction process. Moreover, the proposed system can assign intuitive column names to the columns of the extracted table by leveraging Wikipedia knowledge base for the purpose of table annotation. To improve accuracy of the assignment, we exploit the structural homogeneity of the column values and their co-location information to weed out less likely candidates. This approach requires no human intervention and experimental results have shown its accuracy to be promising. Moreover, the wrapper generation algorithm works in linear time. © 2010 World Scientific Publishing Company.","Information extraction; Missing column name annotation; Wrapper","Back-end database; Colocations; Data extraction; Domain specific; Extraction process; Extraction techniques; Human intervention; Information Extraction; Knowledge base; Linear time; Regular expressions; Relevant patterns; Structural homogeneity; Table structure; Tabular data; Web data integration; Web page; Wikipedia; Wrapper generation algorithms; Data handling; Information analysis; Knowledge based systems; Knowledge management; Websites",2-s2.0-77953253353
"Kazantseva A., Szpakowicz S.","Summarizing short stories",2010,"Computational Linguistics",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049101700&doi=10.1162%2fcoli.2010.36.1.36102&partnerID=40&md5=6f32940a16730fd9843fd35db41c1701","We present an approach to the automatic creation of extractive summaries of literary short stories. The summaries are produced with a specific objective in mind: to help a reader decide whether she would be interested in reading the complete story. To this end, the summaries give the user relevant information about the setting of the story without revealing its plot. The system relies on assorted surface indicators about clauses in the short story, the most important of which are those related to the aspectual type of a clause and to the main entities in a story. Fifteen judges evaluated the summaries on a number of extrinsic and intrinsic measures. The outcome of this evaluation suggests that the summaries are helpful in achieving the original objective. © 2010 Association for Computational Linguistics.",,,2-s2.0-77049101700
"Baker P.","Sociolinguistics and corpus linguistics",2010,"Sociolinguistics and Corpus Linguistics",64,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967430704&partnerID=40&md5=862dfd15682b57c878ba565c05ac0129","Shows how techniques from corpus linguistics can be used in sociolinguistic research. This textbook introduces students to the ways in which techniques from corpus linguistics can be used to aid sociolinguistic research. Corpus linguistics shares with variationist sociolinguistics a quantitative approach to the study of variation or differences between populations. It may also complement qualitative traditions of enquiry such as interactional sociolinguistics. This text covers a range of different topics within sociolinguistics: • Analysing demographic variation • Comparing language use across different cultures • Examining language change over time • Studying transcripts of spoken interactions • Identifying attitudes or discourses. Written for undergraduate and postgraduate students of sociolinguistics, or corpus linguists who wish to use corpora to study social phenomena, this textbook examines how corpora can be drawn on to investigate synchronic variation, diachronic change and the construction of discourses. It refers to several classic corpus-based studies as well as the author's own research. Original analyses of a number of corpora including the British National Corpus, the Survey of English Dialects and the Brown family of corpora are complemented by a new corpus of written British English collected around 2006 for the purposes of writing the book. Techniques of analysis like concordancing, keywords and collocations are discussed, along with corpus annotation and statistical procedures such as chi-squared tests and clustering. Paul Baker takes a critical approach to using corpora in sociolinguistics, outlining the limitations of the approach as well as its advantages. © Paul Baker, 2010.",,,2-s2.0-84967430704
"Al Qady M., Kandil A.","Concept relation extraction from construction documents using natural language processing",2010,"Journal of Construction Engineering and Management",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77149152182&doi=10.1061%2f%28ASCE%29CO.1943-7862.0000131&partnerID=40&md5=cd8a81843620c1cbe4441ab16ffaf6bc","The objective of this research is to present an innovative technique for managing the knowledge contained in construction contract documents to facilitate quick access and efficient use of such knowledge for project management and contract administration tasks. Knowledge Management has become the focus of a lot of scientific research during the second half of the 20th century as researchers discovered the importance of the knowledge resource to business organizations. Despite early expectations of improved document management techniques, document management systems used in the construction industry have failed to deliver the anticipated performance. Recent research attempts to utilize analysis of the contents of documents to improve document categorization and retrieval functions. It is hypothesized that natural language processing can be effectively used to perform document text analysis. The proposed system, technique for concept relation identification using shallow parsing (CRISP), utilizes a shallow parser to extract semantic knowledge from construction contract documents which can be used to improve electronic document management functions such as document categorization and retrieval. When compared with human evaluators, CRISP achieved almost 80% of the average kappa score attained by the evaluators, and approximately 90% of their F -measure score. © 2010 ASCE.","Computer aided operations; Computerization; Construction management; Contract management; Information management; Information systems","20th century; Business organizations; Computer aided operation; Construction contract; Construction documents; Construction management; Contract administration; Document categorization; Document management; Document management systems; Electronic document management; F-measure scores; Knowledge resource; Management information; NAtural language processing; Relation extraction; Scientific researches; Semantic knowledge; Shallow parsing; Text analysis; Computational linguistics; Construction industry; Information retrieval; Information services; Information systems; Knowledge management; Management information systems; Management science; Natural language processing systems; Research; Signal processing; Word processing; Project management",2-s2.0-77149152182
"Bui Q.-C., Nualláin B.T., Boucher C.A., Sloot P.M.A.","Extracting causal relations on HIV drug resistance from literature",2010,"BMC Bioinformatics",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950542062&doi=10.1186%2f1471-2105-11-101&partnerID=40&md5=d7fee9a1b7e3c3d0e29c39078ea7f93e","Background: In HIV treatment it is critical to have up-to-date resistance data of applicable drugs since HIV has a very high rate of mutation. These data are made available through scientific publications and must be extracted manually by experts in order to be used by virologists and medical doctors. Therefore there is an urgent need for a tool that partially automates this process and is able to retrieve relations between drugs and virus mutations from literature.Results: In this work we present a novel method to extract and combine relationships between HIV drugs and mutations in viral genomes. Our extraction method is based on natural language processing (NLP) which produces grammatical relations and applies a set of rules to these relations. We applied our method to a relevant set of PubMed abstracts and obtained 2,434 extracted relations with an estimated performance of 84% for F-score. We then combined the extracted relations using logistic regression to generate resistance values for each <drug, mutation> pair. The results of this relation combination show more than 85% agreement with the Stanford HIVDB for the ten most frequently occurring mutations. The system is used in 5 hospitals from the Virolab project http://www.virolab.org to preselect the most relevant novel resistance data from literature and present those to virologists and medical doctors for further evaluation.Conclusions: The proposed relation extraction and combination method has a good performance on extracting HIV drug resistance data. It can be used in large-scale relation extraction experiments. The developed methods can also be applied to extract other type of relations such as gene-protein, gene-disease, and disease-mutation. © 2010 Bui et al; licensee BioMed Central Ltd.",,"Combination method; Extraction method; Grammatical relations; Logistic regressions; NAtural language processing; Relation extraction; Resistance values; Scientific publications; Extraction; Genes; Natural language processing systems; Viruses; Diseases; Human immunodeficiency virus; anti human immunodeficiency virus agent; antiviral resistance; article; documentation; genetic database; genetics; Human immunodeficiency virus infection; information retrieval; Internet; MEDLINE; methodology; mutation; virus genome; Abstracting and Indexing as Topic; Anti-HIV Agents; Databases, Genetic; Drug Resistance, Viral; Genome, Viral; HIV Infections; Information Storage and Retrieval; Internet; Mutation; PubMed",2-s2.0-77950542062
"Valentin F., Squizzato S., Goujon M., McWilliam H., Paern J., Lopez R.","Fast and efficient searching of biological data resources-using EB-eye",2010,"Briefings in Bioinformatics",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954245304&doi=10.1093%2fbib%2fbbp065&partnerID=40&md5=95d51c34c88b48a4b4c6bcd8394f460f","The EB-eye is a fast and efficient search engine that provides easy and uniform access to the biological data resources hosted at the EMBL-EBI. Currently, users can access information from more than 62 distinct datasets covering some 400 million entries. The data resources represented in the EB-eye include: nucleotide and protein sequences at both the genomic and proteomic levels, structures ranging from chemicals to macro-molecular complexes, gene-expression experiments, binary level molecular interactions as well as reaction maps and pathway models, functional classifications, biological ontologies, and comprehensive literature libraries covering the biomedical sciences and related intellectual property. The EB-eye can be accessed over the web or programmatically using a SOAP Web Services interface. This allows its search and retrieval capabilities to be exploited in workflows and analytical pipe-lines. The EB-eye is a novel alternative to existing biological search and retrieval engines. In this article we describe in detail how to exploit its powerful capabilities. © The Author 2010. Published by Oxford University Press.","Apache Lucene; Biological databases; Integration; Interoperability; Text search; Web services","animal; article; factual database; human; information retrieval; Animals; Databases, Factual; Humans; Information Storage and Retrieval; Animals; Databases, Factual; Humans; Information Storage and Retrieval",2-s2.0-77954245304
"Della Penna G., Magazzeni D., Orefice S.","Visual extraction of information from web pages",2010,"Journal of Visual Languages and Computing",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149164295&doi=10.1016%2fj.jvlc.2009.06.001&partnerID=40&md5=3b616983674fd9223f103892e5a6afb4","In this paper we present a graphical software system that provides an automatic support to the extraction of information from web pages. The underlying extraction technique exploits the visual appearance of the information in the document, and is driven by the spatial relations occurring among the elements in the page. However, the usual information extraction modalities based on the web page structure can be used in our framework, too. The technique has been integrated within the Spatial Relation Query (SRQ) tool. The tool is provided with a graphical front-end which allows one to define and manage a library of spatial relations, and to use a SQL-like language for composing queries driven by these relations and by further semantic and graphical attributes. © 2009 Elsevier Ltd. All rights reserved.","Information extraction; Query languages; Spatial relations; User interfaces for search interaction; Web visual information search",,2-s2.0-75149164295
"Hu C.-L., Cho C.-A., Lin C.-J., Fan C.-W.","Design of mobile group communication system in ubiquitous communication network",2010,"IEEE Transactions on Consumer Electronics",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950683166&doi=10.1109%2fTCE.2010.5439130&partnerID=40&md5=4b18912af94a6184e36a9c8c621a77c6","The advent of ubiquitous IP networks enables users to freely move and continue network services anytime, anywhere by means of mobile handheld devices. This paper exploits network ubiquity and advanced device capability to develop a novel mobile group communication system. Our work presents the design and implementation of a proof-of-concept software system. This system architecture consists of four components: central tracking, handheld tracking, handheld messaging and Web-based administration subsystems. Their functional integration thus fulfills both location tracking and mobile messaging services in support of mobile group communication. After prototype and demonstration, the proposed system is able to offer a variety of novel location-aware and text/voice communication scenarios to mobile social communities in ubiquitous IP network environments1. © 2006 IEEE.","Location tracking; Mobile group communication; Mobile messaging; Ubiquitous service","Functional integration; Group communication systems; Group communications; Handhelds; IP networks; Location tracking; Location-aware; Mobile handheld devices; Mobile messaging; Mobile messaging services; Network services; Proof of concept; Social communities; Software systems; System architectures; Ubiquitous communication; Ubiquitous Service; Web-based administration; Communication systems; Wireless networks",2-s2.0-77950683166
"Bitter C., Elizondo D.A., Yang Y.","Natural language processing: A prolog perspective",2010,"Artificial Intelligence Review",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149115620&doi=10.1007%2fs10462-009-9151-4&partnerID=40&md5=7e1e6490e870bbf5367a0677a69da2c5","Natural language processing (NLP) is a vibrant field of interdisciplinary Computer Science research. Ultimately, NLP seeks to build intelligence into software so that software will be able to process a natural language as skillfully and artfully as humans. Prolog, a general purpose logic programming language, has been used extensively to develop NLP applications or components thereof. This report is concerned with introducing the interested reader to the broad field of NLP with respect to NLP applications that are built in Prolog or from Prolog components. © 2009 Springer Science+Business Media B.V.","Applications of natural language processing; Natural language processing; Prolog","A-prolog; General purpose; Interdisciplinary computer science; Logic programming languages; NAtural language processing; Natural languages; Prolog; Computational linguistics; Computer software; Industrial research; Logic programming; Natural language processing systems; PROLOG (programming language)",2-s2.0-75149115620
"Sonntag D.","Ontologies and adaptivity in dialogue for question answering",2010,"Ontologies and Adaptivity in Dialogue for Question Answering",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957967350&partnerID=40&md5=2790451d033fa43eba809eb40ad669c4","The appeal of being able to ask a question to a mobile internet terminal and receive an answer immediately has been renewed by the broad availability of information on the Web. Ideally, a spoken dialogue system that uses the Web as its knowledge base would be able to answer a broad range of questions. A new generation of natural language dialogue systems is emerging that transforms traditional keyword search engines into semantic answering machines by providing exact and concise answers formulated in natural language instead of today's long lists of document references, which the user has to check by himself for relevant answers. This book presents the anatomy of the fully operational SmartWeb system (funded by the German Federal Ministry of Education and Research with grants totaling 14 million euros) that provides not only an open-domain question answering machine but a multimodal web service interface for coherent dialogue, where questions and commands are interpreted according to the context of the previous conversation. One of the key innovations described in this book is the ability of the system to learn how to predict the probability that it can answer a complex user query in a given time interval. © 2010, Akademische Verlagsgesellschaft AKA GmbH, Heidelberg. All rights reserved.",,"Knowledge based systems; Natural language processing systems; Query processing; Search engines; Semantics; Speech processing; Answering machines; Fully operational; German Federal Ministry of Education and Research; Natural languages; Natural-language dialogue systems; Open domain question answering; Question Answering; Spoken dialogue system; Web services",2-s2.0-77957967350
"Weinbub J., Heinzl R., Stimpfl F., Selberherr S., Schwaha P.","A lightweight material library for scientific computing in C++",2010,"ESM 2010 - 2010 European Simulation and Modelling Conference",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898631918&partnerID=40&md5=a3dc8c2aa700cc7f1ebc84f70f9b8ff4","Simulations in the field of scientific computing require often the availability of large sets of material properties. We propose a convenient approach for a lightweight material library using available open source tools. The presented approach is therefore suited for embedding into larger projects, such as simulators. The XML file format as well as an XML parser library is used to store, load, and manage the data. The location of data items or data sets is specified using XPath query language. Furthermore, an utility is provided for the conversion of the initially untyped data items to the numerical data types required by the simulation package. As performance is an issue in this context, we present a simple use case.",,"Modal analysis; XML; Data items; Lightweight materials; Numerical data; Open source tools; Simulation packages; XML files; XML parser; Query languages",2-s2.0-84898631918
"Ishida T.","Intercultural collaboration using machine translation",2010,"IEEE Internet Computing",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449090834&doi=10.1109%2fMIC.2010.12&partnerID=40&md5=98c0c8bc8fa7b131144284abd6c8f92d","Toru Ishida, Kyoto University, suggests that multilingual collaboration approaches need to be adopted to solve the problem of working in multiple cultures. Machine translation has emerged as one of the significant solutions to solve the problem of understanding different languages of the world. Machine translation systems can be useful when they are customized to suit specific needs of different communities. Users need to combine domain-specific texts with machine translators to customize machine translations. They also need morphological analyzers to analyze input sentences that are to be translated. Training machine translators with parallel texts requires dependency parsers, as users will want to use speech recognition/synthesis and gesture recognition in the future. Worldwide collaboration will be needed to generate all the necessary language services for supporting local schools, including students from different countries.",,"Dependency parser; Domain specific; Kyoto University; Machine translation systems; Machine translations; Morphological analyzer; Parallel text; Gesture recognition; Information theory; Linguistics; Query languages; Speech recognition; Speech transmission; Translation (languages)",2-s2.0-75449090834
"Aguilar R., Pan J., Gries C., San Gil I., Palanisamy G.","A flexible online metadata editing and management system",2010,"Ecological Informatics",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-75949120337&doi=10.1016%2fj.ecoinf.2009.08.012&partnerID=40&md5=e7d806ee11160b755739b22c4a076d4d","Our team developed a metadata editing and management system employing state of the art XML technologies initially aimed at the environmental sciences but with the potential to be useful across multiple domains. We chose a modular and distributed design for scalability, flexibility, options for customizations, and the possibility to add more functionality at a later stage. The system consists of a desktop design tool that generates code for the actual online editor, a native XML database, and an online user access management application. A Java Swing application that reads an XML schema, the design tool provides the designer with options to combine input fields into online forms with user-friendly tags and determine the flow of input forms. Based on design decisions, the tool generates XForm code for the online metadata editor which is based on the Orbeon XForms engine. The design tool fulfills two requirements: First data entry forms based on a schema are customized at design time and second the tool can generate data entry applications for any valid XML schema without relying on custom information in the schema. A configuration file in the design tool saves custom information generated at design time. Future developments will add functionality to the design tool to integrate help text, tool tips, project specific keyword lists, and thesaurus services. Cascading style sheets customize the look-and-feel of the finished editor. The editor produces XML files in compliance with the original schema, however, a user may save the input into a native XML database at any time independent of validity. The system uses the open source XML database eXist for storage and uses a MySQL relational database and a simple Java Server Faces user interface for file and access management. We chose three levels to distribute administrative responsibilities and handle the common situation of an information manager entering the bulk of the metadata but leave specifics to the actual data provider. © 2009 Elsevier B.V. All rights reserved.","EML; Metadata editor; Orbeon; XForms; XML schema parser; XQuery","database; environmental technology; meta-analysis",2-s2.0-75949120337
"Chen Z., Ma J., Rui H., Sun Y., Shao H., Ren Z.","Web page publication date extraction and application",2010,"Journal of Computational Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954296890&partnerID=40&md5=d832d2485a15049928afb9dbe4969362","Publication dates(P-dates for short) of Web pages are often required in many application areas. In this paper, we propose a supervised machine learning approach to find the P-dates, where the linguistic information and format information extracted from the DOM (Document Object Model) tree of Web pages are used as features for learning. Experiments demonstrate our approach significantly outperforms three baseline methods which utilize the first date, the last date, and the latest date respectively in terms of F1 score for both English and Chinese pages. As an application, we study how to add the P-dates in page rank. We propose a model for page rank, which takes the P-dates, the relevance scores between the text of pages and user query, and the important scores of pages into account. Experiments indicate that page rank using P-dates is almost always better than those do not in terms of NDCG(Normalized Discount Cumulative Gain). Copyright © 2010 Binary Information Press.","Machine learning; Page rank; Publication date extraction; Temporal information extraction","Application area; Baseline methods; Document object model; Linguistic information; Machine-learning; Page rank; Page ranks; Relevance score; Supervised machine learning; Temporal information extraction; User query; Web page; Information analysis; Learning systems; Publishing; Websites",2-s2.0-77954296890
"Pham P.T., Moens M.-F., Tuytelaars T.","Cross-media alignment of names and faces",2010,"IEEE Transactions on Multimedia",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949106517&doi=10.1109%2fTMM.2009.2036232&partnerID=40&md5=0aace9bd2499eef865b117c125fe4245","In this paper we report on our experiments on aligning names and faces as found in images and captions of online news websites. Developing accurate technologies for linking names and faces is valuable when retrieving or mining information from multimedia collections. We perform exhaustive and systematic experiments exploiting the (a)symmetry between the visual and textual modalities. This leads to different schemes for assigning names to the faces, assigning faces to the names, and establishing name-face link pairs. On top of that, we investigate generic approaches to the use of textual and visual structural information to predict the presence of the corresponding entity in the other modality. The proposed methods are completely unsupervised and are inspired by methods for aligning phrases and words in texts of different languages developed for constructing dictionaries for machine translation. The results are competitive with state-of-the-art performance on the ""Labeled Faces in the Wild"" dataset in terms of recall values, now reported on the complete dataset, include excellent precision values, and show the value of text and image analysis for identifying the probability of being pictured or named in the alignment process. © 2009 IEEE.","Cross-media mining; Image annotation","Cross-media; Cross-media mining; Data sets; Generic approach; Image annotation; Machine translations; Multimedia collections; Online news; State-of-the-art performance; Structural information; Systematic experiment; Alignment; Computer aided language translation; Information theory; Multimedia systems; Query languages; Speech transmission; Image analysis",2-s2.0-72949106517
"Hagen M.S., Lee E.K.","BIOSPIDA: A Relational Database Translator for NCBI",2010,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964999293&partnerID=40&md5=66d52fb11d64f9029e233cf0a65eaf06","As the volume and availability of biological databases continue widespread growth, it has become increasingly difficult for research scientists to identify all relevant information for biological entities of interest. Details of nucleotide sequences, gene expression, molecular interactions, and three-dimensional structures are maintained across many different databases. To retrieve all necessary information requires an integrated system that can query multiple databases with minimized overhead. This paper introduces a universal parser and relational schema translator that can be utilized for all NCBI databases in Abstract Syntax Notation (ASN.1). The data models for OMIM, Entrez-Gene, Pubmed, MMDB and GenBank have been successfully converted into relational databases and all are easily linkable helping to answer complex biological questions. These tools facilitate research scientists to locally integrate databases from NCBI without significant workload or development time.",,"computer program; gene expression; genetic database; library; Medline; nucleic acid database; nucleotide sequence; United States; Base Sequence; Databases, Genetic; Databases, Nucleic Acid; Gene Expression; National Library of Medicine (U.S.); PubMed; Software; United States",2-s2.0-84964999293
"Jakubíček M., Kovář V.","Czechparl: Corpus of stenographic protocols from Czech parliament",2010,"RASLAN 2010 - Recent Advances in Slavonic Natural Language Processing:  4th Workshop on Recent Advances in Slavonic Natural Language Processing, Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897932278&partnerID=40&md5=d46941706a319ad924c211b0b931300a","Within a single language, there is a large variety of styles used for written text and speech, which differ significantly and have their subtle specifics. Among all of those, the language of politicians represents an integral class that deserves detailed analysis. In this paper we present CzechParl, a corpus we built from stenographic protocols recorded during plenary meetings of the Czech parliament in its modern era from 1993 to 2010. We provide brief statistics of the corpus and discuss its intended future usage and further development. © Tribun EU 2010.",,"Natural language processing systems; Written texts; Linguistics",2-s2.0-84897932278
"Tian F., Ren F.","Learning relation instances for Chinese domain ontology from the web",2010,"IEEJ Transactions on Electrical and Electronic Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952115930&doi=10.1002%2ftee.20516&partnerID=40&md5=70a0248f5f3fcff76a262e108100a0a9","This paper presents a method to extract relation instances from the Internet in order to acquire knowledge that has some relations for domain ontology. We propose an ontology relation instance learning model: data sources are collected though the Web search engine and the extracted instances are constructed in Web ontology language (OWL) by Protege in Chinese. Basically, the extraction of relation instances contains syntactic patterns for filtering concepts and relevance measurement for selection of relation instances. A relevance measurement based on co-occurrence statistics is presented in this paper, which measures the semantic similarity of the measure between candidate instances and predefined domain keywords using Web search engines. In the experiment, we extract festival customs for different festival instances using relation 'has custom' between festival class and custom class in the Chinese festival ontology, and prove the effectiveness of our method. © 2010 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc.","Co-occurrence statistics; Ontology learning; Relation extraction; Relation instance","Information retrieval; Search engines; Semantic Web; World Wide Web; Co-occurrence statistics; Data source; Domain ontologies; Learning models; Learning relations; Measurement-based; Ontology learning; Protege; Semantic similarity; Syntactic patterns; Web ontology language; Web search engines; Ontology",2-s2.0-77952115930
"Zdun U.","A DSL toolkit for deferring architectural decisions in DSL-based software design",2010,"Information and Software Technology",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955779391&doi=10.1016%2fj.infsof.2010.03.004&partnerID=40&md5=bd636afe67334b90c5b8901d0a417f11","A number of mature toolkits and language workbenches for DSL-based design have been proposed, making DSL-based design attractive for many projects. These toolkits preselect many architectural decision options. However, in many cases it would be beneficial for DSL-based design to decide for the DSL's architecture later on in a DSL project, once the requirements and the domain have been sufficiently understood. We propose a language and a number of DSLs for DSL-based design and development that combine important benefits of different DSL toolkits in a unique way. Our approach specifically targets at deferring architectural decisions in DSL-based design. As a consequence, the architect can choose, even late in a DSL project, for options such as whether to provide the DSL as one or more external or embedded DSLs and whether to use an explicit language model or not . © 2010 Elsevier B.V. All rights reserved.","Architectural decisions; Domain-specific language (DSL); Software architecture; Software design","Architectural design; Computer programming languages; Problem oriented languages; Software architecture; Software design; Architectural decision; Design and Development; Domain specific languages; Language model; Language workbenches; Digital subscriber lines",2-s2.0-77955779391
"O'Sullivan D.M., Wilk S.A., Michalowski W.J., Farion K.J.","Automatic indexing and retrieval of encounter-specific evidence for point-of-care support",2010,"Journal of Biomedical Informatics",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954106091&doi=10.1016%2fj.jbi.2010.03.003&partnerID=40&md5=4087621fcdecb6e4559b9bcad462b382","Evidence-based medicine relies on repositories of empirical research evidence that can be used to support clinical decision making for improved patient care. However, retrieving evidence from such repositories at local sites presents many challenges. This paper describes a methodological framework for automatically indexing and retrieving empirical research evidence in the form of the systematic reviews and associated studies from The Cochrane Library, where retrieved documents are specific to a patient-physician encounter and thus can be used to support evidence-based decision making at the point of care. Such an encounter is defined by three pertinent groups of concepts - diagnosis, treatment, and patient, and the framework relies on these three groups to steer indexing and retrieval of reviews and associated studies. An evaluation of the indexing and retrieval components of the proposed framework was performed using documents relevant for the pediatric asthma domain. Precision and recall values for automatic indexing of systematic reviews and associated studies were 0.93 and 0.87, and 0.81 and 0.56, respectively. Moreover, precision and recall for the retrieval of relevant systematic reviews and associated studies were 0.89 and 0.81, and 0.92 and 0.89, respectively. With minor modifications, the proposed methodological framework can be customized for other evidence repositories. © 2010 Elsevier Inc.","Abstracting and indexing as topic; Automated indexing; Computer-assisted; Decision making; Evidence-based medicine; Information storage and retrieval; Metathesaurus; Point-of-care systems; Unified Medical Language System (UMLS)","Abstracting; Automatic indexing; Decision making; Linguistics; Abstracting and indexing; Computer-assisted decision making; Evidence-based medicine; Information storage and retrieval; Point-of-care systems; Unified medical language systems; Medicine; accuracy; article; asthma; Cochrane Library; computer assisted diagnosis; documentation; evidence based medicine; information retrieval; information storage; medical informatics; medical literature; medical research; patient care; priority journal; clinical trial (topic); decision making; hospital information system; human; pediatrics; procedures; Abstracting and Indexing as Topic; Clinical Trials as Topic; Decision Making; Evidence-Based Medicine; Humans; Pediatrics; Point-of-Care Systems",2-s2.0-77954106091
"Sneyers J., Van Weert P., Schrijvers T., Koninck D.L.","As time goes by: Constraint handling rules",2010,"Theory and Practice of Logic Programming",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951219874&doi=10.1017%2fS1471068409990123&partnerID=40&md5=4d089c90c94ba91754466cf3288d77ed","Constraint Handling Rules (CHR) is a high-level programming language based on multiheaded multiset rewrite rules. Originally designed for writing user-defined constraint solvers, it is now recognized as an elegant general purpose language. Constraint Handling Rules related research has surged during the decade following the previous survey by Frhwirth (J. Logic Programming, Special Issue on Constraint Logic Programming, 1998, vol. 37, nos. 1-3, pp. 95-138). Covering more than 180 publications, this new survey provides an overview of recent results in a wide range of research areas, from semantics and analysis to systems, extensions, and applications. © 2009 Cambridge University Press.","Constraint Handling Rules (CHR); Survey","Constraint handling rules; Constraint Handling Rules (CHR); Constraint Logic Programming; Constraint solvers; General purpose languages; High-level programming language; Multiset; Research areas; Rewrite rules; High level languages; Linguistics; Logic programming; Surveys",2-s2.0-77951219874
"Paoletti E.","The migration of power and north-south inequalities: The case of Italy and Libya",2010,"The Migration of Power and North-South Inequalities: The Case of Italy and Libya",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015047052&doi=10.1057%2f9780230299283&partnerID=40&md5=ebaefc33bfabf32bf9b6a5701adb7b36","This book examines negotiations on migration in the Mediterranean. It argues that migration is a bargaining chip which countries in the South use to increase their leverage versus their counterparts in the North. This proposition opens up new understandings reframing relations of inequalities among states. © Emanuela Paoletti 2010. All rights reserved.",,,2-s2.0-85015047052
"Amini M.R., Usunier N.","Incorporating prior knowledge into a transductive ranking algorithm for multi-document summarization",2009,"Proceedings - 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449128595&doi=10.1145%2f1571941.1572087&partnerID=40&md5=b4de353b5f976ed6bb953818369c4b0c","This paper presents a transductive approach to learn ranking functions for extractive multi-document summarization. At the first stage, the proposed approach identifies topic themes within a document collection, which help to identify two sets of relevant and irrelevant sentences to a question. It then iteratively trains a ranking function over these two sets of sentences by optimizing a ranking loss and fitting a prior model built on keywords. The output of the function is used to find further relevant and irrelevant sentences. This process is repeated until a desired stopping criterion is met.","Learning to rank; Multi-document summarization","Document collection; Incorporating prior knowledge; Learning to rank; Multi-document summarization; Ranking algorithm; Ranking functions; Stopping criteria; Information services; Locomotives; Railroad cars; Information retrieval",2-s2.0-72449128595
"Zhao Y., Gao S., Liu H.","Factors influencing dependency parsing of coordinating structure",2009,"2009 International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249106663&doi=10.1109%2fNLPKE.2009.5313780&partnerID=40&md5=9089b6be6c7ac6333d391445e34fb834","Coordinating structure is a special phenomena with high frequence in human languages. It enjoys a special status in linguistic theories especially in dependency grammars, which require the exceptional treatment. This paper proposes three schemes to analyse coordinating structure and builds dependency treebanks for automatic parsing. We find that different schemes to annotate and parse the coordinating structure bring some distinctions in precision and other aspects. The factors, such as the part-of-speech categories matching with their syntactic functions, may play an important role in dependency parsing. We also calculate the mean dependency distance of three schemes, and the result shows that single-directional and smaller mean dependency distance is helpful for better machine learning and higher precision. ©2009 IEEE.","Annotation; Coordinatiing structure; Dependency distance; Dependency grammar; Parser","Automatic parsing; Dependency grammar; Dependency parsing; Human language; Linguistic theory; Machine-learning; Part Of Speech; Syntactic functions; Treebanks; Computational linguistics; Knowledge engineering; Query languages; Natural language processing systems",2-s2.0-72249106663
"Kats L.C.L., De Jonge M., Nilsson-Nyman E., Visser E.","Providing rapid feedback in generated modular language environments: Adding error recovery to scannerless generalized-LR parsing",2009,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249091863&doi=10.1145%2f1640089.1640122&partnerID=40&md5=fa8fc9192b6383ed24b0db9bb4d6af49","Integrated development environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. A heavy burden lies on developers of new languages to provide adequate IDE support. Code generation techniques provide a viable, efficient approach to semi-automatically produce IDE plugins. Key components for the realization of plugins are the language's grammar and parser. For embedded languages and language extensions, constituent IDE plugin modules and their grammars can be combined. Unlike conventional parsing algorithms, scannerless generalized-LR parsing supports the full set of context-free grammars, which is closed under composition, and hence can parse language embeddings and extensions composed from separate grammar modules. To apply this algorithm in an interactive environment, this paper introduces a novel error recovery mechanism, which allows it to be used with files with syntax errors - common in interactive editing. Error recovery is vital for providing rapid feedback in case of syntax errors, as most IDE services depend on the parser from syntax highlighting to semantic analysis and cross-referencing. We base our approach on the principles of island grammars, and derive permissive grammars with error recovery productions from normal SDF grammars. To cope with the added complexity of these grammars, we adapt the parser to support backtracking. We evaluate the recovery quality and performance of our approach using a set of composed languages, based on Java and Stratego. Copyright © 2009 ACM.","Composable languages; Embedded languages; Error recovery; Language extensions; Permissive grammars; sdf; sglr","Code Generation; Embedded Languages; Embeddings; Error recovery; Error recovery mechanisms; Integrated development environment; Interactive editing; Interactive Environments; Interactive feedback; Key component; Language extensions; LR parsing; Modular language; Parsing algorithm; Plug-ins; Programmer productivity; Rapid feedback; Recovery quality; Semantic analysis; Stratego; Syntax errors; Computational linguistics; Computer systems programming; Context free languages; Electric reactors; Errors; Integrodifferential equations; Java programming language; Linguistics; Quality control; Query languages; Recovery; Semantics; Syntactics; Object oriented programming",2-s2.0-72249091863
"Bloom B., Field J., Nystrom N., Östlund J., Richards G., Strniša R., Vitek J., Wrigstad T.","Thorn - Robust, concurrent, extensible scripting on the JVM",2009,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249109830&doi=10.1145%2f1640089.1640098&partnerID=40&md5=6a36ab5cc10dc761459e64bd61e46ceb","Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine. Copyright © 2009 ACM.","Actors; Pattern matching; Scripting","Actors; Data type; Dynamic typing; Java virtual machines; Module systems; Plug-ins; Programming language; Remote services; Scripting languages; Software life cycles; Computer systems programming; Data privacy; Linguistics; Message passing; Object oriented programming; Pattern matching; Query languages; Java programming language",2-s2.0-72249109830
"Beneventano D., Bergamaschi S., Sorrentino S.","Extending WordNet with compound nouns for semi-automatic annotation in data integration systems",2009,"2009 International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2009",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249122528&doi=10.1109%2fNLPKE.2009.5313842&partnerID=40&md5=581ad370524a1853899fc9d24b274825","The focus of data integration systems is on producing a comprehensive global schema successfully integrating data from heterogeneous data sources (heterogeneous in format and in structure). Starting from the ""meanings"" associated to schema elements (i.e. class/attribute labels) and exploiting the structural knowledge of sources, it is possible to discover relationships among the elements of different schemata. Lexical annotation is the explicit inclusion of the ""meaning"" of a data source element according to a lexical resource. Accuracy of semi-automatic lexical annotator tools is poor on real-world schemata due to the abundance of non-dictionary compound nouns. It follows that a large set of relationships among different schemata is discovered, including a great amount of false positive relationships. In this paper we propose a new method for the annotation of non-dictionary compound nouns, which draws its inspiration from works in the natural language disambiguation area. The method extends the lexical annotation module of the MOMIS data integration system. ©2009 IEEE.","Annotation; Compound noun; Data integration; WordNet","Data integration; Data integration system; Data source; False positive; Global schemas; Heterogeneous data sources; Lexical resources; Natural languages; Real-world; Semi-automatic annotation; Semi-automatics; Structural knowledge; Wordnet; Computational linguistics; Integration; Knowledge engineering; Natural language processing systems; Ontology; Query languages; Semantic Web; Data handling",2-s2.0-72249122528
"Saito C., Igarashi A.","Self type constructors",2009,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249119438&doi=10.1145%2f1640089.1640109&partnerID=40&md5=835b29aab7b9bb60e5d52ba8863c00ae","Bruce and Foster proposed the language LOOJ, an extension of Java with the notion of MyType, which represents the type of a self reference and changes its meaning along with inheritance. MyType is useful to write extensible yet type-safe classes for objects with recursive interfaces, that is, ones with methods that take or return objects of the same type as the receiver. Although LOOJ has also generics, MyType has been introduced as a feature rather orthogonal to generics. As a result, LOOJ cannot express an interface that refers to the same generic class recursively but with different type arguments. This is a significant limitation because such an interface naturally arises in practice, for example, in a generic collection class with method map(), which converts a collection to the same kind of collection of a different element type. Altherr and Cremet and Moors, Piessens, and Odersky gave solutions to this problem but they used a highly sophisticated combination of advanced mechanisms such as abstract type members, higher-order type constructors, and F-bounded polymorphism. In this paper, we give another solution by introducing self type constructors, which integrate MyType and generics so that MyType can take type arguments in a generic class. Self type constructors are tailored to writing recursive interfaces more concicely than previous solutions. We demonstrate the expressive power of self type constructors by means of examples, formalize a core language with self type constructors, and prove its type safety. Copyright © 2009 ACM.","Binary methods; Generics; MyType; Type constructor polymorphism","Abstract types; Binary method; Binary methods; Collection class; Element type; Expressive power; Generic class; Higher order; Self-references; Type arguments; Type safety; Computer systems programming; Java programming language; Linguistics; Polymorphism; Query languages; Recursive functions; Object oriented programming",2-s2.0-72249119438
"Yip A., Wang X., Zeldovich N., Kaashoek M.F.","Improving application security with data flow assertions",2009,"SOSP'09 - Proceedings of the 22nd ACM SIGOPS Symposium on Operating Systems Principles",75,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249104474&doi=10.1145%2f1629575.1629604&partnerID=40&md5=c007d358cb1a788e34f7893f14e7e543","Resin is a new language runtime that helps prevent security vulnerabilities, by allowing programmers to specify application-level data flow assertions. Resin provides policy objects, which programmers use to specify assertion code and metadata; data tracking, which allows programmers to associate assertions with application data, and to keep track of assertions as the data flow through the application; and filter objects, which programmers use to define data flow boundaries at which assertions are checked. Resin's runtime checks data flow assertions by propagating policy objects along with data, as that data moves through the application, and then invoking filter objects when data crosses a data flow boundary, such as when writing data to the network or a file. Using Resin, Web application programmers can prevent a range of problems, from SQL injection and cross-site scripting, to inadvertent password disclosure and missing access control checks. Adding a Resin assertion to an application requires few changes to the existing application code, and an assertion can reuse existing code and data structures. For instance, 23 lines of code detect and prevent three previously-unknown missing access control vulnerabilities in phpBB, a popular Web forum application. Other assertions comprising tens of lines of code prevent a range of vulnerabilities in Python and PHP applications. A prototype of Resin incurs a 33% CPU overhead running the HotCRP conference management application. Copyright 2009 ACM.","PHP; Privacy; Python; Security; SQL injection; Web; XSS","Application codes; Application data; Application security; Conference management; Cross site scripting; Data flow; Data-tracking; Lines of code; Run-time checks; Runtimes; Security vulnerabilities; SQL injection; WEB application; Web Forums; Access control; Computer operating systems; Data structures; High level languages; Metadata; Network security; Security systems; Resins",2-s2.0-72249104474
"Bilal M., Khan M.A., Ali R.","Identification of syntactic ambiguities in Pashto text",2009,"2009 International Conference on Emerging Technologies, ICET 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-76549134332&doi=10.1109%2fICET.2009.5353211&partnerID=40&md5=c789327f4c4dc5bc7641f98ae6d82228","Natural languages are inherently ambiguous. For a machine translation system, it is an essential task to resolve the ambiguities in the source language. Before resolution of ambiguities, their identification is an essential task. This research paper is about the classification and identification of syntactic ambiguities in Pashto text. Here, the identification is based on the parse trees build by the parser for the ambiguous phrases. An algorithm is proposed which correctly identifies syntactic ambiguities with a success rate of 64.3%. ©2009 IEEE.","Algorithm; Ambiguity; Pashto; Syntactic ambiguity","Ambiguity; Machine translation systems; Natural languages; Parse trees; Research papers; Source language; Information theory; Linguistics; Query languages; Semiconductor quantum dots; Speech transmission; Text processing; Syntactics",2-s2.0-76549134332
"Gao X., Qi X., Zhang S., Sheng Y., Lv G.","Research on universal GML parser based on IGSD",2009,"Proceedings - 2009 International Conference on Information Engineering and Computer Science, ICIECS 2009",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949590305&doi=10.1109%2fICIECS.2009.5365191&partnerID=40&md5=90ad454294a1f832e2e80cb409f6adbd","Functional structure and framework of a universal GML schema-based parser, along with design and implementation of the integrative grammatical and semantic database(abbreviated to IGSD) that is the core component of GML parser, have been discussed in this paper. IGSD lays the foundation for GML grammar validation and universal parsing of GML data. Whereas universal GML parser based on IGSD would provide a public platform for many GIS researches on spatial data issues, such as storage, compression, index, query, retrieval, transformation, exchange, transmission, integration and sharing, etc. ©2009 IEEE.","GML(Geography Markup Language); IGSD(Integrative Grammatical and Semantic Database); UGSP(Universal GML Schema-based Parser)","Core components; Functional structure; GML(Geography Markup Language); Spatial data; Computer science; Linguistics; Markup languages; Semantics; Database systems",2-s2.0-77949590305
"Zhang C., Huang Y., Griffin T.","Demo paper: Querying geospatial data streams in SECONDO",2009,"GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049104028&doi=10.1145%2f1653771.1653868&partnerID=40&md5=bbb2318b152f9ac160ed361ed80e25de","In this demo paper, we report our experience of designing and implementing a geospatial data stream query engine in SECONDO. Compared with current data stream and moving object query systems, our query engine not only supports point geo-streams that represent continuously moving points, but also line and region geo-streams that represent complex spatio-temporal phenomena elevated from geo-referenced raw readings. We implement the data types, operators, and language extensions as a modular algebra in SECONDO. We also provide an independent SQL parser for specifying continuous queries using SQL-like syntax as well as an Ajax-driven web interface for dynamic visualization of steaming results. © 2009 ACM.","Geospatial data streams; Spatio-temporal databases","Continuous queries; Current data; Data type; Dynamic visualization; Geo-spatial data; Language extensions; Moving objects; Query engines; Query systems; Spatio-temporal; Spatio-temporal database; SQL parser; Web interface; Data communication systems; Geographic information systems; Hydraulics; Information systems; Data visualization",2-s2.0-74049104028
"Giordani A., Moschitti A.","Semantic mapping between natural language questions and SQL queries via syntactic pairing",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651229274&doi=10.1007%2f978-3-642-12550-8_17&partnerID=40&md5=3b1dba3ecd224afc08e89175037adb23","Automatically mapping natural language semantics into programming languages has always been a major and interesting challenge in Computer Science. In this paper, we approach such problem by carrying out mapping at syntactic level and then applying machine learning algorithms to derive an automatic translator of natural language questions into their associated SQL queries. To build the required training and test sets, we designed an algorithm, which, given an initial corpus of questions and their answers, semi-automatically generates the set of possible incorrect and correct pairs. We encode such relational pairs in Support Vector Machines by means of kernel functions applied to the syntactic trees of questions and queries. The accurate results on automatic classification of the above pairs above, suggest that our approach captures the shared semantics between the two languages. © 2010 Springer-Verlag Berlin Heidelberg.","Kernel methods; Natural language processing; Support vector machines","Automatic classification; Automatic translators; Kernel function; Kernel methods; Machine learning algorithms; NAtural language processing; Natural language questions; Natural language semantics; Programming language; Semantic mapping; SQL query; Syntactic trees; Test sets; Computational linguistics; Computer programming languages; Information systems; Learning algorithms; Mapping; Query languages; Semantics; Support vector machines; Syntactics; Natural language processing systems",2-s2.0-78651229274
"Wöhrer A., Lustig T., Brezany P.","Scalable relational query results handling in service oriented architectures",2009,"International Conference for Internet Technology and Secured Transactions, ICITST 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950302758&partnerID=40&md5=907faf79b099cc32d112c3461184b70a","The focus of Grid computing shifted in recent years towards data-intensive applications. Additionally, it became more standard-based and the adherence to and usage of them has to be further fostered in order to enable and ease exchange and interoperability among the research community. The OGSA-DAI framework for data access and integration together with its default XML output format for relational data, namely Java's WebRowSet, are two important representatives of these developments. The research reported in this paper is being conducted in the context of the EU project ADMIRE. The paper's contribution is twofold and targeted on improving the scalability of query result handling in SoA. First, an XML indexing approach is elaborated to provide an out-of-core Java implementation of the standard Java WebRowSet interface to handle big XML files with low main memory consumption. Second, the concept of fine grained data statistics for data preprocessing tasks calculated on-the-fly data service side is presented. Copyright © 2009 by the Institute of Electrical and Electronics Engineers, Inc. All rights reserved.",,"Data access; Data preprocessing; Data services; Data-intensive application; Fine grained; Java implementation; Main memory; OGSA-DAI; On-the-fly; Out-of-core; Query results; Relational data; Relational queries; Research communities; XML files; XML indexing; Grid computing; Information services; Internet; Service oriented architecture (SOA); XML; Markup languages",2-s2.0-77950302758
"Davidov D., Reichart R., Rappoport A.","Superior and efficient fully unsupervised pattern-based concept acquisition using an unsupervised parser",2009,"CoNLL 2009 - Proceedings of the Thirteenth Conference on Computational Natural Language Learning",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862283510&partnerID=40&md5=8eaee64bee3926f110c59d7a6afe37a6","Sets of lexical items sharing a significant aspect of their meaning (concepts) are fundamental for linguistics and NLP. Unsupervised concept acquisition algorithms have been shown to produce good results, and are preferable over manual preparation of concept resources, which is labor intensive, error prone and somewhat arbitrary. Some existing concept mining methods utilize supervised language-specific modules such as POS taggers and computationally intensive parsers. In this paper we present an efficient fully unsupervised concept acquisition algorithm that uses syntactic information obtained from a fully unsupervised parser. Our algorithm incorporates the bracketings induced by the parser into the meta-patterns used by a symmetric patterns and graph-based concept discovery algorithm. We evaluate our algorithm on very large corpora in English and Russian, using both human judgments and WordNetbased evaluation. Using similar settings as the leading fully unsupervised previous work, we show a significant improvement in concept quality and in the extraction of multiword expressions. Our method is the first to use fully unsupervised parsing for unsupervised concept discovery, and requires no languagespecific tools or pattern/word seeds. © 2009 Association for Computational Linguistics.",,"Concept mining; Discovery algorithm; Error prones; Graph-based; Human judgments; Labor intensive; Lexical items; Multiword expressions; PoS taggers; Symmetric patterns; Syntactic information; Mergers and acquisitions; Mining; Natural language processing systems; Algorithms",2-s2.0-84862283510
"Sasaki Y., Thompson P., McNaught J., Ananiadou S.","Three BioNLP tools powered by a biological lexicon",2009,"EACL 2009 - 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888283228&partnerID=40&md5=848dc6410c7545020ea43abbc104175d","In this paper, we demonstrate three NLP applications of the BioLexicon, which is a lexical resource tailored to the biology domain. The applications consist of a dictionary-based POS tagger, a syntactic parser, and query processing for biomedical information retrieval. Biological terminology is a major barrier to the accurate processing of literature within biology domain. In order to address this problem, we have constructed the BioLexicon using both manual and semiautomatic methods. We demonstrate the utility of the biology-oriented lexicon within three separate NLP applications. © 2009 Association for Computational Linguistics.",,"Biomedical information retrieval; Lexical resources; PoS taggers; Semiautomatic methods; Syntactic parsers; Computational linguistics; Natural language processing systems; Biology",2-s2.0-84888283228
"Mitra A., Vieira M.R., Bakalov P., Najjar W., Tsotras V.J.","Boosting XML filtering with a scalable FPGA-based architecture",2009,"CIDR 2009 - 4th Biennal Conference on Innovative Data Systems Research",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858633077&partnerID=40&md5=675f8c20a9a49d6b93abf5a47403491e","The growing amount of XML encoded data exchanged over the Internet increases the importance of XML based publish-subscribe (pub-sub) and content based routing systems. The input in such systems typically consists of a stream of XML documents and a set of user subscriptions expressed as XML queries. The pub-sub system then filters the published documents and passes them to the subscribers. Pub-sub systems are characterized by very high input ratios, therefore the processing time is critical. In this paper we propose a ""pure hardware"" based solution, which utilizes XPath query blocks on FPGA to solve the filtering problem. By utilizing the high throughput that an FPGA provides for parallel processing, our approach achieves drastically better throughput than the existing software or mixed (hardware/software) architectures. The XPath queries (subscriptions) are translated to regular expressions which are then mapped to FPGA devices. By introducing stacks within the FPGA we are able to express and process a wide range of path queries very efficiently, on a scalable environment. Moreover, the fact that the parser and the filter processing are performed on the same FPGA chip, eliminates expensive communication costs (that a multi-core system would need) thus enabling very fast and efficient pipelining. Our experimental evaluation reveals more than one order of magnitude improvement compared to traditional pub/sub systems.",,"Communication cost; Content based routing; Experimental evaluation; Filter processing; Filtering problems; FPGA chips; FPGA devices; FPGA-based architectures; Hardware/software; High throughput; Multi-core systems; Parallel processing; Path queries; Processing time; Pub/sub; Publish-subscribe; Regular expressions; User subscription; XML filtering; XML queries; Xpath queries; Electronic data interchange; Query languages; XML",2-s2.0-84858633077
"Kop C.","Background information for checking completeness of a conceptual database schema",2009,"Proceedings of the 13th IASTED International Conference on Software Engineering and Applications, SEA 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954191782&partnerID=40&md5=9d589972df3b772f5989cb739b8bbe4f","Since a database schema is the backbone of any data centric software, the schema should be continually checked in order to get the proper quality. Using a controlled natural query language is one option to check a database schema together with end users. This paper describes what background information must be considered if a controlled language is used. In that sense, it is strongly related to work on natural language queries for databases. However those research areas need a schema which is already stable and filled with values. In contrary to that, this approach neither assumes that the database schema is stable nor that the stakeholders already collected the values.","Conceptual modelling; Controlled natural language queries; Database design; Schema quality","Background information; Conceptual modelling; Controlled natural language; Data centric; Database design; Database schemas; End users; Natural language queries; Research areas; Computer software selection and evaluation; Linguistics; Query languages; Software engineering; Quality control",2-s2.0-77954191782
"Feng J., Bangalore S.","Effects of word confusion networks on voice search",2009,"EACL 2009 - 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891594279&partnerID=40&md5=90e5dbe4b0b36cf3a6bc5f66733fa414","Mobile voice-enabled search is emerging as one of the most popular applications abetted by the exponential growth in the number of mobile devices. The automatic speech recognition (ASR) output of the voice query is parsed into several fields. Search is then performed on a text corpus or a database. In order to improve the robustness of the query parser to noise in the ASR output, in this paper, we investigate two different methods to query parsing. Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser. We also investigate the results of this improvement on search accuracy. Word confusion-network based query parsing outperforms ASR 1-best based query-parsing by 2.7% absolute and the search performance improves by 1.8% absolute on one of our data sets. © 2009 Association for Computational Linguistics.",,"Automatic speech recognition; Exponential growth; Multiple hypothesis; Search accuracy; Search performance; Text corpora; Voice searches; Word confusion networks; Computational linguistics; Mobile devices; Query processing",2-s2.0-84891594279
"Goyal P., Behera L., McGinnity T.M.","Entailment of causal queries in narratives using action language",2009,"KDIR 2009 - 1st International Conference on Knowledge Discovery and Information Retrieval, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955367901&partnerID=40&md5=8ab3f8bfd0453188f32c7daf413e4cf1","In this paper, Action Language formalism has been used to reason about narratives in a multi agent framework. The actions have been given a semantic frame representation. Hypothetical situations have been dealt using different states for world knowledge and agents' knowledge. A notion of plan recognition has been proposed to answer causal queries. Finally, an algorithm has been proposed for automatically translating a given narrative into the representation and causal query entailment has been shown.","Plan recognition; Question answering","Action language; Multiagent framework; Plan recognition; Question Answering; World knowledge; Information retrieval; Linguistics; Natural language processing systems",2-s2.0-77955367901
"Leveling J., Hartrumpf S.","Integrating methods from IR and QA for geographic information retrieval",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549103580&doi=10.1007%2f978-3-642-04447-2_111&partnerID=40&md5=edf627cdadd65e3ec312604210f433b0","This paper describes the participation of GIRSA at GeoCLEF 2008, the geographic information retrieval task at CLEF. GIRSA combines information retrieval (IR) on geographically annotated data and question answering (QA) employing query decomposition. For the monolingual German experiments, several parameter settings were varied: using a single index or separate indexes for content and geographic annotation, using complex term weighting, adding location names from the topic narrative, and merging results from IR and QA, which yields the highest mean average precision (0.2608 MAP). For bilingual experiments, English and Portuguese topics were translated via the web services Applied Language Solutions, Google Translate, and Promt Online Translator. For both source languages, Google Translate seems to return the best translations. For English (Portuguese) topics, 60.2% (80.0%) of the maximum MAP for monolingual German experiments, or 0.1571 MAP (0.2085 MAP), is achieved. As a post-official experiment, translations of English topics were analysed with a parser. The results were employed to select the best translation for topic titles and descriptions. The corresponding retrieval experiment achieved 69.7% of the MAP of the best monolingual experiment. © 2009 Springer Berlin Heidelberg.",,"Complex term; Geographic information retrieval; Online translators; Parameter setting; Query decomposition; Question Answering; Source language; Information retrieval; Information services; Internet; Linguistics; Natural language processing systems; Query languages; Translation (languages); Experiments",2-s2.0-70549103580
"Rosenmüller M., Kästner C., Siegmund N., Sunkle S., Apel S., Leich T., Saake G.","SQL à la carte - Toward tailor-made data management",2009,"Datenbanksysteme in Business, Technologie und Web, BTW 2009 - 13th Fachtagung des GI-Fachbereichs ""Datenbanken und Informationssysteme"" (DBIS), Proceedings",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70249144845&partnerID=40&md5=0bb016e54aec0f04b63c7442cd0da3dd","The size of the structured query language (SQL) continuously increases. Extensions of SQL for special domains like stream processing or sensor networks come with own extensions, more or less unrelated to the standard. In general, underlying DBMS support only a subset of SQL plus vendor specific extensions. In this paper, we analyze application domains where special SQL dialects are needed or are already in use. We show how SQL can be decomposed to create an extensible family of SQL dialects. Concrete dialects, e.g., a dialect for web databases, can be generated from such a family by choosing SQL features à la carte. A family of SQL dialects simplifies analysis of the standard when deriving a concrete dialect, makes it easy to understand parts of the standard, and eases extension for new application domains. It is also the starting point for developing tailor-made data management solutions that support only a subset of SQL. We outline how such customizable DBMS can be developed and what benefits, e.g., improved maintainability and performance, we can expect from this.",,"Customizable; Data management solution; New applications; Stream processing; Structured query languages; Web database; Concretes; Query languages; Sensor networks; Information management",2-s2.0-70249144845
"Le A.-C., Nguyen P.-T., Vuong H.-T., Pham M.-T., Ho T.-B.","An experimental study on lexicalized statistical parsing for Vietnamese",2009,"KSE 2009 - The 1st International Conference on Knowledge and Systems Engineering",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951432377&doi=10.1109%2fKSE.2009.41&partnerID=40&md5=4b35d9ce06d6fb9dfb8716e5672da0be","Syntactic parsing is a central problem and a challenge in the field of natural language processing. It attracts many studies and consequently there exists the effective parsers for several popular languages such as English and Chinese. For Vietnamese parsing, there have been a few studies focusing on this problem, these studies lack of applying modern techniques, and no popular parser has been released. This paper presents the first study on developing a Vietnamese wide coverage parser based on lexicalized probabilistic context free grammar (LPCFG) and using a standard parsed corpus (similar to Penn Treebank). In this paper the Bikel's parser is modified to analyze Vietnamese. We also provide a comparison based on investigating different parsing models and different linguistic features. The best configuration achieves around 78% of F-score. © 2009 IEEE.",,"Central problems; Experimental studies; F-score; Linguistic features; NAtural language processing; Parsed corpora; Probabilistic context free grammars; Syntactic parsing; Treebanks; Computational linguistics; Context free grammars; Knowledge engineering; Natural language processing systems; Query languages; Standardization; Systems engineering; Formal languages",2-s2.0-77951432377
"Al Hajjar A.E.S., Hajjar M., Zreik K.","A new structured and evolutionary electronic dictionary for the Arabic language: DESELA [Un nouveau dictionnaire électronique structuré et évolutif de la langue arabe: DESELA]",2009,"Patrimoine 3.0 - Actes du douziemz Colloque International sur le Document Electronique, CIDE.12",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875110709&partnerID=40&md5=1dcb840ee0632c5cbc33283fbd976711","In this article, we propose a new structured and progressive electronic dictionary for the Arab language (DESELA) which can be presented in the form of a relational database or in the form of an XML document which can be easily exploitable using suitable query languages. Indeed, many Arab dictionaries are found but are not directly exploitable since they are in flat textual files form. DESELA contains the roots, the prefixes, the suffixes, the infixes, the patterns and the derived words. Moreover, for a given word, it provides links to its root, to their associated affixes, and to its possible pattern. DESELA is supplied automatically from one or several traditional textual dictionaries and is enriched permanently with any Arab textual corpus using system that we built. This system is composed of a parser, a classifier, a comparator and an analyzer. The parser allows transforming a textual source (dictionary or textual corpus) into a set of words. The classifier allows to classify a given word and to add it to DESELA as a root or a derived word. The analyzer allows extracting the affixes and the model from a derived word and of its root. The comparator permits to avoid duplication of roots, affixes or patterns in DESELA. This dictionary can be used to evaluate the information extraction methods from an Arab document, given that; the vocabulary of the Arab language is essentially built from the roots. In general, an Arab word is built from its root while adding to it the affixes (prefix, infix, or suffix) according to a precise pattern. Most methods of information extraction starting from an Arab document proceed conversely by extracting the root from the mot.","Arabic language; Corpus; Dictionary; Information extraction; Root","Arabic languages; Corpus; Electronic dictionaries; Evolutionary electronics; Information Extraction; Information extraction methods; Relational Database; Root; Comparators (optical); Glossaries; Information retrieval; Query languages; Word processing",2-s2.0-84875110709
"Kalpathy-Cramer J., Bedrick S., Hatt W., Hersh W.","Multimodal medical image retrieval OHSU at imageCLEF 2008",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549113744&doi=10.1007%2f978-3-642-04447-2_96&partnerID=40&md5=5b9f9a9df2eada8d33ba0d36ab6c3f68","We present results from the Oregon Health & Science University's participation in the medical retrieval task of ImageCLEF 2008. Our web-based retrieval system was built using a Ruby on Rails framework. Ferret, a Ruby port of Lucene was used to create the full-text based index and search engine. In addition to the textual index of annotations, supervised machine learning techniques using visual features were used to classify the images based on image acquisition modality. Our system provides the user with a number of search options including the ability to limit their search by modality, UMLS-based query expansion, and Natural Language Processing-based techniques. Purely textual runs as well as mixed runs using the purported modality were submitted. We also submitted interactive runs using user specified search options. Although the use of the UMLS metathesaurus increased our recall, our system is geared towards early precision. Consequently, many of our multimodal automatic runs using the custom parser as well as interactive runs had high early precision including the highest P10 and P30 among the official runs. Our runs also performed well using the bpref metric, a measure that is more robust in the case of incomplete judgments. © 2009 Springer Berlin Heidelberg.",,"ImageCLEF; Medical retrieval tasks; Multi-modal; Multimodal medical images; NAtural language processing; Query expansion; Retrieval systems; Ruby on Rails; Supervised machine learning; UMLS metathesaurus; Visual feature; Computational linguistics; Corundum; Image retrieval; Learning algorithms; Natural language processing systems; Ruby; Search engines; Image acquisition",2-s2.0-70549113744
"Deeptimahanti D.K., Babar M.A.","An automated tool for generating UML models from natural language requirements",2009,"ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952151933&doi=10.1109%2fASE.2009.48&partnerID=40&md5=84b58877db97fd83c9fb3be6fda8fa63","This paper describes a domain independent tool, named, UML Model Generator from Analysis of Requirements (UMGAR), which generates UML models like the Use-case Diagram, Analysis class model, Collaboration diagram and Design class model from natural language requirements using efficient Natural Language Processing (NLP) tools. UMGAR implements a set of syntactic reconstruction rules to process complex requirements into simple requirements. UMGAR also provides a generic XMI parser to generate XMI files for visualizing the generated models in any UML modeling tool. With respect to the existing tools in this area, UMGAR provides more comprehensive support for generating models with proper relationships, which can be used for large requirement documents. © 2009 IEEE.","Natural language processing; Requirement engineering; Unified modeling language","Automated tools; Case diagram; Class models; Collaboration diagram; NAtural language processing; Natural language requirements; Requirement engineering; UML Model; UML modeling tools; XMI-file; Automation; Computational linguistics; Computer software; Natural language processing systems; Query languages; XML; Unified Modeling Language",2-s2.0-77952151933
"Apresjan J.D., Boguslavsky I.M., Iomdin L.L., Cinman L.L., Timoshenko S.P.","Semantic paraphrasing for information retrieval and extraction",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549113664&doi=10.1007%2f978-3-642-04957-6_44&partnerID=40&md5=ec3a0046d981119decf7231fc8bc6128","The paper is devoted to the development of a system of synonymous and quasi-synonymous paraphrasing and its practical applications, first of all in the domain of search engine optimization and information extraction. This system is part of the ETAP-3 multifunctional NLP environment created by the Laboratory of Computational Linguistics of the Kharkevich Institute for Information Transmission Problems. Combinatorial dictionaries of Russian, English and some other languages and a rule-driven parser constitute the core of ETAP-3 while a variety of generating modules are used in a number of applications. The paraphrase generator, based on the apparatus of lexical functions, is one such module. We describe the general layout of the paraphrase generator and discuss an experiment that demonstrates its potential as a tool for search optimization. © 2009 Springer-Verlag Berlin Heidelberg.","Information extraction; Information retrieval; Lexical functions; Paraphrase generator","Information extraction; Information retrieval and extraction; Information transmission; Rule-driven; Search engine optimizations; Search optimization; Computational linguistics; Information services; Query languages; Search engines; Information retrieval",2-s2.0-70549113664
"Hoffman D., Wang H.-Y., Chang M., Ly-Gagnon D.","Grammar based testing of HTML injection vulnerabilities in RSS feeds",2009,"TAIC PART 2009 - Testing: Academic and Industrial Conference - Practice and Research Techniques",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949804587&doi=10.1109%2fTAICPART.2009.34&partnerID=40&md5=c6dd98a781013178b87fa8eaa66482dc","Grammar based test generation (GBTG) has seen extensive study and considerable practical use since the 1970s. GBTG was introduced to generate source code for testing compilers from context-free grammars specifying language syntax. More recently, GBTG has been applied to many other testing problems, including the generation of eXtensible Markup Language (XML) documents. Recent research has shown how to integrate covering-array generation into GBTG tools. While the integration offers considerable power to the tester, there are few practical demonstrations in the literature. We present a case study showing how to use grammars and covering arrays for effective input generation. The generated tests have been used to systematically evaluate an RSS feed parser for HTML injection vulnerabilities. © 2009 IEEE.",,"Array generation; Covering arrays; Extensible markup language; Grammar-based testing; Language syntax; Practical use; Source codes; Test generations; Testing compilers; HTML; Hypertext systems; Linguistics; Markup languages; Query languages; Research; Context free languages",2-s2.0-77949804587
"Sak H., Saraçlar M., Güngör T.","Integrating morphology into automatic speech recognition",2009,"Proceedings of the 2009 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2009",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949357555&doi=10.1109%2fASRU.2009.5373386&partnerID=40&md5=a65245819c7ddb8b4f51d10aafda5546","This paper proposes a novel approach to integrate the morphology as a model into an automatic speech recognition (ASR) system for morphologically rich languages. The high out-of-vocabulary (OOV) word rates have been a major challenge for ASR in morphologically productive languages. The standard approach to this problem has been to shift from words to subword units in language modeling, and the only change to the system is in the language model estimated over these units. In contrast, we propose to integrate the morphology as other any knowledge source - such as the lexicon, and the language model - directly into the search network. The morphological parser for a language, implemented as a finite-state lexical transducer, can be considered as a computational lexicon. The computational lexicon represents a dynamic vocabulary in contrast to a static vocabulary generally used for ASR. We compose the transducer for this computational lexicon with a statistical language model over lexical morphemes to obtain a morphology-integrated search network. The resulting search network generates only grammatical word forms and improves the recognition accuracy due to reduced OOV rate. We give experimental results for Turkish broadcast news transcription, and show that it outperforms the 50K and 100K vocabulary word models while the 200K vocabulary word model is slightly better. © 2009 IEEE.",,"Automatic speech recognition; Automatic speech recognition system; Broadcast news; Finite-state; Knowledge sources; Language model; Language modeling; Lexical transducer; Recognition accuracy; Statistical language models; Subword units; Turkishs; Word models; Computational linguistics; Morphology; Natural language processing systems; Query languages; Remelting; Transducers; Speech recognition",2-s2.0-77949357555
"Furr M., An J.-H., Foster J.S., Hicks M.","The Ruby Intermediate Language",2009,"Proceedings of the 5th Dynamic Languages Symposium, DLS '09, Co-located with the 24th Annual ACM Conference on Object-Oriented Programming, Systems, Languages and Applications, OOPSLA 2009",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956333736&doi=10.1145%2f1640134.1640148&partnerID=40&md5=bc0205cb6d3ad521c0841dddbce90edb","Ruby is a popular, dynamic scripting language that aims to ""feel natural to programmers"" and give users the ""freedom to choose"" among many different ways of doing the same thing. While this arguably makes programming in Ruby easier, it makes it hard to build analysis and transformation tools that operate on Ruby source code. In this paper, we present the Ruby Intermediate Language (RIL), a Ruby front-end and intermediate representation that addresses these challenges. RIL includes an extensible GLR parser for Ruby, and an automatic translation into an easy-to-analyze intermediate form. This translation eliminates redundant language constructs, unravels the often subtle ordering among side effecting operations, and makes implicit interpreter operations explicit.We also describe several additional useful features of RIL, such as a dynamic instrumentation library for profiling source code and a dataflow analysis engine. We demonstrate the usefulness of RIL by presenting a static and dynamic analysis to eliminate null pointer errors in Ruby programs. We hope that RIL's features will enable others to more easily build analysis tools for Ruby, and that our design will inspire the creation of similar frameworks for other dynamic languages. Copyright © 2009.","Intermediate Language; Profile-guided analysis; RIL; Ruby","Analysis tools; Automatic translation; Dynamic instrumentation; Dynamic languages; Intermediate languages; Intermediate representations; Language constructs; Profile-guided analysis; RIL; Scripting languages; Source codes; Static and dynamic analysis; Transformation tools; Computer systems programming; Data flow analysis; Dynamic analysis; Dynamic light scattering; Linguistics; Program interpreters; Query languages; Ruby; Technical presentations; Translation (languages); Object oriented programming",2-s2.0-77956333736
"Hartrumpf S., Glöckner I., Leveling J.","Efficient question answering with question decomposition and multiple answer streams",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549087074&doi=10.1007%2f978-3-642-04447-2_49&partnerID=40&md5=debf63e7fd66c909c5b6883dbc0e1c98","The German question answering (QA) system IRSAW (formerly: InSicht) participated in QA@CLEF for the fifth time. IRSAW was introduced in 2007 by integrating the deep answer producer InSicht, several shallow answer producers, and a logical validator. InSicht builds on a deep QA approach: it transforms documents to semantic representations using a parser, draws inferences on semantic representations with rules, and matches semantic representations derived from questions and documents. InSicht was improved for QA@CLEF 2008 mainly in the following two areas. The coreference resolver was trained on question series instead of newspaper texts in order to be better applicable for follow-up questions. Questions are decomposed by several methods on the level of semantic representations. On the shallow processing side, the number of answer producers was increased from two to four by adding FACT, a fact index, and SHASE, a shallow semantic network matcher. The answer validator introduced in 2007 was replaced by the faster RAVE validator designed for logic-based answer validation under time constraints. Using RAVE for merging the results of the answer producers, monolingual German runs and bilingual runs with source language English and Spanish were produced by applying the machine translation web service Promt. An error analysis shows the main problems for the precision-oriented deep answer producer InSicht and the potential offered by the recall-oriented shallow answer producers. © 2009 Springer Berlin Heidelberg.",,"Coreference; Machine translations; Question Answering; Question answering systems; Semantic network; Semantic representation; Source language; Time constraints; Computer aided language translation; Error analysis; Information theory; Linguistics; Query languages; Semantics; Speech transmission; Natural language processing systems",2-s2.0-70549087074
"Demaille A., Levillain R., Sigoure B.","TWEAST: A simple and effective technique to implement concrete-syntax AST rewriting using partial parsing",2009,"Proceedings of the ACM Symposium on Applied Computing",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949105118&doi=10.1145%2f1529282.1529710&partnerID=40&md5=81d38a6cdfe01e9044be20fd104315e5","Abstract Syntax Trees (ASTs) are commonly used to represent an input/output program in compilers and language processing tools. Many of the tasks of these tools consist in generating and rewriting ASTs. Such an approach can become tedious and hard to maintain for complex operations, namely program transformation, optimization, instrumentation, etc. On the other hand, concrete syntax provides a natural and simpler representation of programs, but it is not usually available as a direct feature of the aforementioned tools. We propose a simple technique to implement AST generation and rewriting in general purpose languages using concrete syntax. Our approach relies on extensions made in the scanner and the parser and the use of objects supporting partial parsing called Texts With Embedded Abstract Syntax Trees (TWEASTs). A compiler for a simple language (Tiger) written in C++ serves as an example, featuring transformations in concrete syntax: syntactic desugaring, optimization, code instrumentation such as bounds-checking, etc. Extensions of this technique to provide a full-fledged concrete-syntax rewriting framework are presented as well. Copyright 2009 ACM.","C++; Compiler design; Concrete syntax; Parsing; Program transformation; Rewrite rules","Abstract Syntax Trees; Code instrumentation; Compiler design; Complex operations; Concrete syntax; General purpose languages; Input/output; Language processing; Partial parsing; Program transformations; Rewrite rules; Abstracting; Computer science; Computer software; Hand tools; Instruments; Linguistics; Query languages; Syntactics; XML; Program compilers",2-s2.0-72949105118
"Schneiker C., Seipel D., Wegstein W., Prätor K.","Declarative parsing and annotation of electronic dictionaries",2009,"Natural Language Processing and Cognitive Science - Proceedings of the 6th International Workshop on Natural Language Processing and Cognitive Science - NLPCS 2009 In Conjunction with ICEIS 2009",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549156281&partnerID=40&md5=b664d23b9bdbf5b77f87a2a26731ae75","We present a declarative annotation toolkit based on XML and PROLOG technologies, and we apply it for annotating the Campe Dictionary to obtain an electronic version in XML (TEI). For parsing flat structures, we use a very compact grammar formalism called extended definite clause grammars (EDcG's), which is an extended version of the DcG's that are well-known from the logic programming language PROLOG. For accessing and transforming XML structures, we use the XML query and transformation language FNQUERY. It turned out, that the declarative approach in PROLOG is much more readable, reliable, flexible, and faster than an alternative implementation which we had made in JAVA and XsLT for the TEXTGRID community project.",,"Community project; Definite clause grammars; Electronic dictionaries; Electronic versions; Extended versions; Grammar formalisms; Logic programming languages; XML queries; Computational linguistics; Formal languages; Java programming language; Logic programming; Markup languages; Natural language processing systems; XML; PROLOG (programming language)",2-s2.0-74549156281
"Wu J., Huang S.-Y.","An efficient mapping schema for storing and accessing XML data in relational databases",2009,"International Journal of Web Information Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886387777&doi=10.1108%2f17440080910983574&partnerID=40&md5=6069e3778d07d59bf475c01470ccaf54","Purpose - The purpose of this paper is to reduce the number of join operations for retrieving Extensible Markup Language (XML) data from a relational database. Design/methodology/approach - The paper proposes a new approach to eliminate the join operations for parent-child traversing and/or sibling searching such that the performance of query processing could be improved. The rationale behind the design of the proposed approach is to distribute the structural information into relational databases. Findings - The paper finds that the number of join operations which are needed for processing parent-child traversal and sibling search can be bounded under the proposed approach. It also verifies the capability of the proposed approach by a series of experiments based on the XMark benchmark, for which it has encouraging results. Research limitations/implications - Compared with previous approaches based on the structure encoding method, the proposed approach needs more space to store additional immediate predecessor's IDs. However, the approach has similar performance to others and it is much easier to implement. Practical implications - The experimental results show that the performance of the proposed approach is less than 3 per cent of the well-known MonetDB approach for processing benchmark queries. Moreover, its bulkloading time is much less than that for the MonetDB. There is no doubt that the approach is efficient for accessing XML data with acceptable overheads. Originality/value - This paper contributes to the implementations of XML database systems. © Emerald Group Publishing Limited.","Database management; Extensible Markup Language; Relational databases","Database management; Design/methodology/approach; Encoding methods; Extensible Mark-Up language (XML); Join operation; New approaches; Relational Database; Structural information; Benchmarking; Query languages; XML",2-s2.0-84886387777
"Karimpour R., Ghorbani A., Pishdad A., Mohtarami M., Aleahmad A., Amiri H., Oroumchian F.","Improving persian information retrieval systems using stemming and part of speech tagging",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549100196&doi=10.1007%2f978-3-642-04447-2_10&partnerID=40&md5=ce99aa484aabe179b6df59d2f81a95e5","With the emergence of vast resources of information, it is necessary to develop methods that retrieve the most relevant information according to needs. These retrieval methods may benefit from natural language constructs to boost their results by achieving higher precision and recall rates. In this study, we have used part of speech properties of terms as extra source of information about document and query terms and have evaluated the impact of such data on the performance of the Persian retrieval algorithms. Furthermore the effect of stemming has been experimented as a complement to this research. Our findings indicate that part of speech tags may have small influence on effectiveness of the retrieved results. However, when this information is combined with stemming it improves the accuracy of the outcomes considerably. © 2009 Springer Berlin Heidelberg.","Natural language; Part of speech; Persian information retrieval","Natural languages; Part Of Speech; Part of speech tagging; Part-of-speech tags; Persian information retrieval; Persians; Precision and recall; Query terms; Retrieval algorithms; Retrieval methods; Information retrieval; Information retrieval systems; Information services; Linguistics",2-s2.0-70549100196
"Sun G.-L., Xue Y., Xu Z., Lang F.","Chinese chunking based on coarse-grained part-of-speech features",2009,"2009 International Conference on Asian Language Processing: Recent Advances in Asian Language Processing, IALP 2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950889556&doi=10.1109%2fIALP.2009.54&partnerID=40&md5=674375e31670a33c2b5faffdf6510bb4","Although part-of-speech (POS) is an effective feature for Chinese Chunking, the POS-tagging errors generated by automatic POS tagger leads to almost 10% performance drop in F-score. To solve this problem, this paper presents new features to replace the POS features, namely the coarse-grained part-of-speech features. Combining with the methods of processing out-of-vocabulary words, the new features are utilized in the Chinese chunking model. Experimental results show that the new features can contribute 2.71% performance improvement over the baseline method. © 2009 IEEE.","Chunking; Coarse-grained features; Part-of-speech; Propagated errors","Baseline methods; Coarse-grained; Coarse-grained features; F-score; Out-of-vocabulary words; Part Of Speech; Performance improvements; PoS taggers; Errors; Linguistics; Query languages",2-s2.0-77950889556
"Padioleau Y., Tan L., Zhou Y.","Listening to Programmers - Taxonomies and characteristics of comments in operating system code",2009,"Proceedings - International Conference on Software Engineering",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949880220&doi=10.1109%2fICSE.2009.5070533&partnerID=40&md5=97a2f698b39aaabe279ebe4ff88b70f5","Innovations from multiple directions have been proposed to improve software reliability. Unfortunately, many of the innovations are not fully exploited by programmers. To bridge the gap, this paper proposes a new approach to ""listen"" to thousands of programmers: studying their programming comments. Since comments express programmers' assumptions and intentions, comments can reveal programmers' needs, which can provide guidance (1) for language/-tool designers on where they should develop new techniques or enhance the usability of existing ones, and (2) for programmers on what problems are most pervasive and important so that they should take initiatives to adopt some existing tools or language extensions. We studied 1050 comments randomly sampled from the latest versions of Linux, FreeBSD, and OpenSolaris. We found that 52.6% of these comments could be leveraged by existing or to-be-proposed tools for improving reliability. Our findings include: (1) many comments describe code relationships, code evolutions, or the usage and meaning of integers and integer macros, (2) a significant amount of comments could be expressed by existing annotation languages, and (3) many comments express synchronization related concerns but are not well supported by annotation languages. © 2009 IEEE.",,"Annotation languages; FreeBSD; Language extensions; New approaches; Operating systems; Tool designers; Computer operating systems; Computer software selection and evaluation; Linguistics; Query languages; Software engineering; Taxonomies; Software reliability",2-s2.0-77949880220
"Dagand P.-E., Baumann A., Roscoe T.","Filet-o-Fish: Practical and dependable domain-specific languages for OS development",2009,"Proceedings of the 5th Workshop on Programming Languages and Operating Systems, PLOS 2009, in Conjunction with the 22nd ACM Symposium on Operating Systems Principles, SOSP 2009",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954599554&doi=10.1145%2f1745438.1745446&partnerID=40&md5=39a46e06c847f7feb9887ca54b13929e","We address a persistent problem with using domain-specific languages to write operating systems: the effort of implementing, checking, and debugging the DSL usually outweighs any of its benefits. Because these DSLs generate C by templated string concatenation, they are tedious to write, fragile, and incompatible with automated verification tools. We present Filet-o-Fish (FoF), a semantic language to ease DSL construction. Building a DSL using FoF consists of safely composing semantically-rich building blocks. This has several advantages: input files for the DSL are formal specifications of the system's functionality, automated testing of the DSL is possible via existing tools, and we can prove that the C code generated by a given DSL respects the semantics expected by the developer. Early experience has been good: FoF is in daily use as part of the tool chain of the Barrelfish multicore OS, which makes extensive use of domain-specific languages to generate low-level OS code. We have found that the ability to rapidly generate DSLs we can rely on has changed how we have designed the OS. © 2010 ACM.",,"Automated testing; Automated verification; Building blockes; C codes; Daily use; Domain specific languages; Formal Specification; Input files; Multi core; Operating systems; Semantic language; Templated; Computer operating systems; Problem oriented languages; Query languages; Semantics; Linguistics",2-s2.0-77954599554
"Hao Z., Kun T., Jiang Y.","An inference engine with human memory simulation on individuals' activities",2009,"Proceedings of the International Symposium on Test and Measurement",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951160752&doi=10.1109%2fICTM.2009.5412888&partnerID=40&md5=f50c8ff433123c4dae914f8ef0e40876","One of the challenging tasks in AI is to read and do reasoning on languages in an automatic way as the human does. We present an intelligent inference system to predict people's activities from English sentences in a simulated human memory model. Our system analyses input sentences with refined natural language processing technologies and stores the extracted information as new memory in a simulated human memory model. The new memory is then processed together with old information in self-generated Bayesian networks for inferring new and unified predictions. To ensure that the reasoning is reasonable, we proposed efficient mechanisms to propagate information, to construct and connect Bayesian networks. We use a series of experiment to compare our system to other reasoning systems such as Direct Memory Access (DMAP) system. The results show an improvement of our system on the deduction of more useful information than the compared systems. With a better disambiguation of confusing data in the knowledgebase to avoid some faulty reasoning, our system is also adaptive to different scenarios. ©2009 IEEE.","Artificial intelligence; Bayesian network; Inference; Natural language processing","Bayesian Network inference; Direct memory access; English sentences; Human memory; Inference; Inference systems; Knowledge base; NAtural language processing; Reasoning system; System analysis; Artificial intelligence; Computational linguistics; Distributed parameter networks; Inference engines; Intelligent networks; Natural language processing systems; Query languages; Security of data; Speech recognition; Bayesian networks",2-s2.0-77951160752
"Martínez-González Á., De Pablo-Sánchez C., Polo-Bayo C., Vicente-Díez M.T., Martínez-Fernández P., Martínez-Fernández J.L.","The MIRACLE Team at the CLEF 2008 Multilingual Question Answering Track",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549106244&doi=10.1007%2f978-3-642-04447-2_48&partnerID=40&md5=e8d599ab26a75b807b8ec8d6489efa73","The MIRACLE team participated in the monolingual Spanish and cross-language French to Spanish subtasks at QA@CLEF 2008. For the Spanish subtask, we used an almost completely rebuilt version of our system, designed with the aim of flexibly combining information sources and linguistic annotators for different languages. To allow easy development for new languages, most of the modules do not make any language dependent assumptions. The language dependent knowledge is encapsulated in a rule language developed within the MIRACLE team. By the time of submitting the runs, work on the new version was still ongoing, so we consider the results as a partial test of the possibilities of the new architecture. Subsystems for other languages were not yet available, so we tried a very simple approach for the French to Spanish subtask: questions were translated to Spanish with Babylon, and the output of this translation was fed into our system. The results had an accuracy of 16% for the monolingual Spanish task and 5% for the cross-language task. © 2009 Springer Berlin Heidelberg.",,"Information sources; Question Answering track; Simple approach; Subtasks; Content based retrieval; Query languages; Translation (languages); Linguistics",2-s2.0-70549106244
"Sarasa-Cabezuelo A., Martínez-Avilés A., Sierra J.-L., Fernández-Valmayor A.","A generative approach to the construction of application-specific XML processing components",2009,"Conference Proceedings of the EUROMICRO",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549184149&doi=10.1109%2fSEAA.2009.14&partnerID=40&md5=deb27902bf0dfcbf570cd6b0b8552596","This paper proposes a generative approach to the construction of XML processing components. This approach promotes the high-level description of XML processing tasks with attribute grammars (a high-level formalism used in the definition of computer languages). The components themselves are produced by automatically processing these high-level specifications with a suitable generator. The approach substantially enhances the construction and maintenance of task-specific XML processing components compared to hand-coding or more rigid generative solutions. In order to illustrate the approach, we will show how XLOP (XML Language-Oriented Processing), an XML processing environment based on these concepts, is used for the development of an XML-based courseware system in the e-Learning domain. © 2009 IEEE.","Attribute grammars; Courseware; Generative approach; Software components; XML","Application-Specific; Attribute grammars; Computer language; Courseware; Generative approach; High level description; High level specification; Software component; XML languages; XML processing; Computer software; Computer software maintenance; Context sensitive grammars; Linguistics; Markup languages; Query languages; XML; High level languages",2-s2.0-74549184149
"Hettige B., Karunananda A.S.","Theoretical based approach to English to Sinhala machine translation",2009,"ICIIS 2009 - 4th International Conference on Industrial and Information Systems 2009, Conference Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951551902&doi=10.1109%2fICIINFS.2009.5429832&partnerID=40&md5=4ae1e1ec7da2af74a9dab1e6cef38fe5","Machine translation is a log felt need of the countries those who use English as a second language. Due to inherent complexity of natural languages, most of machine translation systems adopt rather ad-hoc strategies such as word level translation without concerning a proper theoretical basis. This has been a major reason for why the developments in natural language processing could not achieve as expected. This paper presents a theoretical-based approach to English-Sinhala machine translation through the concept of Varanagema (conjugation) in Sinhala Language. The theory of Varanagema in Sinhala language handles major language primitives including noun, verbs and prepositions. The concept of Varanagema also drastically reduces the number of word forms to be stored in the dictionaries of the machine translation system. The design, implementation and results of test versions of English-Sinhala machine translation system has been presented in the paper. ©2009 IEEE.",,"English as a second language; Inherent complexity; Machine translation systems; Machine translations; NAtural language processing; Natural languages; Theoretical basis; Word level; Computational linguistics; Information systems; Information theory; Machine design; Natural language processing systems; Query languages; Speech transmission; Translation (languages)",2-s2.0-77951551902
"Chaussin A., Osawa H., Oomura R., Imai M.","Mundus traducere: Interpretation of natural language using a semantic sensor network",2009,"Proceedings of IEEE Workshop on Advanced Robotics and its Social Impacts, ARSO",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869986596&doi=10.1109%2fARSO.2009.5587065&partnerID=40&md5=8b37c00c135bd633d887f8095a42dccd","Understanding natural language utterances cannot be done by studying only language. Models of the real world are also needed. Those models are usually built around semantic data. As sensor networks allow us to get information about the physical world, we wanted to add said information to the models used to interprete natural language. Adding that kind of physical data to world models will lead to dynamic model that can be deployed in everyday life systems like smart houses. This paper will focus on the theory needed to implement such physical data to semantic data and will present an implementation of previously mentionned theory called Mundus Traducere. This implementation is a Java program that generates formal expressions based on natural language utterances, linking real world objects with their language counter parts. The implementation is not regarded as a stand alone application but as a tool for implementing more complex interfaces. © 2009 IEEE.","Intelligent structures; Natural languages; Semantic network","Complex interface; Formal expressions; Interprete; Java program; Natural languages; Physical data; Physical world; Real-world objects; Semantic data; Semantic network; Semantic sensors; Smart house; Standalone applications; World model; Computer software; Intelligent structures; Models; Query languages; Robotics; Sensor networks; Java programming language",2-s2.0-84869986596
"Tangtulyangkul P., Hocking T.S., Fung C.C.","Intelligent information mining from veterinary clinical records and open source repository",2009,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951127149&doi=10.1109%2fTENCON.2009.5396065&partnerID=40&md5=ba7d365aacdeef97054a7f76d94cc141","This paper reports an implementation of an intelligent mining approach from veterinary clinical records and an external source of information. The system retrieves information from a local veterinary clinical database and then complements this information with related records from an external source, OAIster. It utilizes text-mining, web service technologies and domain knowledge, in order to extract keywords, to retrieve related records from an external source, and to filter the extracted keywords list. This study meets a practical challenge encountered at the School of Veterinary and Biomedical Sciences at Murdoch University. The results indicate that the system can be used to increase the limited knowledge within a local source by complementing it with related records from an external source. Moreover, the system also reduces information overload by only retrieving a set of related information from an external source. Finally, domain knowledge can be used to filter the extracted keywords, in this case, selected medical keywords from the extracted keyword list. ©2009 IEEE.","Clinical database; Information filtering; Keyword extraction; Query; Textmining; Veterinary records; Web services","Biomedical science; Clinical database; Clinical records; Domain knowledge; External sources; Information overloads; Intelligent information; Keyword extraction; Local source; Murdoch university; Open source repositories; Text-mining; Web service technology; Database systems; Web services; Filtration",2-s2.0-77951127149
"Moiseev R., Hayashi S., Saeki M.","Generating assertion code from OCL: A transformational approach based on similarities of implementation languages",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249162860&doi=10.1007%2f978-3-642-04425-0_52&partnerID=40&md5=88dcd2f1163931f1a25b733e8c29e868","The Object Constraint Language (OCL) carries a platform independent characteristic allowing it to be decoupled from implementation details, and therefore it is widely applied in model transformations used by model-driven development techniques. However, OCL can be found tremendously useful in the implementation phase aiding assertion code generation and allowing system verification. Yet, taking full advantage of OCL without destroying its platform independence is a difficult task. This paper proposes an approach for generating assertion code from OCL constraints by using a model transformation technique to abstract language specific details away from OCL high-level concepts, showing wide applicability of model transformation techniques. We take advantage of structural similarities of implementation languages to describe a rewriting framework, which is used to easily and flexibly reformulate OCL constraints into any target language, making them executable on any platform. A tool is implemented to demonstrate the effectiveness of this approach. © 2009 Springer Berlin Heidelberg.","Assertion code; Constraints; OCL; Programming languages","Abstract languages; Code constraints; Code Generation; Implementation languages; Model driven development; Model transformation; Model transformation technique; Object Constraint Language; Platform independence; Platform independent; Programming language; Structural similarity; System verifications; Target language; Transformational approach; Computer software; Linguistics; Models; Object oriented programming; Query languages; High level languages",2-s2.0-77249162860
"Manine A.-P., Alphonse E., Bessières P.","Learning ontological rules to extract multiple relations of genic interactions from text",2009,"International Journal of Medical Informatics",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71849103383&doi=10.1016%2fj.ijmedinf.2009.03.005&partnerID=40&md5=0e08bcad1abdd429f930cf3f09e212b7","Introduction: Information extraction (IE) systems have been proposed in recent years to extract genic interactions from bibliographical resources. They are limited to single interaction relations, and have to face a trade-off between recall and precision, by focusing either on specific interactions (for precision), or general and unspecified interactions of biological entities (for recall). Yet, biologists need to process more complex data from literature, in order to study biological pathways. An ontology is an adequate formal representation to model this sophisticated knowledge. However, the tight integration of IE systems and ontologies is still a current research issue, a fortiori with complex ones that go beyond hierarchies. Method: We propose a rich modeling of genic interactions with an ontology, and show how it can be used within an IE system. The ontology is seen as a language specifying a normalized representation of text. First, IE is performed by extracting instances from natural language processing (NLP) modules. Then, deductive inferences on the ontology language are completed, and new instances are derived from previously extracted ones. Inference rules are learnt with an inductive logic programming (ILP) algorithm, using the ontology as the hypothesis language, and its instantiation on an annotated corpus as the example language. Learning is set in a multi-class setting to deal with the multiple ontological relations. Results: We validated our approach on an annotated corpus of gene transcription regulations in the Bacillus subtilis bacterium. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten semantic relations defined in the ontology. © 2009 Elsevier B.V. All rights reserved.","Genic Interactions; Inductive Logic Programming; Information Extraction; Machine Learning; Ontology","Bacillus Subtilis; Biological entities; Biological pathways; Complex data; Formal representations; Gene transcriptions; Inductive Logic Programming; Inference rules; Information Extraction; Information extraction systems; Machine-learning; Multi-class; Natural language processing; Ontology language; Recall and precision; Research issues; Semantic relations; Specific interaction; Bacteriology; Computational linguistics; Education; Inference engines; Information analysis; Knowledge representation; Laws and legislation; Logic programming; Natural language processing systems; Query languages; Robot learning; Transcription; Ontology; article; Bacillus subtilis; bibliographic database; data extraction; gene interaction; information retrieval; machine learning; natural language processing; ontogeny; priority journal; recall; semantics; transcription regulation; Bacillus subtilis; Gene Expression Regulation, Bacterial; Information Storage and Retrieval; Natural Language Processing",2-s2.0-71849103383
"Hedegaard S., Houen S., Simonsen J.G.","LAIR: A language for automated semantics-aware text sanitization based on frame semantics",2009,"ICSC 2009 - 2009 IEEE International Conference on Semantic Computing",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449097460&doi=10.1109%2fICSC.2009.79&partnerID=40&md5=a2d1aebf1aae852661d6720f74ac3392","We present LAIR: A domain-specific language that enables users to specify actions to be taken upon meeting specific semantic frames in a text, in particular to rephrase and redact the textual content. While LAIR presupposes superficial knowledge of frames and frame semantics, it requires only limited prior programming experience. It neither contain scripting or I/O primitives, nor does it contain general loop constructions and is not Turing-complete. We have implemented a LAIR compiler and integrated it in a pipeline for automated redaction of web pages. We detail our experience with automated redaction of web pages for subjectively undesirable content; initial experiments suggest that using a small language based on semantic recognition of undesirable terms can be highly useful as a supplement to traditional methods of text sanitization. © 2009 IEEE.","Domainspecific languages; Frame semantics; Redaction; Sanitization","Domainspecific languages; General loops; Programming experience; Sanitization; Semantic recognition; Specific semantics; Textual content; Web page; Automation; Character recognition; Linguistics; Problem oriented languages; Query languages; World Wide Web; Semantics",2-s2.0-73449097460
"Adafre S.F., Van Genabith J.","Dublin city university at QA@CLEF 2008",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549106663&doi=10.1007%2f978-3-642-04447-2_41&partnerID=40&md5=02c4872f5275183da95cacefd0f09488","We describe our participation in Multilingual Question Answering at CLEF 2008 using German and English as our source and target languages, respectively. The system was built using UIMA (Unstructured Information Management Architechture) as underlying framework. © 2009 Springer Berlin Heidelberg.",,"Architechture; Dublin City University; Question Answering; Target language; Information management; Query languages; Linguistics",2-s2.0-70549106663
[No author name available],"JCDL'09 - Proceedings of the 2009 ACM/IEEE Joint Conference on Digital Libraries",2009,"Proceedings of the ACM/IEEE Joint Conference on Digital Libraries",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549109138&partnerID=40&md5=e33e92afc0034ef5fc039f6aa03decbd","The proceedings contain 107 papers. The topics discussed include: science teachers' use of online resources and the digital library for earth system education; helping students with information fragmentation, assimilation and notetaking; topic model methods for automatically identifying out-of-scope resources; automatically generating high quality metadata by analyzing the document code of common file types; disambiguating authors in academic publications using random forests; using web information for author name disambiguation; finding topic trends in digital libraries; CEBBIP: a parser of bibliographic information in Chinese electronic books; query parameters for harvesting digital video and associated contextual information; developing a flexible content model for media repositories: a case study; and query-page intention matching using clicked titles and snippets to boost search rankings.",,,2-s2.0-70549109138
"Edelmana S., Solanb Z.","Machine translation using automatically inferred construction-based correspondence and language models",2009,"PACLIC 23 - Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863860850&partnerID=40&md5=f3b7de12e2b63006dcfee430423009b2","We discuss the problem of translation in the wider context of the problem of meaning in cognition and describe a structural statistical machine translation (MT) method motivated by philosophical, cognitive, and computational considerations. Our approach relies on a recently published algorithm capable of learning from a raw corpus a limited yet effective grammar that can be used to construct probabilistic parsers and language models, and on cognitively motivated heuristics for learning construction-based translation models. A pilot system has been implemented and tested successfully on simple English to Hebrew and Spanish to English translation tasks. © 2009 by Shimon Edelman and Zach Solan.","Cognitive linguistics; Grammar inference; Machine translation","Grammar inference; Language model; Machine translations; Pilot system; Statistical machine translation; Translation models; Computer aided language translation; Philosophical aspects; Computational linguistics",2-s2.0-84863860850
"De Coi J.L., Fuchs N.E., Kaljurand K., Kuhn T.","Controlled english for reasoning on the semantic web",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549087875&doi=10.1007%2f978-3-642-04581-3_6&partnerID=40&md5=df5bc5f9ff798e25cfedd1c020f7862e","The existing Semantic Web languages have a very technical focus and fail to provide good usability for users with no background in formal methods. We argue that controlled natural languages like Attempto Controlled English (ACE) can solve this problem. ACE is a subset of English that can be translated into various logic based languages, among them the Semantic Web standards OWL and SWRL. ACE is accompanied by a set of tools, namely the parser APE, the Attempto Reasoner RACE, the ACE View ontology and rule editor, the semantic wiki AceWiki, and the Protune policy framework. The applications cover a wide range of Semantic Web scenarios, which shows how broadly ACE can be applied. We conclude that controlled natural languages can make the Semantic Web better understandable and more usable. © 2009 Springer Berlin Heidelberg.",,"Attempto Controlled English; Controlled natural language; Logic-based languages; Policy framework; Protune; Reasoner; Semantic web languages; Semantic web standards; Formal methods; Linguistics; Ontology; Semantic Web; Semantics",2-s2.0-70549087875
"Brambilla M., Ceri S.","Engineering search computing applications: Vision and challenges",2009,"ESEC-FSE'09 - Proceedings of the Joint 12th European Software Engineering Conference and 17th ACM SIGSOFT Symposium on the Foundations of Software Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949421387&doi=10.1145%2f1595696.1595764&partnerID=40&md5=70554a099c9b7d2b1ef4f4bf60f653ec","Search computing is a novel discipline whose goal is to answer complex, multi-domain queries. Such queries typically require combining in their results domain knowledge extracted from multiple Web resources; therefore, conventional crawling and indexing techniques, which look at individual Web pages, are not adequate for them. In this paper, we sketch the main characteristics of search computing and we highlight how various classical computer science disciplines - including software engineering, Web engineering, service-oriented architectures, data management, and human-computing interaction - are challenged by the search computing approach.","Search computing; Search services; SOA; Software engineering; Web services","Computing applications; Data management; Domain knowledge; Indexing techniques; Main characteristics; Multi domains; Search computing; Search services; Web engineering; Web page; Web resources; Complexation; Computer architecture; Computer software; Human computer interaction; Information services; Service oriented architecture (SOA); Web services; Engineering",2-s2.0-77949421387
"Li S., Gao H., Sun H., Chen F., Feng O., Gao S., Zhang H., Li X., Tan C., Xu W., Chen G., Guo J.","A study of faceted blog distillation - PRIS at TREC 2009 Blog Track",2009,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873457786&partnerID=40&md5=adaddb25f2c9dc2aaa21c2fce6e665ea","This paper describes BUPT (pris) participation in faceted blog distillation task at Blog Track 2009. The system adopts a two-stage strategy in faceted blog distillation task. In the first stage, the system carries out a basic topic relevance retrieval to get the top k blogs for each query. In the second stage, different models are designed to judge the facets and ranking.",,"Topic relevance; Distillation; Information retrieval; Blogs",2-s2.0-84873457786
"Yeh C.-L., Chen Y.-C.","Using topic identification in Chinese information retrieval",2009,"Journal of Internet Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949688487&partnerID=40&md5=8ed1efc354ecafcc834082a32560be41","Information retrieval is to identify documents, from text collections, which are relevant with respect to some query. In current information retrieval systems, users can query with an unordered set of keywords, a question or a sentence. A list of document links matching the query can be retrieved and ordered by relevancy between the query and the documents. In this article, we are concerned with a hypothesis that the discourse-level element, topic, could be used to contribute the calculations of information retrieval. Due to the phenomenon of zero anaphora frequently occurring in Chinese texts, the topics may be omitted and are not expressed on the surface text. The key elements of the centering model of local discourse coherence are employed to extract structures of discourse segments. We propose a topic identification method using the local discourse structure to recover the omissions of topics and identify the topics of documents in the text collection. Then the topic information is inserted into the text for creating better indices. The experiment results are demonstrated on a test collection which is taken from Chinese Information Retrieval Benchmark, version 3.0.","Information retrieval; Natural language processing; Shallow parsing; Topic identification","Centering model; Chinese text; Discourse structure; Key elements; NAtural language processing; Shallow parsing; Surface text; Test Collection; Text collection; Topic identification; Computational linguistics; Information retrieval systems; Information services; Natural language processing systems; Information retrieval",2-s2.0-77949688487
"Ding Z., Jiang M.","Rebuilding web application requirements based on user navigation",2009,"NCM 2009 - 5th International Joint Conference on INC, IMS, and IDC",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549119252&doi=10.1109%2fNCM.2009.37&partnerID=40&md5=584ce6f3486789ddaaf04b1f618d6b74","Use cases have been used to describe functional requirements of information systems in a natural language. In the design phase, use cases can be used to construct design model and then the model can be checked by some static analysis techniques. Since errors could be created while the design is refined and bugs could be planted in the implementation, after going through the software developing process, it is still question if the resulting system satisfies the original requirements. In this paper, we propose a new technique to rebuild use cases for web applications, thus we can compare the generated use cases with the designed use cases. For a web application, due to its special navigation character, we can get rich information in the log file while the user navigates the web. From the log file, we extract raw data that contains basic structures of the use case. After applying Natural Language Processing technique, we determine the boundary of raw data, reduce the data redundancy, and adjust the sentence, finally, we rebuild the use cases. Standford Parser has been employed to perform the sentence analysis for us. A prototype has been developed to support our technique. © 2009 IEEE.",,"Basic structure; Data redundancy; Design models; Design phase; Developing process; Functional requirement; Log file; NAtural language processing; Natural languages; Sentence analysis; User navigation; WEB application; Agriculture; Computational linguistics; Information management; Natural language processing systems; Navigation; Program debugging; Semiconductor storage; World Wide Web",2-s2.0-73549119252
"Werner C., Buschmann C., Brandt Y., Fischer S.","XML compression for web services on resource-constrained devices",2009,"Services and Business Computing Solutions with XML: Applications for Quality Management and Best Processes",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899196035&doi=10.4018%2f978-1-60566-330-2.ch010&partnerID=40&md5=fe078f22b55b8bbea72340175bede10d","Compared to other middleware approaches like CORBA or Java RMI the protocol overhead of SOAP is very high. This fact is not only disadvantageous for several performance-critical applications, but especially in environments with limited network bandwidth or resource-constrained computing devices. Although recent research work concentrated on more compact, binary representations of XML data only very few approaches account for the special characteristics of SOAP communication. In this article we will discuss the most relevant state-of-the-art technologies for compressing XML data. Furthermore, we will present a novel solution for compacting SOAP messages. In order to achieve significantly better compression rates than current approaches, our compressor utilizes structure information from an XML Schema or WSDL document. With this additional knowledge on the ""grammar"" of the exchanged messages, our compressor generates a single custom pushdown automaton, which can be used as a highly efficient validating parser as well as a highly efficient compressor. The main idea is to tag the transitions of the automaton with short binary identifiers that are then used to encode the path trough the automaton during parsing. Our approach leads to extremely compact data representations and is also usable in environments with very limited CPU and memory resources. © 2009, IGI Global.",,,2-s2.0-84899196035
"Verma K.S., Bhattacharyya P.","Context-sensitive semantic smoothing using semantically relatable sequences",2009,"IJCAI International Joint Conference on Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751699913&partnerID=40&md5=8b0ae01b9ff7137ec1ebe44d87d8886b","We propose a novel approach to context sensitive semantic smoothing by making use of an intermediate, ""semantically light"" representation for sentences, called Semantically Relatable Sequences (SRS). SRSs of a sentence are tuples of words appearing in the semantic graph of the sentence as linked nodes depicting dependency relations. In contrast to patterns based on consecutive words, SRSs make use of groupings of non-consecutive but semantically related words. Our experiments on TREC AP89 collection show that the mixture model of SRS translation model and Two Stage Language Model (TSLM) of Lafferty and Zhai achieves MAP scores better than the mixture-model of MultiWord Expression (MWE) translation model and TSLM. Furthermore, a system, which for each test query selects either the SRS or the MWE mixture model based on better query MAP score, shows significant improvements over the individual mixture models.",,"Context sensitive; Dependency relation; Language model; Mixture model; Multiword expressions; Semantic graphs; Semantically-related words; Translation models; Two stage; Artificial intelligence; Computational linguistics; Semantics; Stimulated Raman scattering; Mixtures",2-s2.0-78751699913
"Santos D., Cardoso N., Carvalho P., Dornescu I., Hartrumpf S., Leveling J., Skalban Y.","GikiP at geoCLEF 2008: Joining GIR and QA forces for querying wikipedia",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549087084&doi=10.1007%2f978-3-642-04447-2_118&partnerID=40&md5=d428ce494dedee6e960e74696bd58154","This paper reports on the GikiP pilot that took place in 2008 in GeoCLEF. This pilot task requires a combination of methods from geographical information retrieval and question answering to answer queries to the Wikipedia. We start by the task description, providing details on topic choice and evaluation measures. Then we offer a brief motivation from several perspectives, and we present results in detail. A comparison of participants' approaches is then presented, and the paper concludes with improvements for the next edition. © 2009 Springer Berlin Heidelberg.",,"Evaluation measures; Geographical information; Pilot tasks; Question Answering; Task description; Wikipedia; Information services; Natural language processing systems; Linguistics",2-s2.0-70549087084
"Yang Q., Chen Y., Xue G.-R., Dai W., Yu Y.","Heterogeneous transfer learning for image clustering via the social Web",2009,"ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.",71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956221086&partnerID=40&md5=cda251e582ade3334ebdde1b9f354fc2","In this paper, we present a new learning scenario, heterogeneous transfer learning, which improves learning performance when the data can be in different feature spaces and where no correspondence between data instances in these spaces is provided. In the past, we have classified Chinese text documents using English training data under the heterogeneous transfer learning framework. In this paper, we present image clustering as an example to illustrate how unsupervised learning can be improved by transferring knowledge from auxiliary heterogeneous data obtained from the social Web. Image clustering is useful for image sense disambiguation in query-based image search, but its quality is often low due to imagedata sparsity problem. We extend PLSA to help transfer the knowledge from social Web data, which have mixed feature representations. Experiments on image-object clustering and scene clustering tasks show that our approach in heterogeneous transfer learning based on the auxiliary data is indeed effective and promising. © 2009 ACL and AFNLP.",,"Auxiliary data; Chinese text documents; Feature representation; Feature space; Heterogeneous data; Image clustering; Image search; Learning performance; Learning scenarios; Sparsity problems; Training data; Transfer learning; Web data; Computational linguistics; Natural language processing systems",2-s2.0-77956221086
"Huang W., Liang H., Hu Z., Yang Y.","VoIP network perimeter defense system",2009,"Qinghua Daxue Xuebao/Journal of Tsinghua University",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955786633&partnerID=40&md5=326e6bcc0b3bc15da26d1225e332926c","Detection algorithms against VoIP (voice over internet protocol) network attacks need large amounts of computing resources, with current defense systems running out of computing resources for large system loads. This paper presents a transport layer load balancing algorithm which optimizes the distribution of the network traffic and the back-end server load. The defense system was distributed, and parallel processing to identify the signaling flow and RTP (real-time transport protocol) packet flow. The identified RTP packet flows are then associated with their respective VoIP sessions through an asynchronous query for signaling information. The integrity of the VoIP session data is then ensured by the distributed VoIP network perimeter defense. Back-to-back tests with large packet flow rates show that the packet loss rate with this system is much lower than with a single host based system. The malformed SIP (session initial protocol) signaling flood test showed that existing detection algorithms can be applied in this system without any changes. Attacks can be detected in real-time and the response delay is only one second even for a heavy loads.","Load balance; Network security; Perimeter defense; VoIP (voice over internet protocol)","Back-end servers; Back-test; Computing resource; Defense system; Detection algorithm; Heavy loads; Host-based; Large system; Load balance; Load balancing algorithms; Network attack; Network traffic; Packet flows; Packet loss rates; Parallel processing; Perimeter defense; Real time transport protocols; Response delays; Session initial protocols; Signaling flow; Transport layers; Voice over Internet protocol; Algorithms; Computer crime; Internet; Internet protocols; Internet telephony; Parallel flow; Signal detection; Signaling; Voice/data communication systems; Network security",2-s2.0-77955786633
"Bucuvalas S.P.C., Bader A.M., Nelson T.D., Barnhart R.","Push-button verification and debugging for COBOL-based financial systems",2009,"Proceedings of the 13th IASTED International Conference on Software Engineering and Applications, SEA 2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954203657&partnerID=40&md5=53e76488ac5fb4a679b3d5ee69ce2a81","Formal Methods have been adopted successfully by industries for which safety is paramount or the cost of errors exorbitant. Even so, Formal Methods are virtually unknown in the domain of Financial Services, despite persistent and economically significant software quality challenges. The Babel prototype is introduced to demonstrate an innovative approach to encapsulating powerful Formal Methods into automated Push-Button verification for COBOL-based Financial Services software. The essential adoption barrier for Financial Services is the knowledge of Logic and Computation Theory required for their successful use. Babel's Push-Button approach eliminates Formal Methods' educational prerequisites, simplifies their use, and integrates seamlessly into the existing software development task structure and tools. Babel's Human-Computer Interaction advances are based upon complete and correct static analysis, which in turn are based upon the thesis that the core systems in Financial Services can be implemented on a simpler computational model than core systems in other industries. Babel implements the static analysis and automated verification using symbolic execution, a modified semantic tableau, and subsumption reasoning. Verification tasks are defined in a point-and click-query user interface, in which a visual source trace capability seamlessly links verification failures to root causes in program source.","COBOL; Formal methods; Tools; Verification and validation","Adoption barriers; Automated verification; COBOL; Computational model; Core systems; Financial service; Financial system; Innovative approaches; Root cause; Semantic tableau; Software development; Software Quality; Symbolic execution; Verification and validation; Verification task; Automata theory; Computer software selection and evaluation; Finance; Human computer interaction; Knowledge management; Safety engineering; Software design; Static analysis; User interfaces; Verification; Formal methods",2-s2.0-77954203657
"Yao J., Ma Y., Dai Z., Niu J., Wei H.","Extracting chemical laws from Chinese scientific literatures",2009,"2009 International Conference on Artificial Intelligence and Computational Intelligence, AICI 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949282357&doi=10.1109%2fAICI.2009.376&partnerID=40&md5=fbeee8fcb730a570e64772177f2ef669","The knowledge acquisition bottleneck has become the major impediment to the development and application of effective knowledge systems. In this paper, we study the problem of extracting chemical laws from the abstract of Chinese scientific and formalizing the laws extracted to production rules. This task is different from traditional text based information extraction because how to encode the extracted laws into a computational chemistry expert system is also under our consideration. NLP approaches are used to facilitate the automated extraction of chemical laws. First, tregex are used to identify the named entities on parsing trees and then typed dependencies are utilized to extract operation words and consequence words. The experimental results indicate that our approaches for this task are highly efficient. © 2009 IEEE.","Knowledge acquisition; Law extraction; Text mining","Automated extraction; Knowledge system; Named entities; Production rules; Scientific literature; Text mining; Text-based information; Computational chemistry; Data mining; Expert systems; Knowledge acquisition; Mergers and acquisitions; Natural language processing systems; Artificial intelligence",2-s2.0-77949282357
"Flood D.","Natural Language Processing for spreadsheet information retrieval",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651250570&doi=10.1007%2f978-3-642-12550-8_32&partnerID=40&md5=183cd32e7fc37c6107ad4f0d8240560d","Spreadsheets can be used for a variety of tasks including inventory management. Most spreadsheet applications provide the ability to extract additional information from this data, through techniques such as filtering and PivotTables. These techniques however can be quite complex and time consuming, especially for novice spreadsheet users. We believe that these issues can be addressed through the use of Natural Language Processing, a technology that has already demonstrated its effectiveness in similar domains, such as databases and chart generation. © 2010 Springer-Verlag Berlin Heidelberg.",,"Inventory management; NAtural language processing; Pivot-tables; Spreadsheet applications; Computational linguistics; Information retrieval; Information systems; Spreadsheets; Natural language processing systems",2-s2.0-78651250570
"Petricek T.","Encoding monadic computations in C# using iterators",2009,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874045979&partnerID=40&md5=a93b259a46cefe62648ccdab865e627f","Many programming problems can be easily solved if we express them as computations with some non-standard aspect. This is a very important problem, because today we're struggling for example to eficiently program multi-core processors and to write asynchronous code. Unfortunately main-stream languages such as Java or C# don't support any direct way for encoding unrestricted non- standard computations. In languages like Haskell and F#, this can be done using monads with syntactic extensions they provide and it has been successfully applied to a wide range of real-world problems. In this paper, we present a general way for encoding monadic computations in the C# 2.0 language with a convenient syntax using an existing language feature called iterators. This gives us a way to use well-known non-standard computations enabling easy asynchronous programming or for example the use of software transactional memory in plain C#. Moreover, it also opens monads in general to a wider audience which can help in the search for other useful and previously unknown kinds of computations.",,"Asynchronous programming; Haskell; Iterators; Language features; Monadic computations; Multi-core processor; Programming problem; Real-world problem; Software transactional memory; Encoding (symbols); Information technology; Program processors; Syntactics; Multicore programming",2-s2.0-84874045979
"Mlynkova I.","XML benchmarking: The state of the art and possible enhancements",2009,"Open and Novel Issues in XML Database Applications: Future Directions and Advanced Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899202558&doi=10.4018%2f978-1-60566-308-1.ch014&partnerID=40&md5=daf9e40c5477de77bcdd44d0bb56d906","Since XML technologies have become a standard for data representation, numerous methods for processing XML data emerge every day. Consequently, it is necessary to compare the newly proposed methods with the existing ones, as well as analyze the effect of a particular method when applied to various types of data. In this chapter, the auhtors provide an overview of existing approaches to XML benchmarking from the perspective of various applications and show that to date the problem has been highly marginalized. Therefore, in the second part of the chapter they discuss persisting open issues and their possible solutions. © 2009, IGI Global.",,,2-s2.0-84899202558
"Lim J.-H., Park S., Jang H., Park S.","BioProber2.0: A unified biomedical workbench with mining and probing literatures",2009,"ACM International Conference Proceeding Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74949086323&doi=10.1145%2f1655925.1656129&partnerID=40&md5=72364584e6ec3155f66b6f6dcbb3583a","The number of biomedical literatures is increasing at a considerable rate, and the information is growing continuously and fast as well. Accordingly, information retrieval is more and more important to support biomedical researches. However, it often retrieves too many literatures or too few literatures for the target gene/proteins or relations. And extremely various synonyms of the gene and protein names make information retrieval more difficult to support biomedical researches. To overcome these difficulties, we propose a unified biomedical workbench with mining and probing literatures. The proposed workbench is composed of searching/collecting, literature mining, relation probing, and statistics analysis. It provides searching and collecting literatures of Pubmed articles and USPTO patents. And, to extract biomedical relations, the collected literatures are mined using text mining techniques such as named entity recognition, gene/protein name normalization, and relation extraction. Users can probe their target relations using these extracted relation information, shown in form of relation network. Finally, the workbench provides statistics information of literature meta data such as authors, organizations, publication years and so on. That is, the proposed workbench provides unified literature-based functions from searching to probing, and including text mining and statistics analysis.","Bio text mining; Biomedical workbench; Relation probing; Statistics analysis","Biomedical literature; Biomedical research; Literature mining; Named entity recognition; Protein names; Relation extraction; Relation information; Statistics analysis; Text mining; Text mining techniques; Information retrieval; Information services; Information technology; Natural language processing systems; Character recognition",2-s2.0-74949086323
"Mlynkova I.","Current trends in testing XMLMSs",2009,"Information Systems Development: Towards a Service Provision Society",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052136737&doi=10.1007%2fb137171_56&partnerID=40&md5=2314a267cf21e56fce14b5130b56de8b","Since XML technologies have become a standard for data representation, a huge amount of XMLMSs have emerged as well. Consequently, it is necessary to be able to experimentally test and compare their versatility, behaviour and efficiency. In this chapter we provide an overview of existing approaches to testing XMLMSs and we discuss respective consequences and recommendations. © 2009 Springer Science+Business Media, LLC.","XML benchmarking; XML data generators; XML test suites","Current trends; Data representations; XML data; XML technology; Information systems; Management information systems; Standardization; XML",2-s2.0-80052136737
"Chali Y., Hasan S.A., Joty S.R.","Do automatic annotation techniques have any impact on supervised complex question answering?",2009,"ACL-IJCNLP 2009 - Joint Conf. of the 47th Annual Meeting of the Association for Computational Linguistics and 4th Int. Joint Conf. on Natural Language Processing of the AFNLP, Proceedings of the Conf.",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960602319&partnerID=40&md5=43a10ba42718d0d93500e90a038e937d","In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task). Huge amount of annotated or labeled data is a prerequisite for supervised training. The task of labeling can be accomplished either by humans or by computer programs. When humans are employed, the whole process becomes time consuming and expensive. So, in order to produce a large set of labeled data we prefer the automatic annotation strategy. We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK). The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (Max-Ent). Evaluation results are presented to show the impact. © 2009 ACL and AFNLP.",,"Automatic annotation; Basic elements; Complex questions; Conditional random field; Evaluation results; Labeled data; Main tasks; Maximum entropy; Semantic similarity measures; Similarity measure; Support vector machine (SVM); Whole process; Computational linguistics; Hidden Markov models; Maximum entropy methods; Support vector machines; Natural language processing systems",2-s2.0-79960602319
"Ashish N., Mehrotra S.","XAR: An integrated framework for semantic extraction and annotation",2009,"Cases on Semantic Interoperability for Information Systems Integration: Practices and Applications",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900216062&doi=10.4018%2f978-1-60566-894-9.ch011&partnerID=40&md5=5f2aa8ba610ef76cb79bba48ecf2d317","The authors present the XAR framework that allows for free text information extraction and semantic annotation. The language underpinning XAR, the authors argue, allows for the inclusion of probabilistic reasoning with the rule language, provides higher level predicates capturing text features and relationships, and defines and supports advanced features such as token consumption and stratified negotiation in the rule language and semantics. The XAR framework also allows the incorporation of semantic information as integrity constraints in the extraction and annotation process. The XAR framework aims to fill in a gap, the authors claim, in the Web based information extraction systems. XAR provides an extraction and annotation framework by permitting the integrated use of hand-crafted extraction rules, machine-learning based extractors, and semantic information about the particular domain of interest. The XAR system has been deployed in an emergency response scenario with civic agencies in North America and in a scenario with an IT department of a county level community clinic. © 2010, IGI Global.",,,2-s2.0-84900216062
"Barker P.","Using metanotation as a tool for describing learning systems",2009,"Handbook of Research on Hybrid Learning Models: Advanced Tools, Technologies, and Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898525720&doi=10.4018%2f978-1-60566-380-7.ch003&partnerID=40&md5=3e874e74f4c67a25e1ddc45a44f121b2","Metanotation is a powerful tool for describing systems, objects and processes. This chapter illustrates how this tool can be used to specify the nature and characteristics of learning systems and the various artefacts from which they are composed. It is suggested that messages and messaging systems are the fundamental building blocks from which learning artefacts are created. The chapter therefore discusses the nature of communication and messaging from an educational perspective and then outlines how various types of message artefact can be used to build hybrid learning systems that involve the use of Webs, Wikis, weblogs and electronic books. © 2010, IGI Global.",,,2-s2.0-84898525720
"Van Durme B., Michalak P., Schubert L.K.","Deriving generalized knowledge from corpora using WordNet abstraction",2009,"EACL 2009 - 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891605434&partnerID=40&md5=900c08439dc1149fa14023e96e6e1358","Existing work in the extraction of com-monsense knowledge from text has been primarily restricted to factoids that serve as statements about what may possibly obtain in the world. We present an approach to deriving stronger, more general claims by abstracting over large sets of factoids. Our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types, obtained as WordNet synsets. The results can be construed as generically quantified sentences restricting the semantic type of an argument position of a predicate. © 2009 Association for Computational Linguistics.",,"Semantic types; Synsets; Wordnet; Abstracting; Computational linguistics; Semantics; Ontology",2-s2.0-84891605434
"Liang P., Klein D.","Online EM for unsupervised models",2009,"NAACL HLT 2009 - Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Conference",75,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651340127&partnerID=40&md5=ceb307cfb5ffc079aa09405b849fae28","The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsuper-vised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment. © 2009 Association for Computational Linguistics.",,"Document Classification; EM algorithms; Online EM; Part of speech tagging; Word alignment; Word segmentation; Algorithms; Information retrieval systems; Computational linguistics",2-s2.0-78651340127
"Yang Y., Peng X., Zhao W.","Domain feature model recovery from multiple applications using data access semantics and formal concept analysis",2009,"Proceedings - Working Conference on Reverse Engineering, WCRE",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449121426&doi=10.1109%2fWCRE.2009.15&partnerID=40&md5=fd1a946fbc60c07bf9aca6e8d9942995","Feature models are widely employed in domain specific software development to specify the domain requirements with commonality and variability. A feature model is usually constructed by domain experts after comprehensive domain analysis. In this paper, we propose a method to recover an initial domain feature model from multiple existing domain applications using data access semantics and formal concept analysis (FCA). In the method, we first establish mappings among the database schemas of all the reference implementations. Then, we capture the data access semantics of each method in each reference implementation. Based on the pre-established data mapping, we can mix methods from different applications together and conduct formal concept analysis with the data access semantics as intention. After that, further concept merging/pruning and variability analysis are performed to produce the domain feature model. In order to evaluate the effectiveness of our method, we conduct a case study on three open-source forum applications and present comprehensive analysis and discussions on the results. © 2009 IEEE.","Data access semantics; FCA; Feature model recovery","Commonality and variability; Comprehensive analysis; Data access; Data access semantics; Data mappings; Database schemas; Domain analysis; Domain experts; Domain feature; Domain requirements; Domain specific software development; Feature model recovery; Feature models; Formal Concept Analysis; Initial domains; Multiple applications; Open-source; Reference implementation; Variability analysis; Computer software reusability; Reengineering; Reverse engineering; Semantics",2-s2.0-73449121426
"Olsson F.","On privacy preservation in text and document-based active learning for named entity recognition",2009,"International Conference on Information and Knowledge Management, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049088172&doi=10.1145%2f1651449.1651460&partnerID=40&md5=4777202157341914eac23d534d5fa190","The preservation of the privacy of persons mentioned in text requires the ability to automatically recognize and identify names. Named entity recognition is a mature field and most current approaches are based on supervised machine learning techniques. Such learning requires the presence of labeled examples on which to train; training examples are usually provided to the learner on the form of annotated corpora. Creating and annotating corpora is a tedious, meticulous and error prone process; obtaining good training examples is a hard task in itself. This paper describes the development and in-depth empirical investigation of a method, called BootMark, for bootstrapping the marking up of named entities in textual documents. Experimental results show that BootMark requires a human annotator to manually annotate fewer documents in order to produce a named entity recognizer with a given performance, than would be needed if the documents forming the basis for the recognizer were randomly drawn from the same corpus. The investigation further indicates that the primary gain obtained by BootMark compared to passive learning is in terms of higher recall. Thus, it is argued, the recognizers are suitable for use in privacy preservation applications. Copyright 2009 ACM.","Active machine learning; Data annotation; Named entity recognition; Privacy preservation","Active Learning; Empirical investigation; Error-prone process; Hard task; Machine-learning; Mature fields; Named entities; Named entity recognition; Passive learning; Privacy preservation; Supervised machine learning; Textual documents; Training example; Data privacy; Knowledge management; Learning algorithms; Learning systems; Natural language processing systems; Software agents; Character recognition",2-s2.0-74049088172
"Galvez C.","Standardization of terms applying finite-state transducers (FST)",2009,"Handbook of Research on Digital Libraries: Design, Development, and Impact",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898114561&doi=10.4018%2f978-1-59904-879-6.ch010&partnerID=40&md5=90151fff1d159c430d5c6d6601ac4427","This chapter presents the different standardization methods of terms at the two basic approaches of nonlinguistic and linguistic techniques, and sets out to justify the application of processes based on finite-state transducers (FST). Standardization of terms is the procedure of matching and grouping together variants of the same term that are semantically equivalent. A term variant is a text occurrence that is conceptually related to an original term and can be used to search for information in a text database. The uniterm and multiterm variants can be considered equivalent units for the purposes of automatic indexing. This chapter describes the computational and linguistic base of the finite-state approach, with emphasis on the influence of the formal language theory in the standardization process of uniterms and multiterms. The lemmatization and the use of syntactic pattern-matching, through equivalence relations represented in FSTs, are emerging methods for the standardization of terms. © 2009, IGI Global.",,,2-s2.0-84898114561
"Sokolova M., Szpakowicz S.","Machine learning applications in mega-text processing",2009,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898393547&doi=10.4018%2f978-1-60566-766-9.ch015&partnerID=40&md5=2e7da82a75309e97fd8ac89efeae89f8","This chapter presents applications of machine learning techniques to problems in natural language processing that require work with very large amounts of text. Such problems came into focus after the Internet and other computer-based environments acquired the status of the prime medium for text delivery and exchange. In all cases which the authors discuss, an algorithm has ensured a meaningful result, be it the knowledge of consumer opinions, the protection of personal information or the selection of news reports. The chapter covers elements of opinion mining, news monitoring and privacy protection, and, in parallel, discusses text representation, feature selection, and word category and text classification problems. The applications presented here combine scientific interest and significant economic potential. © 2010, IGI Global.",,,2-s2.0-84898393547
"Rosenmüller M., Apel S., Leich T., Saake G.","Tailor-made data management for embedded systems: A case study on Berkeley DB",2009,"Data and Knowledge Engineering",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749116421&doi=10.1016%2fj.datak.2009.07.013&partnerID=40&md5=412497dcd1154207a640f343c3bf744b","Applications in the domain of embedded systems are diverse and store an increasing amount of data. In order to satisfy the varying requirements of these applications, data management functionality is needed that can be tailored to the applications' needs. Furthermore, the resource restrictions of embedded systems imply a need for data management that is customized to the hardware platform. In this paper, we present an approach for decomposing data management software for embedded systems using feature-oriented programming. The result of such a decomposition is a software product line that allows us to generate tailor-made data management systems. While existing approaches for tailoring software have significant drawbacks regarding customizability and performance, a feature-oriented approach overcomes these limitations, as we will demonstrate. In a non-trivial case study on Berkeley DB, we evaluate our approach and compare it to other approaches for tailoring DBMS. © 2009 Elsevier B.V. All rights reserved.","Embedded systems; Feature-oriented programming; FeatureC++; Software product lines; Tailor-made data management","Customizability; Data management; Data management software; Data management system; Feature-oriented approaches; Feature-oriented programming; Hardware platform; Non-trivial; Resource restrictions; Software Product Line; Software product lines; Decomposition; Embedded software; Management; Network architecture; Research; Embedded systems",2-s2.0-71749116421
"Dünnweber J., Gorlatch S.","Higher-order components for grid programming: Making grids more usable",2009,"Higher-Order Components for Grid Programming: Making Grids More Usable",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891384265&doi=10.1007%2f978-3-642-00841-2&partnerID=40&md5=7b03dc768ce2dcd6c2e3c2f9e9095f4d","A major challenge in grid computing remains the application software development for this new kind of infrastructure. Grid application programmers have to take into account several complicated aspects: distribution of data and computations, parallel computations on different sites and processors, heterogeneity of the involved computers, load balancing, etc. Grid programmers thus demand novel programming methodologies that abstract over such technical details while preserving the beneficial features of modern grid middleware. For this purpose, the authors introduce Higher-Order Components (HOCs). HOCs implement generic parallel/distributed processing patterns, together with the required middleware support, and they are offered to users via a high-level service interface. Users only have to provide the application-specific pieces of their programs as parameters, while low-level implementation details, such as the transfer of data across the grid, are handled by the HOCs. HOCs were developed within the CoreGRID European Network of Excellence and have become an optional extension of the popular Globus middleware. The book provides the reader with hands-on experience, describing a broad collection of example applications from various fields of science and engineering, including biology, physics, etc. The Java code for these examples is provided online, complementing the book. The expected application performance is studied and reported for extensive performance experiments on different testbeds, including grids with worldwide distribution. The book is targeted at graduate students, advanced professionals, and researchers in both academia and industry. Readers can raise their level of knowledge about methodologies for programming contemporary parallel and distributed systems, and, furthermore, they can gain practical experience in using distributed software. Practical examples show how the complementary online material can easily be adopted in various new projects. © Springer-Verlag Berlin Heidelberg 2009. All rights are reserved.",,,2-s2.0-84891384265
"Schwerdfeger A.C., Van Wyk E.R.","Verifiable composition of deterministic grammars",2009,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450228505&doi=10.1145%2f1542476.1542499&partnerID=40&md5=d78007c3c28606e0b98f1c9e0725bd16","There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension. Copyright © 2009 ACM.","Context-aware scanning; Extensible languages; Grammar composition; Language composition; LR parsing","Composition languages; Concrete syntax; Context-Aware; Context-aware scanning; Domain specific; Expert programmers; Language extensions; Lexical ambiguity; Lexical disambiguation; LR parsers; LR parsing; Modular analysis; Parse table; Primary contribution; Regular expressions; Terminal symbols; C (programming language); Computational linguistics; Computer software; Electric reactors; Formal languages; Linguistics; Scanning; Specifications; Query languages",2-s2.0-70450228505
"Atkey R., Lindley S., Yallop J.","Unembedding domain-specific languages",2009,"Haskell'09 - Proceedings of the 2009 ACM SIGPLAN Haskell Symposium",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949120676&doi=10.1145%2f1596638.1596644&partnerID=40&md5=691d0a6745f5de86dc6b3f9419800573","Higher-order abstract syntax provides a convenient way of embedding domain-specific languages, but is awkward to analyse and manipulate directly. We explore the boundaries of higher-order abstract syntax. Our key tool is the unembedding of embedded terms as de Bruijn terms, enabling intensional analysis. As part of our solution we present techniques for separating the definition of an embedded program from its interpretation, giving modular extensions of the embedded language, and different ways to encode the types of the embedded language. Copyright © 2009 ACM.","Domain-specific languages; Higher-order abstract syntax; Type classes; Unembedding","De Bruijn; Domain specific languages; Embedded Languages; Higher-order abstract syntax; Modular extension; Type class; Abstracting; Functional programming; Problem oriented languages; Program interpreters; Query languages; Syntactics; Linguistics",2-s2.0-72949120676
"Cuoq P., Signoles J., Baudin P., Bonichon R.","Experience report: OCaml for an industrial-strength static analysis framework",2009,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450193051&doi=10.1145%2f1596550.1596591&partnerID=40&md5=245817446e8c08fabade50b7e3a25909","This experience report describes the choice of OCaml as the implementation language for Frama-C, a framework for the static analysis of C programs. OCaml became the implementation language for Frama-C because it is expressive. Most of the reasons listed in the remaining of this article are secondary reasons, features which are not specific to OCaml (modularity, availability of a C parser, control over the use of resources. . . ) but could have prevented the use of OCaml for this project if they had been missing. Copyright © 2009 ACM.","Design; Languages; Verification","C programs; Design languages; Experience report; Implementation languages; Functional programming; Linguistics; Static analysis; Query languages",2-s2.0-70450193051
"Piasecki M., Radziszewski A.","Morphosyntactic constraints in the acquisition of linguistic knowledge for polish",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450159671&doi=10.1007%2f978-3-642-04735-0_7&partnerID=40&md5=30b76e4096696dcdc06703aaebc78c03","Many approaches to the construction of language tools and acquisition of linguistic knowledge from corpora assume the application of some robust shallow parser. Construction of such a parser is difficult in the case of inflective languages with relaxed word order like Polish. The goal of the work presented here is to analyse the extent of knowledge that can be expressed in the form of morphosyntactic constraints referring to morphological properties of word forms, and its applications in the automatic extraction of syntactic and semantic knowledge. Basic properties of an extended version of the language of morphosyntactic constraints called JOSKIPI are briefly presented. The application of morphosyntactic constraints as background knowledge for extraction of disambiguation rules for Polish is discussed. A new approach to extraction of lexical semantic relations is presented: it relies on the constraints in identifying lexico-morphosyntactic dependencies among word forms in the text. Finally, a combination of the constraints and statistical analysis in the acquisition of multiword expressions is outlined. © 2009 Springer Berlin Heidelberg.","Annotated corpus; Decision trees; Extraction of multiword expressions; Measures of semantic relatedness; Morphosyntactic constraints; Morphosyntactic tagging; Polish","Annotated corpus; Measures of semantic relatedness; Morphosyntactic constraints; Morphosyntactic tagging; Multiword expressions; Computational linguistics; Decision trees; Mergers and acquisitions; Natural language processing systems; Semantics; Software agents; Query languages",2-s2.0-70450159671
"Dey L., Haque S.M.","Studying the effects of noisy text on text mining applications",2009,"ACM International Conference Proceeding Series",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450161141&doi=10.1145%2f1568296.1568314&partnerID=40&md5=0ad5338dee59db24a767c95130289a62","Text mining aims at deriving high quality information from text in an automated way. Text mining applications rely on Natural Language Processing (NLP) tools like tagger, parser etc. to locate and retrieve relevant information in an application specific manner. Most of these NLP tools however have been designed to work on clean and grammatically correct text. Presently, many organizations are interested to derive information from informally written text that is generated as a result of human communication through emails, or blog posts, web-based reviews etc. These texts are highly noisy and often found to contain mixture of languages. In this study we present some analysis on how noise introduced due to incorrect English affects the performance of some of the NLP tools and thereafter the text mining applications. The text mining application that we focus on is opinion mining. Opinion mining is the most significant text mining application that has to deal with noisy text generated in an unregulated fashion by users. Copyright 2009 ACM.","NLP tools; Noisy text; Text mining","Application specific; High quality information; Human communications; Natural language processing; NLP tools; Opinion mining; Text mining; Written texts; Computational linguistics; Natural language processing systems; Query languages; Mining",2-s2.0-70450161141
"Repenning A., Ioannidou A.","X-expressions in XMLisp: S-expressions and extensible markup language unite",2009,"Proceedings of the 2007 International Lisp Conference, ILC '07",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450195342&doi=10.1145%2f1622123.1622149&partnerID=40&md5=fa1bdd35a378b788de730fb8579233db","XMLisp unites S-expressions with XML into X-expressions that unify the notions of data sharing with computation. Using a combination of the Meta Object Protocol (MOP), readers and printers, X-expressions uniquely integrate XML at a language, not API level, into Lisp in a way that could not be done with other programming languages. Integration at a language level has significant advantages by making XML tangible to the programmer throughout existing Lisp development tools including editors, debuggers, inspectors, listeners and compilers. This integration with Lisp tools enables XML development in the incremental development style Lisp programmers have become accustomed to. This article describes XMLisp in the context of the AgentCubes simulation and game-authoring tool. AgentCubes is the 3D version of AgentSheets system, which is the world's most distributed Lisp-based educational simulation and game-authoring tool. Copyright 2009 ACM.","3D tools.; Meta object protocol; Object-oriented programming; XML","Authoring tool; Data Sharing; Debuggers; Development tools; Educational simulations; Extensible markup language; Incremental development; Language levels; Meta-objects; Programming language; Application programming interfaces (API); Computer systems programming; Linguistics; Markup languages; Object oriented programming; Query languages; Three dimensional; Two term control systems; XML; LISP (programming language)",2-s2.0-70450195342
"Dworak H.","The design and implementation of a documentation generator for the PRISM language",2009,"Proceedings of 2009 4th International Conference on Dependability of Computer Systems, DepCos-RELCOMEX 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449636321&doi=10.1109%2fDepCoS-RELCOMEX.2009.29&partnerID=40&md5=fd22a9f330b7eec5273073e725516517","PRISM is a tool for formal modelling and analysis of systems which exhibit random or probabilistic behaviour. Having developed a documentation generator for its language, we explain the design and the corresponding implementation of our extension to PRISM. The paper consists of a study of the unobtrusive enhancements to the parser of the original code base in order to construct software analogous to Javadoc. On the basis of our findings, one could follow the same pattern to build similar tools for other specification or programming languages. Furthermore, we describe the resulting software, which can facilitate the research within the field of the dependability of computer systems.",,"Formal modelling; Javadoc; Programming language; Computer software; Linguistics; Prisms; Query languages; Mathematical models",2-s2.0-70449636321
"Comparetti P.M., Wondracek G., Kruegel C., Kirda E.","Prospex: Protocol specification extraction",2009,"Proceedings - IEEE Symposium on Security and Privacy",109,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350402293&doi=10.1109%2fSP.2009.14&partnerID=40&md5=714a9103fc16b67ae64556c1916ff18a","Protocol reverse engineering is the process of extracting application-level specifications for network protocols. Such specifications are very useful in a number of security-related contexts, for example, to perform deep packet inspection and black-box fuzzing, or to quickly understand custom botnet command and control (C&C) channels. Since manual reverse engineering is a time-consuming and tedious process, a number of systems have been proposed that aim to automate this task. These systems either analyze network traffic directly or monitor the execution of the application that receives the protocol messages. While previous systems show that precise message formats can be extracted automatically, they do not provide a protocol specification. The reason is that they do not reverse engineer the protocol state machine. In this paper, we focus on closing this gap by presenting a system that is capable of automatically inferring state machines. This greatly enhances the results of automatic protocol reverse engineering, while further reducing the need for human interaction. We extend previous work that focuses on behavior-based message format extraction, and introduce techniques for identifying and clustering different types of messages not only based on their structure, but also according to the impact of each message on server behavior. Moreover, we present an algorithm for extracting the state machine. We have applied our techniques to a number of real-world protocols, including the command and control protocol used by a malicious bot. Our results demonstrate that we are able to extract format specifications for different types of messages and meaningful protocol state machines. We use these protocol specifications to automatically generate input for a stateful fuzzer, allowing us to discover security vulnerabilities in real-world applications. © 2009 IEEE.",,"Behavior-based; Black boxes; Command and control; Deep packet inspection; Human interactions; Message format; Network traffic; Protocol message; Protocol specifications; Real-world; Real-world application; Reverse engineers; Security vulnerabilities; Server behavior; State machine; Contour followers; Internet; Network protocols; Reengineering; Reverse engineering; Specifications; Network security",2-s2.0-70350402293
"Amin M.S., Jamil H.","FastWrap: An efficient wrapper for tabular data extraction from the Web",2009,"2009 IEEE International Conference on Information Reuse and Integration, IRI 2009",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449353701&doi=10.1109%2fIRI.2009.5211578&partnerID=40&md5=4ed845f2fce2e565998bf996d55c807d","In the last few years, several works in the literature have addressed the problem of data extraction from web pages. The importance of this problem derives from the fact that, once extracted, data can be handled in a way similar to instances of a traditional database, which in turn can facilitate application of web data integration and various other domain specific problems. In this paper, we propose a novel table extraction technique that works on web pages generated dynamically from a back-end database. The proposed system can automatically discover table structure by relevant pattern mining from web pages in an efficient way, and can generate regular expression for the extraction process. This approach requires no human intervention and experimental results have shown its accuracy to be promising. Moreover, the algorithm works in linear time to generate the wrapper.",,"Back-end database; Data extraction; Domain specific; Extraction process; Extraction techniques; Human intervention; Linear time; Regular expressions; Relevant patterns; Table structure; Tabular data; Web data integration; Web page; Data handling; World Wide Web; Information use",2-s2.0-70449353701
"Toal R., Smith D.","Convention-based syntactic descriptions",2009,"2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049181071&doi=10.1109%2fCSIE.2009.461&partnerID=40&md5=201b879840aef3d554bd44aeebd20bf5","This paper introduces a formalism for the syntactic description of programming languages. While our work clearly borrows from existing, well-known formalisms such as EBNF, its primary contribution is to provide a notation that can serve both as a concise, formal linguistic description as well as input to a parser generator. This is achieved through the use of notational conventions in place of excessive markup. These conventions are fully defined and enable a syntax to remain concise without relying on informal descriptions. Benefits of the notation include a clear separation of microsyntax and macrosyntax - a trait curiously lacking from many published programming language standards. We also provide a description of the formalism using its own notation. © 2008 IEEE.",,"Parser generators; Primary contribution; Programming language; Computer science; Computer software; Query languages; Syntactics; Linguistics",2-s2.0-71049181071
"Andrei Ş.","Parallel parsing-based reverse engineering",2009,"2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049155877&doi=10.1109%2fCSIE.2009.993&partnerID=40&md5=6f4c8b07d4e8aa8c51ca299e3baf790a","Parsing descriptive programming languages, such as eXtendable Markup Language (XML) and Unified Modeling Language (UML), has been an active area of research. Lam, Ding, and Liu claim that among all important phases of XML (e.g., parsing, access, modification, and serialization), parsing is the most time-consuming one. That motivates investigation of efficient parsing techniques with applications in many computer science areas, including reverse engineering. While there are many works on parsing XML, there is still room for research about UML parsing. UML parsing is still challenging because UML deals with graphical representations, such as class icons, class diagrams, sequence diagrams, state diagrams, and not text representations that are input for traditional parsers. Reverse engineering is an important sub-area in software engineering and it basically means obtaining the UML specification from a source program described in an (object-oriented) programming language. This paper describes a new application of a nontraditional parallel parsing technique for a Uniform Petal Language (UPL) program. Our bidirectional parser takes as input a UPL program, that is, a text representation of a UML specification. The benefits of parsing a UPL program are checking the correctness of the UML specification and obtaining the productions that lead to that UPL program. Consequently, the UPL program can be generated from the associated set of productions. The set of productions associated to a correct UPL program is a very condensed way to store it. Keeping a specification as minimum as possible, but expressive, is an important concept of reverse engineering. We have implemented a bidirectional parser for context free languages, including UPL, in Java programming language, called Parsing Uniform petAl Language (PUPAL), and compare it with CUP, a state-of-the-art parser for Java designed by Hudson, Flannery, and Ananian. The experimental results considered several UPL programs and concluded that PUPAL performs better than CUP. © 2008 IEEE.",,"Active area; Class diagrams; Graphical representations; New applications; Object oriented; Programming language; Sequence diagrams; State diagram; Text representation; UML specifications; C (programming language); Computational linguistics; Computer crime; Computer science; Computer simulation languages; Computer software; Context free languages; Graphic methods; Hypertext systems; Linguistics; Markup languages; Object oriented programming; Query languages; Reengineering; Reverse engineering; Specifications; Unified Modeling Language; XML; Java programming language",2-s2.0-71049155877
"Qiu Q.-Y., Wang Z.-X., Feng P.-E., Zhang J., Zhang H.","Working structure knowledge acquisition from mechanical product patent based on natural language understanding",2009,"2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049123322&doi=10.1109%2fCSIE.2009.848&partnerID=40&md5=140ad0948533e043e75b90cdfc9b569e","According to the increasing demand of patent knowledge in product innovative design, a working structure knowledge acquisition method, from mechanical product patent, was presented. A representing model of concept graph (CG) for patent working structure scheme, composed of technical component, technical relationship and semantic role, was established, which described the concept semantic hierarchy of patent technical component and technical relationship in mechanical field. This model divided CG into product structure composition CG, structure attribute CG and behavior CG. Furthermore, natural language understanding process of product working structure knowledge was put forward. The key technique of syntactic analysis and semantic analysis was illuminated. This method could effectively accelerate processing patent texts and establish a good foundation for sufficient utilization of patent knowledge in product innovation. © 2008 IEEE.","Concept graph; Knowledge acquisition; Patent; Working structure","Concept graph; Innovative design; Key techniques; Mechanical field; Mechanical product; Natural language understanding; Patent; Product innovation; Product structure; Semantic analysis; Semantic hierarchies; Semantic roles; Syntactic analysis; Working structure; Computer science; Knowledge acquisition; Linguistics; Machinery; Mergers and acquisitions; Semantics; Patents and inventions",2-s2.0-71049123322
"Choudhary A., Singh M.","GB theory based hindi to english translation system",2009,"Proceedings - 2009 2nd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449112939&doi=10.1109%2fICCSIT.2009.5234543&partnerID=40&md5=6179e8b375ecc56fa258eb13c723f7e9","Many research organizations in India and abroad have started developing translation systems for the Indian languages recently using conventional approaches like Ruledbased or Exampled-based or hybrid. Very few have tried to identify universality of Government and Binding (GB) theory, which emphasizes common phrase structure for aU the languages. In this paper, a machine translation system based on GB theory is proposed. The system takes Hindi as source language and English as target language. © 2009 IEEE.","Analyzer; GB theory; Hindi to english translation; Synthesizer","Analyzer; Conventional approach; GB theory; Hindi to english translation; Indian languages; Machine translation systems; Phrase structure; Research organization; Source language; Synthesizer; Target language; Translation systems; Computer aided language translation; Computer science; Information technology; Information theory; Linguistics; Query languages; Speech transmission; Translation (languages)",2-s2.0-70449112939
"Ali S., Kiefer S.","Semantic coordination of ambient intelligent medical devices - A case study",2009,"2009 3rd International Conference on Pervasive Computing Technologies for Healthcare - Pervasive Health 2009, PCTHealth 2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350746649&doi=10.4108%2fICST.PERVASIVEHEALTH2009.5975&partnerID=40&md5=8da4aacd4a8369422d23d65d6c88de0a","In this paper, we describe a clinical laboratory scenario, where the next generation Ambient Intelligent (AmI) medical devices semantically coordinate with each other not only for the diagnosis of Pheochromocytoma and/or Neuroblastoma tumors, but also the forwarding of a higher level of interpreted results to a remote health information system, using a 3G mobile device as a gateway, to assist a health professional for meticulous diagnosis. These AmI medical devices are enriched with our SOA based middleware infrastructure, named Semantic Medical Devices Space, which supports ontology based semantic discovery of desired medical devices, and provides Semantic Web Service based interface for exchanging the measurement results.","Ambient intelligence; Medical devices; Semantic interoperability; Semantic web services","3G mobile; Ambient intelligence; Ambient intelligent; Health information systems; Health professionals; Measurement results; Medical devices; Middleware infrastructure; Neuroblastomas; Ontology-based; Pheochromocytoma; Semantic interoperability; Health; Health care; Interoperability; Middleware; Mobile devices; Ontology; Semantic Web; Web services; Semantics",2-s2.0-70350746649
"Htoon E.C., Nyunt T.T.S.","M-filter: Semantic XML data filtering system for multiple queries",2009,"Proceedings of the 2009 8th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2009",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350710257&doi=10.1109%2fICIS.2009.96&partnerID=40&md5=51414cc02eaf12a9fdbaa70dd26cb624","In most web applications, publish-subscribe systems based on XML data filtering have been evolved. Many researchers have developed the various filtering mechanisms for XML data such as XFilter, YFilter, AFilter, Bitmap Filtering and TwigX-Guide. In this paper, we propose a new XML data filtering mechanism, called M-Filter which is based on ontology to get the semantic information. Proposed method intends to provide approximate information and queries which are matched with the incoming XML documents. In this system, all XPath queries are transformed into a single tree with node relation lists to filter the irrelevant paths. On the other hand, XML document is parsed by SAX parser and then prefix tree is built. After that, matching step will be carried out. Finally, we present our experimental results comparing with previous methods. © 2009 IEEE.",,"Filtering mechanism; Multiple queries; Prefix trees; Publish-subscribe systems; Semantic information; WEB application; XML data; XPath queries; Distributed computer systems; Information science; Markup languages; Ontology; Semantics; XML",2-s2.0-70350710257
"Tangtulyangkul P., Fung C.-C.","An intelligent integrated querying system for free-form information extraction from veterinary clinical records",2009,"Proceedings of the 2009 International Conference on Machine Learning and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350708174&doi=10.1109%2fICMLC.2009.5212715&partnerID=40&md5=a416814188736b30e2205903a29ea6e5","The aim of this paper is to report an intelligent integrated query system that provides and uses information from local veterinary clinical records supplemented with information from external resources. The information from the local records is used to remotely retrieve related information from external sources, in order to supplement the existing records for diagnosis and decision purposes. In Murdoch University, a large number of historical clinical data are stored in a veterinary practice management system for several years. It is necessary to provide an efficient system for query and retrieval of data for research work. The proposed solution in this paper involves text-mining (keyword extraction) and web service technologies for enhancing the existing results and to provide a comprehensive set of information related to local clinical records from the internal database. © 2009 IEEE.","Clinical database; Keyword extraction; Query; Text-mining; Veterinary records; Web services","Clinical database; Keyword extraction; Query; Text-mining; Veterinary records; Control theory; Cybernetics; Robot learning; Web services; Database systems",2-s2.0-70350708174
"Balasubramanian N., Allan J.","Syntactic query models for restatement retrieval",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350630415&doi=10.1007%2f978-3-642-03784-9_14&partnerID=40&md5=39f9c892146f85a2818d19a4d10e7f0f","We consider the problem of retrieving sentence level restatements. Formally, we define restatements as sentences that contain all or some subset of information present in a query sentence. Identifying restatements is useful for several applications such as multi-document summarization, document provenance, text reuse and novelty detection. Spurious partial matches and term dependence become important issues for restatement retrieval in these settings. To address these issues, we focus on query models that capture relative term importance and sequential term dependence. In this paper, we build query models using syntactic information such as subject-verb-objects and phrases. Our experimental results on two different collections show that syntactic query models are consistently more effective than purely statistical alternatives. © 2009 Springer.",,"Multi-document summarization; Novelty detection; Partial matches; Query model; Sentence level; Syntactic information; Term importance; Information retrieval; Information services; Towers; Syntactics",2-s2.0-70350630415
"Xi Q., Fisher K., Walker D., Zhu K.Q.","Ad hoc data and the token ambiguity problem",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350686658&doi=10.1007%2f978-3-540-92995-6_7&partnerID=40&md5=af1413c58a6817a86bf2f8fc08f5684f","pads is a declarative language used to describe the syntax and semantic properties of ad hoc data sources such as financial transactions, server logs and scientific data sets. The pads compiler reads these descriptions and generates a suite of useful data processing tools such as format translators, parsers, printers and even a query engine, all customized to the ad hoc data format in question. Recently, however, to further improve the productivity of programmers that manage ad hoc data sources, we have turned to using pads as an intermediate language in a system that first infers a pads description directly from example data and then passes that description to the original compiler for tool generation. A key subproblem in the inference engine is the token ambiguity problem - the problem of determining which substrings in the example data correspond to complex tokens such as dates, URLs, or comments. In order to solve the token ambiguity problem, the paper studies the relative effectiveness of three different statistical models for tokenizing ad hoc data. It also shows how to incorporate these models into a general and effective format inference algorithm. In addition to using a declarative language (pads) as a key intermediate form, we have implemented the system as a whole in ml. © 2009 Springer Berlin Heidelberg.",,"Data format; Data source; Declarative Languages; Financial transactions; Inference algorithm; Intermediate languages; Query engines; Scientific data; Semantic properties; Server logs; Statistical models; Sub-strings; Tokenizing; Tool generation; Ad hoc networks; Data processing; Inference engines; Linguistics; Program compilers; Query languages; Textiles; Typewriters",2-s2.0-70350686658
"Giordani A., Moschitti A.","Syntactic structural kernels for natural language interfaces to databases",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350651612&doi=10.1007%2f978-3-642-04180-8_43&partnerID=40&md5=5e95ef12638311fc81116bc6075c09b5","A core problem in data mining is to retrieve data in a easy and human friendly way. Automatically translating natural language questions into SQL queries would allow for the design of effective and useful database systems from a user viewpoint. Interesting previous work has been focused on the use of machine learning algorithms for automatically mapping natural language (NL) questions to SQL queries. In this paper, we present many structural kernels and their combinations for inducing the relational semantics between pairs of NL questions and SQL queries. We measure the effectiveness of such kernels by using them in Support Vector Machines to select the queries that correctly answer to NL questions. Experimental results on two different datasets show that our approach is viable and that syntactic information under the form of pairs of syntactic tree fragments (from queries and questions) plays a major role in deriving the relational semantics between the two languages. © 2009 Springer.","Kernel methods; Natural language processing; Support vector machines","Core problems; Data sets; Human-friendly; Kernel methods; Machine learning algorithms; Natural language interfaces; Natural language processing; Natural language questions; Natural languages; Relational semantics; SQL query; Syntactic information; Syntactic trees; Computational linguistics; Natural language processing systems; Query languages; Robot learning; Semantics; Support vector machines; Syntactics; Learning algorithms",2-s2.0-70350651612
"Lee K.-Y., Choi S.-K., Kwon O.-W., Roh Y.-H., Kim Y.-G.","Domain adaptation for English-Korean MT system: From patent domain to IT web news domain",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350656264&doi=10.1007%2f978-3-642-00831-3_31&partnerID=40&md5=6e3ae2e47271225233f0dfde4337053c","This paper addresses a method to adapt an existing machine translation (MT) system for a newly targeted translation domain. Especially, we give detailed descriptions of customizing a patent specific English-Korean machine translation system for IT web news domain. The proposed method includes the followings: constructing a corpus from documents of IT web news domain, analyzing characteristics of IT web news sentences according to each viewpoint of MT system modules (tagger, parser, transfer) and translation knowledge, and adapting each MT system modules and translation knowledge considering characteristics of IT web news domain. To evaluate our domain adaptation method, we conducted a human evaluation and an automatic evaluation. The experiment showed promising results for diverse sentences extracted from IT Web News documents. © 2009 Springer Berlin Heidelberg.","Domain adaptation; Machine translation","Automatic evaluation; Domain adaptation; Human evaluation; Machine translation; Machine translation systems; News domain; System modules; Translation knowledge; Computer aided language translation; Information theory; Knowledge based systems; Knowledge management; Linguistics; Patents and inventions; Query languages; Speech recognition; Speech transmission; Telluric prospecting; Translation (languages)",2-s2.0-70350656264
"Chen Z., Motet G.","A language-theoretic view on guidelines and consistency rules of UML",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350641401&doi=10.1007%2f978-3-642-02674-4_6&partnerID=40&md5=af69b23fb7a1e9a2b40f48cef095f2bb","Guidelines and consistency rules of UML are used to control the degrees of freedom provided by the language to prevent faults. Guidelines are used in specific domains (e.g., avionics) to recommend the proper use of technologies. Consistency rules are used to deal with inconsistencies in models. However, guidelines and consistency rules use informal restrictions on the uses of languages, which makes checking difficult. In this paper, we consider these problems from a language-theoretic view. We propose the formalism of C-Systems, short for ""formal language control systems"". A C-System consists of a controlled grammar and a controlling grammar. Guidelines and consistency rules are formalized as controlling grammars that control the uses of UML, i.e. the derivations using the grammar of UML. This approach can be implemented as a parser, which can automatically verify the rules on a UML user model in XMI format. A comparison to related work shows our contribution: a generic top-down and syntax-based approach that checks language level constraints at compile-time. © 2009 Springer Berlin Heidelberg.",,"C-systems; Compile time; Degrees of freedom; Language levels; Syntax-based approach; Topdown; User models; Formal languages; Query languages; Linguistics",2-s2.0-70350641401
"Huang M., Haralick R.M.","A probabilistic graphical model for recognizing np chunks in texts",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350663032&doi=10.1007%2f978-3-642-00831-3_3&partnerID=40&md5=aa10fff654fc403ee8e1c06d6051d920","We present a probabilistic graphical model for identifying noun phrase patterns in texts. This model is derived from mathematical processes under two reasonable conditional independence assumptions with different perspectives compared with other graphical models, such as CRFs or MEMMs. Empirical results shown our model is effective. Experiments on WSJ data from the Penn Treebank, our method achieves an average of precision 97.7% and an average of recall 98.7%. Further experiments on the CoNLL-2000 shared task data set show our method achieves the best performance compared to competing methods that other researchers have published on this data set. Our average precision is 95.15% and an average recall is 96.05%. © 2009 Springer Berlin Heidelberg.","Cliques; Graphical models; NP chunking; Separators","Cliques; Conditional independence assumption; Data sets; Empirical results; GraphicaL model; Graphical models; Noun phrase; NP chunking; Probabilistic graphical models; Treebanks; Feature extraction; Graphic methods; Knowledge based systems; Knowledge management; Linguistics; Query languages; Separators; Speech recognition",2-s2.0-70350663032
"Sanchez Cuadrado J., Molina J.G.","A model-based approach to families of embedded domain-specific languages",2009,"IEEE Transactions on Software Engineering",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549110067&doi=10.1109%2fTSE.2009.14&partnerID=40&md5=13c4c9aa4ca68e5eeca16a21b763a5d5","With the emergence of model-driven engineering (MDE), the creation of domain-specific languages (DSLs) is becoming a fundamental part of language engineering. The development cost of a DSL should be modest compared to the cost of developing a general-purpose programming language. Reducing the implementation effort and providing reuse techniques are key aspects for DSL approaches to be really effective. In this paper, we present an approach to build embedded domain-specific languages applying the principles of model-driven engineering. On the basis of this approach, we will tackle reuse of DSLs by defining families of DSLs, addressing reuse both from the DSL developer and user point of views. A family of DSLs will be built up by composing several DSLs, so we will propose composition mechanisms for the abstract syntax, concrete syntax, and model transformation levels of a DSL's definition. Finally, we contribute a software framework to support our approach, and we illustrate the paper with a case study to demonstrate its practical applicability. © 2009 IEEE.","Domain-specific languages; DSL composition; Families of DSLs; Model-driven development","Abstract syntax; Composition mechanisms; Concrete syntax; Development costs; Domain-specific languages; Embedded domains; General-purpose programming language; Language engineering; Model based approach; Model driven development; Model transformation; Model-driven engineering; Software frameworks; DSL; Modems; Problem oriented languages; Query languages; Syntactics; Telecommunication lines; Linguistics",2-s2.0-73549110067
"Bowden P., Beavis R., Marshall J.","Tandem mass spectrometry of human tryptic blood peptides calculated by a statistical algorithm and captured by a relational database with exploration by a general statistical analysis system",2009,"Journal of Proteomics",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349992387&doi=10.1016%2fj.jprot.2009.08.004&partnerID=40&md5=b33bac607ed92b70f06553a16ffa1ea7","A goodness of fit test may be used to assign tandem mass spectra of peptides to amino acid sequences and to directly calculate the expected probability of mis-identification. The product of the peptide expectation values directly yields the probability that the parent protein has been mis-identified. A relational database could capture the mass spectral data, the best fit results, and permit subsequent calculations by a general statistical analysis system. The many files of the Hupo blood protein data correlated by X!TANDEM against the proteins of ENSEMBL were collected into a relational database. A redundant set of 247,077 proteins and peptides were correlated by X!TANDEM, and that was collapsed to a set of 34,956 peptides from 13,379 distinct proteins. About 6875 distinct proteins were only represented by a single distinct peptide, 2866 proteins showed 2 distinct peptides, and 3454 proteins showed at least three distinct peptides by X!TANDEM. More than 99% of the peptides were associated with proteins that had cumulative expectation values, i.e. probability of false positive identification, of one in one hundred or less. The distribution of peptides per protein from X!TANDEM was significantly different than those expected from random assignment of peptides. © 2009 Elsevier B.V. All rights reserved.",".xml; Database; Human; Parser; Peptide; Plasma; Protein; Structure query language; X!TANDEM","peptide; plasma protein; algorithm; amino acid sequence; article; calculation; peptide analysis; priority journal; protein analysis; protein database; statistical analysis; tandem mass spectrometry; Algorithms; Automatic Data Processing; Blood Proteins; Data Interpretation, Statistical; Databases, Protein; Humans; Peptide Fragments; Peptide Library; Protein Hydrolysates; Software; Tandem Mass Spectrometry; Tryptases",2-s2.0-70349992387
"Kim S.M., Yoo S.I.","DOM tree browsing of a very large XML document: Design and implementation",2009,"Journal of Systems and Software",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749090588&doi=10.1016%2fj.jss.2009.05.043&partnerID=40&md5=fe3516ba16b424244156919a465df948","Browsing the DOM tree of an XML document is an act of following the links among the nodes of the DOM tree to find some desired nodes without any knowledge for search. When the structure of the XML document is not known to a user, browsing is the basic operation performed for referring the contents of the XML document. If the size of the XML document is very large, however, using a general-purpose XML parser for browsing the DOM tree of the XML document to access arbitrary node may suffer from the lack of memory space for constructing the large DOM tree. To alleviate this problem, we suggest a method to browse the DOM tree of a very large XML document by splitting the XML document into n small XML documents and generating sequentially the DOM tree of each of those small n XML documents. For later reference, the information of some nodes accessed from the DOM tree already generated has been also kept using the concept of their virtual nodes. With our suggested approach, the memory space necessary for browsing the DOM tree of a very large XML document is reduced such that it can be managed by a personal computer. © 2009 Elsevier Inc. All rights reserved.","DOM; DOM tree; Very large XML documents; XML","Basic operation; DOM; DOM tree; Memory space; Virtual node; XML parser; Biological materials; Markup languages; Personal computers; XML",2-s2.0-71749090588
"Arisoy E., Saraçlar M.","Analysis of the recognition errors in LVCSR of Turkish [Türkçe GDSKT için konuşma tanima hatalarinin analizi]",2009,"2009 IEEE 17th Signal Processing and Communications Applications Conference, SIU 2009",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350339513&doi=10.1109%2fSIU.2009.5136407&partnerID=40&md5=9646327ef47bb756279d86981545a069","This paper presents the analysis of recognition errors in Large Vocabulary Continuous Speech Recognition (LVCSR) of Turkish. This analysis aims to learn the source of the recognition errors and investigate useful features to rectify them. These features will be used in corrective language models. First, recognition experiments were performed using word and sub-word (morph) language models. Morphs outperformed words for out-of-vocabulary words and achieved 1.5% absolute significant improvements over words. Then, the errors in the recognition output of the morph model were manually labeled according to the predefined error classes. This subjective labeling revealed that errors due to incorrect syntax can be corrected. Therefore, using syntactic dependency relations as features in the corrective language models is expected to yield higher accuracies. ©2009 IEEE.",,"Dependency relation; Language model; Large vocabulary continuous speech recognition; Out-of-vocabulary words; Recognition error; Turkishs; Computational linguistics; Continuous speech recognition; Query languages; Signal processing; Speech processing; Syntactics; Error detection",2-s2.0-70350339513
"Sarasa-Cabezuelo A., Temprado-Battad B., Sierra J.-L., Fernández-Valmayor A.","XML language-oriented processing with XLOP",2009,"Proceedings - International Conference on Advanced Information Networking and Applications, AINA",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350106543&doi=10.1109%2fWAINA.2009.17&partnerID=40&md5=062f62a78599311c940cc336047065e5","This paper presents XLOP (XML Language-Oriented Processing), an environment that permits the description of XML processing applications with attribute grammars (formalism used to describe the semantics of computer languages). The environment also includes a generator able to translate attribute grammar-based specifications into the specification language of CUP, a well-known YACC-like environment for writing language processors. The final goal of XLOP is to facilitate the development and maintenance of XML processing applications, whilst preserving the flexibility of general-purpose XML processing models. Also, since it is based on attribute grammars, XLOP provides a higher abstraction level than other similar translation scheme-based approaches. © 2009 IEEE.","Attribute grammar; CUP; Syntax-directed translation; XLOP; XML processing","Attribute grammar; CUP; Syntax-directed translation; XLOP; XML processing; Context sensitive grammars; Linguistics; Markup languages; Query languages; Specification languages; Specifications; Syntactics; Technical presentations; XML; Translation (languages)",2-s2.0-70350106543
"Adda M., Missaoui R., Valtchev P.","Toward feedback-based web search engine",2009,"Proceedings - International Conference on Advanced Information Networking and Applications, AINA",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350093541&doi=10.1109%2fWAINA.2009.22&partnerID=40&md5=9b02a9b69c830b0da8c1fbce5659a1d6","The capability to easily find useful information on the Web becomes increasingly difficult as the available content increases. To assist users in finding relevant information Web Search Engines are developed. Those engines apply different strategies to measure the relevance of a Web page (its rank). However, page ranking is mainly conducted by relying on automatic assessment criteria. Hence, a gap is created between the effective relevance of a content and the computed one. To reduce this gap, we introduce a framework for feedbackbased web search engine development. To illustrate the effectiveness and the use of the proposed framework, we developed a web search engine prototype called SocialSeeker. © 2009 IEEE.",,"Automatic assessment; Page ranking; Web page; Web search engines; Engines; Information retrieval; Search engines; Technical presentations; World Wide Web",2-s2.0-70350093541
"Chengyang Z., Yan H., Mihalcea R., Cuellar H.","A natural language interface for crime-related spatial queries",2009,"2009 IEEE International Conference on Intelligence and Security Informatics, ISI 2009",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350059300&doi=10.1109%2fISI.2009.5137290&partnerID=40&md5=cac9383e0f13419607beba9772dbdfd8","Web-based mapping applications such as Google Maps or Virtual Earth have become increasingly popular. However, current map search is still keyword-based and supports a limited number of spatial predicates. In this paper, we build towards a natural language query interface to spatial databases to answer crime-related spatial queries. The system has two main advantages compared with interfaces such as Google Maps: (1) It allows query conditions to be expressed in natural language, and (2) It supports a larger number of spatial predicates, such as ""within 3 miles"" and ""close to"". The system is evaluated using a set of crime-related queries run against a dataset that contains many spatial layers in the Denton, Texas area. The results show that our approach significantly outperforms Google Maps when processing complicated spatial queries. ©2009 IEEE.",,"Data sets; Google maps; Mapping applications; Natural language interfaces; Natural language queries; Natural languages; Query conditions; Spatial database; Spatial predicates; Spatial queries; Internet; Intersymbol interference; Law enforcement; Linguistics; Query languages",2-s2.0-70350059300
"Paige R.F., Kolovos D.S., Rose L.M., Drivalos N., Polack F.A.C.","The design of a conceptual framework and technical infrastructure for model management language engineering",2009,"Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS",37,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350041761&doi=10.1109%2fICECCS.2009.14&partnerID=40&md5=c1a4a657004ae64737df81de92f6ba0d","Model management is the discipline of managing artefacts used in Model-Driven Engineering (MDE). A model management framework defines and implements the operations (such as transformation or code generation) required to manipulate MDE artefacts. Modern approaches to model management generally implement these operations via domain-specific languages (DSLs). This paper presents and compares the principles behind three approaches to implementing DSLs for model management and identifies some of the key differences between DSL engineering in general and for model management. It then shows how theory relates to practice by illustrating how DSL design and implementation approaches have been used in practice to build working languages from the Epsilon model management framework. A set of questions for guiding the development of new model management DSLs is summarised, and data on development costs for the different approaches is presented. © 2009 IEEE.",,"Code Generation; Conceptual frameworks; Development costs; Domain specific languages; Implementation approach; Language engineering; Model management; Model-driven engineering; New model; Technical infrastructure; DSL; Engineering; Linguistics; Management; Modems; Query languages; Telecommunication lines; Public key cryptography",2-s2.0-70350041761
"Nilsson J., Löwe W., Hall J., Nivre J.","Natural language parsing for fact extraction from source code",2009,"IEEE International Conference on Program Comprehension",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349984813&doi=10.1109%2fICPC.2009.5090046&partnerID=40&md5=894a703078e93f5f435ae7215ef4796e","We present a novel approach to extract structural information from source code using state-of-the-art parser technologies for natural languages. The parser technology is robust in the sense that it guarantees to produce some output, entailing that even incomplete or incorrect source code as input will get some kind of analysis. This comes at the expense of possibly assigning a partially incorrect analysis for input free of errors. However, an evaluation on source codes of the Java, Python and C/C++ languages shows that the committed errors are few i.e., our accuracy is close to 100%. The error analysis indicates that the majority of the errors remaining are harmless. © 2009 IEEE.",,"Fact extraction; Natural language parsing; Natural languages; Source codes; Structural information; Error analysis; Errors; Linguistics; Query languages; Java programming language",2-s2.0-70349984813
"Hu D.H., Dong F., Wang C.-L.","A semantic context management framework on mobile device",2009,"Proceedings - 2009 International Conference on Embedded Software and Systems, ICESS 2009",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349673709&doi=10.1109%2fICESS.2009.95&partnerID=40&md5=686444344ff62b5849bb34a68854a164","We present a semantic context management framework named ContextTorrent, which can make various types of context information be semantically searchable and sharable among local and remote context-aware applications. We implement this framework on the Google Android platform with its elegant application support. An open source RDF parser has been extended to effectively get RDF triples from files or over the network. Three embedded database systems were evaluated for storing ontology represented contexts in the resource-constrained mobile devices. We use the FOAF ontology schema and a synthetic data set of up to 2500 records to evaluate the context query and storage performance. Ordinary context queries can be replied instantaneously.",,"Context information; Embedded database; Open sources; Remote contexts; Resource-constrained; Semantic context; Storage performance; Synthetic datasets; Database systems; Embedded systems; Mobile devices; Ontology; Portable equipment; Semantic Web; Semantics; Wireless networks; Embedded software",2-s2.0-70349673709
"Demaille A., Durlin R., Pierron N., Sigoure B.","Semantics Driven Disambiguation: A Comparison of Different Approaches",2009,"Electronic Notes in Theoretical Computer Science",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349400462&doi=10.1016%2fj.entcs.2009.09.043&partnerID=40&md5=167349c6596e55a1f1fef33d69bf5d55","Context-sensitive languages such as or can be parsed using a context-free but ambiguous grammar, which requires another stage, disambiguation, in order to select the single parse tree that complies with the language's semantical rules. Naturally, large and complex languages induce large and complex disambiguation stages. If, in addition, the parser should be extensible, for instance to enable the embedding of domain specific languages, the disambiguation techniques should feature traditional software-engineering qualities: modularity, extensibility, scalability and expressiveness. We evaluate three approaches to write disambiguation filters for SDF grammars: algebraic equations with ASF, rewrite-rules with programmable traversals for Stratego, and attribute grammars with TAG (TransformersAttribute Grammar), our system. To this end we introduce Phenix, a highly ambiguous language. Its ""standard"" grammar exhibits ambiguities inspired by those found in the and standard grammars. To evaluate modularity, the grammar is layered: it starts with a small core language, and several layers add new features, new production rules, and new ambiguities. © 2009 Elsevier B.V. All rights reserved.","ASF; attribute grammar; context-free grammar; disambiguation; parsing; program transformation; SDF; Stratego; term rewriting; Transformers","ASF; attribute grammar; disambiguation; parsing; program transformation; SDF; Stratego; term rewriting; Transformers; Computational linguistics; Context sensitive grammars; Context sensitive languages; Linguistics; Query languages; Response time (computer systems); Systems analysis; Context free languages",2-s2.0-70349400462
"Fritzson P., Pop A., Broman D., Aronsson P.","Formal semantics based translator generation and tool development in practice",2009,"Proceedings of the Australian Software Engineering Conference, ASWEC",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349493342&doi=10.1109%2fASWEC.2009.46&partnerID=40&md5=36853d42517dfc00c04c161da2074964","In this paper we report on a long-term research effort to develop and use efficient language implementation generators in practice. The generator is applied to a number of different languages, some of which are used for projects in industry. The used formal specification style is Operational Semantics, primarily in the form called Natural Semantics, represented and supported by a meta-language and tool called the Relational Meta Language (RML), which can generate efficient implementations in C, on par with hand-implemented code. Generating implementations from formal specifications are assumed to give advantages such as: high level descriptions, higher degree of correctness, and consistency between specification and implementation. To what extent can this be realized in practice? Does it scale to large language implementations? To answer some of these questions we have developed specifications of a range of languages: imperative, functional, object-oriented (Java), and equation-based (Modelica). The size of specifications range from half a page to large specifications of 60 000 lines. It turns out to be possible to generate efficient compilers, also for large languages. However, the performance of the generator tool and the user support of the development environment become increasingly important for large specifications. To satisfy such user needs the speed of the generator was increased a factor of ten to reduce turn-around time, and an Eclipse plug-in including a debugger were developed. For very large specifications, the structuring and modularity of the specification itself also become essential for performance and maintainability. © 2009 Crown Copyright.",,"Debuggers; Development environment; Efficient implementation; Formal Semantics; Formal Specification; Generator tool; High level description; Higher-degree; Meta language; Modelica; Natural semantics; Object oriented; Operational semantics; Plug-ins; Research efforts; Tool development; User need; User support; Computer software; Formal methods; Java programming language; Linguistics; Maintainability; Query languages; Report generators; Semantics; Specifications; Object oriented programming",2-s2.0-70349493342
"Kats L.C.L., De Jonge M., Emma N.-N., Visser E.","Providing rapid feedback in generated modular language environments: Adding error recovery to scannerless generalized-LR parsing",2009,"ACM SIGPLAN Notices",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350634086&partnerID=40&md5=2f194d3e7871543791de9458c010c6ba","Integrated development environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. A heavy burden lies on developers of new languages to provide adequate IDE support. Code generation techniques provide a viable, efficient approach to semi-automatically produce IDE plugins. Key components for the realization of plugins are the language's grammar and parser. For embedded languages and language extensions, constituent IDE plugin modules and their grammars can be combined. Unlike conventional parsing algorithms, scannerless generalized-LR parsing supports the full set of context-free grammars, which is closed under composition, and hence can parse language embeddings and extensions composed from separate grammar modules. To apply this algorithm in an interactive environment, this paper introduces a novel error recovery mechanism, which allows it to be used with files with syntax errors-common in interactive editing. Error recovery is vital for providing rapid feedback in case of syntax errors, as most IDE services depend on the parser-from syntax highlighting to semantic analysis and cross-referencing. We base our approach on the principles of island grammars, and derive permissive grammars with error recovery productions from normal SDF grammars. To cope with the added complexity of these grammars, we adapt the parser to support backtracking. We evaluate the recovery quality and performance of our approach using a set of composed languages, based on Java and Stratego. © 2009 ACM.",,"Code Generation; Embedded Languages; Embeddings; Error recovery; Error recovery mechanisms; Integrated development environment; Interactive editing; Interactive Environments; Interactive feedback; Key component; Language extensions; LR parsing; Modular language; Parsing algorithm; Plug-ins; Programmer productivity; Rapid feedback; Recovery quality; Semantic analysis; Stratego; Syntax errors; Computational linguistics; Electric reactors; Errors; Integrodifferential equations; Java programming language; Linguistics; Quality control; Query languages; Semantics; Syntactics; Context free languages",2-s2.0-70350634086
"Bloom B., Field J., Nystrom N., Östlund J., Richards G., Strniša R., Vitek J., Wrigstad T.","Thorn-robust, concurrent, extensible scripting on the JVM",2009,"ACM SIGPLAN Notices",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350630552&partnerID=40&md5=7cc30673f26dbf5be65ed614ae418a53","Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency-though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs-e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine. © 2009 ACM.","Actors; Pattern matching; Scripting","Actors; Data type; Dynamic typing; Java virtual machines; Module systems; Plug-ins; Programming language; Remote services; Scripting; Scripting languages; Software life cycles; Data privacy; Linguistics; Message passing; Pattern matching; Query languages; Java programming language",2-s2.0-70350630552
"Knöll R., Mezini M.","π-a Pattern language",2009,"ACM SIGPLAN Notices",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350656218&partnerID=40&md5=4079e3c2c19c95b062abd298238f0d02","Current programming languages and techniques realize many features which allow their users to extend these languages on a semantic basis: classes, functions, interfaces, aspects and other entities can be defined. However, there is a lack of modern programming languages which are both semantically and syntactically extensible from within the language itself, i.e., with no additional tool or meta-language. In this paper we present π as an approach that aims to overcome this lack. π provides an abstraction mechanism based on parameterized symbols which is capable of semantically and syntactically unifying programming concepts like variables, control-structures, procedures and functions into one concept: the pattern. We have evaluated the abstraction potential and the syntactic extensibility of π by successfully creating patterns for the aforementioned programming concepts. π could serve as a tool for designing new experimental languages and might generally influence the view we have on current programming concepts. © 2009 ACM.","Domain specific languages; Extensibility; Language design; Language extension; Macros; Pattern language; Patterns; Semiotics","Domain specific languages; Extensibility; Language design; Language extension; Pattern language; Patterns; Abstracting; Computer software; Demultiplexing; Semiotics; XML; Query languages",2-s2.0-70350656218
"Levit M., Hakkani-Tür D., Tur G., Gillick D.","IXIR: A statistical information distillation system",2009,"Computer Speech and Language",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349266195&doi=10.1016%2fj.csl.2009.03.006&partnerID=40&md5=1a0ad61ce53e67be1946a194cb1d2408","The task of information distillation is to extract snippets from massive multilingual audio and textual document sources that are relevant for a given templated query. We present an approach that focuses on the sentence extraction phase of the distillation process. It selects document sentences with respect to their relevance to a query via statistical classification with support vector machines. The distinguishing contribution of the approach is a novel method to generate classification features. The features are extracted from charts, compilations of elements from various annotation layers, such as word transcriptions, syntactic and semantic parses, and information extraction (IE) annotations. We describe a procedure for creating charts from documents and queries, while paying special attention to query slots (free-text descriptions of names, organizations, topic, events and so on, around which templates are centered), and suggest various types of classification features that can be extracted from these charts. While observing a 30% relative improvement due to non-lexical annotation layers, we perform a detailed analysis of the contributions of each of these layers to classification performance. © 2009 Elsevier Ltd.","Information distillation; Information extraction; Machine learning; Natural language processing; Question answering","Information distillation; Information extraction; Machine learning; Natural language processing; Question answering; Computational linguistics; Distillation; Distillation equipment; Graphic methods; Information retrieval; Information retrieval systems; Information theory; Natural language processing systems; Robot learning; Text processing; Feature extraction",2-s2.0-67349266195
"Peiravi A., Rahimzadeh M.J.","A novel scalable and storage-efficient architecture for high speed exact string matching",2009,"ETRI Journal",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350140185&doi=10.4218%2fetrij.09.0108.0353&partnerID=40&md5=ebef9c1288f1bc5ae8aae29c4b46fbff","String matching is a fundamental element of an important category of modern packet processing applications which involve scanning the content flowing through a network for thousands of strings at the line rate. To keep pace with high network speeds, specialized hardware-based solutions are needed which should be efficient enough to maintain scalability in terms of speed and the number of strings. In this paper, a novel architecture based upon a recently proposed data structure called the Bloomier filter is proposed which can successfully support scalability. The Bloomier filter is a compact data structure for encoding arbitrary functions, and it supports approximate evaluation queries. By eliminating the Bloomier filter,s false positives in a space efficient way, a simple yet powerful exact string matching architecture is proposed that can handle several thousand strings at high rates and is amenable to on-chip realization. The proposed scheme is implemented in reconfigurable hardware and we compare it with existing solutions. The results show that the proposed approach achieves better performance compared to other existing architectures measured in terms of throughput per logic cells per character as a metric.","Bloomier filter; Content scanning; Pearson's hash; String matching","Approximate evaluation; Arbitrary functions; Bloomier filter; Compact data structure; Content scanning; Efficient architecture; False positive; High rate; Line rate; Logic cells; Network speed; Novel architecture; On chips; Packet processing; Pearson's hash; Reconfigurable hardwares; Specialized hardware; String matching; Data structures; Logic devices; Scalability; Scanning; Packet networks",2-s2.0-70350140185
"Chapman W.W., Cohen K.B.","Current issues in biomedical text mining and natural language processing",2009,"Journal of Biomedical Informatics",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349445262&doi=10.1016%2fj.jbi.2009.09.001&partnerID=40&md5=f7cb234ed59a3796e15c08d0251ccede",[No abstract available],,"bioinformatics; biomedicine; data analysis; data base; editorial; electronic medical record; information processing; information retrieval; language processing; medical research; MEDLINE; priority journal; publication; Biomedical Research; Data Mining; Databases as Topic; Medical Informatics; Natural Language Processing",2-s2.0-70349445262
"Yang H., Keane J., Bergman C.M., Nenadic G.","Assigning roles to protein mentions: The case of transcription factors",2009,"Journal of Biomedical Informatics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349456436&doi=10.1016%2fj.jbi.2009.04.001&partnerID=40&md5=58984717d812a12cf326c3f82bb19208","Transcription factors (TFs) play a crucial role in gene regulation, and providing structured and curated information about them is important for genome biology. Manual curation of TF related data is time-consuming and always lags behind the actual knowledge available in the biomedical literature. Here we present a machine-learning text mining approach for identification and tagging of protein mentions that play a TF role in a given context to support the curation process. More precisely, the method explicitly identifies those protein mentions in text that refer to their potential TF functions. The prediction features are engineered from the results of shallow parsing and domain-specific processing (recognition of relevant appearing in phrases) and a phrase-based Conditional Random Fields (CRF) model is used to capture the content and context information of candidate entities. The proposed approach for the identification of TF mentions has been tested on a set of evidence sentences from the TRANSFAC and FlyTF databases. It achieved an F-measure of around 51.5% with a precision of 62.5% using 5-fold cross-validation evaluation. The experimental results suggest that the phrase-based CRF model benefits from the flexibility to use correlated domain-specific features that describe the dependencies between TFs and other entities. To the best of our knowledge, this work is one of the first attempts to apply text-mining techniques to the task of assigning semantic roles to protein mentions. © 2009 Elsevier Inc. All rights reserved.","Conditional Random Fields; Information extraction; Text mining; Transcription factor identification","Biomedical literature; Conditional random field; Conditional Random Fields; Context information; Cross validation; Curation; Domain specific; F-measure; Gene regulations; Information extraction; Machine-learning; Semantic roles; Shallow parsing; Text mining; TRANSFAC; Biology; Content based retrieval; Information analysis; Mining; Transcription factors; Transcription; transcription factor; accuracy; article; controlled study; data analysis; data extraction; data synthesis; information processing; information system; machine learning; priority journal; protein function; reference database; semantics; Artificial Intelligence; Information Storage and Retrieval; Models, Statistical; Natural Language Processing; Pattern Recognition, Automated; Periodicals as Topic; Reproducibility of Results; Transcription Factors",2-s2.0-70349456436
"Chilingaryan S.","The XMLbench project: Comparison of fast, multi-platform XML libraries",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349318225&doi=10.1007%2f978-3-642-04205-8_4&partnerID=40&md5=f630b5fa8508a07ddaba364ab8af5e68","The XML technologies have brought a lot of new ideas and abilities in the field of information management systems. Nowadays, XML is used almost everywhere: from small configuration files to multi-gigabyte archives of measurements. Many network services are using XML as transport protocol. XML based applications are utilizing multiple XML technologies to simplify software development: DOM is used to create and navigate XML documents, XSD schema is used to check consistency and validity, XSL simplifies transformation between different formats, XML Encryption and Signature establishes secure and trustworthy way of information exchange and storage. These technologies are provided by multiple commercial and open source libraries which are significantly varied in features and performance. Moreover, some libraries are optimized to certain tasks and, therefore, the actual library performance could significantly vary depending on the type of data processed. XMLBench project was started to provide comprehensive comparison of available XML toolkits in their functionality and ability to sustain required performance. The main target was fast C and C++ libraries able to work on multiple platforms. The applied tests compare different aspects of XML processing and are run on few auto-generated data sets emulating library usage for different tasks. The details of test setup and achieved results will be presented. © 2009 Springer Berlin Heidelberg.",,"C++ libraries; Configuration files; Data sets; Information exchanges; Information management systems; Multi-platform; Multiple platforms; Network services; Open-source libraries; Software development; Test setups; Transport protocols; XML based application; XML encryption; XML processing; XML technology; Cryptography; Database systems; Information management; Libraries; Network protocols; Network security; Technical presentations; XML; Markup languages",2-s2.0-70349318225
"Swierstra S.D.","Combinator parsing: A short tutorial",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349113488&doi=10.1007%2f978-3-642-03153-3_6&partnerID=40&md5=7bd7ac9ce81c54128f0c1fb67e3bc7c1","There are numerous ways to implement a parser for a given syntax; using parser combinators is a powerful approach to parsing which derives much of its power and expressiveness from the type system and semantics of the host programming language. This tutorial begins with the construction of a small library of parsing combinators. This library introduces the basics of combinator parsing and, more generally, demonstrates how domain specific embedded languages are able to leverage the facilities of the host language. After having constructed our small combinator library, we investigate some shortcomings of the naïve implementation introduced in the first part, and incrementally develop an implementation without these problems. Finally we discuss some further extensions of the presented library and compare our approach with similar libraries. © 2009 Springer Berlin Heidelberg.",,"Combinator library; Combinators; Domain specific; Embedded Languages; Parser combinators; Programming language; Type systems; Computer software; Libraries; Linguistics; Software design; Query languages",2-s2.0-70349113488
"Bosetti P., Biral F.","Rapid development of a CNC software within Manufacturing Automation courses",2009,"ASME International Mechanical Engineering Congress and Exposition, Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70149117499&doi=10.1115%2fIMECE2008-68420&partnerID=40&md5=b2c6b1a2e3a254a960aa696a3884d4d3","A significant part of a Manufacturing Automation course is devoted to CNC machine tools, their architecture, and the part programming languages (APT, ISO G-code and so on). Nevertheless, it is not trivial to provide the students with a complete understanding of the relationships between the machine dynamics, the path planning strategy, and the control systems. For this reason, a short laboratory course has been developed aimed at the programming of a simplified CNC software with the following functionalities: to parse a part program written in a subset of the ISO G-code; to process the instruction blocks and to generate trapezoidal velocity profiles; to convert the velocity profiles reference lrajectory; to interpolate the reference trajectory at the servo loop control frequency. In order to shorten the development time, the following choices have been made. The parser only implements a small subset of the ISO G-code, which is point-to-point positioning (G00), linear interpolation (G01) and full stop (M30). The velocity profiles are calculated as acceleration-limited trapezoidal profiles with zero-feed velocity boundary conditions. Finally, the system is developed in Ruby, which is an object-oriented scripting language, easy to learn and well suited for rapid prototyping of complex software systems. This is why the project has been named RNC (Ruby Numerical Control). At the course start, the overall system architecture is explained and is translated in the set of Ruby classes that have to be developed, and classes interfaces are mandatorily determined. During the laboratory activity, students work in teams, and each team is encouraged to work separately on the development of each Ruby class. At the end of the development phase, the students can interface the RNC they wrote with a machine tool simulator (developed separately) and use the whole software system to test the accuracy of the tool-tip trajectories as a function of the system parameters (servo loop gains, motors torque, masses and dynamic performance of the virtual machine tool). Moreover, thanks to the object-based architecture of RNC and to the common, pre-determined class interfaces, the students can then swap and mix different implementations of the above reported functionalities, as well as enhanced versions provided by the teacher. With respect to other similar Mathlab/Simulink based solutions, the presented laboratory activity brings a more delailed insighl into a CNC software still limiting the code complexity thanks to the Ruby language and it is only based on open-source tools. Copyright © 2008 by ASME.",,"CAN interface; Class interfaces; CNC machine tools; Code complexity; Complex software systems; Development phase; Development time; Dynamic performance; Feed velocity; Laboratory course; Linear Interpolation; Machine dynamics; Manufacturing Automation; Numerical control; Object based; Object-oriented scripting languages; Open source tools; Part programming; Part programs; Path-planning; Point positioning; Rapid development; Reference trajectories; Servo loops; Software systems; System architectures; Trapezoidal velocity profile; Velocity profiles; Virtual machine tools; Computer control systems; Concurrent engineering; Corundum; Job analysis; Linguistics; Machine tools; Numerical control systems; Object oriented programming; Phase interfaces; Query languages; Rapid prototyping; Ruby; Software prototyping; Structured programming; Students; Tropical engineering; Velocity; Computer software",2-s2.0-70149117499
"Cuoq P., Signoles J., Baudin P., Bonichon R., Canet G., Correnson L., Monate B., Prevosto V., Puccetti A.","Experience report: OCaml for an industrial-strength static analysis framework",2009,"ACM SIGPLAN Notices",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350442621&partnerID=40&md5=1f09e5814a41f2e1a11de53fbf5ef207","This experience report describes the choice of OCaml as the implementation language for Frama-C, a framework for the static analysis of C programs. OCaml became the implementation language for Frama-C because it is expressive. Most of the reasons listed in the remaining of this article are secondary reasons, features which are not specific to OCaml (modularity, availability of a C parser, control over the use of resources. . . ) but could have prevented the use of OCaml for this project if they had been missing. ©2009 ACM.","Design; Languages; Verification","C programs; Experience report; Implementation languages; Languages; Query languages; Static analysis; Linguistics",2-s2.0-70350442621
"Martínez-Ortiz I., Sierra J.-L., Fernández-Manjón B., Fernández-Valmayor A.","Language engineering techniques for the development of e-learning applications",2009,"Journal of Network and Computer Applications",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649550043&doi=10.1016%2fj.jnca.2009.02.005&partnerID=40&md5=a9ba4ddfac84672afa476239ec81f585","In this paper we propose the use of language engineering techniques to improve and systematize the development of e-learning applications. E-learning specifications usually rely on domain-specific languages that describe different aspects of such final e-learning applications. This fact makes it natural to adopt well-established language engineering principles during the construction of these applications. These principles promote the specification of the structure and the runtime behavior of the domain-specific languages as the central part of the development process. This specification can be used to drive different activities: rapid prototyping, provision of authoring notations and tools, automatic model checking of properties, importation/exportation from/to standards, and deployment of running applications. This language engineering concept also promotes active collaboration between instructors (the users of the languages) and developers (the designers and implementers) throughout the development process. In this paper we describe this language-driven approach to the construction of e-learning applications and we illustrate all its aspects using a learning flow sequencing language as a case study. © 2009 Elsevier Ltd. All rights reserved.","Authoring; Domain-specific languages; E-learning applications; Language engineering; Model checking; Rapid prototyping","Authoring; Automatic models; Development process; Domain-specific languages; e-Learning application; E-learning applications; Language engineering; Language-driven approach; Running applications; Runtime; Concurrent engineering; E-learning; Education; Internet; Job analysis; Model checking; Multimedia systems; Query languages; Rapid prototyping; Specifications; Linguistics",2-s2.0-67649550043
"Kwon J., Rao P., Moon B., Lee S.","Fast XML document filtering by sequencing twig patterns",2009,"ACM Transactions on Internet Technology",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350348206&doi=10.1145%2f1592446.1592447&partnerID=40&md5=7c99cf1373b190105e7823cb23ef9469","XML-enabled publish-subscribe (pub-sub) systems have emerged as an increasingly important tool for e-commerce and Internet applications. In a typical pub-sub system, subscribed users specify their interests in a profile expressed in the XPath language. Each new data content is then matched against the user profiles so that the content is delivered only to the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the service is critical to the success of pub-sub systems. In this article, we propose a novel scalable filtering system called iFiST that transforms user profiles of a twig pattern expressed in XPath into sequences using the Prüfer's method. Consequently, instead of breaking a twig pattern into multiple linear paths and matching them separately, FiST performs holistic matching of twig patterns with each incoming document in a bottom-up fashion. FiST organizes the sequences into a dynamic hash-based index for efficient filtering, and exploits the commonality among user profiles to enable shared processing during the filtering phase. We demonstrate that the holistic matching approach reduces filtering cost and memory consumption, thereby improving the scalability of FiST. © 2009 ACM.","Prüfer sequences; Selective dissemination of information; Twig pattern; XML filtering","Bottom-up fashion; Data contents; Document filtering; E-Commerce; Filtering systems; Internet application; Memory consumption; Publish-subscribe; Selective dissemination of information; Sub-systems; Twig pattern; User profile; XML filtering; Electronic commerce; Information dissemination; Information services; Internet; Markup languages; Scalability; XML; Information retrieval systems",2-s2.0-70350348206
"Falcon J., Cook W.R.","Gel: A generic extensible language",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-69049086734&doi=10.1007%2f978-3-642-03034-5_4&partnerID=40&md5=91716fb1918d5c724a1a6f79f2e23401","Both XML and Lisp have demonstrated the utility of generic syntax for expressing tree-structured data. But generic languages do not provide the syntactic richness of custom languages. Generic Extensible Language (Gel) is a rich generic syntax that embodies many of the common syntactic conventions for operators, grouping and lists in widely-used languages. Prefix/infix operators are disambiguated by white-space, so that documents which violate common white-space conventions will not necessarily parse correctly with Gel. With some character replacements and adjusting for mismatch in operator precedence, Gel can extract meaningful structure from typical files in many languages, including Java, Cascading Style Sheets, Smalltalk, and ANTLR grammars. This evaluation shows the expressive power of Gel, not that Gel can be used as a parser for existing languages. Gel is intended to serve as a generic language for creating composable domain-specific languages. © IFIP International Federation for Information Processing 2009.",,"Cascading style sheets; Domain specific languages; Expressive power; Smalltalk; Tree-structured data; Cascading style sheets; Composable; Domain specific languages; Expressive power; Extensible languages; Smalltalk; Tree-structured data; White space; DSL; Gelation; Gels; Java programming language; Linguistics; Markup languages; Modems; Query languages; Syntactics; Telecommunication lines; XML; Computational linguistics; Computer programming languages; LISP (programming language); Problem oriented languages; Syntactics; Trees (mathematics); LISP (programming language); Java programming language",2-s2.0-69049086734
"Fufezan C., Specht M.","P3d - Python module for structural bioinformatics",2009,"BMC Bioinformatics",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349731764&doi=10.1186%2f1471-2105-10-258&partnerID=40&md5=893689ca861ef1d0eee1e05c8002cd7f","Background: High-throughput bioinformatic analysis tools are needed to mine the large amount of structural data via knowledge based approaches. The development of such tools requires a robust interface to access the structural data in an easy way. For this the Python scripting language is the optimal choice since its philosophy is to write an understandable source code. Results: p3d is an object oriented Python module that adds a simple yet powerful interface to the Python interpreter to process and analyse three dimensional protein structure files (PDB files). p3d's strength arises from the combination of a) very fast spatial access to the structural data due to the implementation of a binary space partitioning (BSP) tree, b) set theory and c) functions that allow to combine a and b and that use human readable language in the search queries rather than complex computer language. All these factors combined facilitate the rapid development of bioinformatic tools that can perform quick and complex analyses of protein structures. Conclusion: p3d is the perfect tool to quickly develop tools for structural bioinformatics using the Python scripting language. © 2009 Fufezan and Specht; licensee BioMed Central Ltd.",,"Binary space partitioning trees; Bioinformatic analysis; Bioinformatic tools; Knowledge-based approach; Protein structures; Python scripting languages; Structural bioinformatics; Three dimensional protein structures; Binary trees; C (programming language); Knowledge based systems; Proteins; Tools; Trees (mathematics); Bioinformatics; access to information; article; computer interface; computer language; computer program; data analysis; data mining; high throughput screening; information processing; protein structure; structural bioinformatics; theory; Computational Biology; Databases, Protein; Programming Languages; Protein Conformation; Proteins; Software",2-s2.0-70349731764
"Ortega A., Del Rosal E., Pérez D., Mercaş R., Perekrestenko A., Alfonseca M.","PNEPs, NEPs for context free parsing: Application to natural language processing",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68749106228&doi=10.1007%2f978-3-642-02478-8_59&partnerID=40&md5=5fd6df47c63b690d606d1d60f25a2e44","This work tests the suitability of NEPs to parse languages. We propose PNEP, a simple extension to NEP, and a procedure to translate a grammar into a PNEP that recognizes the same language. These parsers based on NEPs do not impose any additional constrain to the structure of the grammar, which can contain all kinds of recursive, lambda or ambiguous rules. This flexibility makes this procedure specially suited for Natural Languge Processing (NLP). In a first proof with a simplified English grammar, we got a performance (a linear time complexity) similar to that of the most popular syntactic parsers in the NLP area (Early and its derivatives). All the possible derivations for ambiguous grammars were generated. © 2009 Springer Berlin Heidelberg.",,"Context-free; Linear time complexity; NAtural language processing; Syntactic parsers; Backpropagation; Computational linguistics; Linguistics; Natural language processing systems; Neural networks; Query languages; Context free languages",2-s2.0-68749106228
"Chali Y., Joty S.R., Hasan S.A.","Complex question answering: Unsupervised learning approaches and experiments",2009,"Journal of Artificial Intelligence Research",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68349096281&partnerID=40&md5=bd8f1338a651451159029838192f0d67","Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical se-mantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features. ©2009 AI Access Foundation.",,"Bag of words; Basic elements; Complex questions; Complex task; Cosine similarity; Empirical approach; Empirical method; Expectation Maximization; Feature sets; K-means; Kernel function; Local search techniques; Machine learning techniques; Multi-document summarization; Multiple documents; Relative importance; Semantic features; User query; Computer science; Content based retrieval; Feature extraction; Learning algorithms; Natural language processing systems; Semantics; Syntactics; Experiments",2-s2.0-68349096281
"Kurc T., Hastings S., Kumar V., Langella S., Sharma A., Pan T., Oster S., Ervin D., Permar J., Narayanan S., Gil Y., Deelman E., Hall M., Saltz J.","HPC and grid computing for integrative biomedical research",2009,"International Journal of High Performance Computing Applications",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68249112331&doi=10.1177%2f1094342009106192&partnerID=40&md5=9b4a7ab0cfa58328e93881c8a404b194","Integrative biomedical research projects query, analyze, and integrate many different data types and make use of datasets obtained from measurements or simulations of structure and function at multiple biological scales. With the increasing availability of high-throughput and high-resolution instruments, the integrative biomedical research imposes many challenging requirements on software middleware systems. In this paper, we look at some of these requirements using example research pattern templates. We then discuss how middleware systems, which incorporate Grid and high-performance computing, could be employed to address the requirements.","Integrative biomedical research; Multi-scale integrative investigation; System level integrative analysis","Biomedical research; Data sets; Data type; High resolution; High-performance computing; High-throughput; Integrative biomedical research; Middleware system; Multi-scale integrative investigation; Research patterns; System level integrative analysis; Computer science; Grid computing; Middleware; Research",2-s2.0-68249112331
"Guo Q., Zhang M.","Question answering based on pervasive agent ontology and Semantic Web",2009,"Knowledge-Based Systems",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650462455&doi=10.1016%2fj.knosys.2009.06.003&partnerID=40&md5=cb7b8f5049e91fc36b39636cfe817fae","Semantic Web technologies bring new benefits to knowledge-based question answering system. Especially, ontology is becoming the pivotal methodology to represent domain-specific conceptual knowledge in order to promote the semantic capability of a QA system. In this paper we present a QA system in which the domain knowledge is represented by means of ontology. In addition, personalized services are enabled through modeling users' profiles in the form of pervasive agent ontology, and a Chinese Natural Language human-machine interface is implemented mainly through a NL parser in this system. An initial evaluation result shows the feasibility to build such a semantic QA system based on pervasive agent ontology, the effectivity of personalized semantic QA, the extensibility of pervasive agent ontology and knowledge base, and the possibility of self-produced knowledge-based on semantic relations in the pervasive agent ontology. © 2009 Elsevier B.V. All rights reserved.","Pervasive agent ontology; Question answering; Semantic Web; WWW","Agent ontologies; Conceptual knowledge; Domain knowledge; Domain specific; Evaluation results; Human Machine Interface; Knowledge base; Natural languages; Personalized service; Pervasive agent ontology; QA system; Question answering; Question answering systems; Semantic relations; Semantic Web technology; WWW; Knowledge based systems; Man machine systems; Multi agent systems; Natural language processing systems; Ontology; Semantics; Ubiquitous computing; Semantic Web",2-s2.0-67650462455
"Thirunarayan K., Immaneni T.","Integrated retrieval from web of documents and data",2009,"Studies in Computational Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651248226&doi=10.1007%2f978-3-642-02190-9_2&partnerID=40&md5=f2ddfb4574403404a246d2133f7c1925","The Semantic Web is evolving into a property-linked web of data, conceptually different from but contained in the Web of hyperlinked documents. Data Retrieval techniques are typically used to retrieve data from the Semantic Web while Information Retrieval techniques are used to retrieve documents from the Hypertext Web. We present a Unified Web model that integrates the two webs and formalizes connection between them. We then present an approach to retrieving documents and data that captures best of both the worlds. Specifically, it improves recall for legacy documents and provides keyword-based search capability for the Semantic Web. We specify the Hybrid Query Language that embodies this approach, and the prototype system SITAR that implements it. We conclude with areas of future work. © 2009 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-67651248226
"Kozanidis L., Stamou S., Spiros G.","Focusing web crawls on location-specific content",2009,"WEBIST 2009 - Proceedings of the 5th International Conference on Web Information Systems and Technologies",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650524479&partnerID=40&md5=a42e3508bbe688dc153375abfd57d1d6","Retrieving relevant data for location-sensitive keyword queries is a challenging task that has so far been addressed as a problem of automatically determining the geographical orientation of web searches. Unfortunately, identifying localizable queries is not sufficient per se for performing successful location-sensitive searches, unless there exists a geo-referenced index of data sources against which localizable queries are searched. In this paper, we propose a novel approach towards the automatic construction of a geo-referenced search engine index. Our approach relies on a geo-focused crawler that incorporates a structural parser and uses GeoWordNet as a knowledge base in order to automatically deduce the geo-spatial information that is latent in the pages' contents. Based on location-descriptive elements in the page URLs and anchor text, the crawler directs the pages to a location-sensitive downloaded This downloading module resolves the geographical references of the URL location elements and org nizes them into indexable hierarchical structures. The location-aware URL hierarchies are linked to their respective pages, resulting into a geo-referenced index against which location-sensitive queries can be answered.","Focused crawling; Geo-referenced index; Location-sensitive web search","Automatic construction; Data source; Focused crawler; Focused crawling; Geo-referenced index; Geo-spatial; Hierarchical structures; Keyword queries; Knowledge base; Location-aware; Web searches; Information retrieval; Internet; Knowledge based systems; Search engines; Websites",2-s2.0-67650524479
"Lin D.","Combining language modeling and discriminative classification for word segmentation",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650513858&doi=10.1007%2f978-3-642-00382-0_14&partnerID=40&md5=960c132391649db8cae4481cc170a45b","Generative language modeling and discriminative classification are two main techniques for Chinese word segmentation. Most previous methods have adopted one of the techniques. We present a hybrid model that combines the disambiguation power of language modeling and the ability of discriminative classifiers to deal with out-of-vocabulary words. We show that the combined model achieves 9% error reduction over the discriminative classifier alone. © Springer-Verlag Berlin Heidelberg 2009.","Language model; Maximum entropy; Segmentation","Chinese word segmentation; Combined model; Discriminative classifiers; Error reduction; Hybrid model; Language model; Language modeling; Maximum entropy; Out-of-vocabulary words; Segmentation; Word segmentation; Classifiers; Learning systems; Query languages; Text processing; Word processing; Computational linguistics",2-s2.0-67650513858
"Lämmel R., Zaytsev V.","An introduction to grammar convergence",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650495564&doi=10.1007%2f978-3-642-00255-7-17&partnerID=40&md5=8c09f27766bc3c3bcfd00a7d04049bcc","Grammar convergence is a lightweight verification method for establishing and maintaining the correspondence between grammar knowledge ingrained in all kinds of software artifacts, e.g., object models, XML schemas, parser descriptions, or language documents. The central idea is to extract grammars from diverse software artifacts, and to transform the grammars until they become syntactically identical. The present paper introduces and illustrates the basics of grammar convergence. © Springer-Verlag Berlin Heidelberg 2009.",,"Object model; Software artifacts; Verification method; XML schemas; Computer software; Markup languages; Verification; Formal methods",2-s2.0-67650495564
"D'Ulizia A., Ferri F.","Formalization of multimodal languages in pervasive computing paradigm",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650330251&doi=10.1007%2f978-3-642-01350-8_12&partnerID=40&md5=16c3508c6b7856c8a86edd763988d30d","The pervasive computing paradigm provides the user with a uniform computing space available everywhere, any time and in the most appropriate form and modality. These aspects produce the need for user interfaces that are usable, multimodal and personalized for each user. In this paper multimodality is discussed. In particular, features and computational issues of the multimodal interaction are analyzed in order to examine methodological aspects for the definition of multimodal interaction languages for pervasive applications. The multimodality is faced at a grammar level, rather than at a dialogue management level. This means that different unimodal inputs are considered as a unique multimodal input that is sent to the dialogue parser that uses the grammar specification to interpret it, rather than to be distinctly interpreted and then combined. The main objective of the paper is thus to explore multimodal interaction for pervasive applications through the use of a multimodal language rather than through the integration of several unimodal languages. ©Springer-Verlag Berlin Heidelberg 2009.","Multimodal languages; Pervasive computing systems; Relation grammars","Computational issues; Dialogue management; Methodological aspects; Multi-modal; Multi-Modal Interactions; Multi-modality; Multimodal languages; Pervasive applications; Pervasive computing paradigm; Pervasive computing systems; Relation grammars; Unimodal; Computer science; Interactive computer systems; Internet; Linguistics; Query languages; User interfaces",2-s2.0-67650330251
"Barthwal A., Norrish M.","Verified, executable parsing",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650134650&doi=10.1007%2f978-3-642-00590-9_12&partnerID=40&md5=aea2badb3cbb330f8b03d8bc9846ac8b","We describe the mechanisation of an SLR parser produced by a parser generator, covering background properties of context-free languages and grammars, as well as the construction of an SLR automaton. Among the various properties proved about the parser we show, in particular, soundness: if the parser results in a parse tree on a given input, then the parse tree is valid with respect to the grammar, and the leaves of the parse tree match the input; completeness: if the input is in the language of the grammar then the parser constructs the correct parse tree for the input with respect to the grammar; and non-ambiguity: grammars successfully converted to SLR automata are unambiguous. We also develop versions of the algorithms that are executable by automatic translation from HOL to SML. These alternative versions of the algorithms require some interesting termination proofs.",,"Automatic translation; Mechanisation; Parse trees; Parser generators; Termination proof; Automata theory; Context free languages; Linguistics; Machinery; Query languages; Translation (languages); Computational linguistics",2-s2.0-67650134650
"Wachsmuth G.","A formal way from text to code templates",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650139600&doi=10.1007%2f978-3-642-00593-0_8&partnerID=40&md5=c5ec02b88ca2c6bf73511f2191090352","We present an approach to define template languages for generating syntactically correct code. In the first part of the paper, we define the syntax and semantics of a template language for text generation. We use Natural Semantics for expressing both the static and the dynamic semantics of the language. In the second part, we deal with template languages for code generation in a particular target language. We provide construction steps for the syntax and semantics of such languages. The approach is generic and can be applied to any target language.",,"Code Generation; Construction steps; Correct code; Dynamic semantic; Natural semantics; Target language; Computer software; Linguistics; Semantics; Syntactics; Query languages",2-s2.0-67650139600
"Baars A.I., Swierstra S.D., Viera M.","Typed transformations of typed abstract syntax",2009,"Proceedings of the 2009 ACM SIGPLAN Workshop on Types in Language Design and Implementation, TLDI'09",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650085321&doi=10.1145%2f1481861.1481865&partnerID=40&md5=9b904f544f2cf4bafe656e80aee85319","Advantages of embedded domain-specific languages (EDSLs) are one does not have to implement a separate type system nor abstraction mechanism, since these are directly borrowed from host language. Straightforward implementations of embedded specific languages map the semantics of the embedded language a function in the host language. The semantic mappings usually compositional, i.e. they directly follow the syntax of the language.of the questions which arises is whether conventional compilation, such as global analysis and resulting transformations, be applied in the context of EDSLs. The approach weis that, instead of mapping the embedded language directly a function, we first build a representation of the abstract syntax of the embedded program fragment. This syntax tree isanalyzed and transformed, and finally mapped onto function representing its denotational semantics. In this way we run-time ""compilation"" of the embedded language.time transformations on the embedded language can have huge effect on performance. In previous work (Viera et al. 2008) present a case study comparing the Read instances generated Haskells deriving construct with instances on which run-time transformations (precedence resolution, left-factorisation left-corner transformation) have been applied.this paper we present the library, which has an arrow like, which supports in the construction of analyses and transformations,we demonstrate its use in implementing a common expression elemination transformation. The library uses typed syntax to represent fragments of embedded programs containing and binding structures, while preserving the idea the type system of the host language is used to emulate the type of the embedded language. The tricky issue is how to keep collection of mutually recursive structures well-typed while it is transformed. finally discuss the typing rules of Haskell, its extensions those as implemented by the GHC and show that pure System-based systems are sufficiently rich to express what we want to express, albeit at the cost of an increased complexity of the code. © 2009 ACM.","GADT, Meta Programming, Type Systems, Typed Transformations, Common Subexpression Elimination","Abstract syntax; Abstraction mechanism; Corner transformation; Denotational semantics; Embedded domains; Embedded Languages; GADT, Meta Programming, Type Systems, Typed Transformations, Common Subexpression Elimination; Global analysis; Haskell; Library use; Program fragments; Recursive structure; Runtime; Semantic mapping; Specific languages; Syntax tree; System-based; Type systems; Typed syntax; Typing rules; Abstracting; Java programming language; Query languages; Security of data; Semantics; Syntactics; Linguistics",2-s2.0-67650085321
"Nilsson-Nyman E., Ekman T., Hedin G.","Practical scope recovery using bridge parsing",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649949493&doi=10.1007%2f978-3-642-00434-6_7&partnerID=40&md5=6ff93cdbac7f342e8c1c3c9884f8dcce","Interactive development environments (IDEs) increase programmer productivity, but unfortunately also the burden on language implementors since sophisticated tool support is expected even for small domain-specific languages. Our goal is to alleviate that burden, by generating IDEs from high-level language specifications using the JastAdd meta-compiler system. This puts increased tension on scope recovery in parsers, since at least a partial AST is required by the system to perform static analysis, such as name completion and context sensitive search. In this paper we present a novel recovery algorithm called bridge parsing, which provides a light-weight recovery mechanism that complements existing parsing recovery techniques. An initial phase recovers nesting structure in source files making them easier to process by existing parsers. This enables batch parser generators with existing grammars to be used in an interactive setting with minor or no modifications. We have implemented bridge parsing in a generic extensible IDE for JastAdd based compilers. It is independent of parsing technology, which we validate by showing how it improves recovery in a set of typical interactive editing scenarios for three parser generators: ANTLR (LL(variable lookahead) parsers), LPG (LALR(k) parsers), and Beaver (LALR(1) parsers). ANTLR and LPG both contain sophisticated support for error recovery, while Beaver requires manual error productions. Bridge parsing complements these techniques and yields better recovery for all these tools with only minimal changes to existing grammars.",,"Compiler systems; Context-sensitive search; Domain specific languages; Error recovery; Interactive development environment; Interactive editing; Light weight; Look-ahead; Parser generators; Parsing technologies; Programmer productivity; Recovery algorithms; Recovery mechanisms; Source files; Tool support; Computer software; High level languages; Linguistics; Program compilers; Query languages; Recovery",2-s2.0-67649949493
"Bravenboer M., Visser E.","Parse table composition separate compilation and binary extensibility of grammars",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649993168&doi=10.1007%2f978-3-642-00434-6_6&partnerID=40&md5=cc3b50cd768c0a4ab72315e255d57623","Module systems, separate compilation, deployment of binary components, and dynamic linking have enjoyed wide acceptance in programming languages and systems. In contrast, the syntax of languages is usually defined in a non-modular way, cannot be compiled separately, cannot easily be combined with the syntax of other languages, and cannot be deployed as a component for later composition. Grammar formalisms that do support modules use whole program compilation. Current extensible compilers focus on source-level extensibility, which requires users to compile the compiler with a specific configuration of extensions. A compound parser needs to be generated for every combination of extensions. The generation of parse tables is expensive, which is a particular problem when the composition configuration is not fixed to enable users to choose language extensions. In this paper we introduce an algorithm for parse table composition to support separate compilation of grammars to parse table components. Parse table components can be composed (linked) efficiently at runtime, i.e. just before parsing. While the worst-case time complexity of parse table composition is exponential (like the complexity of parse table generation itself), for realistic language combination scenarios involving grammars for real languages, our parse table composition algorithm is an order of magnitude faster than computation of the parse table for the combined grammars.",,"Binary components; Dynamic linking; Grammar formalisms; Language extensions; Module systems; Order of magnitude; Parse table; Programming language; Runtime; Separate compilation; Support modules; Time complexity; Whole-program compilation; Computer software; Linguistics; Program compilers; Syntactics; Query languages",2-s2.0-67649993168
"Van Den Brand M.G.J.","Model-driven engineering meets generic language technology",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649934206&doi=10.1007%2f978-3-642-00434-6_2&partnerID=40&md5=74da67fccb4b0e170593a496d163faed","One of the key points of model-driven engineering is raising the level of abstraction in software development. This phenomenon is not new. In the sixties of the previous century, the first high-level programming languages were developed and they also increased the abstraction level of software development. The development of high-level programming languages initiated research on compilers and programming environments. This research eventually matured into generic language technology: the description of (programming) languages and tooling to generate compilers and programming environments. The model-driven engineering community is developing tools to analyze models and to transform models into code.The application of generic language technology, or at least the lessons learnt by this community, can be beneficial for the model-driven engineering community. By means of a number of case studies it will be shown how generic language technology research can be useful for the development of model-driven engineering technology.",,"Abstraction level; High-level programming language; Keypoints; Language technology; Level of abstraction; Model-driven Engineering; Programming environment; Software development; Transform models; Abstracting; Computer software; Linguistics; Models; Program compilers; Query languages; Research; Software design; Systems analysis; Technology; High level languages",2-s2.0-67649934206
"Payrits S., Dornbach P., Zólyomi I.","XML data binding for C++ using metadata",2009,"International Journal of Web Services Research",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349297470&partnerID=40&md5=b33ef385f3da3944bec945e065914d93","Mapping XML document schemas and Web Service interfaces to programming languages has an important role in effective creation of quality Web Service implementations. The authors present a novel way to map XML data to the C++ programming language. The proposed solution offers more flexibility and more compact code that makes it ideal for embedded environments. The article describes the concept and the architecture of the solution and compares it with existing solutionsThis article is an extended version of the paperfrom ICWS 2006. The authors include a broader comparison with existing tools on Symbian and Linux platforms and evaluate the code size and performance. [Article copies are available for purchase from InfoSci-on Demand.com] Copyright © 2009, IGI Global.","C++; Data Binding; Metadata; Web services; XML","C++; Code size; Compact code; Data Binding; Document schemas; Embedded environment; Extended versions; Linux platform; Programming language; Symbian; Web service interface; XML data; Computer operating systems; Data communication systems; Linguistics; Markup languages; Metadata; Query languages; Web services; XML",2-s2.0-70349297470
"Frasincar F., Borsje J., Levering L.","A semantic web-based approach for building personalized news services",2009,"International Journal of e-Business Research",39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349271241&partnerID=40&md5=072e127dfb211c0a415e574bf5be7962","This article proposes Hermes, a Semantic Web-based frame work for building personalized news services. It makes use of ontologies for knowledge representation, natural language processing techniques for semantic text analysis, and semantic query languages for specifying wanted information. Hermes is supported by an implementation of the framework, the Hermes News Portal, a tool which allows users to have a personalized online access to news items. The Hermes framework and its associated implementation aim at advancing the state-of-the-art of semantic approaches for personalized news services by employing Semantic Web standards, exploiting domain information, using a word sense disambiguation procedure, and being able to express temporal constraints for the desired news items. Copyright © 2009, IGI Global.","Natural Language Processing; News Services; Ontologies; Personalization; Semantic Web",,2-s2.0-70349271241
"Heradio R., Cerrada J.A., Lopez J.C., Coz J.R.","Code Generation with the Exemplar Flexibilization Language",2009,"Electronic Notes in Theoretical Computer Science",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649388082&doi=10.1016%2fj.entcs.2009.05.004&partnerID=40&md5=4179badba1fc3296f55bd03ac50f7d2f","Code Generation is an increasing popular technique for implementing Software Product Lines that produces code from abstract specifications written in Domain Specific Languages (DSLs). This paper proposes to take advantage of the similitude among the products in a domain to generate them by analogy. That is, instead of synthesizing the final code from scratch or transforming the DSL specifications, the final products are obtained by adapting a previously developed domain product. The paper also discusses the capabilities and limitations of several currently available tools and languages to implement this kind of generators and introduce a new language to overcome the limitations. © 2009 Elsevier B.V. All rights reserved.","Code Generation; Domain Specific Language; Software Product Line","Abstract specifications; Code Generation; Domain Specific Language; Domain specific languages; Software Product Line; Abstracting; Computer software; Network architecture; Network components; Query languages; Specifications; Linguistics",2-s2.0-67649388082
"Walsh I., Martin A.J.M., Mooney C., Rubagotti E., Vullo A., Pollastri G.","Ab initio and homology based prediction of protein domains by recursive neural networks",2009,"BMC Bioinformatics",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650898274&doi=10.1186%2f1471-2105-10-195&partnerID=40&md5=a49f30b065dbd23559b34997a0326bfd","Background: Proteins, especially larger ones, are often composed of individual evolutionary units, domains, which have their own function and structural fold. Predicting domains is an important intermediate step in protein analyses, including the prediction of protein structures. Results: We describe novel systems for the prediction of protein domain boundaries powered by Recursive Neural Networks. The systems rely on a combination of primary sequence and evolutionary information, predictions of structural features such as secondary structure, solvent accessibility and residue contact maps, and structural templates, both annotated for domains (from the SCOP dataset) and unannotated (from the PDB). We gauge the contribution of contact maps, and PDB and SCOP templates independently and for different ranges of template quality. We find that accurately predicted contact maps are informative for the prediction of domain boundaries, while the same is not true for contact maps predicted ab initio. We also find that gap information from PDB templates is informative, but, not surprisingly, less than SCOP annotations. We test both systems trained on templates of all qualities, and systems trained only on templates of marginal similarity to the query (less than 25% sequence identity). While the first batch of systems produces near perfect predictions in the presence of fair to good templates, the second batch outperforms or match ab initio predictors down to essentially any level of template quality. We test all systems in 5-fold cross-validation on a large non-redundant set of multi-domain and single domain proteins. The final predictors are state-of-the-art, with a template-less prediction boundary recall of 50.8% (precision 38.7%) within ± 20 residues and a single domain recall of 80.3% (precision 78.1%). The SCOP-based predictors achieve a boundary recall of 74% (precision 77.1%) again within ± 20 residues, and classify single domain proteins as such in over 85% of cases, when we allow a mix of bad and good quality templates. If we only allow marginal templates (max 25% sequence identity to the query) the scores remain high, with boundary recall and precision of 59% and 66.3%, and 80% of all single domain proteins predicted correctly. Conclusion: The systems presented here may prove useful in large-scale annotation of protein domains in proteins of unknown structure. The methods are available as public web servers at the address: http://distill.ucd.ie/shandy/ and we plan on running them on a multi-genomic scale and make the results public in the near future. © 2009 Walsh et al; licensee BioMed Central Ltd.",,"Evolutionary information; Prediction boundaries; Recall and precision; Recursive neural networks; Secondary structures; Single-domain proteins; Solvent accessibility; Structural templates; Forecasting; Neural networks; Proteins; ab initio calculation; accuracy; amino acid sequence; article; artificial neural network; client server application; intermethod comparison; prediction; protein domain; protein secondary structure; sequence homology; algorithm; chemistry; protein database; protein tertiary structure; protein; Algorithms; Databases, Protein; Neural Networks (Computer); Protein Structure, Tertiary; Proteins",2-s2.0-67650898274
"Völker J.","Learning expressive ontologies",2009,"Learning Expressive Ontologies",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014961048&partnerID=40&md5=e24f7d3b47ad9994f6700ec57948920b","This publication advances the state-of-the-art in ontology learning by presenting a set of novel approaches to the semi-automatic acquisition, refinement and evaluation of logically complex axiomatizations. It has been motivated by the fact that the realization of the semantic web envisioned by Tim Berners-Lee is still hampered by the lack of ontological resources, while at the same time more and more applications of semantic technologies emerge from fast-growing areas such as e-business or life sciences. Such knowledge-intensive applications, requiring large scale reasoning over complex domains of interest, even more than the semantic web depend on the availability of expressive, high-quality axiomatizations. This knowledge acquisition bottleneck could be overcome by approaches to the automatic or semi-automatic construction of ontologies. Hence a huge number of ontology learning tools and frameworks have been developed in recent years, all of them aiming for the automatic or semi-automatic generation of ontologies from various kinds of data. However, both the quality and the expressivity of ontologies that can be acquired by the current state-of-the-art in ontology learning so far have failed to meet the expectations of people who argue in favor of powerful, knowledge-intensive applications based on logical inference. This work therefore takes a first, yet important, step towards the semi-automatic generation and maintenance of expressive ontologies. © 2009, Akademische Verlagsgesellschaft AKA GmbH, Heidelberg. All rights reserved.",,,2-s2.0-85014961048
"Pavlič L., Hericko M., Podgorelec V., Rozman I.","Improving design pattern adoption with an ontology-based repository",2009,"Informatica (Ljubljana)",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67049110339&partnerID=40&md5=49d909b9714a84b3387e37091cc46663","In software engineering, an efficient approach towards reuse has become a crucial success factor. Conceptual simple high level approaches to reuse are the most appropriate for performing it in a useful manner. Design patterns are reliable and an effective high level approach that enables developers to produce high quality software in less time. Unfortunately, the rapidly growing number of design patterns has not yet been adequately supported by efficient search and management tools, making the patterns uninviting for a large part of the software development community. In this way, the issue of managing and selecting design patters in a straight-forward way has become the main challenge. In this paper, we propose a possible solution for the improvement of design pattern adoption and present a platform that should give design patterns some new and long-overdue momentum. Using our proposed technique for formal design pattern specifications, we have developed an experimental prototype of a new design pattern repository based on semantic web technologies. A new Ontology- Based Design Pattern Repository (OBDPR) has been developed that can also be used as a platform for introducing advanced services. Some fundamental services - searching, design pattern proposing, verification and training services - have already been developed and many others are proposed. Based on the conducted experiments, it is our strong belief that the proposed approach together with the platform's potential -- can significantly contribute to the improvement of design pattern adoption.","Design pattern repository; Design patterns; Ontologies; Semantic web","Design Pattern; Design pattern repository; Design patterns; Experimental prototype; Formal design; High-level approach; High-quality software; Large parts; Management tool; New design; Ontology-based; Possible solutions; Semantic Web technology; Software development; Success factors; Training services; Computer software reusability; Computer software selection and evaluation; Multi agent systems; Ontology; Semantic Web; Semantics; Software engineering; Design",2-s2.0-67049110339
"Schwerdfeger A.C., Van Wyk E.R.","Verifiable composition of deterministic grammars",2009,"ACM SIGPLAN Notices",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650802341&partnerID=40&md5=54abe25040925b7ffeee9c1f89615ee8","There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension. Copyright © 2009 ACM.","Context-aware scanning; Extensible languages; Grammar composition; Language composition; LR parsing","Context-aware scanning; Extensible languages; Grammar composition; Language composition; LR parsing; Computational linguistics; Electric reactors; Formal languages; Linguistics; Scanning; Specifications; Query languages",2-s2.0-67650802341
"Zhen L., Jiang Z., Liang J.","Knowledge grid-based problem-solving platform",2009,"International Journal of Advanced Manufacturing Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650215404&doi=10.1007%2fs00170-008-1671-6&partnerID=40&md5=c85a0e2228bc53746b4f4d1d9f1b7726","As to how to acquire potentially useful knowledge to aid engineers in problem solving, this paper proposed a platform of problem solving by using the knowledge grid. This paper is mainly concerned with the implementation of problem-solving-oriented knowledge acquisition in the knowledge grid, which is a platform for sharing and managing globally distributed knowledge resources. Based on the knowledge grid, engineers could acquire the knowledge that will be potentially useful and give them some hints or inspiration for problem solving. Some key technologies for the platform's implementation, such as ontology building for product development, semantic register, semantic parser, and query dispatch, are investigated. At last, a working scenario is employed to illustrate the knowledge acquisition process for problem solving in the product development process. © 2008 Springer-Verlag London Limited.","Knowledge grid; Knowledge management; Problem solving; Product development","Distributed knowledge; Key technologies; Knowledge grid; Knowledge grids; Ontology building; Product development process; Knowledge acquisition; Knowledge management; Ontology; Product development; Project management; Semantics; Problem solving",2-s2.0-67650215404
"Asker L., Argaw A.A., Gambäck B., Eyassu Asfeha S., Nigussie Habte L.","Classifying Amharic webnews",2009,"Information Retrieval",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-64749116958&doi=10.1007%2fs10791-008-9080-x&partnerID=40&md5=fbf2b5e595f437555c4a37bbeed20459","We present work aimed at compiling an Amharic corpus from the Web and automatically categorizing the texts. Amharic is the second most spoken Semitic language in the World (after Arabic) and used for countrywide communication in Ethiopia. It is highly inflectional and quite dialectally diversified. We discuss the issues of compiling and annotating a corpus of Amharic news articles from the Web. This corpus was then used in three sets of text classification experiments. Working with a less-researched language highlights a number of practical issues that might otherwise receive less attention or go unnoticed. The purpose of the experiments has not primarily been to develop a cutting-edge text classification system for Amharic, but rather to put the spotlight on some of these issues. The first two sets of experiments investigated the use of Self-Organizing Maps (SOMs) for document classification. Testing on small datasets, we first looked at classifying unseen data into 10 predefined categories of news items, and then at clustering it around query content, when taking 16 queries as class labels. The second set of experiments investigated the effect of operations such as stemming and part-of-speech tagging on text classification performance. We compared three representations while constructing classification models based on bagging of decision trees for the 10 predefined news categories. The best accuracy was achieved using the full text as representation. A representation using only the nouns performed almost equally well, confirming the assumption that most of the information required for distinguishing between various categories actually is contained in the nouns, while stemming did not have much effect on the performance of the classifier. © 2009 Springer Science+Business Media, LLC.","Semitic languages; Text classification; Web mining",,2-s2.0-64749116958
"Lee H.-H., Lee W.-S.","Selectivity-sensitive shared evaluation of multiple continuous XPath queries over XML streams",2009,"Information Sciences",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149156172&doi=10.1016%2fj.ins.2009.01.022&partnerID=40&md5=56e5631ac1329eae28e2dcfc805de193","One of the primary issues confronting XML message brokers is the difficulty associated with processing a large set of continuous XPath queries over incoming XML streams. This paper proposes a novel system designed to present an effective solution to this problem. The proposed system transforms multiple XPath queries before their run-time into a new data structure, called an XP-table, by sharing their common constraints. An XP-table is matched with a stream relation (SR) transformed from a target XML stream by a SAX parser. This arrangement is intended to minimize the run-time workload of continuous query processing. In addition, an early-query-termination strategy is proposed as an improved alternative to the basic approach. It optimizes query processing by arranging the evaluation sequence of the member-lists (m-lists) of an XP-table adaptively and offers increased efficiency, especially in cases of low selectivity. System performance is estimated and verified through a variety of experiments, including comparisons with previous approaches such as YFilter and LazyDFA. The proposed system is practically linear-scalable and stable for evaluating a set of XPath queries in a continuous and timely fashion. Crown Copyright © 2009.","Adaptive optimization; Early-query-termination strategy; Multiple continuous XPath queries; Selectivity; Stream relation; XML stream; XP-table","Data structures; Query processing; Windows operating system; XML; Adaptive optimization; Early-query-termination strategy; Multiple continuous XPath queries; Selectivity; Stream relation; XML stream; XP-table; Markup languages",2-s2.0-63149156172
"Zhong Z., Zhang Z., Dai M., Cao J., Shi J.","Real-time conversion method for flowchart and similar C language",2009,"Dongnan Daxue Xuebao (Ziran Kexue Ban)/Journal of Southeast University (Natural Science Edition)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649601308&doi=10.3969%2fj.issn.1001-0505.2009.03.016&partnerID=40&md5=9fbd1026fbea3fc048076787d64b6333","A new real-time conversion method for a flowchart and a similar C language is proposed to realize the versatility of a programming platform of educational robots by the integration of graphics and text programming. During the process of converting the flowchart to a similar C language, the module tree is scanned according to the relationships between modules; moreover, the converting results are achieved and checked by the library function and the rule of the similar C language. During the process of converting the similar C language to the flowchart, an XML (eXtensible markup language) code is generated by lexical analysis, syntax analysis and semantic analysis; then, a flowchart is built via an XML parser. Besides, real-time error checks are carried out to highlight the error line and prompt the user of the wrong reason. Finally, an example of the conversion of a typical structure is given to illustrate the validity and efficiency of the method.","EXtensible markup language; Flowchart; Real-time conversion; Similar C language","C language; Educational robot; Error checks; EXtensible markup language; Flowchart; Lexical analysis; Library functions; Real-time conversion; Semantic analysis; Similar C language; Syntax analysis; XML (extensible markup language); XML parser; Cantilever beams; Flowcharting; Hypertext systems; Linguistics; Query languages; Robot programming; XML; Markup languages",2-s2.0-67649601308
"Kahn Jr. C.E., Rubin D.L.","Automated Semantic Indexing of Figure Captions to Improve Radiology Image Retrieval",2009,"Journal of the American Medical Informatics Association",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-65349187289&doi=10.1197%2fjamia.M2945&partnerID=40&md5=3c9989f05e473a2fcd32d4379f14bb5a","Objective: We explored automated concept-based indexing of unstructured figure captions to improve retrieval of images from radiology journals. Design: The MetaMap Transfer program (MMTx) was used to map the text of 84,846 figure captions from 9,004 peer-reviewed, English-language articles to concepts in three controlled vocabularies from the UMLS Metathesaurus, version 2006AA. Sampling procedures were used to estimate the standard information-retrieval metrics of precision and recall, and to evaluate the degree to which concept-based retrieval improved image retrieval. Measurements: Precision was estimated based on a sample of 250 concepts. Recall was estimated based on a sample of 40 concepts. The authors measured the impact of concept-based retrieval to improve upon keyword-based retrieval in a random sample of 10,000 search queries issued by users of a radiology image search engine. Results: Estimated precision was 0.897 (95% confidence interval, 0.857-0.937). Estimated recall was 0.930 (95% confidence interval, 0.838-1.000). In 5,535 of 10,000 search queries (55%), concept-based retrieval found results not identified by simple keyword matching; in 2,086 searches (21%), more than 75% of the results were found by concept-based search alone. Conclusion: Concept-based indexing of radiology journal figure captions achieved very high precision and recall, and significantly improved image retrieval. © 2009 J Am Med Inform Assoc.",,"accuracy; algorithm; article; automation; imaging system; medical information; medical literature; Abstracting and Indexing as Topic; Algorithms; Confidence Intervals; Information Storage and Retrieval; Medical Illustration; Periodicals as Topic; Radiology; Semantics; Unified Medical Language System",2-s2.0-65349187289
"Vandervalk B.P., McCarthy E.L., Wilkinson M.D.","Moby and moby 2: Creatures of the deep (Web)",2009,"Briefings in Bioinformatics",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63349096555&doi=10.1093%2fbib%2fbbn051&partnerID=40&md5=8194563f9938bf565545253d2f49f789","Facile and meaningful integration of data from disparate resources is the 'holy grail' of bioinformatics. Some resources have begun to address this problem by providing their data using Semantic Web standards, specifically the Resource Description Framework (RDF) and the Web Ontology Language (OWL). Unfortunately, adoption of Semantic Web standards has been slow overall, and even in cases where the standards are being utilized, interconnectivity between resources is rare. In response, we have seen the emergence of centralized 'semantic warehouses' that collect public data from third parties, integrate it, translate it into OWL/RDF and provide it to the community as a unified and queryable resource. One limitation of the warehouse approach is that queries are confined to the resources that have been selected for inclusion. A related problem, perhaps of greater concern, is that the majority of bioinformatics data exists in the 'Deep Web' - that is, the data does not exist until an application or analytical tool is invoked, and therefore does not have a predictable Web address. The inability to utilize Uniform Resource Identifiers (URIs) to address this data is a barrier to its accessibility via URI-centric Semantic Web technologies. Here we examine 'The State of the Union' for the adoption of Semantic Web standards in the health care and life sciences domain by key bioinformatics resources, explore the nature and connectivity of several community-driven semantic warehousing projects, and report on our own progress with the CardioSHARE/Moby-2 project, which aims to make the resources of the Deep Web transparently accessible through SPARQL queries. © The Author 2009. Published by Oxford University Press.","Data integration; Distributed SPARQL; OWL; RDF; Semantic Web; Semantic Web services","access to information; bioinformatics; conference paper; data mining; information dissemination; information processing; information retrieval; information service; online system; reference database; semantics; standard; standardization; Algorithms; Computational Biology; Databases, Factual; Humans; Internet; Semantics; Software; User-Computer Interface; Vocabulary, Controlled",2-s2.0-63349096555
"Deléger L., Namer F., Zweigenbaum P.","Morphosemantic parsing of medical compound words: Transferring a French analyzer to English",2009,"International Journal of Medical Informatics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949121339&doi=10.1016%2fj.ijmedinf.2008.07.016&partnerID=40&md5=de3919abcab9fdeae884af9ef19ddf8f","Purpose: Medical language, as many technical languages, is rich with morphologically complex words, many of which take their roots in Greek and Latin-in which case they are called neoclassical compounds. Morphosemantic analysis can help generate definitions of such words. The similarity of structure of those compounds in several European languages has also been observed, which seems to indicate that a same linguistic analysis could be applied to neo-classical compounds from different languages with minor modifications. Methods: This paper reports work on the adaptation of a morphosemantic analyzer dedicated to French (DériF) to analyze English medical neo-classical compounds. It presents the principles of this transposition and its current performance. Results: The analyzer was tested on a set of 1299 compounds extracted from the WHO-ART terminology. 859 could be decomposed and defined, 675 of which successfully. Conclusion: An advantage of this process is that complex linguistic analyses designed for French could be successfully transposed to the analysis of English medical neoclassical compounds, which confirmed our hypothesis of transferability. The fact that the method was successfully applied to a Germanic language such as English suggests that performances would be at least as high if experimenting with Romance languages such as Spanish. Finally, the resulting system can produce more complete analyses of English medical compounds than existing systems, including a hierarchical decomposition and semantic gloss of each word. © 2008 Elsevier Ireland Ltd. All rights reserved.","English; French; Morphosemantic analysis; Natural language processing; Neoclassical compounds; Word definition","English; French; Morphosemantic analysis; Natural language processing; Neoclassical compounds; Word definition; Computational linguistics; Hierarchical systems; Information theory; Natural language processing systems; Terminology; Query languages; analyzer; article; France; language; medical documentation; natural language processing; priority journal; recall; semantics; Terminology as Topic; Translating",2-s2.0-62949121339
"Araujo L.","Stochastic parsing and evolutionary algorithms",2009,"Applied Artificial Intelligence",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651218839&doi=10.1080%2f08839510902830650&partnerID=40&md5=0cea789f150140254388f813b1083877","This article aims to show the effectiveness of evolutionary algorithms in automatically parsing sentences of real texts. Parsing methods based on complete search techniques are limited by the exponential increase of the size of the search space with the size of the grammar and the length of the sentences to be parsed. Approximated methods, such as evolutionary algorithms, can provide approximate results, adequate to deal with the indeterminism that ambiguity introduces in natural language processing. This work investigates different alternatives to implement an evolutionary bottom-up parser. Different genetic operators have been considered and evaluated. We focus on statistical parsing models to establish preferences among different parses. It is not our aim to propose a new statistical model for parsing but a new algorithm to perform the parsing once the model has been defined. The training data are extracted from syntactically annotated corpora (treebanks) which provide sets of lexical and syntactic tags as well as the grammar in which the parsing is based. We have tested the system with two corpora: Susanne and Penn Treebank, obtaining very encouraging results.",,"Approximate results; Complete search; Exponential increase; Genetic operators; NAtural language processing; Parsing methods; Search spaces; Statistical models; Syntactically annotated corpus; Training data; Treebank; Treebanks; Evolutionary algorithms; Formal languages; Natural language processing systems; Computational linguistics",2-s2.0-67651218839
"Vetter G.R.","Commercial free and open source software: Knowledge production, hybrid appropriability, and patents",2009,"Fordham Law Review",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949149788&partnerID=40&md5=96761eb1e7cb80e727cbdca6aaced17d",[No abstract available],,,2-s2.0-66949149788
"Schmidt F., Schmid M., Thiede B., Pleiner K.-P., Böhme M., Jungblut P.R.","Assembling proteomics data as a prerequisite for the analysis of large scale experiments",2009,"Chemistry Central Journal",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149119145&doi=10.1186%2f1752-153X-3-2&partnerID=40&md5=bf3fe877803322cde362c0df5b38a3f8","Background. Despite the complete determination of the genome sequence of a huge number of bacteria, their proteomes remain relatively poorly defined. Beside new methods to increase the number of identified proteins new database applications are necessary to store and present results of large- scale proteomics experiments. Results. In the present study, a database concept has been developed to address these issues and to offer complete information via a web interface. In our concept, the Oracle based data repository system SQL-LIMS plays the central role in the proteomics workflow and was applied to the proteomes of Mycobacterium tuberculosis, Helicobacter pylori, Salmonella typhimurium and protein complexes such as 20S proteasome. Technical operations of our proteomics labs were used as the standard for SQL-LIMS template creation. By means of a Java based data parser, post-processed data of different approaches, such as LC/ESI-MS, MALDI-MS and 2-D gel electrophoresis (2-DE), were stored in SQL-LIMS. A minimum set of the proteomics data were transferred in our public 2D-PAGE database using a Java based interface (Data Transfer Tool) with the requirements of the PEDRo standardization. Furthermore, the stored proteomics data were extractable out of SQL-LIMS via XML. Conclusion. The Oracle based data repository system SQL-LIMS played the central role in the proteomics workflow concept. Technical operations of our proteomics labs were used as standards for SQL-LIMS templates. Using a Java based parser, post-processed data of different approaches such as LC/ESI-MS, MALDI-MS and 1-DE and 2-DE were stored in SQL-LIMS. Thus, unique data formats of different instruments were unified and stored in SQL-LIMS tables. Moreover, a unique submission identifier allowed fast access to all experimental data. This was the main advantage compared to multi software solutions, especially if personnel fluctuations are high. Moreover, large scale and high-throughput experiments must be managed in a comprehensive repository system such as SQL-LIMS, to query results in a systematic manner. On the other hand, these database systems are expensive and require at least one full time administrator and specialized lab manager. Moreover, the high technical dynamics in proteomics may cause problems to adjust new data formats. To summarize, SQL-LIMS met the requirements of proteomics data handling especially in skilled processes such as gel-electrophoresis or mass spectrometry and fulfilled the PSI standardization criteria. The data transfer into a public domain via DTT facilitated validation of proteomics data. Additionally, evaluation of mass spectra by post-processing using MS-Screener improved the reliability of mass analysis and prevented storage of data junk.",,,2-s2.0-62149119145
"Delort J.-Y.","Automatically characterizing salience using readers' feedback",2009,"Journal of Digital Information",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-61449197683&partnerID=40&md5=35d4d0f973f121f4bd4f6096b8908d63","Salience is an important characteristic of information influencing users' cognitive and emotional states. For example, salient parts of a document are those that readers will find moving or provoking. This article analyzes the main characteristics of salience and the different meanings of the concept in information retrieval and linguistics. It also presents a generic approach for identifying linguistically salient segments in a text using readers' textual feedback. The method, supports any kind of text and. textual feedback. We evaluated the effectiveness of the method with a. corpus of blog posts and readers' comments. Our preliminary experiments show that the method has promising results with an fscore of 0.65. The method could also be used on 90% of commented posts which proves that it can be used on a large scale.","Feedback; Information retrieval; Salience",,2-s2.0-61449197683
"Klavans J.L., Sheffield C., Abels E., Lin J., Passonneau R., Sidhu T., Soergel D.","Computational linguistics for metadata building (CLiMB): Using text mining for the automatic identification, categorization, and disambiguation of subject terms for image metadata",2009,"Multimedia Tools and Applications",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59849109123&doi=10.1007%2fs11042-008-0253-9&partnerID=40&md5=89da3f3c7ffe16b5e536a673fb4f33da","In this paper, we present a system using computational linguistic techniques to extract metadata for image access. We discuss the implementation, functionality and evaluation of an image catalogers' toolkit, developed in the Computational Linguistics for Metadata Building (CLiMB) research project. We have tested components of the system, including phrase finding for the art and architecture domain, functional semantic labeling using machine learning, and disambiguation of terms in domain-specific text vis a vis a rich thesaurus of subject terms, geographic and artist names. We present specific results on disambiguation techniques and on the nature of the ambiguity problem given the thesaurus, resources, and domain-specific text resource, with a comparison of domain-general resources and text. Our primary user group for evaluation has been the cataloger expert with specific expertise in the fields of painting, sculpture, and vernacular and landscape architecture. © 2008 Springer Science+Business Media, LLC.","Image access; Lexical disambiguation; Metadata extraction; Natural language processing (NLP); Subject cataloging; Word sense disambiguation (WSD)","Automation; Computational linguistics; Electronic data interchange; Image processing; Information theory; Learning systems; Linguistics; Machine components; Metadata; Project management; Semantics; Thesauri; Image access; Lexical disambiguation; Metadata extraction; Natural language processing (NLP); Subject cataloging; Word sense disambiguation (WSD); Natural language processing systems",2-s2.0-59849109123
"Juhász Z.","Automatic segmentation and comparative study of motives in eleven folk song collections using self-organizing maps and multidimensional mapping",2009,"Journal of New Music Research",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-75149195244&doi=10.1080%2f09298210903029830&partnerID=40&md5=3428dae371f63dcef30069ec33ef295c","A data-based system for automatic segmentation of large folk song corpora is described in this article. The algorithm is based on a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 11 cultures in Eurasia were determined. The analysis of the overlaps between the cultures allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution. These groups are connected by the cultures of the Carpathian Basin, which in itself assures the unbroken structure of the system of connections. The mapping of the motive contours into points of an appropriate three-dimensional space opened the possibility to analyse the musical structures of the typical motives in different cultures. Based on the segmentation algorithm, we also defined a melody similarity measure, determining local similarities between the closest motive contours. © 2009 Taylor & Francis.",,,2-s2.0-75149195244
"Mosqueira-Rey E., Alonso-Betanzos A., Guijarro-Berdiñas B., Alonso-Ríos D., Lago-Pineiro J.","A Snort-based agent for a JADE multi-agent intrusion detection system",2009,"International Journal of Intelligent Information and Database Systems",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449088978&doi=10.1504%2fIJIIDS.2009.023041&partnerID=40&md5=3b50c54aa7d1a060689384cff2518306","We describe the design of a misuse detection agent, one of the distinct agents in a multi-agent-based intrusion detection system. This system is being implemented in JADE, a well-known multi-agent platform based in Java. The agent analyses the packets in the network connections using a packet sniffer and then creates a data model based on the information obtained. This data model is the input to a rule-based inference engine agent, which uses the Rete algorithm for pattern matching and the rules of the signature-based intrusion detection system, Snort. Specifically, an implementation in Java language - the Drools-JBoss Rules - was used and a parser was implemented that converts Snort rules into Drools rules. The use of object-oriented techniques, together with design patterns, means that the agent is flexible, easily configurable and extensible. Copyright © 2009, Inderscience Publishers.","Drools-JBoss; Intelligent agents; Intrusion detection; JADE; MD; Misuse detection; Multi-agent systems; Network packet sniffing; Snort","Agents; Artificial intelligence; Computer crime; Inference engines; Intelligent agents; Java programming language; Multi agent systems; Pattern matching; Query processing; Security of data; Silicate minerals; Drools-JBoss; JADE; MD; Misuse detection; Network packet sniffing; Snort; Intrusion detection",2-s2.0-60449088978
"Carvalho P.C., Fischer J.S.G., Chen E.I., Domont G.B., Carvalho M.G.C., Degrave W.M., Yates III J.R., Barbosa V.C.","GO Explorer: A gene-ontology tool to aid in the interpretation of shotgun proteomics data",2009,"Proteome Science",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62549085665&doi=10.1186%2f1477-5956-7-6&partnerID=40&md5=6e5d2e50c6a752121b22a1be9b98ac54","Background: Spectral counting is a shotgun proteomics approach comprising the identification and relative quantitation of thousands of proteins in complex mixtures. However, this strategy generates bewildering amounts of data whose biological interpretation is a challenge. Results: Here we present a new algorithm, termed GO Explorer (GOEx), that leverages the gene ontology (GO) to aid in the interpretation of proteomic data. GOEx stands out because it combines data from protein fold changes with GO over-representation statistics to help draw conclusions. Moreover, it is tightly integrated within the PatternLab for Proteomics project and, thus, lies within a complete computational environment that provides parsers and pattern recognition tools designed for spectral counting. GOEx offers three independent methods to query data: an interactive directed acyclic graph, a specialist mode where key words can be searched, and an automatic search. Its usefulness is demonstrated by applying it to help interpret the effects of perillyl alcohol, a natural chemotherapeutic agent, on glioblastoma multiform cell lines (A172). We used a new multi-surfactant shotgun proteomic strategy and identified more than 2600 proteins; GOEx pinpointed key sets of differentially expressed proteins related to cell cycle, alcohol catabolism, the Ras pathway, apoptosis, and stress response, to name a few. Conclusion: GOEx facilitates organism-specific studies by leveraging GO and providing a rich graphical user interface. It is a simple to use tool, specialized for biologists who wish to analyze spectral counting data from shotgun proteomics. GOEx is available at http://pcarvalho.com/ patternlab. © 2009 Carvalho et al; licensee BioMed Central Ltd.",,"RhoA guanine nucleotide binding protein; RhoB guanine nucleotide binding protein; apoptosis; article; catabolism; cell cycle; data analysis software; genetic algorithm; GO Explorer; human; human cell; protein analysis; protein expression; protein folding; proteomics; shotgun proteomics; stress",2-s2.0-62549085665
"Beetz J., Van Leeuwen J., De Vries B.","IfcOWL: A case of transforming EXPRESS schemas into ontologies",2009,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",93,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68349154149&doi=10.1017%2fS0890060409000122&partnerID=40&md5=c7747d560e734da00c4cc439139b474b","Ontologies have been successfully applied as a semantic enabler of communication between both users and applications in fragmented, heterogeneous multinational business environments. In this paper we discuss the underlying principles, their current implementation status, and most importantly, their applicability to problems in the building information modeling domain. We introduce the development of an ontology for the building and construction sector based on the industry foundation classes. We discuss several approaches of lifting modeling information that is based on the express family of languages for data modeling onto a logically rigid and semantically enhanced ontological level encoded in the W3C Ontology Web Language. We exemplify the added value of such formal notation of building models by providing several examples where generic query and reasoning algorithms can be applied to problems that otherwise have to be manually hard-wired into applications for processing building information. Furthermore, we show how the underlying resource description framework and the set of technologies evolving around it can be tailored to the need of distributed collaborative work in the building and construction industry. Copyright © 2009 Cambridge University Press.","Building Information Modeling; Knowledge Representation; Model-Driven Architectures; Ontologies","Added values; Building and construction; Building Information Modeling; Building model; Collaborative Work; Data modeling; Formal notations; Industry Foundation Classes - IFC; Model-Driven Architectures; Multinational business; Ontology web language; Reasoning algorithms; Resource description framework; Schemas; Underlying principles; Architectural design; Construction industry; Knowledge representation; Linguistics; Model buildings; Query languages; Systems analysis; Ontology",2-s2.0-68349154149
"Habash N., Dorr B., Monz C.","Symbolic-to-statistical hybridization: Extending generation-heavy machine translation",2009,"Machine Translation",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77149159452&doi=10.1007%2fs10590-009-9056-7&partnerID=40&md5=1dae7da5421c21e30da5b538a3ae7f18","The last few years have witnessed an increasing interest in hybridizing surface-based statistical approaches and rule-based symbolic approaches to machine translation (MT). Much of that work is focused on extending statistical MT systems with symbolic knowledge and components. In the brand of hybridization discussed here, we go in the opposite direction: adding statistical bilingual components to a symbolic system. Our base system is Generation-heavy machine translation (GHMT), a primarily symbolic asymmetrical approach that addresses the issue of Interlingual MT resource poverty in source-poor/target-rich language pairs by exploiting symbolic and statistical target-language resources. GHMT's statistical components are limited to target-language models, which arguably makes it a simple form of a hybrid system. We extend the hybrid nature of GHMT by adding statistical bilingual components. We also describe the details of retargeting it to Arabic-English MT. The morphological richness of Arabic brings several challenges to the hybridization task. We conduct an extensive evaluation of multiple system variants. Our evaluation shows that this new variant of GHMT-a primarily symbolic system extended with monolingual and bilingual statistical components-has a higher degree of grammaticality than a phrase-based statistical MT system, where grammaticality is measured in terms of correct verb-argument realization and long-distance dependency translation. © 2010 Springer Science+Business Media B.V.","Arabic-English machine translation; Generation-heavy machine translation; Hybrid machine translation; Statistical machine translation","Base systems; Heavy machines; Higher-degree; Hybrid machine translation; Language model; Language pairs; Language resources; Long-distance dependencies; Machine translations; Multiple systems; Rule based; Statistical approach; Statistical machine translation; Surface-based; Symbolic knowledge; Symbolic systems; Computational linguistics; Hybrid systems; Information theory; Query languages; Speech recognition; Speech transmission; Statistics; Telluric prospecting; Translation (languages)",2-s2.0-77149159452
"Zhang T., Jouault F., Attiogbé C., Bézivin J., Li X.-D.","MDE-based mode transformation: From MARTE model to FIACRE model",2009,"Ruan Jian Xue Bao/Journal of Software",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-61649117334&doi=10.3724%2fSP.J.1001.2009.00214&partnerID=40&md5=e23944a9387833522085e24972c7062f","This paper presents a representative case study of bridging UML/MARTE (unified modeling language/modeling and analysis of real time and embedded systems) to FIACRE (intermediate format for the architectures of embedded distributed components), and discusses in detail two sub-problems of semantic mapping and syntactic transformation respectively. At the semantic level, the semantic mapping rules are developed using model transformation technology so as to implement the transformation between metamodels. At the syntactic level, the concrete syntax rules are built on the metamodels so that textual programs could be generated. Based on the case study, the general framework and corresponding key techniques are discussed. In addition, both the advantages and deficiencies of the work are concluded. © by Institute of Software, the Chinese Academy of Sciences. All rights reserved.","FIACRE (intermediate format for the architectures of embedded distributed components); Formal method; Heterogeneous; MARTE (modeling and analysis of real time and embedded systems); MDE (model-driven engineering)","Concrete syntaxes; FIACRE (intermediate format for the architectures of embedded distributed components); Heterogeneous; Key techniques; MARTE (modeling and analysis of real time and embedded systems); MDE (model-driven engineering); Meta-models; Mode transformations; Model transformations; Representative case; Semantic levels; Semantic mappings; Sub-problems; Syntactic transformations; Unified modeling; Formal methods; Fourier transforms; Information theory; Integrated circuits; Real time systems; Semantics; Syntactics; Systems analysis; Embedded systems",2-s2.0-61649117334
"Raveh B., Enosh A., Schueler-Furman O., Halperin D.","Rapid sampling of molecular motions with prior information constraints",2009,"PLoS Computational Biology",39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-61449189835&doi=10.1371%2fjournal.pcbi.1000295&partnerID=40&md5=8db3d02822ac4253dd536d58456c6f17","Proteins are active, flexible machines that perform a range of different functions. Innovative experimental approaches may now provide limited partial information about conformational changes along motion pathways of proteins. There is therefore a need for computational approaches that can efficiently incorporate prior information into motion prediction schemes. In this paper, we present PathRover, a general setup designed for the integration of prior information into the motion planning algorithm of rapidly exploring random trees (RRT). Each suggested motion pathway comprises a sequence of low-energy clash-free conformations that satisfy an arbitrary number of prior information constraints. These constraints can be derived from experimental data or from expert intuition about the motion. The incorporation of prior information is very straightforward and significantly narrows down the vast search in the typically high-dimensional conformational space, leading to dramatic reduction in running time. To allow the use of state-of-the-art energy functions and conformational sampling, we have integrated this framework into Rosetta, an accurate protocol for diverse types of structural modeling. The suggested framework can serve as an effective complementary tool for molecular dynamics, Normal Mode Analysis, and other prevalent techniques for predicting motion in proteins. We applied our framework to three different model systems. We show that a limited set of experimentally motivated constraints may effectively bias the simulations toward diverse predicates in an outright fashion, from distance constraints to enforcement of loop closure. In particular, our analysis sheds light on mechanisms of protein domain swapping and on the role of different residues in the motion.",,"binding protein; chaperone; cyanovirin N; ribose binding protein; unclassified drug; protein; algorithm; alpha helix; article; beta sheet; conceptual framework; conformation; crystal structure; information system; ligand binding; molecular dynamics; motion analysis system; process optimization; protein secondary structure; structural bioinformatics; chemistry; computer program; protein conformation; Protein Conformation; Proteins; Software",2-s2.0-61449189835
"Amir H.J., Fariborz M.","Query wikification: Mining structured queries from unstructured information needs using wikipedia-based semantic analysis",2009,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032088&partnerID=40&md5=a5ef74630aac255447aff4b546801f42","Combining the language model and inference network, as implemented in the Indri search engine, is efficient and verified approach. In this retrieval model, the user's information need is exhibited as Indri's Structural Query Language. Although the SQL allows expert users to richly represent its information needs but unfortunately, the complicacy of SQLs make them unpopular in the WEB for ordinary ones. Automatically detecting the concepts in a user's information need and generate a richly structured equivalent query is a good solution. It needs a concept repository and a way to extracting appropriate concepts from the user's information need. We utilize Wikipedia as a great, multilingual, free-content encyclopedia for our knowledge base and also some state of the art algorithms for extracting Wikipedia's concepts from the user's information need. This process is called ""Query Wikification"". Mining Wikipedia concept repository help us to propose a solution that supports usability in multilingual environments, cross-language retrievals, scalability and covering erratum, various equivalents and synonyms of a concept. Experimental results verify that our automatic structured query construction is an efficient and scalable method that has a very good potential to apply on the WEB. Our experiments over TEL corpus in CLEF2009 achieves +23% improvement in Mean Average Precision and retrieves more than 600 relevant documents against the Indri baselines. In Persian track, we evaluated a simple stemmer so-called ""Perstem"", a stemmer and light morphological analyzer for Persian language. Our experimental results show that using this stemmer in indexing and retrieval phase can significantly improve both precision (+91%) and recall (+43%).","Indri structure query language; Meta-language indexing; Perstem; Query wikification; Wikipedia knowledge-base; Wm-wikifier","Computational linguistics; Digital libraries; Indexing (of information); Information retrieval systems; Information science; Knowledge based systems; Natural language processing systems; Query languages; Query processing; Scalability; Search engines; Semantics; Social networking (online); Websites; Meta language; Perstem; Query wikification; Structure query languages; Wikipedia knowledge; Wm-wikifier; Data mining",2-s2.0-84922032088
"Su-Cheng H., Chien-Sing L.","Efficient preprocesses for fast storage and query retrieval in native XML database",2009,"IETE Technical Review (Institution of Electronics and Telecommunication Engineers, India)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62349117485&doi=10.4103%2f0256-4602.48466&partnerID=40&md5=9535c7b42aeeeaf7947947c857453481","XML (extensible mark-up language) has emerged as one of the popular data representation standards for information storage and -exchange. In this paper, we propose an extended INLAB architecture, INLAB2, focusing on preprocessing the XML -document for fast native storage and accurate query retrieval. Firstly, we propose our xParse parser to check the well-formedness of an XML document. Next, we use a ( self-end) labeling scheme to encode each -element in the XML database, by its positional information, to establish parent-child (P-C) or -ancestor-descendant (A-D) relationships between nodes. Subsequently, our TwigINLAB2 algorithm is used to optimize query retrieval. TwigINLAB2 is a generalization of TwigStack, the stack-based algorithm for matching twig query. However, the TwigStack algorithm is efficient for A-D relationship queries only. Thus, in order to overcome this limitation, we enhance query retrieval by utilizing indices to speed up the matching and merging phases. Experimental results indicate that TwigINLAB2 can, on an average, process twig queries 23% better than the TwigStack algorithm and 10% better than TwigINLAB1, in terms of execution time.","Indexing; Labeling; Native XML database; Retrieval; Storage; XML query","Indexing; Native XML database; Retrieval; Storage; XML query; Algorithms; Indexing (of information); Labeling; Markup languages; XML",2-s2.0-62349117485
"Sarhan A.","A proposed architecture for dynamically built NLIDB systems",2009,"International Journal of Knowledge-Based and Intelligent Engineering Systems",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013574597&doi=10.3233%2fJAD-2009-0174&partnerID=40&md5=9d34e8f5440f7d60c9a8b20745f4a78c","Natural Language Interface to Database (NLIDB) systems have provided an easy access to database system without the need for the user to use formal query languages, such as SQL. Database query languages can be difficult to the non-expert users and learning these formal queries takes a lot of time. In NLIDB, users can type a question or a sentence in their natural language (such as English or Arabic) then it will be converted through a special natural language interface interrupter into formal database query. A major problem that faces the NLIDB designers is the identification of the tables that contain the required information and the desired attributes in the query. Most of the existing systems use static templates for the queries, in which tables’ names are embedded in the template. However, this requires large code that considers all possible query templates. The main objective of this paper is to introduce a dynamic approach to determine the tables name that the attributes involved in the query belong to by representing the database schema as a graph which is used to determine the tables’ names. This will minimize the templates used to build the queries thus minimizing the code size and effort to track and build all possible queries. We also propose a dynamic component that helps in building any NLIDB called database management. This component is linked to all the NLIDB system’s components and will automatically build the dictionaries and database schema graph. The proposed approach was applied to a case study called Mall Shopper’s Guide (MSG) to investigate its effectiveness. © 2009 - IOS Press and the authors.","Database; Natural language; NLIDB; Parser; Semantic dictionary; Semantic graph",,2-s2.0-85013574597
"Groppe S., Groppe J., Böttcher S., Wycisk T., Gruenwald L.","Optimizing the execution of XSLT stylesheets for querying transformed XML data",2009,"Knowledge and Information Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62149140965&doi=10.1007%2fs10115-008-0144-4&partnerID=40&md5=b2cacf638d2be054b3424ba3bf21b423","We have to deal with different data formats whenever data formats evolve or data must be integrated from heterogeneous systems. These data when implemented in XML for data exchange cannot be shared freely among applications without data transformation. A common approach to solve this problem is to convert the entire XML data from their source format to the applications' target formats using the transformations rules specified in XSLT stylesheets. However, in many cases, not all XML data are required to be transformed except for a smaller part described by a user's query (application). In this paper, we present an approach that optimizes the execution time of an XSLT stylesheet for answering a given XPath query by modifying the XSLT stylesheet in such a way that it would (a) capture only the parts in the XML data that are relevant to the query and (b) process only those XSLT instructions that are relevant to the query. We prove the correctness of our optimization approach, analyze its complexity and present experimental results. The experimental results show that our approach performs the best in terms of execution time, especially when many cost-intensive XSLT instructions can be excluded in the XSLT stylesheet. © Springer-Verlag London Limited 2008.","Optimization; Query reformulation; Transformation; Views; XML; XSLT",,2-s2.0-62149140965
"Bani-Ahmad S.","On context-driven online search-phrase suggesters for large textual document repositories",2009,"Proceedings of the IADIS International Conference Information Systems 2009, IS 2009",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944054261&partnerID=40&md5=44e0bbb3c845ea0d3c540f5bc59ff18a","Guiding user's choice of search-terms and search-phrases is useful as it allows for successful search. Studies show that user spend considerable amount of time choosing search terms and, probably modifying their chosen terms in order to focus search outputs. This means that each search-session is composed of multiple, probably unsuccessful search queries. This in turns degrades scalability of online textual document repositories. In this paper we propose SISSTOR, a prototype of Scalable Inline Search-phrase Suggestor for Textual-document Online Repositories. We achieve scalability through making suggestions from contexts (topics) of interest of the current user. Our proposal has two major offline steps; the pre-analysis step in which the documents at hand are (i) processed to extract frequent search phrases from text, and (ii) are put into contexts such that each context represents a topic. At search time, topics of interest are gradually identified as the user enter more keywords and even parts-of-words. Suggestions are made out of the identified topics. We experimentally evaluate SISSTOR against its basic design principles. For that we use a repository of more than 14,891 publications from the computer science domain. Our proposal promises high-quality suggestions with no startup requirements as the case in Google Suggest which requires having a search history to make suggestions from. © 2009 IADIS.","Information retrieval; Search-phrase suggester; Textual-document digital libraries","Data mining; Information retrieval; Information systems; Scalability; Design Principles; Online repositories; Search history; Search queries; Search sessions; Search-phrase suggester; Successful search; Textual documents; Digital libraries",2-s2.0-84944054261
"Yuan C., Ren F., Wang X., Zhong Y.","Function labeling for unparsed Chinese text",2009,"IEEJ Transactions on Electronics, Information and Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72549118754&doi=10.1541%2fieejeiss.129.1593&partnerID=40&md5=91db5a398be68c9ceeac8114efa503ba","This paper presents a work of function labeling for unparsed Chinese text. Unlike other attempts that utilize the full parse trees, we propose an effective way to recognize function labels directly based on lexical information, which is easily scalable for languages that lack sufficient parsing resources. Furthermore, we investigate a general method to iteratively simplify a sentence, thus transferring complicated sentence into structurally simple pieces. By means of a sequence learning model with hidden Markov support vector machine, we achieve the best F-measure of 87.40 on the text from Penn Chinese Treebank resources - a statistically significant improvement over the existing Chinese function labeling systems. © 2009 The Institute of Electrical Engineers of Japan.","Chinese language processing; Function labeling; Lexical features; Sequence learning","Linguistics; Query languages; Chinese language; Chinese language processing; Chinese text; F-measure; General method; Lexical features; Lexical information; Parse trees; Penn Chinese Treebank; Sequence learning; Labeling; Labeling; Languages; Processing",2-s2.0-72549118754
"Arisoy E., Saraçlar M.","Lattice extension and vocabulary adaptation for turkish LVCSR",2009,"IEEE Transactions on Audio, Speech and Language Processing",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350482314&doi=10.1109%2fTASL.2008.2006655&partnerID=40&md5=b315dc8b620cca9a0f63852306dede09","This paper presents two-pass speech recognition techniques to handle the out-of-vocabulary (OOV) problem in Turkish newspaper content transcription. OOV words are assumed to be replaced by acoustically ""similar"" in-vocabulary (IV) words during decoding. Therefore, the first pass recognition lattice is used as the prior knowledge to adapt the vocabulary and the search space for the second pass. Vocabulary adaptation and lattice extension are performed with words similar to the hypothesis lattice words. These words are selected from a fallback vocabulary using distance functions that take the agglutinative language characteristics of Turkish into account. Morphology-based and phonetic-distance-based similarity functions respectively yield 1.9% and 4.6% absolute accuracy improvements. Statistical sub-word units are also utilized to handle the OOV problem encountered in the word-based system. Using sub-words alleviates the OOV problem and improves the recognition accuracy-OOV accuracy improved from 0% to 60.2%. However, this introduces ungrammatical items to the recognition output. Since automatically derived sub-word units do not provide explicit morphological features, the lattice extension strategy is modified to correct these ungrammatical items. Lattice extension for sub-words reduces the word error rate to 32.3% from 33.9%. This improvement is statistically significant at p = 0.002 as measured by the NIST MAPSSWE significance test. © 2008 IEEE.","Agglutinative languages; Lattice extension; Speech recognition; Vocabulary adaptation","Absolute accuracy; Agglutinative language; Agglutinative languages; Distance functions; Distance-based; Lattice extension; Morphological features; OOV problem; Prior knowledge; Recognition accuracy; Search spaces; Significance test; Similarity functions; Turkishs; Two-pass speech; Vocabulary adaptation; Word error rate; Word-based systems; Linguistics; Query languages; Speech recognition",2-s2.0-70350482314
"Kokkinaki A.I., Christofi S., Pavlou P., Christofi P., Vassiliades V.M., Paraskeva C.","Design and implementation issues of a semantic web enabled search engine",2009,"Proceedings of the IADIS International Conference WWW/Internet 2009, ICWI 2009",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946021310&partnerID=40&md5=0cda2f73cfe9001567ff90e83983e38a","This paper presents design and implementation issues related to a semantic web search system that is being specifically developed for the needs of the business community in Cyprus. More specifically, a market research has pointed out that content related to Cyprus products and/or services as well as content business registered under a domain name in the Cyprus republic can not be easily retrieved from existing search engines. A semantic web search engine system has been proposed to address this issue. The semantic web search engine system will be used primarily by the business community. For this purpose, an ontology has been developed for Cyprus products and services based on international standards. The ontology will be used with instantiations of Cyprus specific content. Complementary to that, a Web Crawler, an Ontology Parser and an Ontology Indexer have been implemented. An Ontology ranker is being developed to address issues related to the presentation of the results. © 2009 IADIS.","Cyprus; Search engine; Semantic web","Information retrieval; Search engines; Semantic Web; Websites; Business community; Cyprus; Design and implementations; International standards; Market researches; Products and services; Semantic web search; Web crawlers; World Wide Web",2-s2.0-84946021310
"Bernard G., Rosset S., Galibert O., Bilinski E., Adda G.","The LIMSI participation to the QAst 2009 track",2009,"CEUR Workshop Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922022698&partnerID=40&md5=c367fd35a389b5d3bbcd510d5a229fd7","We present in this paper the three LIMSI question-answering systems on speech transcripts which participated to the QAst 2009 evaluation. These systems are based on a complete and multi-level analysis of both queries and documents. These systems use an automatically generated research descriptor. A score based on those descriptors is used to select documents and snippets. Three different methods are tried to extract and score candidate answers, and we present in particular a tree transformation based ranking method. We participated to all the tasks and submitted 30 runs (for 24 sub-tasks). The evaluation results for manual transcripts range from 27%to 36% for accuracy depending on the task and from 20% to 29% for automatic transcripts.","Question answering; Speech transcriptions","Artificial intelligence; Natural language processing systems; Speech transmission; Transcription; Automatically generated; Evaluation results; Multi-level analysis; Question Answering; Question answering systems; Speech transcriptions; Speech transcripts; Tree transformation; Digital libraries",2-s2.0-84922022698
"Adda M., Missaoui R., Valtchev P.","Web search based on web communities feedback data",2009,"Lecture Notes in Business Information Processing",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249156726&doi=10.1007%2f978-3-642-01187-0_14&partnerID=40&md5=cf9579ae18df1678146ce4348956700d","The capability to easily find relevant information becomes increasingly difficult as the available content increases. Web Search Engines aim to assist users in finding pertinent information. To measure the relevance of a Web page (its rank), different strategies are used. However, page ranking is mainly conducted by relying on automatic assessment criteria. Hence, a gap is created between the effective relevance of a content and the computed one. To reduce this gap, we introduce a framework for feedback-based web search engine development. To illustrate the effectiveness and the use of the proposed framework, we developed a web search engine prototype called SocialSeeker. Finally, we evaluated our approach from the end-user perspective and the results shown that feedback data can improve search engine results. © 2009 Springer Berlin Heidelberg.","Feedback data; Search engine; Social-based filtering; Web communities","Engines; Information retrieval; Search engines; Automatic assessment; End-user perspective; Page ranking; Search engine results; Social-based filtering; Web communities; Web community; Web page; Web search engines; Web searches; World Wide Web",2-s2.0-67249156726
"Arısoy E., Can D., Parlak S., Saraçlar M., Sak H.","Turkish Broadcast News Transcription and Retrieval",2009,"IEEE Transactions on Audio, Speech and Language Processing",54,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008055179&doi=10.1109%2fTASL.2008.2012313&partnerID=40&md5=690b135a4388789094b9a4bc61d3b90c","This paper summarizes our recent efforts for building a Turkish Broadcast News transcription and retrieval system. The agglutinative nature of Turkish leads to a high number of out-of-vocabulary (OOV) words which in turn lower automatic speech recognition (ASR) accuracy. This situation compromises the performance of speech retrieval systems based on ASR output. Therefore using a word-based ASR is not adequate for transcribing speech in Turkish. To alleviate this problem, various sub-word-based recognition units are utilized. These units solve the OOV problem with moderate size vocabularies and perform even better than a 500 K word vocabulary as far as recognition accuracy is concerned. As a novel approach, the interaction between recognition units, words and sub-words, and discriminative training is explored. Sub-word models benefit from discriminative training more than word models do, especially in the discriminative language modeling framework. For speech retrieval, a spoken term detection system based on automata indexation is utilized. As with transcription, retrieval performance is measured under various schemes incorporating words and sub-words. Best results are obtained using a cascade of word and sub-word indexes together with term-specific thresholding. © 2009 IEEE","Discriminative training; language modeling (LM); morphologically rich languages; speech recognition; spoken term detection",,2-s2.0-85008055179
"Mattingley J., Boyd S.","Automatic code generation for real-time convex optimization",2009,"Convex Optimization in Signal Processing and Communications",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926113841&doi=10.1017%2fCBO9780511804458.002&partnerID=40&md5=80f2e625ee1e569e347c09e9323dca29","This chapter concerns the use of convex optimization in real-time embedded systems, in areas such as signal processing, automatic control, real-time estimation, real-time resource allocation and decision making, and fast automated trading. By “embedded” we mean that the optimization algorithm is part of a larger, fully automated system, that executes automatically with newly arriving data or changing conditions, and without any human intervention or action. By “real-time” we mean that the optimization algorithm executes much faster than a typical or generic method with a human in the loop, in times measured in milliseconds or microseconds for small and medium size problems, and (a few) seconds for larger problems. In real-time embedded convex optimization the same optimization problem is solved many times, with different data, often with a hard real-time deadline. In this chapter we propose an automatic code generation system for real-time embedded convex optimization. Such a system scans a description of the problem family, and performs much of the analysis and optimization of the algorithm, such as choosing variable orderings used with sparse factorizations and determining storage structures, at code generation time. Compiling the generated source code yields an extremely efficient custom solver for the problem family. We describe a preliminary implementation, built on the Python-based modeling framework CVXMOD, and give some timing results for several examples. Introduction Advisory optimization Mathematical optimization is traditionally thought of as an aid to human decision making. © Cambridge University Press 2010.",,"Automatic programming; Automation; Codes (symbols); Convex optimization; Decision making; Digital storage; Embedded systems; Optimization; Program compilers; Signal processing; Automatic code generations; Human decision making; Mathematical optimizations; Optimization algorithms; Optimization problems; Real-time embedded systems; Real-time estimation; Storage structures; Real time systems",2-s2.0-84926113841
"Scott M.L.","Programming Language Pragmatics: Third Edition",2009,"Programming Language Pragmatics: Third Edition",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957766071&doi=10.1016%2fB978-0-12-374514-9.X0001-8&partnerID=40&md5=ddaa45c487880ded31ae05ccd8e46393","Programming Language Pragmatics is the most comprehensive programming language textbook available today. Taking the perspective that language design and language implementation are tightly interconnected, and that neither can be fully understood in isolation, this critically acclaimed and bestselling book has been thoroughly updated to cover the most recent developments in programming language design. With a new chapter on run-time program management and expanded coverage of concurrency, this new edition provides both students and professionals alike with a solid understanding of the most important issues driving software development today. Classic programming foundations text now updated to familiarize students with the languages they are most likely to encounter in the workforce, including including Java 7, C++, C# 3.0, F#, Fortran 2008, Ada 2005, Scheme R6RS, and Perl 6. New and expanded coverage of concurrency and run-time systems ensures students and professionals understand the most important advances driving software today. Includes over 800 numbered examples to help the reader quickly cross-reference and access content. © 2009 Elsevier Inc. All rights reserved.",,,2-s2.0-84957766071
"Koehn P.","Statistical machine translation",2009,"Statistical Machine Translation",72,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928706421&doi=10.1017%2fCBO9780511815829&partnerID=40&md5=c01e1c20a2fba703584948786a4f7b9c","This introductory text to statistical machine translation (SMT) provides all of the theories and methods needed to build a statistical machine translator, such as Google Language Tools and Babelfish. In general, statistical techniques allow automatic translation systems to be built quickly for any language-pair using only translated texts and generic software. With increasing globalization, statistical machine translation will be central to communication and commerce. Based on courses and tutorials, and classroom-tested globally, it is ideal for instruction or self-study, for advanced undergraduates and graduate students in computer science and/or computational linguistics, and researchers in natural language processing. The companion website provides open-source corpora and tool-kits. © P. Koehn 2010.",,"Computation theory; Computational linguistics; Computer aided language translation; Linguistics; Natural language processing systems; Open source software; Statistics; Students; Teaching; Automatic translation; Companion website; Generic softwares; Graduate students; Language pairs; Language tools; Statistical machine translation; Statistical techniques; Translation (languages)",2-s2.0-84928706421
"Caldwell D., Lee S., Mandelbaum Y.","Adaptive parsing of router configuration languages",2008,"2008 IEEE Internet Network Management Workshop, INM 2008",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849099214&doi=10.1109%2fINETMW.2008.4660333&partnerID=40&md5=5eb6a7fdaa80907f5de9a3e3cec1d8eb","Network functionality is growing increasingly complex, making network configuration management a steadily growing challenge. Router configurations capture and reflect all levels of network operation, and it is highly challenging to manage the detailed configurations of the potentially huge number of routers that run a network. One source of difficulty is the constant evolution of router configuration languages. For some languages, particularly Cisco's IOS command language, and its relatives, these changes demand frequent maintenance of configuration parsers in any configuration management tool. The essential problem is that config parsers understand a statically determined set of inputs, requiring human intervention to modify that set. We propose an alternative design for router configuration parsers: adaptive parsers. Such parsers can infer the configuration language based on real configs and automatically adapt to changes in the config language, all with minimal human involvement. We present the design of such a parser and discuss its prototype implementation for the Cisco IOS configuration language. We have validated our prototype's accuracy and efficiency by running it on the configuration files of Tier-1 ISP networks. Our results show that from only 81 configuration files, we can learn enough IOS to successfully parse all of the 819 IOS configurations in under 10 minutes.",,"Alternative designs; Command languages; Configuration files; Configuration languages; Configuration management tools; Essential problems; Human interventions; Network configurations; Network functionalities; Network operations; Prototype implementations; Router configurations; Administrative data processing; Internet; Internet service providers; Linguistics; Management information systems; Metropolitan area networks; Query languages; Routers; Software prototyping; Technical presentations; Network management",2-s2.0-57849099214
"Sarasa-Cabezuelo A., Navarro-Iborra A., Sierra J.-L., Fernández-Valmayor A.","Building a syntax directed processing environment for XML documents by combining SAX and JavaCC",2008,"Proceedings - International Workshop on Database and Expert Systems Applications, DEXA",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849115807&doi=10.1109%2fDEXA.2008.32&partnerID=40&md5=851cc1561eef10173d4f6d24e972f4b3","In this paper we show how to integrate JavaCC, a popular translator-generation tool, with any standard XML parsing environment supporting the SAX specification. This integration lets developers build efficient XML processing applications which act as left-to-right, one-pass translators. The integration also facilitates the maintenance of these applications, since they are specified as syntax-directed translation schemas instead of being directly programmed in a general-purpose programming language. This integration proposal also allows for exploiting the modularization capabilities of the SAX-based underlying processing framework, which is capable of piping several translators that are working concurrently. This concurrent processing facilitates the modularization of complex processing tasks in more affordable, simpler translators, which can be developed and maintained using separated translation schemas. © 2008 IEEE.","JavaCC; SAX; Syntax-directed translation; XML pipelines; XML processing","Administrative data processing; Applications; Computer programming languages; Database systems; Decision support systems; Expert systems; Industrial applications; Integration; Markup languages; Modular construction; Pipelines; Problem solving; Program translators; Query languages; Syntactics; Translation (languages); XML; Complex processing; Concurrent processing; Environment supporting; Generation tools; JavaCC; Modularization; Processing environments; Programming languages; SAX; Schemas; Syntax-directed translation; XML documents; XML parsing; XML processing; Pipeline processing systems",2-s2.0-57849115807
"Kirasić D., Basch D.","Ontology-based design pattern recognition",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849113989&doi=10.1007%2f978-3-540-85563-7-50&partnerID=40&md5=f5a9d4751bf6e128c0a7954265ddc734","This paper presents ontology-based architecture for pattern recognition in the context of static source code analysis. The proposed system has three subsystems: parser, OWL ontologies and analyser. The parser subsystem translates the input code to AST that is constructed as an XML tree. The OWL ontologies define code patterns and general programming concepts. The analyser subsystem constructs instances of the input code as ontology individuals and asks the reasoner to classify them. The experience gained in the implementation of the proposed system and some practical issues are discussed. The recognition system successfully integrates the knowledge representation field and static code analysis, resulting in greater flexibility of the recognition system. © 2008 Springer-Verlag Berlin Heidelberg.","Description logics; Formal pattern definition; Knowledge-based system; Ontology-based system; OWL application; Static code analysis","Description logics; Formal pattern definition; Knowledge-based system; OWL application; Static code analysis; Data description; Information theory; Knowledge based systems; Knowledge engineering; Knowledge representation; Markup languages; Pattern recognition; Ontology",2-s2.0-57849113989
"Snyder B., Barzilay R.","Cross-lingual propagation for morphological analysis",2008,"Proceedings of the National Conference on Artificial Intelligence",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749176853&partnerID=40&md5=8e06587002bbd7f99f418925640d5e6a","Multilingual parallel text corpora provide a powerful means for propagating linguistic knowledge across languages. We present a model which jointly learns linguistic structure for each language while inducing links between them. Our model supports fully symmetrical knowledge transfer, utilizing any combination of supervised and unsupervised data across language barriers. The proposed non-parametric Bayesian model effectively combines cross-lingual alignment with target language predictions. This architecture is a potent alternative to projection methods which decompose these decisions into two separate stages. We apply this approach to the task of morphological segmentation, where the goal is to separate a word into its individual morphemes. When tested on a parallel corpus of Hebrew and Arabic, our joint bilingual model effectively incorporates all available evidence from both languages, yielding significant performance gains. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Bayesian models; Knowledge transfers; Language barriers; Linguistic knowledges; Linguistic structures; Morphological analyses; Morphological segmentations; Parallel corpora; Parallel texts; Performance gains; Projection methods; Target languages; Unsupervised datum; Artificial intelligence; Bayesian networks; Bionics; Information management; Information retrieval systems; Knowledge based systems; Knowledge management; Query languages; Software agents; Speech recognition; Linguistics",2-s2.0-57749176853
"Cairó O., Guardati S.","Automatic translation in two phases: Recognition and interpretation",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749189982&doi=10.1007%2f978-3-540-85565-1-11&partnerID=40&md5=eaf9a497b685b56f475f4c4a72ba939d","This paper presents an approach to automatic translation in two steps. The first one is recognition (syntactic) and the last one is interpretation (semantic). In the recognition phase, we use volatile grammars. They represent an innovation to logic-based grammars. For the interpretation phase, we use componential analysis and the laws of integrity. Componential analysis defines the sense of the lexical parts. The rules of integrity, on the other hand, are in charge of refining, improving and optimizing translation. We applied this framework for general analysis of romance languages and for the automatic translation of texts from Spanish into other neo-Latin languages and vice versa. © 2008 Springer-Verlag Berlin Heidelberg.","Automatic translation; Natural language; Volatile grammars","Information theory; Knowledge based systems; Knowledge engineering; Linguistics; Query languages; Semantics; Automatic translation; Logic-based grammars; Natural language; Romance languages; Spanishs; Two phases; Volatile grammars; Translation (languages)",2-s2.0-57749189982
"Mooney R.J.","Learning to connect language and perception",2008,"Proceedings of the National Conference on Artificial Intelligence",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749107432&partnerID=40&md5=a1d46cd4037ec1351968158fdd206a7c","To truly understand language, an intelligent system must be able to connect words, phrases, and sentences to its perception of objects and events in the world. Current natural language processing and computer vision systems make extensive use of machine learning to acquire the probabilistic knowledge needed to comprehend linguistic and visual input. However, to date, there has been relatively little work on learning the relationships between the two modalities. In this talk, I will review some of the existing work on learning to connect language and perception, discuss important directions for future research in this area, and argue that the time is now ripe to make a concerted effort to address this important, integrative AI problem. Copyright © 2008.",,"Concerted efforts; Future researches; Important directions; Machine learnings; Natural Language Processing; Probabilistic knowledges; Artificial intelligence; Bionics; Computational linguistics; Computer vision; Education; Image processing; Intelligent systems; Learning systems; Linguistics; Visual communication; Natural language processing systems",2-s2.0-57749107432
"Bani-Ahmad S., Ozsoyoglu G.","On content-driven search-keyword suggesters for literature digital libraries",2008,"Proceedings of the ACM International Conference on Digital Libraries",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649143141&doi=10.1145%2f1378889.1378893&partnerID=40&md5=544ca68cf98df793e5f984d5cc454142","We propose and evaluate a ""content-driven search keyword suggester"" for keyword-based search in literature digital libraries. Suggesting search keywords at an early stage, i.e., while the user is entering search terms, is helpful for constructing more accurate, less ambiguous, and focused search keywords for queries. Our search keyword suggestion approach is based on an a priori analysis of the publication collection in the digital library at hand, and consists of the following steps. We (i) parse the document collection using the Link Grammar parser, a syntactic parser of English, (ii) group publications based on their ""most-specific"" research topics, (iii) use the parser output to build a hierarchical structure of simple and compound tokens to be used to suggest search terms, (iv) use TextRank, a text summarization tool, to assign topic-sensitive scores to keywords, and (v) use the identified research-topics to help user aggregate search keywords prior to the actual search query execution. We experimentally show that tine proposed framework, which is optimized to work on literature digital libraries, promises a more scalable, high quality, and user-friendly search-keyword suggester when compared to its competitors. We validate our proposal experimentally using a subset of the ACM SIGMOD Anthology digital library as a testbed, and by employing the research-pyramid model to identify the ""most-specific"" research topics. Copyright 2008 ACM.","Literature digital libraries; Search-keyword suggester","Competition; Computational linguistics; Formal languages; Image retrieval; Libraries; A-priori; Document collections; Focused searches; Hierarchical structures; High qualities; Keyword-based searches; Literature digital libraries; Research topics; Search keywords; Search queries; Search terms; Search-keyword suggester; Syntactic parsers; Test beds; Text summarizations; Digital libraries",2-s2.0-57649143141
"Manine A.-P., Alphonse E., Bessières P.","Information extraction as an ontology population task and its application to genie interactions",2008,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649224064&doi=10.1109%2fICTAI.2008.117&partnerID=40&md5=9c2a285e75ee93fddedafd193dc4293b","Ontologies are a well-motivated formal representation to model knowledge needed to extract and encode data from text. Yet, their tight integration with Information Extraction (IE) systems is still a research issue, a fortiori with complex ones that go beyond hierarchies. In this paper, we introduce an original architecture where IE is specified by designing an ontology, and the extraction process is seen as an Ontology Population (OP) task. Concepts and relations of the ontology define a normalized text representation. As their abstraction level is irrelevant for text extraction, we introduced a Lexical Layer (LL) along with the ontology, i.e. relations and classes at an intermediate level of normalization between raw text and concepts. On the contrary to previous IE systems, the extraction process only involves normalizing the outputs of Natural Language Processing (NLP) modules with instances of the ontology and the LL. AH the remaining reasoning is left to a query module, which uses the inference rules of the ontology to derive new instances by deduction. In this context, these inference rules subsume classical extraction rules or patterns by providing access to appropriate abstraction level and domain knowledge. To acquire those rules, we adopt an Ontology Learning (OL) perspective, and automatically acquire the inference rules with relational Machine Learning (ML). Our approach is validated on a genie interaction extraction task from a Bacillus subtilis bacterium text corpus. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten conceptual relations in the ontology. © 2008 IEEE.",,"Abstraction levels; Bacillus Subtilis; Conceptual relations; Domain knowledges; Extraction processes; Extraction rules; Formal representations; IE systems; Inference rules; Information extractions; Intermediate levels; Natural Language Processing; Ontology populations; Query modules; Research issues; Text corpora; Text extractions; Text representations; Abstracting; Artificial intelligence; Bacteriology; Computational linguistics; Flow interactions; Information analysis; Knowledge based systems; Knowledge representation; Learning systems; Natural language processing systems; Population statistics; Theorem proving; Ontology",2-s2.0-57649224064
"Kong J., Ates K.L., Zhang K., Gu Y.","Adaptive mobile interfaces through grammar induction",2008,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649221828&doi=10.1109%2fICTAI.2008.41&partnerID=40&md5=dfc401a8d5573ce017781c0aa5597879","This paper presents a grammar-induction based approach to partitioning a Web page into several small pages while each small page fits not only spatially but also logically for mobile browsing. Our approach proceeds in three steps: (1) using the grammar induction technique to generate a graph grammar, which formalizes design policies for presenting information in a clear and logic structure; (2) based on the graph grammar, a graph parser parses a Web page to recover the hierarchical logic structure underlying that Web page; (3) the extracted logic structure models the content organization in the Web page, and is used to partition the Web page into several small pages for mobile displays. © 2008 IEEE.",,"Content organizations; Design policies; Grammar inductions; Graph grammars; Logic structures; Mobile Displays; Mobile interfaces; Three steps; Web pages; Artificial intelligence; Computational grammars; Computational linguistics; Context sensitive grammars; Graph theory; World Wide Web; Formal languages",2-s2.0-57649221828
"Stewart R., Scott G., Zelevinsky V.","Idea navigation: Structured browsing for unstructured text",2008,"Conference on Human Factors in Computing Systems - Proceedings",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649205526&doi=10.1145%2f1357054.1357332&partnerID=40&md5=49beda80f3d3e17362b6062331919298","Traditional interfaces for information access do not fully support queries that rely on semantic relationships between terms. To better support such queries, we introduce a system that automatically extracts subject-verb-object concepts from unstructured text documents and dynamically presents them to the user as navigable refinements. This approach, which we call ""idea navigation,"" makes subject-verb-object querying as simple as selecting successive refinements. It also supports exploratory search by providing a view of the most common ideas in the current result set. First-time users of a prototype system successfully used idea navigation to solve realistic search tasks, demonstrating its effectiveness. Copyright 2008 ACM.","Exploratory search; Faceted browsing; Information retrieval","Exploratory search; Faceted browsing; First-time users; Information access; Prototype systems; Search tasks; Semantic relationships; Successive refinements; Text documents; Human engineering; Image retrieval; Information retrieval; Information services; Information theory; Navigation",2-s2.0-57649205526
"Bamman D., Crane G.","Building a dynamic lexicon from a digital library",2008,"Proceedings of the ACM International Conference on Digital Libraries",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649200254&doi=10.1145%2f1378889.1378892&partnerID=40&md5=23ea9553f22a95379a80233ef48c6fbc","We describe here in detail our work toward creating a dynamic lexicon from the texts in a large digital library. By leveraging a small structured knowledge source (a 30,457 word treebank), we are able to extract selectional preferences for words from a 3.5 million word Latin corpus. This is promising news for low-resource languages and digital collections seeking to leverage a small human investment into much larger gain. The library architecture in which this work is developed allows us to query customized subcorpora to report on lexical usage by author, genre or era and allows us to continually update the lexicon as new texts are added to the collection. Copyright 2008 ACM.","Digital libraries; Lexicography; Syntactic parsing","Digital collections; Lexicography; Selectional preferences; Structured knowledge; Syntactic parsing; Treebank; Information management; Knowledge management; Libraries; Syntactics; Digital libraries",2-s2.0-57649200254
"Nguyen L.-M., Shimazu A., Phan X.H., Nguyen P.T.","Online structured learning for semantic parsing with synchronous and λ - synchronous context free grammars",2008,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649155519&doi=10.1109%2fICTAI.2008.96&partnerID=40&md5=7e65d25f91216caa399fc0c364cc42a3","We formulate semantic parsing as a parsing problem on a synchronous context free grammar (SCFG) which is automatically built on the corpus of natural language sentences and the representation of semantic outputs. We then present an online learning framework for estimating the synchronous SCFG grammar. In addition, our online learning methods for semantic parsing problems are also extended to deal with the case, in which the semantic representation could be represented under λ- calculus. Experimental results in the domain of semantic parsing show advantages in comparison with previous works. ©2008 IEEE.",,"Artificial intelligence; Context free grammars; Context free languages; E-learning; Formal languages; Functions; Information theory; Internet; Semantics; Do-mains; Natural languages; On-line learnings; Online learning methods; Parsing problems; Semantic parsing; Semantic representations; Structured learnings; Computational linguistics",2-s2.0-57649155519
"Zhao J., Kan M.-Y., Theng Y.L.","Math information retrieval: User requirements and prototype implementation",2008,"Proceedings of the ACM International Conference on Digital Libraries",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649174646&doi=10.1145%2f1378889.1378921&partnerID=40&md5=3e50e7c9da1267d9f17b96b791da873d","We report on the user requirements study and preliminary implementation phases in creating a digital library that indexes and retrieves educational materials on math. We first review the current approaches and resources for math retrieval, then report on the interviews of a small group of potential users to properly ascertain their needs. While preliminary, the results suggest that meta-search and resource categorization are two basic requirements for a math search engine. In addition, we implement a prototype categorization system and show that the generic features work well in identifying the math contents from the webpage but perform less well at categorizing them. We discuss our long term goals, where we plan to investigate how math expressions and text search may be best integrated. Copyright 2008 ACM.","Interaction histories; Math information retrieval; Niche search engines; User requirement analysis; Web classification","Information retrieval; Information services; Libraries; Natural language processing systems; Search engines; Software prototyping; World Wide Web; Interaction histories; Math information retrieval; Niche search engines; User requirement analysis; Web classification; Digital libraries",2-s2.0-57649174646
"Hou Q., Zhou K., Guo B.","BSGP: Bulk-synchronous GPU programming",2008,"SIGGRAPH'08: International Conference on Computer Graphics and Interactive Techniques, ACM SIGGRAPH 2008 Papers 2008",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649083013&partnerID=40&md5=b7fad58c1abfc43f25a8324e30113718","We present BSGP, a new programming language for general purpose computation on the GPU. A BSGP program looks much the same as a sequential C program. Programmers only need to supply a bare minimum of extra information to describe parallel processing on GPUs. As a result, BSGP programs are easy to read, write, and maintain. Moreover, the ease of programming does not come at the cost of performance. A well-designed BSGP compiler converts BSGP programs to kernels and combines them using optimally allocated temporary streams. In our benchmark, BSGP programs achieve similar or better performance than well-optimized CUDA programs, while the source code complexity and programming time are significantly reduced. To test BSGP's code efficiency and ease of programming, we implemented a variety of GPU applications, including a highly sophisticated X3D parser that would be extremely difficult to develop with existing GPU programming languages. © 2008 ACM.","Bulk synchronous parallel programming; Programable graphics hardware; Stream processing; Thread manipulation","Bulk synchronous parallel programming; C programs; General purpose; GPU programming; Parallel processing; Programable graphics hardware; Programming languages; Programming time; Source codes; Stream processing; Thread manipulation; Benchmarking; C (programming language); Computer graphics; Computer hardware description languages; Computer software; Computers; Exposure controls; Interactive computer graphics; Linguistics; Programming theory; Query languages; Parallel programming",2-s2.0-57649083013
"Thiemann P., Neubauer M.","Macros for context-free grammars",2008,"PPDP'08 - Proceedings of the 10th International ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57549105496&doi=10.1145%2f1389449.1389465&partnerID=40&md5=408210d5971598c09116d7497ef38fec","Current parser generators are based on context-free grammars. Because such grammars lack abstraction facilities, the resulting specifications are often not easy to read. Fischer's macro grammars extend context-free grammars with macro-like productions thus providing the equivalent of procedural abstraction. However, their use is hampered by the lack of an efficient, off-the-shelf parsing technology for macro grammars. We define specialization for macro grammars to enable reuse of parsing technology for context-free grammars while facilitating the specification of a language with a macro grammar. This specialization yields context-free rales, but it does not always terminate. We present a sound and complete static analysis that applies to any macro grammar and decides whether specialization terminates for it and thus yields a (finite) context-free grammar. The analysis is based on an intuitive notion of self-embedding nonterminals, which is easy to check by hand. We have implemented the analysis as part of a preprocessing tool that transforms a Yacc grammar extended with macro productions to a standard Yacc grammar. Copyright © 2008 ACM.","Languages; Theory","Abstracting; Computational grammars; Computational linguistics; Computer programming; Context free grammars; Formal languages; Fourier transforms; Linguistics; Programming theory; Query languages; Specifications; Context-free; Languages; Parser generators; Parsing technologies; Pre-processing tools; Procedural abstractions; Theory; Context free languages",2-s2.0-57549105496
"Sunkle S., Kuhlemann M., Siegmund N., Rosenmüller M., Saake G.","Generating highly customizable SQL parsers",2008,"EDBT'08 Workshop on Software Engineering for Tailor-made Data Management, SETMDM",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349173054&doi=10.1145%2f1385486.1385495&partnerID=40&md5=0d7166f070ffdd6ed559375261e44acb","Database technology and the Structured Query Language (SQL) have grown enormously in recent years. Applications from different domains have different requirements for using database technology and SQL. The major problem of current standards of SQL is complexity and unmanageability. In this paper we present an approach based on software product line engineering which can be used to create customizable SQL parsers and consequently different SQL dialects. We present an overview of how SQL can be decomposed in terms of features and compose different features to create different parsers for SQL. Copyright 2008 ACM.","Embedded systems; Feature-oriented programming; Tailor-made data management","Embedded systems; Integrated circuits; Management information systems; Production engineering; Query languages; Software engineering; Current standards; Customizable; Database technologies; Different domains; Feature-oriented programming; Software product line engineerings; Structured Query languages; Tailor-made data management; Database systems",2-s2.0-57349173054
"Wu F., Weld D.S.","Automatically refining the wikipedia infobox ontology",2008,"Proceeding of the 17th International Conference on World Wide Web 2008, WWW'08",196,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349152295&doi=10.1145%2f1367497.1367583&partnerID=40&md5=9502fe56860088d20e6b00479e36c57e","The combined efforts of human volunteers have recently extracted numerous facts from Wikipedia, storing them as machine-harvestable object-attribute-value triples in Wikipedia infoboxes. Machine learning systems, such as Kylin, use these infoboxes as training data, accurately extracting even more semantic knowledge from natural language text. But in order to realize the full power of this information, it must be situated in a cleanly-structured ontology. This paper introduces KOG, an autonomous system for refining Wikipedia's infobox-class ontology towards this end. We cast the problem of ontology refinement as a machine learning problem and solve it using both SVMs and a more powerful joint-inference approach expressed in Markov Logic Networks. We present experiments demonstrating the superiority of the joint-inference approach and evaluating other aspects of our system. Using these techniques, we build a rich ontology, integrating Wikipedia's infobox-class schemata with WordNet. We demonstrate how the resulting ontology may be used to enhance Wikipedia with improved query processing and other features.","Markov logic networks; Ontology; Semantic web; Wikipedia","Information theory; Internet; Knowledge based systems; Learning systems; Query processing; Refining; Robot learning; Semantic Web; Semantics; World Wide Web; Autonomous systems; Human volunteers; Machine learning problems; Machine Learning systems; Markov logic networks; Natural language texts; Semantic knowledges; Training datums; Wikipedia; WordNet; Ontology",2-s2.0-57349152295
"Shoaib Jameel M., Akshat A., Singh C.T.","Enhancements in query evaluation and page summarization of the thinking algorithm",2008,"Proceedings - International Symposium on Information Technology 2008, ITSim",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349129549&doi=10.1109%2fITSIM.2008.4632050&partnerID=40&md5=9836afdad6b7fa25a2c8efe738d66236","This paper explores a unique way in which The Thinking Algorithm adds an extra logical substrate to a web search query using artificial intelligence. Instead of just going after keyword searching, the algorithm tries to assess the motives of the user behind entering a query. The algorithm tries to find the reasons as to why a user has entered a particular query by adding this question with every query: ""Sitting in a particular region, why has the person entered such a query?"" The Compounded Uniqueness Level applies the concept of geo-location searches. The algorithm allots competency level to the user from the query term using topic trees. The query parsers and indexers in the algorithm are more magnetized into extracting meaningful information from queries and web pages than just indexing the words present in a web page. Clustering the search results help to resolve the ambiguity in a user's query. © 2008 IEEE.",,"Artificial intelligence; Information technology; Trees (mathematics); Keyword searching; Query evaluations; Query terms; Search results; Web pages; Web search queries; World Wide Web",2-s2.0-57349129549
"Benedikt M., Alan Jeffrey J., Ley-Wild R.","Stream firewalling of XML constraints",2008,"Proceedings of the ACM SIGMOD International Conference on Management of Data",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149127375&doi=10.1145%2f1376616.1376667&partnerID=40&md5=0398650e295636364ea6fef58ae8bfff","As XML-based messages have become common in many client-server protocols, there is a need to protect application servers from invalid or dangerous messages. This leads to the XML stream, ftrewalling problem; that of applying integrity constraints against a large number of simultaneous streams. We conduct the first investigation of a constraint engine optimized for the generation of XML stream firewalls. We isolate a class of DTDs and XPath constraints which support the generation of low-space filters, and provide algorithms for generating firewalls with low per-input-character time and per-stream space. We give experimental results which show that we have achieved these goals in practice. Copyright 2008 ACM.","Query processing; Streams; XML; Xpath","Application Servers; Integrity constraints; Space filters; Streams; Xml streams; Xpath; Computer system firewalls; Constrained optimization; Markup languages; Query processing; Servers; XML",2-s2.0-57149127375
"Fisher K., Walker D., Zhu K.","LearnPADS: Automatic tool generation from ad hoc data",2008,"Proceedings of the ACM SIGMOD International Conference on Management of Data",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149123931&doi=10.1145%2f1376616.1376759&partnerID=40&md5=8827c9ed09ae0add2f8c477456d067d5","In this demonstration, we will present LearnPADS, a fully automatic system for generating ad hoc data processing tools. When presented with a collection of ad hoc data, the system (1) analyzes the data, (2) infers a pads [4, 5] description, (3) generates parser, printer, validation and traversal libraries and (4) links these libraries with format-independent tool suites to form stand-alone applications. These applications provide statistical analysis, XML conversion, CSV conversion, the ability to query with the Galax XQuery engine [3], and the ability to graph selected data elements, all directly from ASCII ad hoc data without human intervention. SIGMOD attendees will see both the user experience with LEARNPADS and the internals of the multi-phase inference algorithm which lies at the heart of the system.","Algorithms; Languages","Applications.; Automatic systems; Automatic tools; Data elements; Human interventions; Inference algorithms; Languages; Processing tools; Statistical analysis; Tool suites; User experiences; Ad hoc networks; Data storage equipment; Inference engines; Libraries; Markup languages; Data processing",2-s2.0-57149123931
"Haw S.-C., Lee C.-S.","INLAB2: Fast native XML storage and query retrieval",2008,"Proceedings of 2008 3rd International Conference on Intelligent System and Knowledge Engineering, ISKE 2008",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349096405&doi=10.1109%2fISKE.2008.4730896&partnerID=40&md5=cf8ba722bab45d16c1007711ef333f10","The past few years have seen a dramatic increase in the popularity and adoption of XML in Internet technology. With this revolution, an effective way for storing and querying XML documents is crucial. In this paper, we propose the INLAB2 architecture for fast native XML storage and accurate query retrieval. 1NLAB2 comprises of five main components, namely XML Parser, XML Encoder, XML Indexer, Data Manager and Query Processor. All these components are coupled tightly and play an important role in answering queries specified by the user. Extensive experiments quantify the practical benefits of TwigINLAB2, the main algorithm in Query Processor, which amounts to a significant performance gain compared to other approaches. © 2008 IEEE.",,"Answering queries; Internet technologies; Performance gains; Query processors; Query retrievals; XML parsers; Xml storages; Intelligent control; Intelligent systems; Knowledge engineering; XML; Markup languages",2-s2.0-60349096405
"Hong T.-P., Dong J.-S., Lin W.-Y.","An integrated OWL data mining and query system",2008,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949159772&doi=10.1109%2fICSMC.2008.4811283&partnerID=40&md5=391634b6e2a674d12c3780bb039a6750","In this paper, we propose an integrated OWL data mining and query system architecture for expressing the mined knowledge in the OWL format and for effectively answering users' queries. The proposed system consists of five sub-systems: query parser, rule inference system, ontology management system, knowledge generation system, and knowledge management system. We expect the architecture can provide users to query mined knowledge through the internet and with a more machine-understandable format. © 2008 IEEE.","Association rules; Data mining; Ontology; OWL language; Semantic web","Generation systems; Knowledge management system; Ontology management; OWL language; Query systems; Rule inference; Sub-systems; Association rules; Associative processing; Control theory; Cybernetics; Knowledge based systems; Knowledge management; Linguistics; Management; Ontology; Semantics; Semantic Web",2-s2.0-69949159772
"Benting W.","Sms-based urban public traffic query service",2008,"Proceedings of the 2008 2nd International Conference on Future Generation Communication and Networking, FGCN 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650245326&doi=10.1109%2fFGCNS.2008.94&partnerID=40&md5=7c4c8af518b49757f96e4fb418b0ee74","Mobile information services will play an important role in our future work and private life. Enabling mobility in urban and populous areas needs novel techniques for individual traffic planning. However, though there already are WAP-based traffic systems featuring route planning, their usability is not often universal. We presents a SMS-based urban public traffic information system(SMS-UPTS) offering exact transfer query service and supporting most mobile devices. The SMS-UPTS makes use of bus station library model and least transfer path search algorithm. The least transfer path search algorithm is consisted of direct reach path search algorithm and least transfer converse search algorithm(CS-LT). Bus station library model improves veracity of Chinese parse word algorithm and flexibility of traffic information system, and direct reach path search algorithm can immediately get paths whose bus stations are on the same line, and CS-LT algorithm is based on point to point least transfer algorithm(P2P-LT) which makes use of adjacent and reachable matrixes, it greatly improves transfer algorithm performance. And the implementation results show that the performance of P2P-LS algorithm is better than Dijkstra algorithm and the SMS-UPTS has good efficiency. © 2008 IEEE.",,"Algorithm performance; Bus stations; Dijkstra algorithms; Individual traffic; LS algorithm; Mobile information services; Novel techniques; Path search; Point to point; Query service; Reachable matrix; Route planning; Search Algorithms; Traffic information systems; Traffic queries; Traffic systems; Transfer paths; Bus terminals; Buses; Cellular telephone systems; Information services; Information systems; Mobile devices; Traffic control; Wireless networks; Learning algorithms",2-s2.0-67650245326
"Jančura J., Průša D.","Generic framework for integration of programming languages into NetBeans IDE",2008,"Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950892812&doi=10.1145%2f1328408.1328426&partnerID=40&md5=46b0799fb9b2010083b6d48db8c2b10c","We present a generic framework that can be used for an easy integration of editing and visualization support for a programming language or files with a structure into NetBeans IDE. Needed features are defined using a simple declarative language. It is also possible to provide custom Java methods to enhance the definition's capabilities. The concept aims at good maintenance and performance of implemented languages. We proved it to work well by integrating over twenty languages.","Framework for visualization of programming languages; Parser","Declarative Languages; Generic frameworks; Java methods; NetBeans; Parser; Programming language; Computer aided software engineering; Computer software selection and evaluation; Integration; Integrodifferential equations; Java programming language; Query languages; Semantics; Visualization; Linguistics",2-s2.0-77950892812
"Sommerlad P., Zgraggen G., Corbat T., Felber L.","Retaining comments when refactoring code",2008,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63349104535&doi=10.1145%2f1449814.1449817&partnerID=40&md5=ee58800983464d8b67b6887f69f436b3","Today's software developer depends on automated refactoring tools. While implementing such refactoring tools based on the Eclipse platform for a variety of languages, we faced the problem that carefully crafted formatting and comments can disappear when refactoring source code. This is annoying. Therefore, a useful refactoring tool should keep comments and move them along with the source code they belong to. We present different approaches on how to retain comments when refactoring source code. Usually comments are not present in an abstract syntax tree (AST) used by an integrated development environment (IDE). We achieve comment retention by associating them with nodes in an AST. The different attempts on gathering comments and implementing that association are shown and evaluated. Details of comment handling in different refactoring plug-ins that we have implemented for Eclipse [3], are given as well as a brief comparison with comment handling when refactoring with the Eclipse Java Development Tools [4]. We hope, that this paper enables others to implement comment-preserving refactoring tools for more languages and IDEs. Every programmer, language and IDE deserves good refactoring support or they might become endangered.","Ast; C++; Comments; Eclipse; Parser; Refactoring; Ruby","Computer software; Computer systems programming; Corundum; Integrodifferential equations; Linguistics; Query languages; Ruby; Trees (mathematics); Web services; Ast; C++; Comments; Eclipse; Parser; Refactoring; Object oriented programming",2-s2.0-63349104535
"David V.","Preparing for C++0x",2008,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63349083947&doi=10.1145%2f1449814.1449853&partnerID=40&md5=cdbd6a9a0c481e4f8edf5156ed3328cd","C++0x brings us new features that extend the C++ language syntax. This requires new tools to parse and work with the language. However, a C++ front-end is non trivial to write from scratch. We use an extensible C++ front-end to add concepts.","C++; C++0x; Concepts; Parsing; Program transformation; Reusable Parser","Computer software reusability; Computer systems programming; Linguistics; Query languages; Systems analysis; C++; C++0x; Concepts; Parsing; Program transformation; Reusable Parser; Object oriented programming",2-s2.0-63349083947
"Liu F., Deng S., Wang N., Li X.","A CS grammar based query form information extraction method",2008,"Proceedings - International Symposium on Computer Science and Computational Technology, ISCSCT 2008",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62349085203&doi=10.1109%2fISCSCT.2008.190&partnerID=40&md5=c3fe285ec2f668690b80834604c5aa8f","Nowadays, the number of Web databases has experienced an increase at a surprising rate. Data in the Web databases are hidden behind query forms. As the general reptiles are difficult to search these data, massive resources have been wasted. In order to integrate Web databases and provide a convenience to users' query, one of important problems in this research area is to understand what a query form says. This paper introduces a form information extraction method which is established on the basis of the analysis. By observing a large number of Web pages containing query forms, we found the basic structure of them and confirmed the existence of a syntax which guides the creation of them. So we established a method to extract query form information, captured the syntax through a derived grammar-Code Sequence grammar and designed an automaton parser to understand query forms automatically. © 2008 IEEE.","CS grammar; Deep Web; Query form; Web database","Basic structures; Code sequences; CS grammar; Deep Web; Information extraction methods; Query form; Research areas; Web database; Web pages; Computers; Information analysis; Syntactics; Websites; Computational methods",2-s2.0-62349085203
"Lana-Serrano S., Villena-Román J., González-Cristóbal J.C., Goñi-Menoyo J.M.","MIRACLE at GeoCLEF Query Parsing 2007: Extraction and classification of geographical information",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349819788&doi=10.1007%2f978-3-540-85760-0-99&partnerID=40&md5=c40901df8a226edf4b6798d80f16f9b5","This paper describes the participation of MIRACLE research consortium at the Query Parsing task of GeoCLEF 2007. Our system is composed of three main modules. The first one is the Named Geo-entity Identifier, whose objective is to perform the geo-entity identification and tagging, i.e., to extract the ""where"" component of the geographical query, if there is any. Then, the Query Analyzer parses this tagged query to identify the ""what"" and ""geo-relation"" components by means of a rule-based grammar. Finally, a two-level multiclassifier first decides whether the query is indeed a geographical query and, should it be positive, then determines the query type according to the type of information that the user is supposed to be looking for: map, yellow page or information. © 2008 Springer-Verlag Berlin Heidelberg.","Classification; Gazetteer; Geographical entity recognition; Geographical IR; Geonames; Linguistic Engineering; Query classifier; Tagging; WordNet","Classification; Gazetteer; Geographical entity recognition; Geographical IR; Geonames; Linguistic Engineering; Query classifier; Tagging; WordNet; Classifiers; Computational linguistics; Content based retrieval; Formal languages; Information services; Learning systems; Ontology; Semantic Web; Linguistics",2-s2.0-70349819788
"Cai D., Lin X., Ji D., Yang J.","NP tree matching for English-Chinese translation of patent titles",2008,"2008 International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650547000&doi=10.1109%2fNLPKE.2008.4906759&partnerID=40&md5=69fc0368f2072d2178f6b38db60c7678","This paper proposes a method of NP tree matching to realize the translation of English-Chinese patent titles. Firstly a bilingual example database for patent titles is built. English parse trees are produced by English parser, forming NP tree database. The input patent title to be translated is firstly parsed into a tree. Then NP trees are searched for which match with the input NP tree in NP tree database. If similar NP trees exist, HowNet is used to find the best NP tree by calculating word semantic similarity. Final translations are obtained through calculating cohesion of candidate words. If there are no similar NP trees, subtrees that match the input NP tree are searched for and translations generate by subtree substitutions recursively. Experimental results show that our method outperforms a baseline Pharaoh, by using BLEU evaluation system. ©2008 IEEE.","HowNet; NP translation pattern; NP tree; Patent terms","English parser; Evaluation system; HowNet; If there are; NP translation pattern; NP tree; Parse trees; Patent terms; Semantic similarity; Subtree; Subtrees; Tree database; Tree-matching; Computational linguistics; Database systems; Knowledge engineering; Natural language processing systems; Patents and inventions; Translation (languages)",2-s2.0-67650547000
"Wu J., Huang S.-Y.","XPred: A new model-mapping-schema-based approach for efficient access to XML data",2008,"Proceedings of the 10th International Conference on Information Integration and Web-based Applications and Services, iiWAS 2008",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349097969&doi=10.1145%2f1497308.1497336&partnerID=40&md5=aec375179ec1e186188b487dbe57d4c8","Extensible markup language (XML) has become an active research topic in recent years. Many excellent model-mapping-schema-based approaches have been proposed to translate and manipulate XML documents in relational databases. However, most previous work has a potential performance problem for retrieving XML data from a relational database, because a large number of join operations are needed. In this paper, we introduce a new model-mapping-schema-based approach, called XPred, to reduce significant join costs for providing real-time access to XML data. The basic idea is to store the structural information distributely into nodes to reduce the number of join operations when processing user queries. In particular, for every node in a given XML document, we store its predecessor's information within itself. It can eliminate the join operations for parent-child traversing such that the performance of query processing can be improved. The capability of our proposed approach was verified by a series of simulation experiments based on the XMark [1][2], for which we have some encouraging experimental results. Copyright 2008 ACM.","Model mapping schema; Query processing; Relational databases; XML","Basic idea; Extensible markup language; Join operation; Model mapping schema; New model; Performance problems; Real-time access; Relational Database; Relational databases; Research topics; Schema-based approach; Simulation experiments; Structural information; User query; XML data; Hypertext systems; Information retrieval; Mapping; Query languages; Query processing; Web services; XML; Markup languages",2-s2.0-70349097969
"Mlýnková I., Pokorný J.","Usermap-an Adaptive Enhancing of User-Driven XML-to-Relational Mapping Strategies",2008,"Conferences in Research and Practice in Information Technology Series",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350395738&partnerID=40&md5=7c1d9894b6e6b6c084415e24a133e0c7","As the XML has become a standard for data representation, it is inevitable to propose and implement techniques for efficient managing of XML data. A natural alternative is to exploit features of (object-)relational database systems, i.e. to rely on their long theoretical and practical history. The main concern of such techniques is the choice of an appropriate XML-to-relational mapping strategy. In this paper we focus on enhancing of user-driven techniques which leave the mapping decisions in hands of users who specify their requirements using schema annotations. We describe our prototype implementation called UserMap which is able to exploit the annotations more deeply searching the user-specified ""hints"" in the rest of the schema and applies an adaptive method on the remaining schema fragments. Using a sample set of supported fixed mapping methods we discuss problems related to query evaluation for storage strategies generated by the system, in particular correction of the candidate set of annotations and related query translation. And finally, we describe the architecture of the whole system. © 2008, Australian Computer Society, Inc.","Adaptivity; Similarity; User-driven strategy; XML-to-relational mapping","Adaptive methods; Adaptivity; Data representations; Mapping method; Mapping strategy; Prototype implementations; Query evaluation; Query translations; Sample sets; Similarity; User-driven strategy; XML data; Query processing; Relational database systems; XML; Digital storage",2-s2.0-70350395738
"Ghodke S., Bird S.","Querying linguistic annotations",2008,"ADCS 2008 - Proceedings of the Thirteenth Australasian Document Computing Symposium",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858384201&partnerID=40&md5=23cf3bc1761225e1453904af69220a5e","Over the past decade, a variety of expressive linguistic query languages have been developed. The most scalable of these have been implemented on top of an existing database engine. However, with the arrival of efficient, wide-coverage parsers, it is feasible to parse text on a scale that is several orders of magnitude larger. We show that the existing database approach will not scale up, and speculate on a new approach that leverages proximity search in the context of an IR engine. We also propose a simple syntax for querying linguistic annotations, avoiding the usability problems with existing tree query languages.","Information retrieval; Natural language techniques and documents; XML document standards","Database engine; Document standards; IR engines; Natural language techniques; Orders of magnitude; Scale-up; Usability problems; Data mining; Information retrieval; Query languages; Query processing; Linguistics",2-s2.0-84858384201
"Telea A., Voinea L.","An interactive reverse engineering environment for large-scale C++ code",2008,"SOFTVIS 2008 - Proceedings of the 4th ACM Symposium on Software Visualization",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63449083987&doi=10.1145%2f1409720.1409732&partnerID=40&md5=5582788a459cf5d245b3d710c1b30f41","Few toolsets for reverse-engineering and understanding of C++ code provide parsing and fact extraction, querying, analysis and code metrics, navigation, and visualization of source-code-level facts in a way which is as easy-to-use as integrated development environments (IDEs) are for forward engineering. We present an interactive reverse-engineering environment (IRE) for C and C-H-which allows to set up the fact extraction process, apply user-written queries and metrics, and visualize combined query results, metrics, code text, and code structure. Our IRE tightly couples a fast, tolerant C++ fact extractor, an open query system, and several scalable dense-pixel visualizations in a novel way, offering an easy way to analyze and examine large code bases. We illustrate our IRE with several examples, focusing on the added value of the integrated, visual reverse-engineering approach. © 2008 ACM.",,"Added values; C++ codes; Code metrics; Code structures; Engineering environments; Fact extractions; Integrated development environments; Large code basis; Query results; Query systems; Tool sets; Reengineering; Reverse engineering; Visualization; Computer software",2-s2.0-63449083987
"Hassan H., Simaan K., Way A.","A syntactic language model based on incremental ccg parsing",2008,"2008 IEEE Workshop on Spoken Language Technology, SLT 2008 - Proceedings",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649537432&doi=10.1109%2fSLT.2008.4777876&partnerID=40&md5=dc0e3458a6890df736276e77b9180e59","Syntactically-enriched language models (parsers) constitute a component in applications such as machine translation speech-recognition. To maintain a useful level of , existing parsers are non-incremental and must span combinatorially growing space of possible structures as every word is processed. This prohibits their incorporation standard linear-time decoders. In this paper, we an incremental, linear-time dependency parser based Combinatory Categorial Grammar (CCG) and classification . We devise a deterministic transform of CCGbank derivations into incremental ones, and train parser on this data. We discover that a cascaded, incremental provides an appealing balance between efficiency accuracy. ©2008 IEEE.","Grammar; Language Modeling; Natural languages","Combinatory categorial grammar; Grammar; Language model; Language Modeling; Machine translations; Natural languages; Time dependency; Computational linguistics; Computer aided language translation; Information theory; Speech recognition; Speech transmission; Query languages",2-s2.0-67649537432
"Ng S.C.S., Wong A.K.Y.","RCR- a novel model for effective computer-aided tcm (traditional chinese medicine) learning over the web",2008,"Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951495851&doi=10.1109%2fCSSE.2008.113&partnerID=40&md5=0523be56c88ca6b1c782ea3a13f8864e","The Rehearsal with Cross-Referencing (RCR) model is proposed to help Traditional Chinese Medicine (TCM) students and practitioners learn effectively anytime and anywhere over the web. The RCR has a 3-layer architecture. The bottom layer is normally the TCM ontology created by domain experts with consensus certification. The middle layer is the semantic net that logically represents the knowledge subsumption hierarchy in the bottom layer. The semantic net is the machine processable form of this hierarchy. The top layer is the syntactical layer where queries are formed. It is for human understanding and management. A query input from this layer invokes the parser in the middle layer that works with the semantic net or DOM (document object model) tree. Through the RCR graphical user interface, a learner can select parameters (e.g. symptoms) freely. These selected parameters become those in the implicit (i.e. formed automatically by the system) query construct that invokes the parser. By selecting the same parameters the learner can visualize the parsing mechanism repeatedly and this moves the newly acquired knowledge in short-term memory of the learner's brain into the long-term section; that is rehearsing. Since the knowledge in the ontological layer is logically the same as the semantic net, they are transitive. Every semantic path in the DOM tree should have the corresponding XML construct in theontological layer - they are transitive. This transitivity makes cross-referencing across layers possible. The cross-referencing isimportant for learning and debugging because it helps the uservisualize the subsumption hierarchy of concepts, attributes andtheir associations in the TCM ontology. © 2008 IEEE.","3-layer architecture; E-learning; Formatting; Keywords-component; Rehearsal and cross-referencing; TCM","3-layer architectures; Formatting; Keywords-component; Rehearsal and cross-referencing; TCM; Computer aided instruction; E-learning; Graphical user interfaces; Medicine; Ontology; Query processing; Semantics; Software engineering; XML; Learning systems",2-s2.0-79951495851
"Ahmed K., Younus N., Rauf B., Ashraf J.","Efficient and robust content based searching based on XML technology",2008,"2008 International Conference on Computational Intelligence for Modelling Control and Automation, CIMCA 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449560630&doi=10.1109%2fCIMCA.2008.80&partnerID=40&md5=fc203df69a2517a4df6fa804b5ccdfba","eXtensible Markup Language (XML) is a sub part of the Standard Generalized Markup Language (SGML) and is a mother of Wireless Markup Language (WML). This technology is effective for storage, searching and retrieval of data based on query. The bottleneck in the normal search engines is the lackness of proper metadata which proves to be an obstacle in the quick data retrieval. The XML has been replacing the traditional slow techniques quickly. The proposed system performs the efficient tag basedonline searching using Simple Application Programming Interface for XML (SAX) parser. Syste provides option to the user to store the entire document as an XML user-defined column. Fast search via indices is made for XML elements and this result in faster retrieval. Proposed system uses a SAX based component which uses the developed XML files for the retrieval. Component has been designed in such a way to get precise and efficient results. © 2008 IEEE.",,"Content-based searching; Data retrieval; Extensible markup language; Fast search; Standard generalized markup languages; System use; Wireless markup language; XML files; XML technology; Application programming interfaces (API); Artificial intelligence; Hypertext systems; Linguistics; Metadata; Query languages; Search engines; SGML; XML; Markup languages",2-s2.0-70449560630
"Coppola B., Moschitti A., Tonelli S., Riccardi G.","Automatic framenet-based annotation of conversational speech",2008,"2008 IEEE Workshop on Spoken Language Technology, SLT 2008 - Proceedings",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649543735&doi=10.1109%2fSLT.2008.4777843&partnerID=40&md5=5eb4f341242d2eaa9837d61d363e75f6","Current Spoken LanguageUnderstanding technology is based on a simple concept annotation of word sequences, where the interdependencies between concepts and their compositional semantics are neglected. This prevents an effective handling of language phenomena, with a consequential limitation on the design of more complex dialog systems. In this paper, we argue that shallow semantic representation as formulated in the Berkeley FrameNet Project may be useful to improve the capability of managing more complex dialogs. To prove this, the first step is to show that a FrameNet parser of sufficient accuracy can be designed for conversational speech. We show that exploiting a small set of FrameNetbased manual annotations, it is possible to design an effective semantic parser. Our experiments on an Italian spoken dialog corpus, created within the LUNA project, show that our approach is able to automatically annotate unseen dialog turns with a high accuracy.©2008 IEEE.","Computational semantics; Learning models; Spoken dialog systems","Compositional semantics; Computational semantics; Dialog systems; FrameNet; Learning models; Manual annotation; Semantic representation; Spoken dialog systems; Education; Learning systems; Linguistics; Query languages; Semantics; Speech recognition; Computational linguistics",2-s2.0-67649543735
"Medeiros S., Ierusalimschy R.","A parsing machine for PEGs",2008,"DLS'08: Proceedings of the 2008 Symposium on Dynamic Languages",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59249096385&doi=10.1145%2f1408681.1408683&partnerID=40&md5=0755d8053c1df44a356300f6e03ea872","Parsing Expression Grammar (PEG) is a recognition-based foundation for describing syntax that renewed interest in top-down parsing approaches. Generally, the implementation of PEGs is based on a recursive-descent parser, or uses a memoization algorithm. We present a new approach for implementing PEGs, based on a virtual parsing machine, which is more suitable for pattern matching. Each PEG has a corresponding program that is executed by the parsing machine, and new programs are dynamically created and composed. The virtual machine is embedded in a scripting language and used by a patternmatching tool. We give an operational semantics of PEGs used for pattern matching, then describe our parsing machine and its semantics. We show how to transform PEGs to parsing machine programs, and give a correctness proof of our transformation. © 2008 ACM.","Parsing Expression Grammars; Parsing machine; Pattern matching","Dynamic light scattering; Dynamic loads; Formal languages; Fourier transforms; Information theory; Linguistics; Pattern matching; Polyethylene glycols; Query languages; Semantics; Correctness proofs; Machine programs; Memoization; New approaches; Operational semantics; Parsing Expression Grammars; Parsing machine; Scripting languages; Top-down parsing; Virtual machines; Computational linguistics",2-s2.0-59249096385
"Pérez Andrés F., De Lara J., Guerra E.","Domain specific languages with graphical and textual views",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749159161&doi=10.1007%2f978-3-540-89020-1-7&partnerID=40&md5=affdce4f30e756aecf86b2e755193c6a","We show our approach for the definition of Domain Specific Languages integrating both graphical and textual views. The approach is based on the meta-modelling concepts provided by the AToM3 tool. In this way, the language designer starts building the meta-model of the complete language. Then, he can select (possibly overlapping) submodels of the meta-model to define the different diagram types (i.e. language viewpoints). By default, the viewpoint is assigned a graphical concrete syntax, although a textual one can also be given. This is performed by selecting (or creating) triple graph grammar rules to translate from the viewpoint meta-model to a DSL called Textual that contains the most common elements of textual languages (such as expressions or operators). From a Textual model, a parser is automatically generated, where the semantic actions of the EBNF grammar are graph grammar rules, derived from the viewpoint meta-model. In this way, the parsing results in a model conformant to the viewpoint meta-model, which can be seamlessly integrated with other graphical and textual views. © 2008 Springer Berlin Heidelberg.",,"Context sensitive grammars; Formal languages; Graph theory; Helium; Information theory; Query languages; Automatically generated; Concrete syntaxes; Domain specifics; Graph grammars; Semantic actions; Textual languages; Triple Graph Grammars; Linguistics",2-s2.0-56749159161
"Finifter M., Mettler A., Sastry N., Wagner D.","Verifiable functional purity in Java",2008,"Proceedings of the ACM Conference on Computer and Communications Security",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349266256&doi=10.1145%2f1455770.1455793&partnerID=40&md5=41af9ba221cd5dffeecb662989436449","Proving that particular methods within a code base are functionally pure-deterministic and side-effect free-would aid verification of security properties including function invertibility, reproducibility of computation, and safety of untrusted code execution. Until now it has not been possible to automatically prove a method is functionally pure within a high-level imperative language in wide use, such as Java. We discuss a technique to prove that methods are functionally pure by writing programs in a subset of Java called Joe-E; a static verifier ensures that programs fall within the subset. In Joe-E, pure methods can be trivially recognized from their method signature. To demonstrate the practicality of our approach, we refactor an AES library, an experimental voting machine implementation, and an HTML parser to use our techniques. We prove that their top-level methods are verifiably pure and show how this provides high-level security guarantees about these routines. Our approach to verifiable purity is an attractive way to permit functional-style reasoning about security properties while leveraging the familiarity, convenience, and legacy code of imperative languages. Copyright 2008 ACM.","Determinism; Object-capabilities; Pure functions; Static analysis","Determinism; Imperative languages; Invertibility; Legacy code; Level method; Object-capabilities; Pure functions; Reproducibilities; Security properties; Side effect; Untrusted code; Computer software; High level languages; Linguistics; Markup languages; Query languages; Static analysis; Java programming language",2-s2.0-70349266256
"Jin P., Lian J., Zhao X., Wan S.","TISE: A temporal search engine for web contents",2008,"Proceedings - 2008 2nd International Symposium on Intelligent Information Technology Application, IITA 2008",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949110196&doi=10.1109%2fIITA.2008.132&partnerID=40&md5=b4a7941566feb9eb53a3a9a3970c87a5","In this paper, we present a temporal search engine supporting content time retrieval for Web pages, which is called TISE. The main purpose of TISE is to support the Web search on temporal information embedded in Web pages. Compared with commercial search engines such as Google and Baidu, and other temporal search prototypes, which mainly focus on the creation or crawled time of Web pages, our system concentrates on the extraction and search on content time of Web pages, and can provide more meaningful time-based search facilities, such as temporal relation query. In detail, TISE is based on a unified temporal ontology of Web pages, in which different types of time are defined. We introduce a new type of time ""primary time"" to denote the most appropriate time describing the content of a Web page. After an overview of the general features of TISE, we discuss the architecture of TISE and some key modules. And finally, experiment results and analysis is conducted based on fix types of temporal-text queries on TISE and www.baidu.com. The experiment results show that TISE is more efficient when processing temporal-text Web queries. © 2008 IEEE.",,"Information retrieval; Information technology; Ontology; Search engines; Telecommunication networks; Search facilities; Temporal informations; Temporal relations; Text queries; Web contents; Web pages; Web searches; World Wide Web",2-s2.0-62949110196
"Kmoch O., Pokorný J.","XSLT implementation in a relational environment",2008,"MCCSIS'08 - IADIS Multi Conference on Computer Science and Information Systems; Proceedings of Informatics 2008 and Data Mining 2008",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449123584&partnerID=40&md5=324fe5b95ea605f2f2d263495388afec","In this paper we present an approach to processing XSL transformations in a relational DBMS environment. XML data to be transformed are imported into a relational database and processed by relational tools. It means that the XSLT algorithm is implemented in a relational engine by SQL queries and by procedural extensions to SQL. Output XML documents are again stored in the relational storage. Such a solution increases the effectiveness of application development, particularly in the cases when large volumes of XML data have to be processed. The project, called xslt4db, uses the Firebird DBMS and is freely distributed under LGPL license. © 2008 IADIS.","Relational database; SQL; XPath; XSLT","Computers; Fourier transforms; Information management; Information systems; XML; Application development; Relational database; SQL; SQL queries; XML data; XML documents; XPath; XSL transformations; XSLT; Markup languages",2-s2.0-58449123584
"Van Durme B., Qian T., Schubert L.","Class-driven attribute extraction",2008,"Coling 2008 - 22nd International Conference on Computational Linguistics, Proceedings of the Conference",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959755439&partnerID=40&md5=dd8076e53146a7fee06cb201cae810da","We report on the large-scale acquisition of class attributes with and without the use of lists of representative instances, as well as the discovery of unary attributes, such as typically expressed in English through prenominal adjectival modification. Our method employs a system based on compositional language processing, as applied to the British National Corpus. Experimental results suggest that document-based, open class attribute extraction can produce results of comparable quality as those obtained using web query logs, indicating the utility of exploiting explicit occurrences of class labels in text. © 2008. Licensed under the Creative Commons.",,"Attribute extraction; British national corpora; Class labels; Language processing; Query logs; System-based; User interfaces; Computational linguistics",2-s2.0-79959755439
"Ahn H.S., Baek Y.M., Choi J.Y.","Block type modular robot: Mom's friend",2008,"IFAC Proceedings Volumes (IFAC-PapersOnline)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961019531&doi=10.3182%2f20080706-5-KR-1001.1740&partnerID=40&md5=4a7cb68889d92199caacad93ab53a036","Nowadays, people have various personalities and the things they want are also diverse. In the case of clothes, it is possible to produce multifariously. However it is impossible to produce variously for robot industry because the demand for robots is low and the price is high. It is needed to give the right of choice to people and the modular system can be the solution. One of the most important purposes of modulation is reusability of modules and functions. If each module has own functionality separately, people can select the modules which have the functionality they want. In this paper, we introduce a block type modular service robot system, 'Mom's friend'. People choose the function block modules which have different operating system, function, and shape and heap up the block modules, like playing with blocks, for assembling the robot. Mom's Friend can recognize which block module is assembled in real-time by a self-diagnosis system and then a main supervising system connects the new block module to the whole robot system. There is no need to set up or control by people. Each block module has a pair of connection socket array for communication with other block modules. Mom's Friend makes various types of robot and it is possible to assemble any block module. We implemented eight block modules for building block type reconfigurable intelligent modular robot. SLAM module is for movement of robot. It controls 2 motors and uses sonar sensors, IR scanners, and one compass sensor. It also has obstacle avoidance function and real time map builder. Power module is a module that includes batteries to supply electric power. It supplies both 12V and 24V using lead batteries. Main module manages the connections of all modules. It gets the status of modules using many kinds of communications (e.g. TCP/IP, USB and Serial). It sends action commands of robot to each module depending on received commands and status. Vision module recognizes faces, characters and objects. Each function is implemented using optimal algorithms, because image processing needs high performance for recognition. Face processor detects faces by Haar detection algorithm and recognizes faces by OfflinePCA and SVDD algorithm. We use SIFT algorithm to recognize objects. When the robot tries to grip an object, it recognizes what the object is, and calculates how far the object is. Home appliance control module operates various home appliances using Bluetooth communication. Manipulator module controls both arms and hands. It grips an object and uses instruments. It also expresses gestures to show robot emotions to users. Head module is for emotion expression and getting two camera images. As robot neck has Pan-Tilt system, face tracking is possible. It expresses about 20 types of emotions using eyes and a mouth. Mom's friend has internal emotion and it responses directly and indirectly to external stimulus. If it knows a user's face, it has like-dislike degree to user, and expresses an emotion. Speech module recognizes users' speech in two languages (Korean and English). It has STT(Speech To Text), TTS(Text To Speech) engines. STT engine finds almost similar word in speech map. We use HMM algorithm for speech recognition. As it recognizes users' speech, it transfers commands to main module through communication network. Main scheduler manages to communicate among each module by TCP/IP communication. Although operating systems are different from other modules, one module can communicate with other modules. For proper communication, we define mark up language and develop a parser to manage these. It needs thread synchronization among processes. We assembled various types of robot using these block modules and ascertained the superiority and efficiency of the introduced modular robot system. Copyright © 2007 International Federation of Automatic Control All Rights Reserved.","Intelligent robotics","Building blockes; Camera images; Communication networks; Control module; Detection algorithm; Electric power; Emotion expression; External stimulus; Face Tracking; Function Block; HMM algorithm; Home appliances; Intelligent robotics; Lead-battery; Main module; Modular robot systems; Modular system; Obstacle avoidance; Operating systems; Optimal algorithm; Power module; Re-configurable; Real time; Robot emotions; Robot system; Self-diagnosis; Service robot system; Sonar sensor; Speech module; TCP/IP communication; Text to speech; Thread synchronization; Tilt systems; Vision modules; Algorithms; Bluetooth; Cellular telephone systems; Computer operating systems; Domestic appliances; Human computer interaction; Image processing; Internet protocols; Linguistics; Method of moments; Modular robots; Query languages; Reusability; Robotics; Sensors; Speech recognition; Underwater acoustics; Speech communication",2-s2.0-79961019531
"Wang M., Yu S., Duan H.","Exploiting salient word dependency for Chinese NP identification: A study on classifier noun phrase",2008,"2008 International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650394422&doi=10.1109%2fNLPKE.2008.4906763&partnerID=40&md5=455677790b0e9b1a90910eaff86ba8db","NP identification is a challenging subtask of NLP. The reported literatures mainly focus on Base Noun Phrase and Maximal-length Noun Phrase, and deal with them as a sequence labeling problem. In this paper, unlike existing perspective, we concentrate on a special subcategory of Chinese NP, Classifier Noun Phrase (CNP), and present a new approach which uses salient word dependency, such as classifier-noun collocation, for CNP identification. The experiment result is encouraging. Our study shows that salient relations between words should be fully utilized in NP identification as well as other NLP applications.","Classifier noun phrase; Classifier-noun collocation; NP identification","New approaches; Noun phrase; NP identification; Sequence Labeling; Classifiers; Highway planning; Knowledge engineering; Labels; Learning systems; Natural language processing systems; Query languages; Computational linguistics",2-s2.0-67650394422
"Abdel Monem A., Shaalan K., Rafea A., Baraka H.","Generating Arabic text in multilingual speech-to-speech machine translation framework",2008,"Machine Translation",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71249119549&doi=10.1007%2fs10590-009-9054-9&partnerID=40&md5=0617d8890e42b0762e3430130791bbb7","The interlingual approach to machine translation (MT) is used successfully in multilingual translation. It aims to achieve the translation task in two independent steps. First, meanings of the source-language sentences are represented in an intermediate language-independent (Interlingua) representation. Then, sentences of the target language are generated from those meaning representations. Arabic natural language processing in general is still underdeveloped and Arabic natural language generation (NLG) is even less developed. In particular, Arabic NLG from Interlinguas was only investigated using template-based approaches. Moreover, tools used for other languages are not easily adaptable to Arabic due to the language complexity at both the morphological and syntactic levels. In this paper, we describe a rule-based generation approach for task-oriented Interlingua-based spoken dialogue that transforms a relatively shallow semantic interlingual representation, called interchange format (IF), into Arabic text that corresponds to the intentions underlying the speaker's utterances. This approach addresses the handling of the problems of Arabic syntactic structure determination, and Arabic morphological and syntactic generation within the Interlingual MT approach. The generation approach is developed primarily within the framework of the NESPOLE! (NEgotiating through SPOken Language in E-commerce) multilingual speech-to-speech MT project. The IF-to-Arabic generator is implemented in SICStus Prolog. We conducted evaluation experiments using the input and output from the English analyzer that was developed by the NESPOLE! team at Carnegie Mellon University. The results of these experiments were promising and confirmed the ability of the rule-based approach in generating Arabic translation from the Interlingua taken from the travel and tourism domain. © 2009 Springer Science+Business Media B.V.","Arabic natural language processing; Interlingua; Machine translation; Natural language generation; Rule-based text generation","Arabic natural language processing; Arabic texts; Carnegie Mellon University; E-Commerce; Evaluation experiments; Input and outputs; Interchange formats; Intermediate languages; Language complexity; Machine translations; Natural language generation; Rule based; Rule-based approach; SICStus Prolog; Spoken dialogue; Spoken languages; Syntactic structure; Target language; Template-based; Travel and tourism; Computational linguistics; Computer aided language translation; Electronic commerce; Information theory; Mobile telecommunication systems; Natural language processing systems; Query languages; Speech recognition; Speech transmission; Syntactics; Telluric prospecting; Translation (languages)",2-s2.0-71249119549
"Oury N., Swierstra W.","The power of Pi",2008,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59249086782&doi=10.1145%2f1411204.1411213&partnerID=40&md5=2d9b13f0977a880c6fffdaeaf9f9f882","This paper exhibits the power of programming with dependent types by dint of embedding three domain-specific languages: Cryptol, a language for cryptographic protocols; a small data description language; and relational algebra. Each example demonstrates particular design patterns inherent to dependently-typed programming. Documenting these techniques paves the way for further research in domain-specific embedded type systems. Copyright © 2008 ACM.","Design; Languages; Theory","Cryptographic protocols; Dependent types; Design; Design patterns; Domain specifics; Domain-Specific Languages; Languages; Relational algebras; Small datum; Theory; Type systems; Algebra; Computer programming; Cryptography; Embedded systems; Linguistics; Programming theory; Query languages; Functional programming",2-s2.0-59249086782
"Yuan C., Wang X., Ren F.","Exploiting lexical information for function tag labeling",2008,"2008 International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650403819&doi=10.1109%2fNLPKE.2008.4906787&partnerID=40&md5=ba9786170ea75b317cd81d86e3da3cb1","This paper proposes an novel approach to annotate function tags for unparsed text. What distinguishes our work from other attempts in such task is that we assign function tags directly basing on lexical information other than on parsed trees. In order to demonstrate the effectiveness and versatility of our method, we investigate two statistical models for automatic annotation, one is log-linear maximum entropy model and the other is margin maximum based support vector machine model, which achieve the best F-score of 82.8 and 86.4 respectively when tested on the text from Penn Chinese Treebank. We also quantity the effect of POS tagger accuracy on system performance. Our results indicate that the function tag types could be determined via flexible and powerful feature representations from words, POS tags and word position indicators, and that, similarly to syntactic parsing, the main difficulty lies in complex constituents with long-distance dependency. ©2008 IEEE.","Chinese language; Function tagging; Unparsed text","Automatic annotation; Chinese language; F-score; Feature representation; Function tags; Lexical information; Long-distance dependencies; Maximum entropy models; Penn Chinese Treebank; PoS taggers; Statistical models; Syntactic parsing; Tag types; Unparsed text; Computational linguistics; Knowledge engineering; Labels; Plasma guns; Query languages; Natural language processing systems; Information Retrieval; Languages; Mathematical Models; Statistical Analysis",2-s2.0-67650403819
"Niebler E.","Proto: A compiler construction toolkit for DSELs",2008,"LCSD 2007 - Proceedings of the 2007 ACM SIGPLAN Symposium on Library-Centric Software Design",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952340530&doi=10.1145%2f1512762.1512767&partnerID=40&md5=7dcd50b40cca5d4b64fd817d28a7162d","A Domain-Specific Embedded Language (DSEL) is a miniature language-within-a-language for solving problems in a particular domain. One technique for creating efficient and expressive DSELs in C++ is to use expression templates, but this technique is not for the faint of heart. Such libraries are difficult to write and maintain due to the esoteric nature of template meta-programming, and difficult to use because of the often impenetrable compiler error messages they generate. Existing tools help somewhat, but do not provide the support that language designers have come to expect: something like BNF for defining the language's grammar and associated semantic actions. This paper describes Proto, a C++ library that implements a compiler construction toolkit for embedded languages. The benefits of grammar-based DSELs are shown by contrasting them to other existing approaches to DSEL design. The nature of embedded languages with constrained grammars and their implications for a embedded compiler construction toolkit is briefly explored. Some examples are shown where library interfaces can be made more expressive through the use of grammar-based DSELs. © 2007 ACM.",,"C++ libraries; Compiler construction; Domain specific; Embedded Languages; Error messages; Expression templates; Meta Programming; Semantic action; Computer software; Design; Linguistics; Object oriented programming; Program compilers; Query languages; Software design",2-s2.0-77952340530
"Martin T., Shen Y., Azvine B.","Incremental evolution of fuzzy grammar fragments to enhance instance matching and text mining",2008,"IEEE Transactions on Fuzzy Systems",21,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149106234&doi=10.1109%2fTFUZZ.2008.925920&partnerID=40&md5=3f61ac0b0f161b4bbd61e28351821a23","In many applications, it is useful to extract structured data from sections of unstructured text. A common approach is to use pattern matching (e.g., regular expressions) or more general grammar-based techniques. In cases where exact templates or grammar fragments are not known, it is possible to use machine learning approaches, based on words or n-grams, to identify the structured data. This is generally a two-stage (train/use) process that cannot easily cope with incremental extensions of the training set. In this paper, we combine a fuzzy grammar-based approach with incremental learning. This enables a set of grammar fragments to evolve incrementally, each time a new example is given, while guaranteeing that it can parse previously seen examples. We propose a novel measure of overlap between fuzzy grammar fragments that can also be used to determine the degree to which a string is parsed by a grammar fragment. This measure of overlap allows us to compare the range of two fuzzy grammar fragments (i.e., to estimate and compare the sets of strings that fuzzily conform to each grammar) without explicitly parsing any strings. A simple application shows the method's validity. © 2008 IEEE.","Entity extraction; Evolving system; Extensible markup language (XML); Grammar fragments; Guzzy sets; Incremental learning; Instance matching; Tagging; Text Mining","Computational linguistics; Data mining; Education; Hypertext systems; Learning systems; Linguistics; Markup languages; Pattern matching; Query languages; XML; Entity extraction; Evolving system; Extensible markup language (XML); Grammar fragments; Guzzy sets; Incremental learning; Instance matching; Tagging; Text Mining; Formal languages",2-s2.0-52149106234
"Zou Y., Zeng X., Han X., Zhang K.","Context-attributed graph grammar framework for specifying visual languages",2008,"Journal of Southeast University (English Edition)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59649117697&partnerID=40&md5=9007a4ccff97b232f71aac9f07b75982","Since the specifications of most of the existing context-sensitive graph grammars tend to be either too intricate or not intuitive, a novel context-sensitive graph grammar formalism, called context-attributed graph grammar (CAGG), is proposed. In order to resolve the embedding problem, context information of a graph production in the CAGG is represented in the form of context attributes of the nodes involved. Moreover, several properties of a set of confluent CAGG productions are characterized, and then an algorithm based on them is developed to decide whether or not a set of productions is confluent, which provides the foundation for the design of efficient parsing algorithms. It can also be shown through the comparison of CAGG with several typical context-sensitive graph grammars that CAGG is more succinct and, at the same time, more intuitive than the others, making it more suitably and effortlessly applicable to the specification of visual languages.","Confluence; Context-attributed; Graph grammar; Parsing; Visual language","Computational linguistics; Context sensitive grammars; Formal languages; Graph theory; Linguistics; Production; Query languages; Specifications; Confluence; Context-attributed; Graph grammar; Parsing; Visual language; Context sensitive languages",2-s2.0-59649117697
"Sanders B.A., Deumens E., Lotrich V., Ponton M.","Refactoring a language for parallel computational chemistry",2008,"Proceedings of the 2nd Workshop on Refactoring Tools, WRT '08, in conjunction with the Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA 2008",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249102141&doi=10.1145%2f1636642.1636653&partnerID=40&md5=7dc35258729fc3df3900a00c1aefc1ff","We describe a project to provide refactoring support for the SIAL programming language. SIAL is a domain specific parallel programing language designed to express quantum chemistry computations. It incorporates language support for the loop parallelism and distributed array parallel design patterns. In contrast to refactorings typically undertaken for object-oriented programs which have the goal of improving the code structure, SIAL refactorings are usually done to improve the performance or to allow larger problems to be solved. Copyright © 2008 ACM.","Computational chemistry; Refactoring; SIAL","Code structure; Computational chemistry; Domain specific; Object-oriented program; Parallel design; Programming language; Refactorings; Computer systems programming; Linguistics; Query languages; Object oriented programming",2-s2.0-72249102141
"Kats L.C.L., Bravenboer M., Visser E.","Mixing source and bytecode a case for compilation by normalization",2008,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63549097326&doi=10.1145%2f1449764.1449772&partnerID=40&md5=7480bf31ef6a54a1255af323a9add9a6","Language extensions increase programmer productivity by providing concise, often domain-specific syntax, and support for static verification of correctness, security, and style constraints. Language extensions can often be realized through translation to the base language, supported by preprocessors and extensible compilers. However, various kinds of extensions require further adaptation of a base compiler's internal stages and components, for example to support separate compilation or to make use of low-level primitives of the platform (e.g., jump instructions or unbalanced synchronization). To allow for a more loosely coupled approach, we propose an open compiler model based on normalization steps from a high-level language to a subset of it, the core language. We developed such a compiler for a mixed Java and (core) bytecode language, and evaluate its effectiveness for composition mechanisms such as traits, as well as statement-level and expression-level language extensions. Copyright © 2008 ACM.","Bytecode; Compilers; Domain-specific languages; Dryad compiler; Embedded languages; Iterators; Java; Language extensions; Meta programming; SDF; Source tracing; Stratego; Traits","Bytecode; Compilers; Domain-specific languages; Dryad compiler; Embedded languages; Iterators; Java; Language extensions; Meta programming; SDF; Source tracing; Stratego; Traits; Computer systems programming; High level languages; Java programming language; Linguistics; Program compilers; Query languages; Response time (computer systems); Object oriented programming",2-s2.0-63549097326
"DuPlain R., Benson J., Sessoms E.","Data vault: providing simple web access to NRAO data archives",2008,"Proceedings of SPIE - The International Society for Optical Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949111813&doi=10.1117%2f12.789402&partnerID=40&md5=30e76153c1185b3abbdc7a4ccf80d77b","In late 2007, the National Radio Astronomy Observatory (NRAO) launched Data Vault, a feature-rich web application for simplified access to NRAO data archives. This application allows users to submit a Google-like free-text search, and browse, download, and view further information on matching telescope data. Data Vault uses the model-view-controller design pattern with web.py, a minimalist open-source web framework built with the Python Programming Language. Data Vault implements an Ajax client built on the Google Web Toolkit (GWT), which creates structured JavaScript applications. This application supports plug-ins for linking data to additional web tools and services, including Google Sky. NRAO sought the inspiration of Google's remarkably elegant user interface and notable performance to create a modern search tool for the NRAO science data archive, taking advantage of the rapid development frameworks of web.py and GWT to create a web application on a short timeline, while providing modular, easily maintainable code. Data Vault provides users with a NRAO-focused data archive while linking to and providing more information wherever possible. Free-text search capabilities are possible (and even simple) with an innovative query parser. NRAO develops all software under an open-source license; Data Vault is available to developers and users alike.","Agile development; Data archive; Free-text search; Google Web Toolkit (GWT); Model-view-controller; Python framework; Rapid application development; Web application; Web.py","Agile development; Data archive; Free-text search; Google Web Toolkit (GWT); Model-view-controller; Python framework; Rapid application development; Web application; Web.py; Astronomical satellites; Astrophysics; Computer programming; Computer software; Controllers; High level languages; Java programming language; Management information systems; Radar astronomy; Radio astronomy; User interfaces; World Wide Web",2-s2.0-66949111813
"Kalpathy-Cramer J., Hersh W.","Medical image retrieval and automatic annotation: OHSU at ImageCLEF 2007",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349788974&doi=10.1007%2f978-3-540-85760-0-79&partnerID=40&md5=5ad54d8576db46c5c6ba9251d9d9e099","Oregon Health and Science University participated in the medical retrieval and medical annotation tasks of ImageCLEF 2007. In the medical retrieval task, we created a web-based retrieval system built on a full-text index of both image and case annotations. The text-based search engine was implemented in Ruby using Ferret, a port of Lucene and a custom query parser. In addition to this textual index of annotations, supervised machine learning techniques using visual features were used to classify the images based on image acquisition modality. All images were annotated with the purported modality. Purely textual runs as well as mixed runs using the purported modality were submitted, with the latter performing among the best of all participating research groups. In the automatic annotation task, we used the 'gist' technique to create the feature vectors. Using statistics derived from a set of multi-scale oriented filters, we created a 512-dimensional vector. PCA was then used to create a 100-dimensional vector. This feature vector was fed into a two layer neural network. Our error rate on the 1000 test images was 67.8 using the hierarchical error calculations. © 2008 Springer-Verlag Berlin Heidelberg.",,"Automatic annotation; Dimensional vectors; Error calculations; Error rate; Feature vectors; Full-text index; ImageCLEF; Medical annotation; Medical image retrieval; Medical retrieval; Medical retrieval tasks; Multiscales; Oriented filter; Research groups; Retrieval systems; Supervised machine learning; Test images; Two layers; Visual feature; Content based retrieval; Corundum; Image acquisition; Information retrieval; Information services; Learning algorithms; Search engines; Vectors; Linguistics",2-s2.0-70349788974
"Kate R.J.","Transforming meaning representation grammars to improve semantic parsing",2008,"CoNLL 2008 - Proceedings of the Twelfth Conference on Computational Natural Language Learning",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865067279&partnerID=40&md5=811ac13f5143c0d51efc86d6a1e18ed5","A semantic parser learning system learns to map natural language sentences into their domain-specific formal meaning representations, but if the constructs of the meaning representation language do not correspond well with the natural language then the system may not learn a good semantic parser. This paper presents approaches for automatically transforming a meaning representation grammar (MRG) to conform it better with the natural language semantics. It introduces grammar transformation operators and meaning representation macros which are applied in an error-driven manner to transform an MRG while training a semantic parser learning system. Experimental results show that the automatically transformed MRGs lead to better learned semantic parsers which perform comparable to the semantic parsers learned using manually engineered MRGs. © 2008.",,"Domain specific; Grammar transformation; Natural language semantics; Natural languages; Representation languages; Semantic parsing; Learning systems; Natural language processing systems; Personnel training; Semantics",2-s2.0-84865067279
"Haddad C., Desai B.C.","Bilingual question answering using CINDI-QA at QA@CLEF 2007",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349828902&doi=10.1007%2f978-3-540-85760-0-37&partnerID=40&md5=374fe9f52148bb15e82b69fc08485eb6","This article presents the first participation of the CINDI group in the Multiple Language Question Answering Cross Language Evaluation Forum (QA@CLEF). We participated in a track using French as source language and English as target language. CINDI-QA first uses an online translation tool to convert the French input question into an English sentence. Second, a Natural Language Parser extracts keywords such as verbs, nouns, adjectives and capitalized entities from the query. Third, synonyms of those keywords are generated thanks to a Lexical Reference module. Fourth, our integrated Searching and Indexing component localises the candidate answers from the QA@CLEF data collection. Finally, the candidates are matched against our existing set of templates to decide on the best answer to return to the user. Out of eight runs submitted this year, CINDI-QA ranked second and third with an overall accuracy of 13%. © 2008 Springer-Verlag Berlin Heidelberg.","Bilingual; English; French; Question answering; Questions beyond factoids","Bilingual; English; French; Question answering; Questions beyond factoids; Computational linguistics; Information services; Natural language processing systems; Linguistics",2-s2.0-70349828902
"Guo Y., Shao Z.","The cognitive interactionist approach of sentence parsing with simple recurrent networks",2008,"Proceedings - 2008 2nd International Symposium on Intelligent Information Technology Application, IITA 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949119020&doi=10.1109%2fIITA.2008.198&partnerID=40&md5=64853fa6b93bfbb3f723f9a96f5b966c","Sentence parsing has a long research history in the fields of machine learning and natural language processing. The state-of-the-art techniques for tackling this task are mostly based on statistical language learning. How human parsing sentences is also an important research topic attracting research efforts for decades in the field of cognitive psychology. Some behavioristic experiments have convinced that the interactionist approach is rational and effective to simulate human parsing mechanism. This paper presents a sentence parser, the Interactionist Parser, which incorporated the cognitive interactionist approach with semantic information and simple recurrent networks, to extend and enrich the techniques for sentence parsing. Thinking of the parsing efficiency, the semantic information of two word types, noun and verb, are included during the parsing procedure in current stage. The experimental results demonstrate that the Interactionist Parser has comparability with the state-of-the-art parsing techniques based on statistical language learning. © 2008 IEEE.",,"Cognitive psychologies; Language learning; Machine-learning; Natural language processing; Parsing procedures; Research efforts; Research history; Research topics; Semantic informations; Sentence parsing; Simple recurrent networks; Computational linguistics; Information technology; Information theory; Learning systems; Natural language processing systems; Semantics; Industrial research",2-s2.0-62949119020
"Maglaras D., Doerr M., Ware A., Vassilakis K., Papadourakis G.","A formal mechanism for analysis and re-implementation of legacy programs",2008,"Proceedings of the IASTED International Conference on Software Engineering, SE 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62849096013&partnerID=40&md5=5598f6887e17c521a3aa25dd05264037","The need for information exchange is growing rapidly day by day and the formal development tools have been proven insufficient to serve this need. But what will happen with all those large software applications that have been developed in the past, using legacy development tools such as 3rd GLs, which are still working in business? There are many reasons why these applications need to be modified or reconstructed in order to keep on running effectively. This paper presents a formal mechanism for analysis and re-implementation of legacy (mainly data-processing) application systems. This mechanism forms the implementation of an automated procedure in the analysis of old software systems and is based on a software tool that will be able to extract useful information from their source code. The steps, which will be followed, are: Initially, a parser will extract the necessary information from the sources of the old application and store it in a text file in a form of transactions. Then a special tool will read the text file and execute the transactions to a specially configured database. Finally, another tool will be configured in order to retrieve the information from the database by executing queries to it.","Analysis; Engineering; Re-implementation; Software","Analysis; Application systems; Automated procedures; Development tools; Formal development; Information exchanges; Re-implementation; Software; Software applications; Software systems; Software tools; Source codes; Text files; Applications; Computer software; Data processing; Spontaneous emission; Legacy systems",2-s2.0-62849096013
"Baglioni M., Giovannetti E., Masserotti M.V., Renso C., Spinsanti L.","Ontology-supported querying of geographical databases",2008,"Transactions in GIS",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349086232&doi=10.1111%2fj.1467-9671.2008.01136.x&partnerID=40&md5=29f3bde8dc5c23147b9e1d09d0665153","Querying geographical information systems has been recognized as a difficult task for non-expert users. Furthermore, user queries are often characterized by semantic aspects not directly managed by traditional spatial databases or GIS. Examples of such semantic geospatial queries are the use of implicit spatial relations between objects, or the reference of domain concepts not explicitly represented in data. To handle such queries, we envisage a system that translates natural language queries into spatial SQL statements on a database, thus improving standard GIS with new semantic capabilities. Within this general objective, the contribution of this article is to introduce a methodology to handle semantic geospatial queries issued over a spatial database. This approach captures semantics from an ontology built upon the spatial database and enriched by domain concepts and properties specifically defined to represent the localization of objects. Some examples of the use of the methodology in the urban domain are presented. © 2009 Blackwell Publishing Ltd.",,"database; GIS; software; spatial data",2-s2.0-61349086232
"Song Y.-I., Han K.-S., Kim S.-B., Park S.-Y., Rim H.-C.","A novel retrieval approach reflecting variability of syntactic phrase representation",2008,"Journal of Intelligent Information Systems",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849090688&doi=10.1007%2fs10844-007-0045-0&partnerID=40&md5=3835ab8e121cfc32a9557c826662f08c","In this paper, we introduce variability of syntactic phrases and propose a new retrieval approach reflecting the variability of syntactic phrase representation. With variability measure of a phrase, we can estimate how likely a phrase in a given query would appear in relevant documents and control the impact of syntactic phrases in a retrieval model. Various experimental results over different types of queries and document collections show that our retrieval model based on variability of syntactic phrases is very effective in terms of retrieval performance, especially for long natural language queries. © 2007 Springer Science+Business Media, LLC.","Head-modifier pair; Information retrieval; Retrieval model; Structural language model; Syntactic phrase; Variability of phrase; Weighting phrases","Computational linguistics; Image retrieval; Information retrieval; Information services; Linguistics; Natural language processing systems; Head-modifier pair; Retrieval model; Structural language model; Syntactic phrase; Variability of phrase; Weighting phrases; Syntactics",2-s2.0-55849090688
"Oostdijk N.","Improving the lexical coverage of english compound adjectives improving the lexical coverage of english compound adjectives in syntactic parsing",2008,"Computational Linguistics in the Netherlands 2007 - Selected Papers from the 18th CLIN Meeting, CLIN 2007",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870484145&partnerID=40&md5=09b98fcaef043ff6d822712c0dfa0ac4","The present paper addresses the question how in syntactic parsing the coverage of words in previously unseen text may be improved. The adjectives in English are presented here as a case study. Working on the assumption that most new words that are introduced into the language are constructed on the basis of already existing words through the application of word-formation processes, we investigate the role that different word-formation processes play, more specifically in the formation of adjectives in English. An analysis of adjectives in the BNC shows that in the case of adjectives compounding is the word-formation process that is most productive. Moreover, compound adjectives are not formed by combining bases at will; rather, a limited set of fairly simple rules apply that restrict the co-occurrence of bases. This makes it feasible to develop an approach for handling compound adjectives which is rather effective, as is evident from the results from a first implementation where of a set of 30,561 compound adjectives derived from the BNC, 88.68% were correctly identified as such. Incorporation of the rules in the grammar underlying the Pelican parser accounts for a 7.65% increase in the parser's coverage of a subset of 10,123 sentences taken from the Leipzig corpus. Copyright ©2008 by the authors.",,"Co-occurrence; Leipzig; Lexical coverage; Simple rules; Syntactic parsing; Computational linguistics; Syntactics; Natural language processing systems",2-s2.0-84870484145
"Volanschi N., Rinderknecht C.","Unparsed patterns: Easy user-extensibility of program manipulation tools",2008,"Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950896214&doi=10.1145%2f1328408.1328425&partnerID=40&md5=3d40bf58c612a9205effa4ab47cb1873","Pattern matching in concrete syntax is very useful in program manipulation tools. In particular, user-defined extensions to such tools are written much easier using concrete syntax patterns. A few advanced frameworks for language development implement support for concrete syntax patterns, but mainstream frameworks used today still do not support them. This prevents most existing program manipulation tools from using concrete syntax matching, which in particular severely limits the writing of tool extensions to a few language experts. This paper argues that the major implementation obstacle to the pervasive use of concrete syntax patterns is the pattern parser. We propose an alternative approach based on ""unparsed patterns"", which are concrete syntax patterns that can be efficiently matched without being parsed. This lighter approach gives up static checks that parsed patterns usually do. In turn, it can be integrated within any existing parser-based software tool, almost for free. One possible consequence is enabling a widespread adoption of extensible program manipulation tools by the majority of programmers. Unparsed patterns can be used in any programing language, including multi-lingual environments. To demonstrate our approach, we implemented it both as a minimal patch for the gcc compiler, allowing to scan source code for user-defined patterns, and as a stand alone prototype called matchbox. Copyright © 2008 ACM.","Pattern matching; Source code; Unparsed patterns","Alternative approach; Concrete syntax; Language development; Manipulation tools; Scan sources; Software tool; Source code; Source codes; Stand -alone; Linguistics; Pattern matching; Semantics; Syntactics",2-s2.0-77950896214
"Ahlers D., Boll S.","Retrieving address-based locations from the web",2008,"International Conference on Information and Knowledge Management, Proceedings",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954342831&doi=10.1145%2f1460007.1460015&partnerID=40&md5=9024a90340e57029074de87b83b2c84a","Geospatial search for the Web determines the relation of documents' contents to a location within a region. For some pedestrian scenarios, information at a higher granularity down to individual buildings is necessary. In this paper, we describe a process for the extraction and simultaneous verification of precise addresses on German Web pages by a validating parser. We describe how an address-level location extraction can be aided by an extensive use of previous geographic knowledge and the use of its structure. The analysis of address structure, components and dependencies leads to the design of a geoparser that determines valid addresses within unstructured Web content. We further discuss some noteworthy issues that arise within the process.","Address extraction; Geographic web information retrieval; Local search; Location; Location-based web search; Spatial search","Address extraction; Local search; Location based; Spatial search; Web information retrieval; Web searches; Information retrieval; Knowledge management; Location; World Wide Web",2-s2.0-77954342831
"Muresan G.","An integrated approach to interaction design and log analysis",2008,"Handbook of Research on Web Log Analysis",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900606714&doi=10.4018%2f978-1-59904-974-8.ch012&partnerID=40&md5=9863a50fcf902f73a75dcad1ab60f5ae","In this chapter, we describe and discuss a methodological framework that integrates analysis of interaction logs with the conceptual design of the user interaction. It is based on (i) formalizing the functionality that is supported by an interactive system and the valid interactions that can take place; (ii) deriving schemas for capturing the interactions in activity logs; (iii) deriving log parsers that reveal the system states and the state transitions that took place during the interaction; and (iv) analyzing the user activities and the system's state transitions in order to describe the user interaction or to test some research hypotheses. This approach is particularly useful for studying user behavior when using highly interactive systems. We present the details of the methodology, and exemplify its use in a mediated retrieval experiment, in which the focus of the study is on studying the information-seeking process and on finding interaction patterns. © 2009, IGI Global.",,,2-s2.0-84900606714
"Guzman-Arenas A., Cuevas-Rasgado A.-D.","Knowledge acquisition with OM - A heuristic solution",2008,"ICEIS 2008 - Proceedings of the 10th International Conference on Enterprise Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349114725&partnerID=40&md5=7c8163e8c5d38debde6b8e7b55a438a9","Knowledge scattered through the Web inside unstructured documents (text documents) can not be easily interpreted by computers. To do so, knowledge contained from them must be extracted by a parser or a person and poured into a suitable data structure, the best form to do this, are with ontologies. For an appropriate merging of these ""individual"" ontologies, we consider repetitions, redundancies, synonyms, meronyms, different level of details, different viewpoints of the concepts involved, and contradictions, a large and useful ontology could be constructed. This paper presents OM algorithm, an automatic ontology merger that achieves the fusion of two ontologies without human intervention. Through repeated application of OM, we can get a growing ontology of a knowledge topic given. Using OM we hope to achieve automatic knowledge acquisition. There are two missing tasks: the conversion of a given text to its corresponding ontology (by a combination of syntactic and semantic analysis) is not yet automatically done; and the exploitation of the large resulting ontology is still under development.","Knowledge acquisition; Knowledge models; Knowledge representation; Ontology fusion; Ontology merging","Automatic knowledge acquisition; Automatic ontology; Heuristic solutions; Human intervention; Knowledge model; Knowledge models; Level of detail; Ontology merging; Repeated application; Semantic analysis; Text document; Unstructured documents; Artificial intelligence; Data structures; Decision support systems; Decision theory; Information systems; Knowledge acquisition; Knowledge representation; Merging; Semantics; Ontology",2-s2.0-55349114725
"Chen J., Guo W., Pan F., Chang F., Song R., Lin H.","DUTIR at TREC 2008 blog track",2008,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873450875&partnerID=40&md5=294d95a0890420adc640e148e1b41dc5","For opinion finding task our method of the combination of 5 Windows method and Pseudo Relevance Feedback behaves well, achieving an improvement of over 20% on the baseline adhoc results. For the polarity task we develop two different methods. One is a classification method, and the other uses queries to retrieve positive and negative documents respectively. In Blog Distillation task, Pseudo Relevance Feedback method helps improve the result a little, however, since its dependence on the top 10 retrieval result, the method still need to be improved in order to get better result.",,"Classification methods; Negative documents; Pseudo relevance feedback; Distillation; Image retrieval; Internet",2-s2.0-84873450875
"Chen J., Zaïane O.R., Goebel R.","An unsupervised approach to cluster web search results based on word sense communities",2008,"Proceedings - 2008 IEEE/WIC/ACM International Conference on Web Intelligence, WI 2008",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949145718&doi=10.1109%2fWIIAT.2008.24&partnerID=40&md5=8c9b02b6615efe09158df61eab19bb95","Effectively organizing web search results into clusters is important to facilitate quick user navigation to relevant documents. Previous methods may rely on a training process and do not provide a measure for whether page clustering is actually required. In this paper, we reformalize the clustering problem as a word sense discovery problem. Given a query and a list of result pages, our unsupervised method detects word sense communities in the extracted keyword network. The documents are assigned to several refined word sense communities to form clusters. We use the modularity score of the discovered keyword community structure to measure page clustering necessity. Experimental results verify our method's feasibility and effectiveness. © 2008 IEEE.",,"Information retrieval; Clustering problems; Community structures; Form clusters; Relevant documents; Training process; Unsupervised approaches; Unsupervised methods; User navigations; Web searches; Word sense; World Wide Web",2-s2.0-62949145718
"Lopes N., Fernandes C., Abreu S.","Representing and querying multiple ontologies with contextual logic programming",2008,"Computer Science and Information Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349659845&doi=10.2298%2fCSIS0802039L&partnerID=40&md5=fa7bcdd46b63adbeb212c939a06e58f4","The system presented in this paper uses Contextual Logic Programming as a computational hub for representing and reasoning over knowledge modeled by web ontologies, integrating the approach with similar mechanisms which we already developed. As a result of its Logic Programming heritage, the system may also recursively interrogate other ontologies or data repositories, providing a semantic integration of multiple sources. The components required to behave as a SPARQL query engine are explained and examples of integration of different sources are shown - in particular, the case of multiple OWL ontologies is discussed.","Logic Programming; Ontologies (OWL); SPARQL",,2-s2.0-70349659845
"Ma J., Zhang Y., He J.","Efficiently finding web services using a clustering semantic approach",2008,"ACM International Conference Proceeding Series",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953742470&doi=10.1145%2f1361482.1361487&partnerID=40&md5=dcaee41c69aa01527de9dc7a2d71af68","Efficiently finding Web services on the Web is a challenging issue in service-oriented computing. Currently, UDDI is a standard for publishing and discovery of Web services, and UDDI registries also provide keyword searches for Web services. However, the search functionality is very simple and fails to account for relationships between Web services. Firstly, users are overwhelmed by the huge number of irrelevant returned services. Secondly, the intentions of users and the semantics in Web services are ignored. Inspired by the success of partitioning approach used in the database design, we used a novel clustering semantic algorithm to eliminate irrelevant services with respect to a query. Then we utilized Probabilistic Latent Semantic Analysis (PLSA), a machine learning method, to capture the semantics hidden behind the words in a query, and the descriptions in the services, so that service matching can be carried out at the concept level. This paper reports upon the preliminary experimental evaluation that shows improvements over recall and precision. Copyright 2008 ACM.","machine learning; web service; web services matching","Concept levels; Database design; Experimental evaluation; Keyword search; machine learning; Machine learning methods; Novel clustering; Probabilistic latent semantic analysis; Recall and precision; Search functionality; Semantic approach; Service matching; Service oriented computing; UDDI registries; Clustering algorithms; Learning systems; Publishing; Semantics; Web services; Semantic Web",2-s2.0-77953742470
"Dědek J., Vojtáš P.","Computing aggregations from linguistic web resources: A case study in Czech Republic sector/traffic accidents",2008,"Proceedings - The 2nd International Conference on Advanced Engineering Computing and Applications in Sciences, ADVCOMP 2008",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650101335&doi=10.1109%2fADVCOMP.2008.17&partnerID=40&md5=44409ac4cab181e2804ad476af085e0c","Semantic computing aims to connect the intention of humans with computational content. We present a study of a problem of this type: extract information from large number of similar linguistic web resources to compute various aggregations (sum, average,...). In our motivating example we calculate the sum of injured people in traffic accidents in a certain period in a certain region. We restrict ourselves to pages written in Czech language. Our solution exploits existing linguistic tools created originally for a syntactically annotated corpus, Prague Dependency Treebank (PDT 2.0). We propose a solutions which learns tree queries to extract data from PDT2.0 annotations and transforms the data in an ontology. This method is not limited to Czech language and can be used with any structured linguistic representation. We present a proof of concept of our method. This enables to compute various aggregations over linguistic web resources. © 2008 IEEE.",,"Computational contents; Czech language; Czech Republic; Linguistic representations; Linguistic web; Prague Dependency Treebank; Proof of concept; Semantic Computing; Syntactically annotated corpus; Traffic accidents; Accidents; Computer aided engineering; Ontology; World Wide Web; Linguistics",2-s2.0-67650101335
"Haskins D., Haskins D.M.","The pulsed turbine rotor engine VTOL propulsion concept and applications: Capturing the elusive jet-powered flying car and redesigning a radical variant of the V-22 osprey",2008,"SAE Technical Papers",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877501374&doi=10.4271%2f2008-01-2269&partnerID=40&md5=b741cc865a4e34e75f9fe8e43b922db0","The objective of this abstract presentation is to introduce the potential of a new gas turbine engine design: The Pulsed Turbine Rotor Engine, a ""fuel injected"" turbine engine. One such potential is to develop a highly responsive low-medium jet propulsion engine that can finally make a jet-powered flying car practical and affordable, while paving the way for a new light-weight, jet-powered VTOL military assault vehicle. The pursuit of this viable concept to a successfully designed and functioning prototype of the Pulsed Turbine Rotor Engine will lead to opening a whole new chapter in aviation VTOL history. Copyright © 2008 SAE International.",,"Flying car; Jet propulsion; Light weight; Propulsion concept; Turbine rotor; Aviation; Exhibitions; Fuel purification; Machine design; Military vehicles; Turbine components; Jet engines",2-s2.0-84877501374
"Mlynkova I.","XML benchmarking",2008,"MCCSIS'08 - IADIS Multi Conference on Computer Science and Information Systems; Proceedings of Informatics 2008 and Data Mining 2008",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449085556&partnerID=40&md5=7234fcb1c965575704595f35f46e0476","Since XML technologies have become a standard for data representation, a huge amount of methods for processing XML data occurs every day. Consequently, it is necessary to compare the newly proposed methods with the existing ones, as well as to analyze the behaviour of a particular method on various types of data. In this paper we provide an overview of existing approaches to XML benchmarking and we show that the problem has been highly marginalized so far. Consequently, we further discuss persisting open issues and their possible solutions. © 2008 IADIS.","Synthetic XML data; XML benchmarking; XML data generating","Benchmarking; Computers; Information management; Information systems; Markup languages; XML; Data representations; Synthetic XML data; XML benchmarking; XML data generating; Data processing",2-s2.0-58449085556
"Verberne S., Lou B., Oostdijk N., Coppen P.-A.","Using syntactic information for improving why-question answering",2008,"Coling 2008 - 22nd International Conference on Computational Linguistics, Proceedings of the Conference",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053398494&partnerID=40&md5=4f20143ac32de185b7a865cbc313b520","In this paper, we extend an existing paragraph retrieval approach to why-question answering. The starting-point is a system that retrieves a relevant answer for 73% of the test questions. However, in 41% of these cases, the highest ranked relevant answer is not ranked in the top-10. We aim to improve the ranking by adding a re-ranking module. For re-ranking we consider 31 features pertaining to the syntactic structure of the question and the candidate answer. We find a significant improvement over the baseline for both success@10 and MRR@150. The most important features for re-ranking are the baseline score, the presence of cue words, the question's main verb, and the relation between question focus and document title. © Suzan Verberne, 2008.",,"Re-ranking; Syntactic information; Syntactic structure; Computational linguistics; Syntactics",2-s2.0-80053398494
"Clegg A.B., Shepherd A.J.","Syntactic pattern matching with GraphSpider and MPL",2008,"3rd International Symposium on Semantic Mining in Biomedicine, SMBM 2008 - Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-68649094880&partnerID=40&md5=f49658ac41c75eafffb38dc3e5fef785","We present MPL (Metapattern Language), a new formalism for defining patterns over dependency-parsed text, and GraphSpider, a matching engine for extracting dependency subgraphs which match against MPL patterns. Using a regexp-like syntax, MPL allows the definition of subgraphs matching user-specified patterns which can be constrained by word or word class, part-of-speech tag, dependency type and direction, and presence of named variables in particular locations. Although MPL and GraphSpider are general-purpose, we developed a set of patterns to capture biomolecular interactions which achieved very high precision results (92.6% at 31.2% recall) on the LLL Challenge corpus. MPL specifications and pattern sets, and the GraphSpider software, are available on SourceForge: http://graphspider.sf.net/.",,"Biomolecular interactions; High precision; Matching engines; Meta pattern; Part Of Speech; Pattern set; SourceForge; Subgraphs; Syntactic patterns; Pattern matching; Semantics; Syntactics",2-s2.0-68649094880
"Berzins V., Martell C., Luqi, Adams P.","Innovations in natural language document processing for requirements engineering",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449096201&doi=10.1007%2f978-3-540-89778-1_11&partnerID=40&md5=692d307859cc16b88ec614012c2c785f","This paper evaluates the potential contributions of natural language processing to requirements engineering. We present a selective history of the relationship between requirements engineering (RE) and natural-language processing (NLP), and briefly summarize relevant recent trends in NLP. The paper outlines basic issues in RE and how they relate to interactions between a NLP front end and system-development processes. We suggest some improvements to NLP that may be possible in the context of RE and conclude with an assessment of what should be done to improve likelihood of practical impact in this direction. © 2008 Springer Berlin Heidelberg.","Ambiguity; Domain-specific methods; Gaps; Natural language; Requirements","Ambiguity; Domain-specific methods; Gaps; Natural language; Requirements; Computational linguistics; Innovation; Natural language processing systems; Professional aspects; Requirements engineering; Semantics; Linguistics",2-s2.0-58449096201
"Filippova K., Strube M.","Sentence fusion via dependency graph compression",2008,"EMNLP 2008 - 2008 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference: A Meeting of SIGDAT, a Special Interest Group of the ACL",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956313019&partnerID=40&md5=091c784fcf79e445786e64997660d180","We present a novel unsupervised sentence fusion method which we apply to a corpus of biographies in German. Given a group of related sentences, we align their dependency trees and build a dependency graph. Using integer linear programming we compress this graph to a new tree, which we then linearize. We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments. In an evaluation with human judges our method outperforms the fusion approach of Barzilay & McKeown (2005) with respect to readability. © 2008 Association for Computational Linguistics.",,"Dependency graphs; Dependency trees; Fusion methods; Integer Linear Programming; Semantic compatibility; Wikipedia; Computational linguistics; Integer programming; Plant extracts; Semantics; Trees (mathematics); Natural language processing systems",2-s2.0-77956313019
"Hunkeler U., Scotton P.","A quality-of-information-aware framework for data models in wireless sensor networks",2008,"2008 5th IEEE International Conference on Mobile Ad-Hoc and Sensor Systems, MASS 2008",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650697119&doi=10.1109%2fMAHSS.2008.4660118&partnerID=40&md5=aa35991f52f74c7ba1566af9e40e63c5","Wireless sensor networks are used to monitor a given environment, such as indoor heating conditions or the micro-climate of glaciers. They offer a low-cost solution that provides a high data density. Usually the user of such a sensor network has a good idea of how, knowing the environment, the sensed values should behave. This idea can be expressed as a data model. Such models can be used to detect anomalies, compress data, or combine data from many inexpensive sensors to increase the quality of the measurements. This paper presents a framework to process arbitrary sensor-network data models. The framework can then be used to distribute the model processing into the wireless sensor network. Quality of information criteria are used to determine the performance of the models. A prototype of the framework is presented together with a comparison of two existing stochastic data model approaches for wireless sensor networks. © 2008 IEEE.",,"Data density; Data models; Indoor heating; Information-aware; Low-cost solution; Micro-climate; Model processing; Network data; Quality of information; Stochastic data; Ad hoc networks; Models; Routing protocols; Sensor networks; Stochastic models; Wireless telecommunication systems; Wireless sensor networks",2-s2.0-67650697119
"Ciszak L.","Application of clustering and association methods in data cleaning",2008,"Proceedings of the International Multiconference on Computer Science and Information Technology, IMCSIT 2008",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349324255&doi=10.1109%2fIMCSIT.2008.4747224&partnerID=40&md5=722fdd553da1c6633ff428e39537c3c0","Data cleaning is a process of maintaining data quality in information systems. Current data cleaning solutions require reference data to identify incorrect or duplicate entries. This article proposes usage of data mining in the area of data cleaning as effective in discovering reference data and validation rules from the data itself. Two algorithms designed by the author for data attribute correction have been presented. Both algorithms utilize data mining methods. Experimental results show that both algorithms can effectively clean text attributes without external reference data. © 2008 IEEE.",,"Association methods; Current data; Data attributes; Data cleaning; Data mining methods; Data quality; Reference data; Text attributes; Information technology; Mining; Computer science",2-s2.0-70349324255
"Powers D.M.W.","Minors as miners modelling and evaluating ontological and linguistic learning",2008,"Conferences in Research and Practice in Information Technology Series",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870505672&partnerID=40&md5=d152d2009982608a3f3a53ec759205ba","Growing up is in large measure learning about the world and our social and linguistic environment. We might call this data mining, although it is far more multimodal and immersive than most applications. This paper describes computational research into how children learn, with a particular focus on evaluation in both supervised and unsupervised paradigms. Conversely, we gain additional insight into association mining by considering psycholinguistic experiments that quantify the way human association by both adults and children relate to a variety of association measures. Learning and evaluation are not dealt with in isolation, but a program of formal and application-based evaluation is expounded and exemplified to show how to evaluate discovered patterns with and without a gold standard. In this context, some serious issues with current evaluation techniques and accuracy measures are identified and the unbiased techniques identified. © 2008, Australian Computer Society; Inc.","Audiovisual Speech Recognition; Bookmaker Informedness and Markedness; Brain computer interface; Cognitive linguistics; Computational psycholinguistics; Data mining; Deltap; Natural language learning; Receiver operating characteristics; Signal processing; Speech processing; Text mining","Audio visual speech recognition; Bookmaker Informedness and Markedness; Computational psycholinguistics; Deltap; Natural language learning; Receiver operating characteristics; Text mining; Brain computer interface; Information technology; Linguistics; Natural language processing systems; Research; Signal processing; Speech processing; Data mining",2-s2.0-84870505672
"Zhou D., He Y., Kwoh C.K.","From biomedical literature to knowledge: Mining protein-protein interactions",2008,"Studies in Computational Intelligence",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549105464&doi=10.1007%2f978-3-540-70778-3_17&partnerID=40&md5=71b79f5f452ad2154416a65ac0f6669b","To date, more than 16 million citations of published articles in biomedical domain are available in the MEDLINE database. These articles describe the new discoveries which accompany a tremendous development in biomedicine during the last decade. It is crucial for biomedical researchers to retrieve and mine some specific knowledge from the huge quantity of published articles with high efficiency. Researchers have been engaged in the development of text mining tools to find knowledge such as protein-protein interactions, which are most relevant and useful for specific analysis tasks. This chapter provides a road map to the various information extraction methods in biomedical domain, such as protein name recognition and discovery of protein-protein interactions. Disciplines involved in analyzing and processing unstructured-text are summarized. Current work in biomedical information extracting is categorized. Challenges in the field are also presented and possible solutions are discussed. © 2008 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-59549105464
"Kessler J.S.","Polling the blogosphere: A rule-based approach to belief classification",2008,"ICWSM 2008 - Proceedings of the 2nd International Conference on Weblogs and Social Media",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890717150&partnerID=40&md5=02e5a349c03c036569ffe794d4ec56f1","The research described here is part of a larger project with the objective of determining if a writer believes a proposition to be true or false. This task requires a deep understanding of a proposition's semantic context, which is far beyond NLP's state of the art. In light of this difficulty, this paper presents a shallow semantic framework that addresses the sub-problem of finding a proposition's truth-value at the sentence level. The framework consists of several classes of linguistic elements that, when linked to a proposition through specific lexico-syntactic connectors, change its truth-value. A pilot evaluation of a system implementing this framework yields promising results. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Blogospheres; Rule-based approach; Semantic context; Semantic framework; Sentence level; State of the art; Truth values; Semantics; Social networking (online); Classification (of information)",2-s2.0-84890717150
"Shin H.","Achieving low latency of multimedia content browsing in UPnP AV architectures",2008,"Proceedings - The 2nd International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies, UBICOMM 2008",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650088669&doi=10.1109%2fUBICOMM.2008.41&partnerID=40&md5=1716abc2b19100630acaebed425d62ac","UPnP AV solutions have suffered from latency performance problems in user interfaces to browse multimedia contents in remote hosts. This study shows how to improve the round-trip latency by way of caching and aggressive link-based prefetching of the response of the Browse request. Caching reduces the latency dramatically, and aggressive link-based prefetching lead to 100% cache hit rate. Although it is commonly believed that aggressive link-based prefetching is a naïve approach and causes numerous redundancies, we presented our prefetching strategy with the proper number of cache entries and proposed cache replacement algorithm is very effective and efficient. Through the evaluation of our research results, we present a dramatic performance improvement, approximately 300,000% faster than ever, without many redundancies. © 2008 IEEE.",,"Cache hit rates; Cache replacement algorithm; Latency performance; Low latency; Multimedia contents; Performance improvements; Prefetching; Remote host; Research results; UPnP AV; Quality assurance; Redundancy; User interfaces; Ubiquitous computing",2-s2.0-67650088669
"Priyantha N.B., Kansal A., Goraczko M., Zhao F.","Tiny web services: Design and implementation of interoperable and evolvable sensor networks",2008,"SenSys'08 - Proceedings of the 6th ACM Conference on Embedded Networked Sensor Systems",185,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866484192&doi=10.1145%2f1460412.1460438&partnerID=40&md5=28d8bcfede1b5b5f24e7bf8851c3ff95","We present a web service based approach to enable an evolutionary sensornet system where additional sensor nodes may be added after the initial deployment. The functionality and data provided by the new nodes is exposed in a structured manner, so that multiple applications may access them. The result is a highly inter-operable system where multiple applications can share a common evolving sensor substrate. A key challenge in using web services on resource constrained sensor nodes is the energy and bandwidth overhead of the structured data formats used in web services. Our work provides a detailed evaluation of the overheads and presents an implementation on a representative sensor platform with 48k of ROM, 10k of RAM and a 802.15.4 radio. We identify design choices that optimize the web service operation on resource constrained sensor nodes, including support for low latency messaging and sleep modes, quantifying trade-offs between the design generality and resource efficiency. We also prototyped an example application, for home energy management, demonstrating how evolutionary sensor networks can be supported with our approach. © 2008 ACM.","battery life; tcp/ip; web services","Bandwidth overheads; Battery life; Evolvable; Home energy managements; Low latency; Multiple applications; Resource efficiencies; Resource-constrained; Sensor platform; Sensor substrates; Sensornet; Service-based; SLEEP mode; Structured data; tcp/ip; Web service operations; Design; Embedded systems; Energy management; Sensor networks; Sensor nodes; Web services; Websites",2-s2.0-84866484192
"Fey G., Drechsler R.","Robustness and usability in modern design flows",2008,"Robustness and Usability in Modern Design Flows",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891425411&doi=10.1007%2f978-1-4020-6536-1&partnerID=40&md5=cf41b3df05563dbba91ab0c07e0fa89d","The size of technically producible integrated circuits increases continuously. But the ability to design and verify these circuits does not keep up with this development. Therefore today's design flow has to be improved to achieve a higher productivity. In Robustness and Usability in Modern Design Flows the current design methodology and verification methodology are analyzed, a number of deficiencies are identified and solutions suggested. Improvements in the methodology as well as in the underlying algorithms are proposed. An in-depth presentation of preliminary concepts makes the book self-contained. Based on this foundation major design problems are targeted. In particular, a complete tool flow for Synthesis for Testability of SystemC descriptions is presented. The resulting circuits are completely testable and test pattern generation in polynomial time is possible. Verification issues are covered in even more detail. A whole new paradigm for formal design verification is suggested. This is based upon design understanding, the automatic generation of properties and powerful tool support for debugging failures. All these new techniques are empirically evaluated and experimental results are provided. As a result, an enhanced design flow is created that provides more automation (i.e. better usability) and reduces the probability of introducing conceptual errors (i.e. higher robustness). © 2008 Springer Science + Business Media B.V., All Rights Reserved.",,,2-s2.0-84891425411
"Carter P., Lee D., Orengo C.","Chapter 1 Target Selection in Structural Genomics Projects to Increase Knowledge of Protein Structure and Function Space",2008,"Advances in Protein Chemistry and Structural Biology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957152047&doi=10.1016%2fS0065-3233%2807%2975001-5&partnerID=40&md5=d0c28624ca1b4e088239055460908009","Structural genomics aims to solve the three-dimensional structures of proteins at a rapid rate and in a cost-effective manner, with the hope of significantly impacting on the life sciences, biotechnology, and drug discovery in the long-term. Structural genomics initiatives started in Japan in 1997 with the advent of the Protein Folds Project. Since then many new initiatives have begun worldwide, with diverse aims motivating the selection of proteins for structure determination. In this chapter, we consider the biological goals of high-throughput structural biology, while focusing on the Protein Structure Initiative in the United States. This is the most productive of the structural genomics initiatives, having solved 3,363 new structures between September 2000 and October 2008. © 2008 Elsevier Inc. All rights reserved.",,"protein; chemistry; genomics; metabolism; protein conformation; review; Genomics; Protein Conformation; Proteins",2-s2.0-77957152047
"Schäfer U., Uszkoreit H., Federmann C., Marek T., Zhang Y.","Extracting and querying relations in scientific papers",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56549131093&doi=10.1007%2f978-3-540-85845-4_16&partnerID=40&md5=d34b190325ed6d0ee3c9d929c746ce7c","High-precision linguistic and semantic analysis of scientific texts is an emerging research area. We describe methods and an application for extracting interesting factual relations from scientific texts in computational linguistics and language technology. We use a hybrid NLP architecture with shallow preprocessing for increased robustness and domain-specific, ontology-based named entity recognition, followed by a deep HPSG parser running the English Resource Grammar (ERG). The extracted relations in the MRS (minimal recursion semantics) format are simplified and generalized using WordNet. The resulting 'quriples' are stored in a database from where they can be retrieved by relation-based search. The query interface is embedded in a web browser-based application we call the Scientist's Workbench. It supports researchers in editing and online-searching scientific papers. © 2008 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Computational linguistics; Information theory; Linguistics; Ontology; Reconnaissance aircraft; Semantics; Do-mains; Hpsg parsers; Language technologies; Minimal recursion semantics; Named Entity recognitions; Pre-processing; Query interfaces; Research areas; Scientific papers; Scientific texts; Semantic analyses; WordNet; Natural language processing systems",2-s2.0-56549131093
"Kirchmair J., Markt P., Distinto S., Schuster D., Spitzer G.M., Liedl K.R., Langer T., Wolber G.","The Protein Data Bank (PDB), its related services and software tools as key components for in silico guided drug discovery",2008,"Journal of Medicinal Chemistry",62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149136433&doi=10.1021%2fjm8005977&partnerID=40&md5=2fd7d45878ba7aab27253fcd9900db7a",[No abstract available],,"computer model; computer program; data mining; drug research; Internet; medical technology; Protein Data Bank; protein structure; quality control; review; Computer-Aided Design; Databases, Protein; Drug Discovery; Information Storage and Retrieval; Models, Molecular; Online Systems; Software",2-s2.0-57149136433
"Kallmeyer L., Parmentier Y.","On the relation between Multicomponent Tree Adjoining Grammars with Tree Tuples (TT-MCTAG) and Range Concatenation Grammars (RCG)",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449112181&doi=10.1007%2f978-3-540-88282-4_25&partnerID=40&md5=74c161b1b51a7a436fd8a5a5065f6cf8","This paper investigates the relation between TT-MCTAG, a formalism used in computational linguistics, and RCG. RCGs are known to describe exactly the class PTIME; simple RCG even have been shown to be equivalent to linear context-free rewriting systems, i.e., to be mildly context-sensitive. TT-MCTAG has been proposed to model free word order languages. In general, it is NP-complete. In this paper, we will put an additional limitation on the derivations licensed in TT-MCTAG. We show that TT-MCTAG with this additional limitation can be transformed into equivalent simple RCGs. This result is interesting for theoretical reasons (since it shows that TT-MCTAG in this limited form is mildly context-sensitive) and, furthermore, even for practical reasons: We use the proposed transformation from TT-MCTAG to RCG in an actual parser that we have implemented. © 2008 Springer-Verlag Berlin Heidelberg.",,"Computational linguistics; Equivalence classes; Linguistics; Natural language processing systems; Nuclear propulsion; Query languages; Translation (languages); Model free; Range concatenation grammars; Rewriting systems; Tree adjoining grammars; Automata theory",2-s2.0-56449112181
"Mazanek S., Maier S., Minas M.","Auto-completion for diagram editors based on graph grammars",2008,"Proceedings - 2008 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC 2008",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349153666&doi=10.1109%2fVLHCC.2008.4639094&partnerID=40&md5=c192f7d3d80fd0bf60a48cda4b6f9591","Graphs are known to be well-suited as an intermediate data structure in diagram editors. The syntax of a particular visual language can be defined by means of a graph grammar. In recent work we have proposed approaches to graph completion: given a possibly ""incomplete"" graph, this graph is modified in such a way that the resulting graph is a member of the grammar's language. In this paper we describe how graph completion can be used to realize diagram completion, an important requirement for the realization of content assist in diagram editors. With our approach, the advantages of free-hand and structured editing can be effectively combined: drawing of diagrams with maximal freedom and powerful guidance whenever needed. © 2008 IEEE.",,"Computational geometry; Context sensitive grammars; Data structures; File organization; Formal languages; Linguistics; Query languages; Diagram editors; Graph grammars; Intermediate datums; Structured editing; Visual languages; Graph theory",2-s2.0-56349153666
"Meredith P.O., Jin D., Chen F., Roşu G.","Efficient monitoring of parametric context-free patterns",2008,"ASE 2008 - 23rd IEEE/ACM International Conference on Automated Software Engineering, Proceedings",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56249134236&doi=10.1109%2fASE.2008.25&partnerID=40&md5=346455e67aff445051276b814314ae28","Recent developments in runtime verification and monitoring show that parametric regular and temporal logic specifications can be efficiently monitored against large programs. However, these logics reduce to ordinary finite automata, limiting their expressivity. For example, neither can specify structured properties that refer to the call stack of the program. While context-free grammars (CFGs) are expressive and well-understood, existing techniques for monitoring CFGs generate large runtime overhead in real-life applications. This paper shows, for the first time, that monitoring parametric CFGs is practical (with overhead on the order of 10% or lower for average cases, several times faster than the state-of-the-art). We present a monitor synthesis algorithm for CFGs based on an LR(1) parsing algorithm, modified with stack cloning to account for good prefix matching. In addition, a logic-independent mechanism is introduced to support matching against the suffixes of execution traces. © 2008 IEEE.",,"Applications.; Average cases; Efficient monitoring; Execution traces; Large programs; Parsing algorithms; Prefix matching; Recent developments; Runtime overheads; Runtime verifications; Synthesis algorithms; Temporal logic specifications; Cloning; Finite automata; Genetic engineering; Software engineering; Temporal logic",2-s2.0-56249134236
"Moon H.-J., Yoo J.-W.","Free-traversing syntactic and semantic comparison on semi-structured languages",2008,"Proceedings - 2008 International Conference on Convergence and Hybrid Information Technology, ICHIT 2008",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849108846&doi=10.1109%2fICHIT.2008.241&partnerID=40&md5=c372ca477bf66ca2c8e1752de45e63c6","Tree traversing for syntactic and semantic comparison causes expensive time and space consumption. Internet, heterogeneous computing environments, and ubiquitous computing technologies all cause an explosive increase of Web data, and most Web data is written in semi-structured language format. With the growth of Web data usage and the importance of the management, comparison techniques such as similarity detection are more and more needed for efficient information and database management. This paper introduces a free-traversing technique without tree traversing on parse trees generated by the corresponding language parser to analyze its syntactic and semantic meaning. This free-traversing technique uses DIES (Direct Invariant Encoding Scheme) encoding method and has similar results with DFS (Depth First Search) of parse tree traversing. We use XML schema DTDs to evaluate our free-traversing technique. We adopt some of ontological technologies, and apply LCS (Longest Common String) and LNS (Longest Nesting common String) structure extraction methods. With this free-traversing technique, semi-structured Web data management can be much easier and faster than existing tree traversing methods. © 2008 IEEE.",,"Administrative data processing; Cellular radio systems; Computer systems; Encoding (symbols); Information technology; Information theory; Linguistics; Management information systems; Markup languages; Query languages; Semantics; Syntactics; Technology; Ubiquitous computing; XML; Comparison techniques; Database managements; Depth first searches; Encoding methods; Encoding schemes; Heterogeneous computing; Parse trees; Similarity detections; Space consumptions; Structure extractions; Structured languages; Ubiquitous Computing technologies; Web data managements; Web datums; Xml schemata; Trees (mathematics)",2-s2.0-55849108846
"Imamura M., Takayama Y., Akiyoshi M., Komoda N.","Term knowledge acquisition using the structure of headline sentencesfrom information equipments operating manuals",2008,"ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849146354&partnerID=40&md5=3a30129a9e81c290bdd4572580bb6638","This paper proposes a method for automatically extracting term knowledge such as case relations and IS-A relations between words in the headline sentences of operating manuals for information equipments. The proposed method acquires term knowledge by the following iterative processing: the case relation extraction using correspondence relations between surface cases and deep cases; the case and IS-A relation extraction using compound word structures; the IS-A relation extraction using correspondence between the case structures in the hierarchical headline sentences. The distinctive feature of our method is to extract new case relations and IS-A relations by comparison and matching the case relations extracting from the super and sub headline sentences using the headline hierarchy. We have confirmed that the proposed method has achieved 92.4% recall and 96.8% precision for extracting case relations, and 93.9% recall and 89.9% precision for extracting IS-A relations from an operating manual of a car navigation system.","Case frame; Document structure; Linguistic knowledge acquisition; Operating manual","Information retrieval systems; Knowledge acquisition; Mergers and acquisitions; Case frame; Case relations; Case structures; Distinctive features; Document structure; Iterative processing; Linguistic knowledge acquisition; Operating manual; Relation extractions; Word structures; Feature extraction",2-s2.0-55849146354
"Guan D., Yuan W., Lee Y.-K., Gavrilov A., Lee S.","Improving supervised learning performance by using fuzzy clustering method to select training data",2008,"Journal of Intelligent and Fuzzy Systems",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349136231&partnerID=40&md5=7e95ede30a7531d96f8b359bd9632a8a","The crucial issue in many classification applications is how to achieve the best possible classifier with a limited number of labeled data for training. Training data selection is one method which addresses this issue by selecting the most informative data for training. In this work, we propose three data selection mechanisms based on fuzzy clustering method: center-based selection, border-based selection and hybrid selection. Center-based selection selects the samples with high degree of membership in each cluster as training data. Border-based selection selects the samples around the border between clusters. Hybrid selection is the combination of center-based selection and border-based selection. Compared with existing work, our methods do not require much computational effort. Moreover, they are independent with respect to the supervised learning algorithms and initial labeled data. We use fuzzy c-means to implement our data selection mechanisms. The effects of them are empirically studied on a set of UCI data sets. Experimental results indicate that, compared with random selection, hybrid selection can effectively enhance the learning performance in all the data sets, center-based selection shows better performance in certain data sets, border-based selection does not show significant improvement.","Border-based selection; Center-based selection; Classification; Data selection; Fuzzy clustering; Hybrid selection","Data reduction; Flow of solids; Fuzzy sets; Fuzzy systems; Learning algorithms; Learning systems; Supervised learning; Border-based selection; Center-based selection; Classification; Data selection; Hybrid selection; Fuzzy clustering",2-s2.0-55349136231
"Costagliola G., Deufemia V., Risi M.","Using error recovery techniques to improve sketch recognition accuracy",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249101996&doi=10.1007%2f978-3-540-88188-9_16&partnerID=40&md5=697b54865fba606820ec5d19f38ec9ca","Sketching is an activity that produces informal documents containing hand-drawn shapes highly variable and ambiguous. In this paper we present a diagrammatic sketch recognizer that is able to cope with the recognition of in-accurate hand-drawn symbols by exploiting error recovery techniques as developed for programming language compilers. The error recovery algorithms are able to interact with recognizers automatically generated from grammar specifications in order to obtain the information on missing or misrecognized strokes. © 2008 Springer-Verlag Berlin Heidelberg.","Error recovery; LR parsing; Sketch recognition","Color image processing; Computer programming languages; Query languages; Technical presentations; Automatically generated; Diagrammatic sketches; Error recoveries; Error recovery; Error recovery algorithms; LR parsing; Programming languages; Sketch recognition; Sketch recognitions; Drawing (graphics)",2-s2.0-55249101996
"Guan D., Yuan W., Lee Y.-K., Lee S.","Training data selection based on fuzzy C-means",2008,"IEEE International Conference on Fuzzy Systems",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249099342&doi=10.1109%2fFUZZY.2008.4630456&partnerID=40&md5=ee9931023bffc1fb0b5e9bcff83b373b","The performance of supervised learning could be improved when valuable data are selected for training. In this paper, we proposed three data selection methods based on fuzzy c-means algorithm. They are: center-based selection, border-based selection and bin-based selection. In center-based selection, the data with high degree of membership in each cluster are selected for training. In border-based selection, the data around the borders between clusters are selected. In bin-based selection, the data in each cluster are sorted based on their membership degrees. Then for each cluster, the sorted data are divided into bins. Finally, there is one data selected from each bin for training. The effects of them are empirically studied on a set of UCI data sets. Experimental results indicate that bin-based selection could effectively improve the performance of learning compared to randomly selecting training samples. © 2008 IEEE.",,"Data reduction; Fuzzy clustering; Fuzzy logic; Fuzzy systems; Data selections; Data sets; Degree of memberships; Fuzzy c-means; Fuzzy c-means algorithms; Membership degrees; Training datums; Training samples; Bins",2-s2.0-55249099342
"Huang S.S., Zook D., Smaragdakis Y.","Domain-specific languages and program generation with meta-AspectJ",2008,"ACM Transactions on Software Engineering and Methodology",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56149111605&doi=10.1145%2f1416563.1416566&partnerID=40&md5=b68e60e34d7ec186734f6d281319cb26","Meta-AspectJ (MAJ) is a language for generating AspectJ programs using code templates. MAJ itself is an extension of Java, so users can interleave arbitrary Java code with AspectJ code templates. MAJ is a structured metaprogramming tool: a well-typed generator implies a syntactically correct generated program. MAJ promotes a methodology that combines aspect-oriented and generative programming. A valuable application is in implementing small domain-specific language extensions as generators using unobtrusive annotations for syntax extension and AspectJ as a back-end. The advantages of this approach are twofold. First, the generator integrates into an existing software application much as a regular API or library, instead of as a language extension. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language. In addition to its practical value, MAJ offers valuable insights to metaprogramming tool designers. It is a mature metaprogramming tool for AspectJ (and, by extension, Java): a lot of emphasis has been placed on context-sensitive parsing and error reporting. As a result, MAJ minimizes the number of metaprogramming (quote/unquote) operators and uses type inference to reduce the need to remember type names for syntactic entities. © 2008 ACM.","Domain-specific languages; Language extensions; Metaprogramming; Program synthesis; Program transformation; Program verification","Domain-specific languages; Language extensions; Metaprogramming; Program synthesis; Program transformation; Program verification; Application programming interfaces (API); Codes (symbols); Computer programming; Computer programming languages; Computer software; Graphical user interfaces; Linguistics; Query languages; Syntactics; XML; Java programming language",2-s2.0-56149111605
"Zhen L., Jiang Z., Huang G.Q., Liang J.","Knowledge acquisition for product development in knowledge grid",2008,"Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57549104854&doi=10.1243%2f09544062JMES672&partnerID=40&md5=ddf6feb9258b241a4567cc8b1287bcda","This paper is mainly concerned with the implementation of innovative product development-oriented knowledge acquisition in a knowledge grid. The architecture of a knowledge acquisition platform based on the theory of inventive problem solving (TRIZ) and ontology is proposed at first. Then the key techniques analysed are: ontology building for product development, semantic register, semantic parser, query dispatch, and knowledge acquisition based on contradiction matrix. Finally, working scenarios are employed to illustrate the knowledge acquisition process. © IMechE 2008.","Grid; Knowledge acquisition; Knowledge grid; Product development","Information theory; Mergers and acquisitions; Ontology; Product development; Project management; Semantics; Contradiction matrixes; Grid; Innovative product developments; Inventive problems; Key techniques; Knowledge grid; Knowledge grids; Ontology buildings; Semantic registers; Knowledge acquisition",2-s2.0-57549104854
"Calegari S., Sanchez E.","Object-fuzzy concept network: An enrichment of ontologies in semantic information retrieval",2008,"Journal of the American Society for Information Science and Technology",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55449121032&doi=10.1002%2fasi.20945&partnerID=40&md5=1278591c309b53d9ce8155e082587784","This article shows how a fuzzy ontology-based approach can improve semantic documents retrieval. After formally defining a fuzzy ontology and a fuzzy knowledge base, a special type of new fuzzy relationship called (semantic) correlation, which links the concepts or entities in a fuzzy ontology, is discussed.These correlations, first assigned by experts, are updated after querying or when a document has been inserted into a database. Moreover, in order to define a dynamic knowledge of a domain adapting itself to the context, it is shown how to handle a tradeoff between the correct definition of an object, taken in the ontology structure, and the actual meaning assigned by individuals. The notion of a fuzzy concept network is extended, incorporating database objects so that entities and documents can similarly be represented in the network. Information retrieval (IR) algorithm, using an object-fuzzy concept network (O-FCN), is introduced and described. This algorithm allows us to derive a unique path among the entities involved in the query to obtain maxima semantic associations in the knowledge domain. Finally, the study has been validated by querying a database using fuzzy recall, fuzzy precision, and coefficient variant measures in the crisp and fuzzy cases.",,,2-s2.0-55449121032
"Cherubini A., Crespi Reghizzi S., Pradella M.","Regional languages and tiling: A unifying approach to picture grammars",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249124094&doi=10.1007%2f978-3-540-85238-4_20&partnerID=40&md5=786da1a49411dc197d07499d52fd6dd5","Several classical models of picture grammars based on array rewriting rules can be unified and extended by a tiling based approach. The right part of a rewriting rule is formalized by a finite set of permitted tiles. We focus on a simple type of tiling, named regional, and define the corresponding regional tile grammars. They include both Siromoney's (or Matz's) Kolam grammars, and their generalization by Průša. Regionally defined pictures can be recognized with polynomial time complexity by an algorithm extending the CKY one for strings. Regional tile grammars and languages are strictly included into the tile grammars and languages, and are incomparable with Giammarresi-Restivo tiling systems (or Wang's tilings). © 2008 Springer-Verlag Berlin Heidelberg.","2D language; CKY algorithm; Picture grammar; Picture language; Syntactic pattern recognition; Tiling","Boolean functions; Building materials; Computer science; Computers; Feature extraction; Foundations; Pattern recognition; Polynomial approximation; Query languages; Set theory; 2D language; CKY algorithm; Picture grammar; Picture language; Syntactic pattern recognition; Tiling; Linguistics",2-s2.0-54249124094
"Romeikat R., Roser S., Müllender P., Bauer B.","Translation of QVT relations into QVT operational mappings",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249169559&doi=10.1007%2f978-3-540-69927-9_10&partnerID=40&md5=8299fb25b7fbd8bc6ffc4a7f70e6a5b9","Model transformations play a key role in Model-Driven Engineering solutions. To efficiently develop, specify, and manage model transformations, it is often necessary to use a combination of languages that stand for different transformation approaches. To provide a basis for such hybrid model transformation specification solutions, we developed and implemented a translation of the declarative QVT Relations into the imperative QVT Operational Mappings language. © Springer-Verlag Berlin Heidelberg 2008.",,"Query languages; Translation (languages); Declarative; Engineering solutions; Hybrid models; Model transformations; Qvt relations; Linguistics",2-s2.0-54249169559
"Hughes G., Bultan T.","Interface grammars for modular software model checking",2008,"IEEE Transactions on Software Engineering",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249131782&doi=10.1109%2fTSE.2008.72&partnerID=40&md5=be10f50b0e70a82ae68fc0b71c14f614","Verification techniques relying on state enumeration (e.g., model checking) face two important challenges: the state-space explosion, an exponential increase in the state space as the number of components increases; and environment generation, modeling components that are either not available for analysis, or that cannot be handled by the verification tool in use. We propose a semi-automated approach for attacking these problems. In our approach, interfaces for the components not under analysis are specified using a specification language based on grammars. Specifically, an interface grammar for a component specifies the sequences of method invocations that are allowed by that component. We have built an compiler that takes the interface grammar for a component as input and generates a stub for that component. The stub thus generated can be used to replace that component during state space exploration, either to moderate state space explosion, or to provide an executable environment for the component under verification. We conducted a case study by writing an interface grammar for the Enterprise JavaBeans (EJB) persistence interface, and using the resulting stub to check EJB clients using the JPF model checker. Our results show that EJB clients can be verified efficiently with JPF using our approach, whereas they cannot be verified with JPF directly since JPF cannot handle EJB components. © 2008 IEEE.","Interface grammars; Model checking; Modular verification; Specification languages","Computational grammars; Linguistics; Query languages; Space research; Specification languages; Specifications; State space methods; And environments; Automated approaches; Case studies; Enterprise javabeans; Exponential increases; Interface grammars; Method invocations; Model checkers; Modeling components; Modular softwares; Modular verification; Number of components; Space explosions; State enumerations; State spaces; Verification techniques; Verification tools; Model checking",2-s2.0-54249131782
"Roy C.K., Cordy J.R.","NICAD: Accurate detection of near-miss intentional clones using flexible pretty-printing and code normalization",2008,"IEEE International Conference on Program Comprehension",157,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71149111492&doi=10.1109%2fICPC.2008.41&partnerID=40&md5=7ceb9db10a499d3f1e12f7c0041d3efa","This paper examines the effectiveness of a new language-specific parser-based but lightweight clone detection approach. Exploiting a novel application of a source transformation system, the method accurately finds near-miss clones using an efficient text line comparison technique. The transformation system assists the method in three ways. First, using agile parsing it provides user-specified flexible pretty- printing to remove noise, standardize formatting and break program statements into parts such that potential changes can be detected as simple linewise text differences. Second, it provides efficient flexible extraction of potential clones to be compared using island grammars and agile parsing to select granularities and enumerate potential clones. Third, using transformation rules it provides flexible code normalization to allow for local editing differences between similar code segments and filtering out of uninteresting parts of potential clones. In this paper we introduce the theory and practice of the framework and demonstrate its use in finding function clones in C code. Early experiments indicate that the method is capuble of finding near-miss clones with high precision and recall, and with reasonable performance. © 2008 IEEE.",,"Clone detection; Code normalization; Code segments; High-precision; International conferences; Program comprehension; Program statements; Source transformation; Transformation rules; Transformation systems; Agile manufacturing systems; Cloning; Printing; Query languages; Codes (symbols)",2-s2.0-71149111492
"Hindle A., Godfrey M.W., Holt R.C.","Reading beside the lines: Indentation as a proxy for complexity metrics",2008,"IEEE International Conference on Program Comprehension",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71149116829&doi=10.1109%2fICPC.2008.13&partnerID=40&md5=9ac4241fa47d8a240e9dea07e4769a38","Maintainers face the daunting task of wading through a collection of both new and old revisions, trying to ferret out revisions which warrant personal inspection. One can rank revisions by size/lines of code (LOC), but often, due to the distribution of the size of changes, revisions will be of similar size. If we can't rank revisions by LOC perhaps we can rank by Halstead's and McCabe's complexity metrics? However, these metrics are problematic when applied to code fragments (revisions) written in multiple languages: special parsers are required which may not support the language or dialect used; analysis tools may not understand code fragments. We propose using the statistical moments of indentation as a lightweight, language independent, revision/difffriendly metric which uctuully proxies classical complexity metrics. We have extensively evaluated our approach against the entire CVS histories of the 278 of the most popular and most active SourceForge projects. We found that our results are linearly correlated and rank-correlated with traditional measures of complexity, suggesting that measuring indentation is a cheap and accurate proxy for code complexity of revisions. Thus ranking revisions by the standard deviation and summation of indentation will be very similar to ranking revisions by complexity. © 2008 IEEE.",,"Analysis tools; Code complexity; Code fragments; Complexity metrics; International conferences; Measures of complexity; Multiple languages; Program comprehension; SourceForge; Standard deviation; Statistical moments; Codes (symbols); Linguistics; Query languages; Java programming language",2-s2.0-71149116829
"Angkawattanawit N., Haruechaiyasak C., Marukatat S.","Thai Q-Cor: Integrating word approximation and soundex for thai query correction",2008,"5th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2008",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949113786&doi=10.1109%2fECTICON.2008.4600387&partnerID=40&md5=36b446ac1edc85d6b2af8852dd0ff542","Nowadays, Internet is widely used almost all over the world including Thailand. People can find web site or information that they need by using search engines like Sansarn or Google. When users type words or phrases into the search box, sometimes they are not satisfied with the returned results. One of the most important problems is misspelled query due to typographical and cognitive errors. To address these errors, we propose Thai query correction system called Thai Q-Cor that is able to verify an inaccurate query and correct it. Our system is composed of two correction modules: Word Approximation and Soundex. Word Approximation module is used for resolving typographical errors by using beam search technique. A user's query will be calculated for error scores by comparing with the words in the dictionary. The words that have the minimal error score will be returned. Soundex module is used for fixing cognitive error by using phoneme search. The user's query must be first converted to phoneme format. The words in the dictionary which have the same phoneme format as the user's query will be returned. Our preliminary result demonstrates the potential of Thai Q-Cor system for repairing the inaccurate user's queries. ©2008 IEEE.",,"All over the world; Beam search; Cognitive errors; Correction system; International conferences; Minimal errors; Thailand; Typographical errors; Web site; Computer software; Errors; Information services; Information technology; Internet; Polynomial approximation; Search engines; Telecommunication systems; World Wide Web; Technology",2-s2.0-52949113786
"Jin B.-H., Cao D.-L., Ren X., Yu S., Dai B.-J.","High performance XML parser OnceXMLParser",2008,"Ruan Jian Xue Bao/Journal of Software",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249124790&doi=10.3724%2fSP.J.1001.2008.02728&partnerID=40&md5=1dd54d6f02d76b179b52d305a9a57c13","An XML (extensible markup language) parser is the fundamental software for analyzing and processing XML documents. The implementation of a high performance full-validating XML parser is studied in this paper. This research develops a OnceXMLParser which supports three kinds of parsing models. It passes the rigorous XML conformance testing and API (application programming interface) conformance testing. OnceXMLParser adopts light-weighted architecture and is optimized on many aspects including efficient lexical analysis, statistical automaton implementation, reasonable resource allocation strategies and some fine tunings on language level. Performance testing results show that OnceXMLParser has outstanding parsing efficiency.","Extensible markup language; Parser; Performance tuning","Application programming interfaces (API); High performance liquid chromatography; Hypertext systems; Linguistics; Planning; Query languages; Resource allocation; Tuning; XML; Application programmings; Conformance testing; Extensible markup language; Extensible Markup languages; Language levels; Lexical analyses; Parser; Performance tuning; Resource allocation strategies; Testing results; XML documents; XML parsers; Markup languages",2-s2.0-55249124790
"Wang Y.-H., Lin C.-H.","Applying semantic agents to message communication in e-learning environment",2008,"International Journal of Distance Education Technologies",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949108821&partnerID=40&md5=ab82d97c438f0bdfea7b66952e4ec3cc","A traditional distance learning system requires supervisors or teachers always available on online to facilitate and monitor a learner's progress by answering questions and guiding users. We presents an English chat room system in which students discuss course contents and ask questions to and receive from teachers and other students. The mechanism contains an agent that detects syntax errors in sentences written by the online the user and also checks the semantics of a sentence. The agent can thus offer recommendations to the user and, then, analyze the data of the learner corpus. When users query the system, this system will attempt to find the answers from the knowledge ontology that is stored in the records of previous user comments. With the availability of automatic supervisors, messages can be monitored and syntax or semantic mistakes can be corrected to resolve learner-related problems. Copyright © 2008, IGI Global.","Distance learning; Link grammar; Semantic check; Syntax parser","Agents; Distance education; E-learning; Information theory; Internet; Learning systems; Multimedia systems; Ontology; Semantics; Supervisory personnel; Syntactics; Teaching; Chat rooms; Course contents; Distance learning; Distance learning systems; E - learnings; Knowledge ontologies; Link grammar; Message communications; Semantic agents; Semantic check; Syntax errors; Syntax parser; Education",2-s2.0-55949108821
"Wu Y.-C., Yang J.-C.","A robust passage retrieval algorithm for video question answering",2008,"IEEE Transactions on Circuits and Systems for Video Technology",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55149086325&doi=10.1109%2fTCSVT.2008.2002831&partnerID=40&md5=cc45106bdeb5afd7f397745bb607c88e","In this paper, we present a robust passage retrieval algorithm to extend the conventional text question answering (Q/A) to videos. Users interact with our videoQ/A system through natural language queries, while the top-ranked passage fragments with associated video clips are returned as answers. We compare our method with five of the high-performance ranking algorithms that are portable to different languages and domains. The experiments were evaluated with 75.3 h of Chinese videos and 253 questions. The experimental results showed that our method outperformed the second best retrieval model (language models) in relatively 1.43% in mean reciprocal rank (MRR) score and 11.36% when employing a Chinese word segmentation tool. By adopting the initial retrieval results from the retrieval models, our method yields an improvement of at least 5.94% improvement in MRR score. This makes it very attractive for the Asia-like languages since the use of a well-developed word tokenizer is unnecessary. © 2008 IEEE.","Multimedia retrieval; Question answering (Q/A); Video question answering (videoQ/A)","Computational linguistics; Linguistics; Query languages; Chinese word segmentations; Do-mains; Language models; Mean reciprocal ranks; Multimedia retrieval; Natural language queries; Passage retrievals; Performance rankings; Question Answering; Question answering (Q/A); Retrieval models; Tokenizer; Video clips; Video question answering (videoQ/A); Natural language processing systems",2-s2.0-55149086325
"Koch C., Scherzinger S., Schmidt M.","XML prefiltering as a string matching problem",2008,"Proceedings - International Conference on Data Engineering",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52649107512&doi=10.1109%2fICDE.2008.4497471&partnerID=40&md5=621c74f89fa0bf78d5a6fe2696813080","We propose a new technique for the efficient search and navigation in XML documents and streams. This technique takes string matching algorithms designed for efficient keyword search in flat strings into the second dimension, to navigate in tree structured data. We consider the important XML data management task of prefiltering XML documents (also called XML projection) as an application for our approach. Different from existing prefiltering schemes, we usually process only fractions of the input and get by with very economical consumption of both main memory and processing time. Our experiments reveal that, already on low-complexity problems such as XPath filtering, inmemory query engines can experience speed-ups by two orders of magnitude. © 2008 IEEE.",,"Chlorine compounds; Data storage equipment; Information management; Management information systems; Markup languages; Technology; XML; Data engineering; Efficient search; International conferences; Keyword searches; Low-complexity; Main memories; Orders-of-magnitude; Pre-filtering; Pre-filtering schemes; Processing Time; Query engines; Speed-ups; String matching; String matching algorithms; Tree-structured data; XML data; XML documents; XML prefiltering; Trees (mathematics)",2-s2.0-52649107512
"Harrington B., Clark S.","ASKNet: Creating and evaluating large scale integrated semantic networks",2008,"Proceedings - IEEE International Conference on Semantic Computing 2008, ICSC 2008",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149104232&doi=10.1109%2fICSC.2008.24&partnerID=40&md5=2cdf2e9e274ec0943a41b832a9ff685d","Extracting semantic information from multiple natural language sources and combining that information into a single unified resource is an important and fundamental goal for natural language processing. Large scale resources of this kind can be useful for a wide variety of tasks including question answering, word sense disambiguation and knowledge discovery. A single resource representing the information in multiple documents can provide significantly more semantic information than is available from the documents considered independently. In this paper we describe the ASKNet system, which extracts semantic information from a large number of English texts, and combines that information into a large scale semantic network using spreading activation based techniques. Evaluation of largescale semantic networks is a difficult problem. In order to evaluate ASKNet we have developed a novel evaluation metric and applied it to networks created from randomly chosen DUC articles. The results are highly promising: almost 80% precision for the semantic core of the networks. © 2008 IEEE.",,"Artificial intelligence; Computational linguistics; Linguistics; Natural language processing systems; Query languages; Semantics; Evaluation metric; International conferences; Knowledge discovery; Multiple documents; NAtural language processing; Natural languages; Question Answering; Semantic information; Semantic networks; Word-sense disambiguation; Information theory",2-s2.0-52149104232
"Assawamekin N., Sunetnanta T., Pluempitiwiriyawej C.","Resolving multiperspective requirements traceability through ontology integration",2008,"Proceedings - IEEE International Conference on Semantic Computing 2008, ICSC 2008",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-52149088733&doi=10.1109%2fICSC.2008.13&partnerID=40&md5=b676d1fa08fe734e00572a3e212ff584","In a software development process, different stakeholders may deal with different pieces of software requirements depending on their perspectives or perception of their problems. Each of the stakeholders may define his/her requirements in his/her own point of view using different terminologies. However, a variety of stakeholders will need to intemperate, collaborate or trace requirements among each other in order to achieve a common goal of their development. In this situation, ontology can play an essential role in communication among diverse stakeholders in the course of an integrating system. In this paper, we propose an alternative multiperspective requirements traceability (MPRT) framework to automatically generate traceability relationships of multiperspective requirements artifacts. Requirements ontology is designed and built as a knowledge management mechanism to represent multiperspective requirements artifacts in a common way, which intervene mutual ""understanding"" among various stakeholders. Ontology matching takes two ontologies and produces correspondences (i.e., equivalence, more general, less general, mismatch and overlapping) between the concepts of ontologies that correspond semantically to each other. The traceability relationships are automatically generated when a match is found in the ontologies. © 2008 IEEE.","Interoperability; Multiperspective software development; Ontology; Requirements traceability","Administrative data processing; Industrial management; Information management; Information theory; Integration; Knowledge management; Management information systems; Management science; Semantics; Automatically generated; Integrating systems; International conferences; Interoperability; Multiperspective software development; Ontology integration; Ontology matching; Requirements traceability; Software development processes; Software requirements; Trace requirements; Ontology",2-s2.0-52149088733
"Pavlič L., Heričko M., Podgorelec V.","Improving design pattern adoption with ontology-based design pattern repository",2008,"Proceedings of the International Conference on Information Technology Interfaces, ITI",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51949088001&doi=10.1109%2fITI.2008.4588487&partnerID=40&md5=da26e0999bb4538551d1ec43c673c7ce","Design patterns are a proven way to build high-quality software. The number of design patterns is rising rapidly, while management and searching facilities seems not to catch up. This is why selecting a suitable design pattern is not always an easy task. This issue is especially clear for less experienced developers. In this paper we present our approach to cope with the presented issue - an experiment prototype of a new design pattern repository, based on semantic web technologies. Since new Ontology-Based Design Pattern Repository is a work in progress we point out its potentials for improving design pattern adoption.","Design pattern repository; Design patterns; Ontologies; Semantic web","Information technology; Information theory; Ontology; Real time systems; Semantic Web; Technology; Design pattern repository; Design patterns; High-quality software; International conferences; New design; Ontologies; Semantic web technologies; Work-in-progress; Software design",2-s2.0-51949088001
"Mazanek S., Minas M.","Graph parser combinators",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849084265&doi=10.1007%2f978-3-540-85373-2_1&partnerID=40&md5=869ff170afdfd824b665a8acdced2f85","A graph language can be described by a graph grammar in a manner similar to a string grammar known from the theory of formal languages. Unfortunately, graph parsing is known to be computationally expensive in general. There are quite simple graph languages that crush most general-purpose graph parsers. In this paper we present graph parser combinators, a new approach to graph parsing inspired by the well-known string parser combinators. The basic idea is to define primitive graph parsers for elementary graph components and a set of combinators for the construction of more advanced graph parsers. Using graph parser combinators special-purpose graph parsers can be composed conveniently. Thereby, language-specific performance optimizations can be incorporated in a flexible manner. © Springer-Verlag Berlin Heidelberg 2008.","Functional programming; Graph parsing; Parser combinators; Visual languages","Formal languages; Linguistics; Query languages; Basic idea; Combinators; Computationally expensive; Functional languages; Functional programming; Graph grammars; Graph parsing; International symposium; New approaches; Parser combinators; Performance optimizations; Simple graph; Visual languages; Graph theory",2-s2.0-51849084265
"Brkić M., Matetić M.","VoiceXML for Slavic languages application development",2008,"2008 Conference on Human System Interaction, HSI 2008",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849120661&doi=10.1109%2fHSI.2008.4581424&partnerID=40&md5=546d3c86e569278587bc424382bf279b","Since Croatian language, as well as other Slavic languages, is essentially very different from English, we are in the need of developing language specific tools and systems. In this paper we point out the benefits that speech applications have and describe our future platform. Furthermore, we create domain-specific dialogues using VoiceXML. VoiceXML is a language for creating voice user interfaces. We opted for VoiceXML because it simplifies application development by permitting developers to use familiar Web techniques, tools and infrastructure. ©2008 IEEE.","Slavic languages; VoiceXML; Weather forecast system; Web-based system","Flow interactions; Internet telephony; Linguistics; Query languages; User interfaces; Application development; Croatian language; Domain-specific; Human-system interaction; Slavic languages; Speech applications; Voice user interfaces; VoiceXML; Weather forecast system; Web-based system; Cellular telephone systems",2-s2.0-51849120661
"Liu C., Wang H., McClean S., Liu J., Wu S.","Pruning dependency trees in a syntactic information retrieval model",2008,"ICALIP 2008 - 2008 International Conference on Audio, Language and Image Processing, Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849122158&doi=10.1109%2fICALIP.2008.4590035&partnerID=40&md5=f5c56b87eb68fa3d008f0499a6663d06","Natural language processing (NLP) techniques are believe to have the potential to aid information retrieval (IR) in terms of retrieval accuracy. In previous work, we report a proof of concept study on a new approach to NLP-based IR, proposed as a syntactic IR model (SIR). In SIR, Documents and queries are represented on the basis of syntactic parse trees, which are generated by a natural language parser. Based on this tree structured representation of documents and queries, the matching between a document and a query is executed on their tree representations, with tree comparison as the key operation. In this paper, we extend the dataset of the IR experiment for testing SIR into full documents. Additionally, the raw parse trees output by the parser are pruned before being fed into the indexing process of SIR. This operation is necessary and has the similar role of the stop words list strategy in the term based IR index construction. Experimental results show that retrieval accuracy is improved if the pruning operation is applied. © 2008 IEEE.",,"Artificial intelligence; Computational linguistics; Digital image storage; Image processing; Imaging systems; Imaging techniques; Indexing (of information); Information analysis; Information retrieval; Information science; Information services; Linguistics; Nonlinear programming; Optical data processing; Search engines; Statistical tests; Syntactics; Data sets; Index construction; Indexing process; International conferences; Natural Language Processing; Natural languages; New approaches; Parse trees; Proof of concepts; Retrieval accuracy; Retrieval models; Syntactic information; Tree comparison; Tree-structured representation; Natural language processing systems",2-s2.0-51849122158
"Dědek J., Vojtáš P.","Linguistic extraction for semantic annotation",2008,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849091067&doi=10.1007%2f978-3-540-85257-5_9&partnerID=40&md5=a13b6af4a347429e569da4e578acc70f","Bottleneck for semantic web services is lack of semantically annotated information. We deal with linguistic information extraction from Czech texts from the Web for semantic annotation. The method described in the paper exploits existing linguistic tools created originally for a syntactically annotated corpus, Prague Dependency Treebank (PDT 2.0). We propose a system which captures text of web-pages, annotates it linguistically by PDT tools, extracts data and stores the data in an ontology. We focus on the third phase - data extraction - and present methods for learning queries over linguistically annotated data. Our experiments in the domain of reports of traffic accidents enable e.g. summarization of the number of injured people. This serves as a proof of concept of our solution. More experiments, for different queries and different domain are planned in the future. This will improve third party semantic annotation of web resources. © 2008 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-51849091067
"Feldman R., Regev Y., Gorodetsky M.","A modular information extraction system",2008,"Intelligent Data Analysis",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849135749&partnerID=40&md5=ba416119d9bfd63b72427a7cd13e261f","In today's information age, the amount of text documents available electronically (on the Web, on corporate intranets, on news wires and elsewhere) is overwhelming. Search engines and information retrieval, while useful to find documents that satisfy a certain query, offer little help with analyzing the unstructured documents themselves. Text Mining is the automated process of analyzing unstructured, natural language text in order to discover information and knowledge that are difficult to retrieve. Information Extraction (IE) centers on finding entities and relations in free text and provides a solid foundation for text mining. In this paper we present a modular IE system, based on the DIAL language. DIAL allows users to implement IE solutions for various domains rapidly, based on a common Natural Language Processing (NLP) infrastructure. We demonstrate in detail an implementation of a system for extracting relations in the intelligence news domain. We present an evaluation of our system and discuss enhancements for other domains, such as emails. © 2008 IOS Press. All rights reserved.","Information extraction; Natural language processing; Text mining applications",,2-s2.0-51849135749
"Ahn G.-J., Xu W., Zhang X.","Systematic policy analysis for high-assurance services in SELinux",2008,"Proceedings - 2008 IEEE Workshop on Policies for Distributed Systems and Networks, POLICY 2008",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849128388&doi=10.1109%2fPOLICY.2008.18&partnerID=40&md5=6318285cafcdc897fd276ce9a11af5cf","Identifying and protecting the trusted computing base (TCB) of a system is an important task to provide high-assurance services since a set of trusted subjects should be legitimately articulated for target applications. In this paper, we present a formal policy analysis framework to identify TCB with the consideration of specific security goals. We also attempt to model information flows between domains in SELinux policies and detect security violations among information flows using Colored Petri Nets. © 2008 IEEE.",,"Applications.; Colored Petri Nets; Distributed systems; High-assurance; Information flows; Model information; Policy analysis; Security goals; Security violations; SELinux; Trusted computing base; Marine biology; Petri nets; Graph theory",2-s2.0-51849128388
"Zhou D.","Exploiting structure recurrence in XML processing",2008,"Proceedings - 8th International Conference on Web Engineering, ICWE 2008",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51749087696&doi=10.1109%2fICWE.2008.46&partnerID=40&md5=18e7601b8021b56ec592a6062aa2796a","Transmitting, parsing, and transforming XML documents (and messages) are particularly costly in cellular environments because of the limitations in handset and access network capabilities. A big part of XML processing cost is caused by the processing of the structure of the documents. By structure we refer to the entity resulted from an XML document after removing the text nodes and attribute values of the document. While XML is a flexible, extensible language, real-world data exchanged in XML often exhibit some degree of stability in its organization. In other words, a computer receiving an XML data item of certain structure is likely to encounter the same structure among future data items. Since most structure-related processing is identical for data items with identical structure, it is thus evident that the overall performance of XML processing will improve if redundancy in structure related processing can be reduced. In this paper we present the concept of Structure Encoding and the approaches to quickly identifying recurring structures, including one relying on collision-resistant hash function. The paper then describes in detail techniques to improving the performance of XML transmission, tokenization, parsing, and transformation by using Structure Encoding. Evaluation experiments with our prototype implementation and industry benchmark suite demonstrate huge performance improvement potential in the presence of structure recurrence: up to 7 times faster for DOM-style parsing, up to 38 times faster for transformation, and up to 97.4% in size reduction when 20% of the text and attribute values change. In the worst case when there is no structure recurrence, structure encoding causes an overhead of about 11.1% for DOM-style parsing and about 8.9% for transformation. © 2008 IEEE.",,"Data items; XML documents; XML processing; Biological materials; Computer networks; Encoding (symbols); Functions; Information management; Markup languages; Query languages; XML; Data processing",2-s2.0-51749087696
"Adji T.B., Baharudin B., Zamin N.","Applying link grammar formalism in the development of English-Indonesian machine translation system",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049109458&doi=10.1007%2f978-3-540-85110-3_3&partnerID=40&md5=41b2f3a3772d8a9ca8061d65ff37124e","In this paper, we present a Machine Translation (MT) system from English to Indonesian by applying Link Grammar (LG) formalism. The Annotated Disjunct (ADJ) technique available in the LG formalism is utilized to map English sentences into equivalent Indonesian sentences. The ADJ is a promising technique to deal with target languages that do not have grammar formalism, parser, and corpus available like Indonesian language. An experimental evaluation shows that the applicability of LG for Indonesian language worked as expected. We have also discussed some significant issues to be considered in future development. © 2008 Springer-Verlag Berlin Heidelberg.","Annotated disjunct; Link grammar; Natural language processing; Parsing algorithm","Administrative data processing; Artificial intelligence; Classification (of information); Computer aided language translation; Computer networks; Information management; Information theory; Knowledge management; Management information systems; Query languages; Speech transmission; Translation (languages); Annotated disjunct; International conferences; Link grammar; Natural language processing; Parsing algorithm; Mathematical techniques",2-s2.0-51049109458
"Lyon D.A.","Multi-threaded data mining of EDGAR CIKs (central index keys) from ticker symbols",2008,"IPDPS Miami 2008 - Proceedings of the 22nd IEEE International Parallel and Distributed Processing Symposium, Program and CD-ROM",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049113199&doi=10.1109%2fIPDPS.2008.4536453&partnerID=40&md5=51078ae265542723d36c84c9e93bbf89","This paper describes how use the Java Swing HTMLEditorKit to perform multi-threaded web data mining on the EDGAR system (Electronic Data-Gathering, Analysis, and Retrieval system). EDGAR is the SEC's (U.S. Securities and Exchange Commission) means of automating the collection, validation, indexing, acceptance, and forwarding of submissions. Some entities are regulated by the SEC (e.g. publicly traded firms) and are required, by law, to file with the SEC. Our focus is on making use of EDGAR to get information about company filings. These offers are filed with companies, using their Central Index Key (CIK). The CIK is used on the SEC's computer system to identify entities that filed a disclosure with the SEC. We show how to map a stock ticker symbol into a CIK. The methodology for converting the web data source into internal data structures is based on using HTML as the input into a context-sensitive parser-call-back facility. Screen scraping is a popular means of data mining, but the unstructured nature of HTML pages makes this a challenge. The stop-and-wait nature of HTTP queries, as well as the non-deterministic nature of the response time, adversely impacts performance. We show that a combination of caching and multi-threading can improve performance by several orders of magnitude. ©2008 IEEE.",,"Administrative data processing; Codes (symbols); Computer networks; Computer systems; Data mining; Data storage equipment; Decision support systems; Distributed parameter networks; File organization; HTML; HTTP; Knowledge management; Markup languages; Mining; Multi-threaded; Parallel and distributed processing; Data structures",2-s2.0-51049113199
"Telea A., Voinea L.","SOLIDFX: An integrated reverse engineering environment for C++",2008,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249140205&doi=10.1109%2fCSMR.2008.4493339&partnerID=40&md5=3205ac7b3b37a32db90dd8be62bfb100","Many C++ extractors exist that produce syntax trees, call graphs, and metrics from C++ code, yet few offer integrated querying, navigation, and visualization of sourcecode-level facts to the end-user. We present an interactive reverse engineering environment which supports reverseengineering tasks on C/C++ code, e.g. set up the extraction process, apply user-written queries on the extracted facts, and visualize query results, much like classical forward-engineering IDEs do. We illustrate our environment with several examples of reverse-engineering analyses. © 2008 IEEE.",,"Codes (standards); Codes (symbols); Computer programming languages; Computer software maintenance; Maintenance; Process engineering; Reengineering; Reverse engineering; Technology; Trees (mathematics); Call graphs; End users; Engineering environment; European; Extraction processing; Integrated querying; Query results; Syntax trees; Computer software",2-s2.0-50249140205
"Oury N., Swierstra W.","The power of Pi",2008,"ACM SIGPLAN Notices",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650085024&partnerID=40&md5=abd940991f51c5f2c79b9965f4472899","This paper exhibits the power of programming with dependent types by dint of embedding three domain-specific languages: Cryptol, a language for cryptographic protocols; a small data description language; and relational algebra. Each example demonstrates particular design patterns inherent to dependently-typed programming. Documenting these techniques paves the way for further research in domain-specific embedded type systems. Copyright © 2008 ACM.","Design; Languages; Theory","Cryptographic protocols; Dependent types; Design Pattern; Domain specific; Domain specific languages; Languages; Relational algebra; Small data; Theory; Type systems; Cryptography; Embedded systems; Query languages; Linguistics",2-s2.0-67650085024
"Nishiyama T., Minamide Y.","A translation from the HTML DTD into a regular hedge grammar",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249188346&doi=10.1007%2f978-3-540-70844-5_13&partnerID=40&md5=e9b903ac5ddd032dccedfe2dd8d50009","The PHP string analyzer developed by the second author approximates the string output of a program with a context-free grammar. By developing a procedure to decide inclusion between context-free and regular hedge languages, Minamide and Tozawa applied this analysis to checking the validity of dynamically generated XHTML documents. In this paper, we consider the problem of checking the validity of dynamically generated HTML documents instead of XHTML documents. HTML is not specified by an XML schema language, but by an SGML DTD, and we can omit several kinds of tags in HTML documents. We formalize a subclass of SGML DTDs and develop a translation into regular hedge grammars. Thus we can validate dynamically generated HTML documents. We have implemented this translation and incorporated it in the PHP string analyzer. The experimental results show that the validation through this translation works well in practice. © 2008 Springer-Verlag Berlin Heidelberg.",,"Context-free; Context-free grammar; Html documents; International conferences; XML-Schema; Fences; Gaussian noise (electronic); HTML; Information management; Linguistics; Markup languages; Query languages; SGML; Translation (languages); Context free languages",2-s2.0-50249188346
"Moors A., Piessens F., Odersky M.","Generics of a higher kind",2008,"ACM SIGPLAN Notices",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650067076&partnerID=40&md5=9e207ae68c3e17ebcddfc9353a872bcb","With Java 5 and C3 2.0, first-order parametric polymorphism was introduced in mainstream object-oriented programming languages under the name of generics. Although the first-order variant of generics is very useful, it also imposes some restrictions: it is possible to abstract over a type, but the resulting type constructor cannot be abstracted over. This can lead to code duplication. We removed this restriction in Scala, by allowing type constructors as type parameters and abstract type members. This paper presents the design and implementation of the resulting type constructor polymorphism. Furthermore, we study how this feature interacts with existing object-oriented constructs, and show how it makes the language more expressive. Copyright © 2008 ACM.","Higher-kinded types; Higher-order genericity; Scala; Type constructor polymorphism","Abstract types; Code duplication; First-order; First-order variant; Higher-kinded types; Higher-order genericity; Object oriented; Object-oriented programming languages; Parametric polymorphism; Scala; Type constructor polymorphism; Abstracting; Linguistics; Polymorphism; Query languages; Object oriented programming",2-s2.0-67650067076
"Standley D.M., Toh H., Nakamura H.","Functional annotation by sequence-weighted structure alignments: Statistical analysis and case studies from the protein 3000 structural genomics project in Japan",2008,"Proteins: Structure, Function and Genetics",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51349135440&doi=10.1002%2fprot.22015&partnerID=40&md5=2100b740eab00f68e4bb126398d6f38b","A method to functionally annotate structural genomics targets, based on a novel structural alignment scoring function, is proposed. In the proposed score, position-specific scoring matrices are used to weight structurally aligned residue pairs to highlight evolutionarily conserved motifs. The functional form of the score is first optimized for discriminating domains belonging to the same Pfam family from domains belonging to different families but the same CATH or SCOP superfamily. In the optimization stage, we consider four standard weighting functions as well as our own, the ""maximum substitution probability"" and combinations of these functions. The optimized score achieves an area of 0.87 under the receiver-operating characteristic curve with respect to identifying Pfam families within a sequence-unique benchmark set of domain pairs. Confidence measures are then derived from the benchmark distribution of true-positive scores. The alignment method is next applied to the task of functionally annotating 230 query proteins released to the public as part of the Protein 3000 structural genomics project in Japan. Of these queries, 78 were found to align to templates with the same Pfam family as the query or had sequence identities ≥30%. Another 49 queries were found to match more distantly related templates. Within this group, the template predicted by our method to be the closest functional revive was often not the most structurally similar. Several nontrivial cases are discussed in detail. Finally, 103 queries matched templates at the fold level, but not the family or superfamily level, and remain functionally uncharacterized. © 2008 Wiley-Liss, Inc.","Maximum substitution probability; Position-specific scoring matrix; ROC curve; Structural genomics; Structure alignment","article; gene sequence; genetic conservation; Japan; multigene family; priority journal; protein domain; protein family; protein folding; sequence homology; structural genomics; technique; Amino Acid Motifs; Animals; Computational Biology; Conserved Sequence; Genomics; Humans; Japan; Protein Structure, Tertiary; Proteins; Sequence Alignment",2-s2.0-51349135440
"Huang M., Ding S., Wang H., Zhu X.","Mining physical protein-protein interactions from the literature",2008,"Genome Biology",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049110333&doi=10.1186%2fgb-2008-9-s2-s12&partnerID=40&md5=88e455e10f7ae7eec6427213fc5c3755","Background: Deciphering physical protein-protein interactions is fundamental to elucidating both the functions of proteins and biological processes. The development of high-throughput experimental technologies such as the yeast two-hybrid screening has produced an explosion in data relating to interactions. Since manual curation is intensive in terms of time and cost, there is an urgent need for text-mining tools to facilitate the extraction of such information. The BioCreative (Critical Assessment of Information Extraction systems in Biology) challenge evaluation provided common standards and shared evaluation criteria to enable comparisons among different approaches. Results: During the benchmark evaluation of BioCreative 2006, all of our results ranked in the top three places. In the task of filtering articles irrelevant to physical protein interactions, our method contributes a precision of 75.07%, a recall of 81.07%, and an AUC (area under the receiver operating characteristic curve) of 0.847. In the task of identifying protein mentions and normalizing mentions to molecule identifiers, our method is competitive among runs submitted, with a precision of 34.83%, a recall of 24.10%, and an F1 score of28.5%. In extracting protein interaction pairs, our profile-based method was competitive on the SwissProt-only subset (precision = 36.95%, recall = 32.68%, and F1 score = 30.40%) and on the entire dataset (30.96%, 29.35%, and26.20%, respectively). From the biologist's point of view, however, these findings are far from satisfactory. The error analysis presented in this report provides insight into how performance could be improved: three-quarters of false negatives were due to protein normalization problems (532/698), and about one-quarter were due to problems with correctly extracting interactions for this system. Conclusion: We present a text-mining framework to extract physical protein-protein interactions from the literature. Three key issues are addressed, namely filtering irrelevant articles, identifying protein names and normalizing them to molecule identifiers, and extracting protein-protein interactions. Our system is among the top three performers in the benchmark evaluation of BioCreative 2006. The tool will be helpful for manual interaction curation and can greatly facilitate the process of extracting protein-protein interactions. © 2008 Huang et al; licensee BioMed Central Ltd.",,"analytical error; article; bioinformatics; comparative study; controlled study; data extraction; data mining; data synthesis; false negative result; molecular biology; performance measurement system; protein analysis; protein database; protein protein interaction; quality control; receiver operating characteristic; SWISS-PROT; system analysis; bibliographic database; Databases, Bibliographic; Databases, Protein; Protein Interaction Mapping",2-s2.0-51049110333
"Rinaldi F., Kappeler T., Kaljurand K., Schneider G., Klenner M., Clematide S., Hess M., von Allmen J.-M., Parisot P., Romacker M., Vachon T.","OntoGene in BioCreative II",2008,"Genome Biology",31,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049105083&doi=10.1186%2fgb-2008-9-s2-s13&partnerID=40&md5=6802f80c97f3f5929f204679484e6602","Background: Research scientists and companies working in the domains of biomedicine and genomics are increasingly faced with the problem of efficiently locating, within the vast body of published scientific findings, the critical pieces of information that are needed to direct current and future research investment. Results: In this report we describe approaches taken within the scope of the second BioCreative competition in order to solve two aspects of this problem: detection of novel protein interactions reported in scientific articles, and detection of the experimental method that was used to confirm the interaction. Our approach to the former problem is based on a high-recall protein annotation step, followed by two strict disambiguation steps. The remaining proteins are then combined according to a number of lexico-syntactic filters, which deliver high-precision results while maintaining reasonable recall. The detection of the experimental methods is tackled by a pattern matching approach, which has delivered the best results in the official BioCreative evaluation. Conclusion: Although the results of BioCreative clearly show that no tool is sufficiently reliable for fully automated annotations, a few of the proposed approaches (including our own) already perform at a competitive level. This makes them interesting either as standalone tools for preliminary document inspection, or as modules within an environment aimed at supporting the process of curation of biomedical literature. © 2008 Rinaldi et al; licensee BioMed Central Ltd.",,"accuracy; article; automation; bioinformatics; controlled study; data mining; data synthesis; experimentation; genomics; human; information retrieval; information storage; molecular biology; nonhuman; protein analysis; protein interaction; protein processing; biology; documentation; gene; health care organization; Internet; methodology; reproducibility; Abstracting and Indexing as Topic; Computational Biology; Genes; Internet; Protein Interaction Mapping; Reproducibility of Results; Societies, Scientific",2-s2.0-51049105083
"Baumgartner Jr. W.A., Lu Z., Johnson H.L., Caporaso J.G., Paquette J., Lindemann A., White E.K., Medvedeva O., Cohen K.B., Hunter L.","Concept recognition for extracting protein interaction relations from biomedical text",2008,"Genome Biology",27,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049090818&doi=10.1186%2fgb-2008-9-s2-s9&partnerID=40&md5=b35e24c181e3f10351d5c7dc8b04aa4e","Background: Reliable information extraction applications have been a long sought goal of the biomedical text mining community, a goal that if reached would provide valuable tools to benchside biologists in their increasingly difficult task of assimilating the knowledge contained in the biomedical literature. We present an integrated approach to concept recognition in biomedical text. Concept recognition provides key information that has been largely missing from previous biomedical information extraction efforts, namely direct links to well defined knowledge resources that explicitly cement the concept's semantics. The BioCreative II tasks discussed in this special issue have provided a unique opportunity to demonstrate the effectiveness of concept recognition in the field of biomedical language processing. Results: Through the modular construction of a protein interaction relation extraction system, we present several use cases of concept recognition in biomedical text, and relate these use cases to potential uses by the benchside biologist. Conclusion: Current information extraction technologies are approaching performance standards at which concept recognition can begin to deliver high quality data to the benchside biologist. Our system is available as part of the BioCreative Meta-Server project and on the internet http://bionlp.sourceforge.net. © 2008 Baumgartner et al; licensee BioMed Central Ltd.",,"article; automation; data extraction; gene identification; gene mapping; genetic database; human; information processing; information retrieval; medical research; molecular recognition; natural language processing; nonhuman; performance; protein analysis; protein database; protein interaction; automated pattern recognition; bibliographic database; gene; information retrieval; medical research; Biomedical Research; Databases, Bibliographic; Genes; Information Storage and Retrieval; Pattern Recognition, Automated; Protein Interaction Mapping",2-s2.0-51049090818
"Warth A., Ohshima Y., Yamamiya T., Wallace S.","Toward a more scalable end-user scripting language",2008,"Proceedings - 6th International Conference on Creating, Connecting and Collaborating through Computing, C5 2008",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50049113895&doi=10.1109%2fC5.2008.33&partnerID=40&md5=a298641a5f9ff6f13de2a222c961d937","End-user scripting languages are relatively easy to learn, but have limited expressive power. Tile-based scripting systems are particularly accessible to beginners, but usually are very limited in scope and usually lack extensibility, and for some tasks the tile idiom becomes cumbersome. Conventional programming languages used by computer professionals are far more powerful, but at the cost of additional complexity and limited environmental support, which place them out of the casual programmer's reach. This paper presents TileScript, an attempt to combine the accessibility of a tile-based programming interface with the leverage of a full textual programming language and with a simple means of extension, making it potentially an appealing tool for the novice programmer without sacrificing any expressiveness. All TileScript programs, whether built originally with tiles or textually, can always be edited both graphically via a drag-and-drop tile interface and textually, and the user can freely switch back and forth between tile and textual representations at any time. Additionally, TileScript 's simple yet powerful extensibility mechanisms allow the language to be used to tackle problems that would normally be out of the scope of an end-user scripting language. © 2008 IEEE.",,"Computer professionals; Drag-and-drop; End-user scripting; Expressive power; International conferences; Novice programmer; Programming interfaces; Programming languages; Textual representations; BASIC (programming language); Building materials; Chlorine compounds; Computer software; Computers; Costs; Finance; Java programming language; Linguistics; Query languages; Computer programming languages",2-s2.0-50049113895
"Cuevas R.R.M., Honda W.Y., De Lucena D.J., Paraboni I., Oliveira P.R.","Portuguese pronoun resolution: Resources and evaluation",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49949114942&doi=10.1007%2f978-3-540-78135-6_29&partnerID=40&md5=37ef45eb4963db459ea2b0d318e9b7e5","Despite being one of the most widely-spoken languages in the world, Portuguese remains a relatively resource-poor language, for which only in recently years NLP tools such as parsers, taggers and (fairly) large corpora have become available. In this work we describe the task of pronominal co-reference annotation and resolution in Portuguese texts, in which we take advantage of information provided by a tagged corpus and a simple annotation tool that has been developed for this purpose. Besides developing some of these basic resources from scratch, our ultimate goal is to investigate the multilingual resolution of Portuguese personal pronouns to improve the accuracy of their translations to both Spanish and English in an underlying MT project. © 2008 Springer-Verlag Berlin Heidelberg.",,"Computational linguistics; Laws and legislation; Query languages; Software agents; Text processing; Word processing; Intelligent text processing; International conferences; Pronoun resolution; Spanish; Spoken languages; Linguistics",2-s2.0-49949114942
"Thomo A., Venkatesh S., Ye Y.Y.","Visibly pushdown transducers for approximate validation of streaming XML",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49949086720&doi=10.1007%2f978-3-540-77684-0_16&partnerID=40&md5=c043de454a6d93497cc008f665509eeb","Visibly Pushdown Languages (VPLs), recognized by Visibly Pushdown Automata (VPAs), are a nicely behaved family of context-free languages. It has been shown that VPAs are equivalent to Extended Document Type Definitions (EDTDs), and thus, they provide means for elegantly solving various problems on XML. Especially, it has been shown that VPAs are the apt device for streaming XML. One of the important problems about XML that can be addressed using VPAs is the validation problem in which we need to decide whether an XML document conforms to the specification given by an EDTD. In this paper, we are interested in solving the approximate version of this problem, which is to decide whether an XML document can be modified by a tolerable number of edit operations to yield a valid one with respect to a given EDTD. For this, we define Visibly Pushdown Transducers (VPTs) that give us the framework for solving this problem under two different semantics for edit operations on XML. While the first semantics is a generalization of edit operations on strings, the second semantics is new and motivated by the special nature of XML documents. Usings VPTs, we give streaming algorithms that solve the problem under both the semantics. These algorithms use storage space that only depends on the size of the EDTD and the number of tolerable errors. Furthermore, they can check approximate validity of an incoming XML document in a single pass over the document, using auxilliary stack space that is proportional to the depth of the XML document. © 2008 Springer-Verlag Berlin Heidelberg.",,"Automata theory; Context free languages; Foundations; Information theory; Linguistics; Markup languages; Semantics; Transducers; XML; Document Type definitions; International symposium; Knowledge systems; Push down; Single pass; Storage spaces; Streaming algorithms; Streaming XML; Validation problem; Visibly Pushdown Automata; Visibly pushdown languages; XML documents; Information management",2-s2.0-49949086720
"Kovács L., Barabás P.","Generalization analysis of the CL and MM-based classifications",2008,"SAMI 2008 6th International Symposium on Applied Machine Intelligence and Informatics - Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49749132341&doi=10.1109%2fSAMI.2008.4469195&partnerID=40&md5=a650ffe432cefacb653d0061b54a235f","Computational linguistics covers the statistical and logical modeling of languages using computer-based software-hardware tools. An important component in CL systems is the morphological parser. The scope of our study is to build a statistical method to learn the rules of word inflection. The pre-requirement regarding the language is that the language uses words which are sequences of characters. A key factor of the required clustering algorithm is the cost efficiency. After analysis of the alternatives, two methods were selected to perform further refinement and adaptation: the observable Markov Model method and the formal concept analysis method. © 2008 IEEE.",,"Cost efficiencies; Formal Concept Analysis; Hardware tools; Informatics; International symposium; Key factors; Logical modeling; Machine intelligence; Markov modelling; Clustering algorithms; Computational linguistics; Cost effectiveness; Linguistics; Markov processes; Query languages; Statistical methods; Computer hardware",2-s2.0-49749132341
"Morneau M., Mineau G.W.","Employing a domain specific ontology to perform semantic search",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48949103754&doi=10.1007%2f978-3-540-70596-3_17&partnerID=40&md5=762efa188433469c20997e92ae655d6a","Increasing the relevancy of Web search results has been a major concern in research over the last years. Boolean search, metadata, natural language based processing and various other techniques have been applied to improve the quality of search results sent to a user. Ontology-based methods were proposed to refine the information extraction process but they have not yet achieved wide adoption by search engines. This is mainly due to the fact that the ontology building process is time consuming. An all inclusive ontology for the entire World Wide Web might be difficult if not impossible to construct, but a specific domain ontology can be automatically built using statistical and machine learning techniques, as done with our tool: SeseiOnto. In this paper, we describe how we adapted the SeseiOnto software to perform Web search on the Wikipedia page on climate change. SeseiOnto, by using conceptual graphs to represent natural language and an ontology to extract links between concepts, manages to properly answer natural language queries about climate change. Our tests show that SeseiOnto has the potential to be used in domain specific Web search as well as in corporate intranets. © 2008 Springer-Verlag.",,"Artificial intelligence; Climate change; Climate control; Climatology; Computer software; Information retrieval; Information services; Information theory; Internet; Learning algorithms; Learning systems; Linguistics; Metadata; Multi agent systems; Ontology; Search engines; Websites; Conceptual graphs; Conceptual structures; Domain-specific; Domain-specific ontologies; Information Extraction; International conferences; Knowledge visualization; Machine-learning techniques; Natural language queries; Natural languages; Ontology building; Search results; Semantic search; Specific domain; Time consuming; Web searches; Wikipedia; World Wide Web",2-s2.0-48949103754
"Régis-Gianas Y., Pottier F.","A Hoare logic for call-by-value functional programs",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48949099469&doi=10.1007%2f978-3-540-70594-9_17&partnerID=40&md5=051d2320f787d1f029aa2604ebb6d473","We present a Hoare logic for a call-by-value programming language equipped with recursive, higher-order functions, algebraic data types, and a polymorphic type system in the style of Hindley and Milner. It is the theoretical basis for a tool that extracts proof obligations out of programs annotated with logical assertions. These proof obligations, expressed in a typed, higher-order logic, are discharged using off-the-shelf automated or interactive theorem provers. Although the technical apparatus that we exploit is by now standard, its application to call-by-value functional programming languages appears to be new, and (we claim) deserves attention. As a sample application, we check the partial correctness of a balanced binary search tree implementation. © 2008 Springer-Verlag.",,"BASIC (programming language); Computer programming; Computer software; Computers; Functional programming; Linguistics; Mathematical techniques; Model predictive control; Predictive control systems; Query languages; Recursive functions; Standards; Algebraic data types; Binary search trees; Functional programming languages; Functional programs; Higher-order functions; Higher-order logics; Hoare logics; Interactive theorem provers; International conferences; Partial correctness; Program construction; Programming languages; Proof obligations; Type systems; Computer programming languages",2-s2.0-48949099469
"Hou Q., Zhou K., Guo B.","BSGP: Bulk-synchronous GPU programming",2008,"ACM Transactions on Graphics",48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249137934&doi=10.1145%2f1360612.1360618&partnerID=40&md5=35a39c7b317f90a392e67061676788c5","We present BSGP, a new programming language for general purpose computation on the GPU. A BSGP program looks much the same as a sequential C program. Programmers only need to supply a bare minimum of extra information to describe parallel processing on GPUs. As a result, BSGP programs are easy to read, write, and maintain. Moreover, the ease of programming does not come at the cost of performance. A well-designed BSGP compiler converts BSGP programs to kernels and combines them using optimally allocated temporary streams. In our benchmark, BSGP programs achieve similar or better performance than well-optimized CUDA programs, while the source code complexity and programming time are significantly reduced. To test BSGP's code efficiency and ease of programming, we implemented a variety of GPU applications, including a highly sophisticated X3D parser that would be extremely difficult to develop with existing GPU programming languages. © 2008 ACM.","Bulk synchronous parallel programming; Programable graphics hardware; Stream processing; Thread manipulation","Bulk synchronous parallel programming; GPU programming; Parallel processing; Programable graphics hardware; Programming languages; Programming time; Source coding; Stream processing; Thread manipulation; BASIC (programming language); Benchmarking; C (programming language); Chlorine compounds; Codes (standards); Codes (symbols); Computer software; Computers; Exposure controls; Linguistics; Query languages; Computer programming languages",2-s2.0-49249137934
"Kong J., Zhao C.-Y.","Visual language techniques for software development",2008,"Ruan Jian Xue Bao/Journal of Software",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50149122678&doi=10.3724%2fSP.J.1001.2008.01902&partnerID=40&md5=6128955abf03c29a602c8c1b6556abd8","Visual language techniques have exhibited more advantages in describing various software artifacts than one-dimensional textual languages during software development, ranging from the requirement analysis and design to testing and maintenance, as diagrammatic and graphical notations have been well applied in modeling system. In addition to an intuitive appearance, graph grammars provide a well-established foundation for defining visual languages with the power of precise modeling and verification on computers. This paper discusses the issues and techniques for a formal foundation of visual languages, reviews related practical graphical environments, presents a spatial graph grammar formalism, and applies the spatial graph grammar to defining behavioral semantics of UML diagrams and developing a style-driven framework for software architecture design.","Graph grammar; Software architecture; UML semantics; Visual language","Computer software maintenance; Context sensitive grammars; Formal languages; Formal logic; Foundations; Graph theory; Information theory; Linguistics; Query languages; Software architecture; Software testing; Unified Modeling Language; Well testing; Behavioral semantics; Graph grammar; Graph grammars; Graphical environments; Graphical notations; Modeling systems; One-dimensional; Precise modeling; Requirement analysis; Software architecture design; Software artifacts; Software development; Spatial graph grammar formalism; U ML diagrams; UML semantics; Visual language; Visual languages; Software design",2-s2.0-50149122678
"Silvasti P.","XML-document-filtering automaton",2008,"Proceedings of the VLDB Endowment",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349160070&partnerID=40&md5=739b46513d28a16ed33ccd0fd735639e","In a publish-subscribe system based on filtering of XML documents subscribers specify their interests with profiles expressed in the XPath language. The system processes a stream of XML documents and delivers to subscribers a notification or content of documents that match the profiles. We present a new XML-document-filtering algorithm that is based on the classic Aho-Corasick pattern-matching automaton. The automaton has a size linear in the sum of the sizes of the filters. We assume that the XML documents all conform to a given DTD; our algorithm utilizes the DTD in the preprocessing phase of the automaton to prune out descendant axes (//) and wildcards (*) from the XPath filters. The XPath subset currently supported consists of linear XPath expressions without predicates. In the case of a 683 MB protein-sequence database, we obtained a through-put of 18.8 MB/sec for 50 000 filters and 17.0 MB/sec for 500 000 filters, using a SAX parser with a throughput of 27 MB/sec. © 2008 VLDB Endowment.",,"Aho-Corasick; Preprocessing phase; Publish-subscribe systems; System process; XPath expressions; Information retrieval systems; XML; Automata theory",2-s2.0-70349160070
"Rus V., McCarthy P.M., McNamara D.S., Graesser A.C.","A study of textual entailment",2008,"International Journal on Artificial Intelligence Tools",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51549098376&doi=10.1142%2fS0218213008004096&partnerID=40&md5=e446017be86f63609b211eff8cd84b40","In this paper we study a graph-based approach to the task of Recognizing Textual Entailment between a Text and a Hypothesis. The approach takes into account the full lexico-syntactic context of both the Text and Hypothesis and is based on the concept of subsumption. It starts with mapping the Text and Hypothesis on to graph structures that have nodes representing concepts and edges representing lexico-syntactic relations among concepts. An entailment decision is then made on the basis of a subsumption score between the Text-graph and Hypothesis-graph. The results obtained from a standard entailment test data set were promising. The impact of synonymy on entailment is quantified and discussed. An important advantage to a solution like ours is its ability to be customized to obtain high-confidence results. © 2008 World Scientific Publishing Company.","Graph subsumption; Natural language processing; Syntactic dependencies; Textual entailment","Statistical tests; Graph subsumption; Natural language processing; Syntactic dependencies; Textual entailment; Graph theory",2-s2.0-51549098376
"Cavendish D., Candan K.S.","Distributed XML processing: Theory and applications",2008,"Journal of Parallel and Distributed Computing",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-45449109568&doi=10.1016%2fj.jpdc.2008.04.003&partnerID=40&md5=5013f8e86de22338a593308887d41b90","Basic message processing tasks, such as well-formedness checking and grammar validation, common in Web service messaging, can be off-loaded from the service providers' own infrastructures. The traditional ways to alleviate the overhead caused by these tasks is to use firewalls and gateways. However, these single processing point solutions do not scale well. To enable effective off-loading of common processing tasks, we introduce the Prefix Automata SyStem - PASS, a middleware architecture which distributively processes XML payloads of web service SOAP messages during their routing towards Web servers. PASS is based on a network of automata, where PASS-nodes independently but cooperatively process parts of the SOAP message XML payload. PASS allows autonomous and pipelined in-network processing of XML documents, where parts of a large message payload are processed by various PASS-nodes in tandem or simultaneously. The non-blocking, non-wasteful, and autonomous operation of PASS middleware is achieved by relying on the prefix nature of basic XML processing tasks, such as well-formedness checking and DTD validation. These properties ensure minimal distributed processing management overhead. We present necessary and sufficient conditions for outsourcing XML document processing tasks to PASS, as well as provide guidelines for rendering suitable applications to be PASS processable. We demonstrate the advantages of migrating XML document processing, such as well-formedness checking, DTD parsing, and filtering to the network via event driven simulations. © 2008 Elsevier Ltd. All rights reserved.","Distributed processing; Filtering; Grammar validation; Prefix computations; SOAP message processing; Well-formedness; XML document processing","Computer networks; Filtration; Gateways (computer networks); Information management; Information retrieval systems; Information services; Markup languages; Metropolitan area networks; Middleware; Network architecture; Network protocols; Robots; Terminology; Translation (languages); Web services; World Wide Web; XML; (e ,2e) theory; (e ,3e) process; (PL) properties; (semi)automata; Autonomous operations; Distributed processing; Elsevier (CO); Event-driven simulations; In network processing; message processing; middleware architecture; Necessary and sufficient conditions; Non-blocking; Off loading; Service provider (SP); Soap messaging; Web servers; Well formedness; XML documents; XML processing; Pipeline processing systems",2-s2.0-45449109568
"Neukirchen H., Zeiss B., Grabowski J.","An approach to quality engineering of TTCN-3 test specifications",2008,"International Journal on Software Tools for Technology Transfer",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48349147433&doi=10.1007%2fs10009-008-0075-0&partnerID=40&md5=b5094aeedc9ce09e2bab56e8b294a213","Experience with the development and maintenance of large test suites specified using the Testing and Test Control Notation (TTCN-3) has shown that it is difficult to construct tests that are concise with respect to quality aspects such as maintainability or usability. The ISO/IEC standard 9126 defines a general software quality model that substantiates the term ""quality"" with characteristics and subcharacteristics. The domain of test specifications, however, requires an adaption of this general model. To apply it to specific languages such as TTCN-3, it needs to be instantiated. In this paper, we present an instantiation of this model as well as an approach to assess and improve test specifications. The assessment is based on metrics and the identification of code smells. The quality improvement is based on refactoring. Example measurements using our TTCN-3 tool TRex demonstrate how this procedure is applied in practise. © Springer-Verlag 2008.","Code smells; Metrics; Quality model; Refactoring; Test specification; TTCN-3","Computer software selection and evaluation; Maintainability; Specifications; Standards; Evolution (CO); General model; ISO/IEC; Metrics (CO); Quality aspects; Quality engineering; Quality improvement (QI); Refactoring; Software-quality; Special sections; Specific languages; Test Automation; Test specification (TS); Test suites; Testing and Test Control Notation (TTCN); Testing",2-s2.0-48349147433
"Bräuer M., Demuth B.","Model-level integration of the OCL standard library using a pivot model with generics support",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47849088620&doi=10.1007%2f978-3-540-69073-3_20&partnerID=40&md5=2f17a8d9f51d306c7205a1bf0cca1a49","OCL 2.0 specifies a standard library of predefined types and associated operations. A model-level representation of the library is required to reference its elements within the abstract syntax model created by an OCL parser. Existing OCL engines build this model in the implementation code which severely limits reusability, flexibility and maintainability. To address these problems, we show how a common pivot model with explicit support for template types can help to externalize the definition of the standard library and integrate it with instances of arbitrary domain-specific modeling languages. We exemplify the feasibility of our approach with a prototypical implementation for the Dresden OCL2 Toolkit and present a tailored EMF editor for modeling the OCL types and operations. We limit our discussion to the model level, i.e., we do not consider an implementation of the standard library for an execution engine. © 2008 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Bioinformatics; Computer science; Maintainability; Models; Software engineering; Abstract syntax; Domain specific modeling languages (DSMLS); Dresden; Execution engines; Heidelberg (CO); Lecture Notes; OCL 2.0; OCL standard library; Prototypical implementations; Standard libraries; Standards",2-s2.0-47849088620
"Hallett C., Hardcastle D.","Towards a bootstrapping NLIDB system",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749131611&doi=10.1007%2f978-3-540-69858-6_20&partnerID=40&md5=9f694feb56e3067b2cb6d8fda332d5fd","This paper presents the results of a feasibility study for a bootstrapping natural language database query interface which uses natural language generation (nlg) technology to address the interpretation problem faced by existing nlidb systems. In particular we assess the feasibility of automatically acquiring the requisite semantic and linguistic resources for the nlg component using the database metadata and data content, a domain-specific ontology and a corpus of associated text documents, such as end-user manuals, for example. © 2008 Springer-Verlag Berlin Heidelberg.",,"Decision making; Information science; Information systems; Information theory; Linguistics; Metadata; Ontology; Planning; Resource allocation; Data contents; Domain-specific ontologies; End users; Feasibility studies; Heidelberg (CO); International conferences; Linguistic resources; Natural language database (NLDB); Natural language generation (NLG); Natural languages; Text documents; Database systems",2-s2.0-47749131611
"Minock M., Olofsson P., Näslund A.","Towards building robust natural language interfaces to databases",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749121071&doi=10.1007%2f978-3-540-69858-6_19&partnerID=40&md5=0937283c203966f752e79f9ec5c2cd64","We seek to give everyday technical teams the capability to build robust natural language interfaces to their databases, for subsequent use by casual users. We present an approach to the problem which integrates and streamlines earlier work based on light annotation and authoring tools. We model queries in a higher-order version of Codd's tuple calculus and we use synchronous grammars extended with lambda functions to represent semantic grammars. The results of configuration can be applied directly to SQL based databases with general n-ary relations. We have fully implemented our approach and we present initial empirical results for the Geoquery 250 corpus. © 2008 Springer-Verlag Berlin Heidelberg.",,"Database systems; Information science; Information systems; Information theory; Authoring tools; Empirical results; Heidelberg (CO); Higher order; International conferences; Natural language interface (NLI); Natural languages; Synchronous grammars; Linguistics",2-s2.0-47749121071
"Tok W.H., Bressan S., Lee M.-L.","Twig'n join: Progressive query processing of multiple XML streams",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47249132442&doi=10.1007%2f978-3-540-78568-2_45&partnerID=40&md5=bbe9eee0ae532ae0fc41b2225efec298","We propose a practical approach to the progressive processing of (FWR) XQuery queries on multiple XML streams, called Twig'n Join (or TnJ). The query is decomposed into a query plan combining several twig queries on the individual streams, followed by a multi-way join and a final twig query. The processing is itself accordingly decomposed into three pipelined stages progressively producing streams of XML fragments. Twig'n Join combines the advantages of the recently proposed TwigM algorithm and our previous work on relational result-rate based progressive joins. In addition, we introduce a novel dynamic probing technique, called Result-Oriented Probing (ROP), which determines an optimal probing sequence for the multi-way join. This significantly reduces the amount of redundant probing for results. We comparatively evaluate the performance of Twig'n Join using both synthetic and real-life data from standard XML query processing benchmarks. We show that Twig'n Join is indeed effective and efficient for processing multiple XML streams. © 2008 Springer-Verlag Berlin Heidelberg.","Progressive join; XML","Advanced applications; Heidelberg (CO); Individual (PSS 544-7); International conferences; Multi-way join; Probing sequence; Progressive processing; Progressive query (PQ); Real-life data; Twig queries; XML query processing; XML streaming; Benchmarking; Data processing; Database systems; Information management; Markup languages; Query processing; Rivers; Standards; XML; Pipeline processing systems",2-s2.0-47249132442
"Henriksson J., Heidenreich F., Johannes J., Zschaler S., Aßmann U.","Extending grammars and metamodels for reuse: The Reuseware approach",2008,"IET Software",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849138167&doi=10.1049%2fiet-sen%3a20070060&partnerID=40&md5=f685539daf9d819f30cd7f7c0833f670","The trend towards domain-specific languages leads to an ever-growing plethora of highly specialised languages. Developers of such languages focus on their specific domains rather than on the technical challenges of language design. The generic features of languages are rarely included in special-purpose languages. One very important feature is the ability to formulate partial programs in separate encapsulated entities, which can be composed into complete programs in a well-defined manner. A language-independent approach is presented that adds useful constructs for defining components. The authors discuss the underlying concepts and describe a composition environment and tool supporting these ideas-the Reuseware Composition Framework. To evaluate this approach, the authors enrich the (Semantic) Web query language Xcerpt with an additional useful reuse concept - modules. © 2008 The Institution of Engineering and Technology.",,"Information theory; Linguistics; Structure (composition); (OTDR) technology; Domain specific language (DSL); Generic features; language designs; Languages (traditional); Meta-models; Technical challenges; Query languages",2-s2.0-45849138167
"Dubey A., Jalote P., Aggarwal S.K.","Learning context-free grammar rules from a set of program",2008,"IET Software",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849086903&doi=10.1049%2fiet-sen%3a20070061&partnerID=40&md5=d0b5abac91a0727c012e8f3261b8d8a1","The grammar of a programming language is important because it is used in developing program analysis and modification tools. Sometimes programs are written in dialects-minor variations of standard languages. Normally, grammars of standard languages are available but grammars of dialects may not be available. A technique for reverse engineering context-free grammar rules is presented. The proposed technique infers rules from a given set of programs and an approximate grammar is generated using an iterative approach with backtracking. The correctness of the approach, is explained and a set of optimisations proposed to make the approach more efficient. The approach and the optimisations are experimentally verified on a set of programming languages. © 2008 The Institution of Engineering and Technology.",,"Computational grammars; Computer programming languages; Computer software; Context free grammars; Learning systems; Linguistics; Query languages; Reengineering; Reverse engineering; Standards; (OTDR) technology; Context-free grammar (CFG); Iterative approaches; Languages (traditional); Program analysis; Programming language; Variations of; Context free languages",2-s2.0-45849086903
"Vilares J., Alonso M.A., Vilares M.","Extraction of complex index terms in non-English IR: A shallow parsing based approach",2008,"Information Processing and Management",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44449174059&doi=10.1016%2fj.ipm.2007.12.005&partnerID=40&md5=44ecf59785902046363db719c84a4ef6","The performance of information retrieval systems is limited by the linguistic variation present in natural language texts. Word-level natural language processing techniques have been shown to be useful in reducing this variation. In this article, we summarize our work on the extension of these techniques for dealing with phrase-level variation in European languages, taking Spanish as a case in point. We propose the use of syntactic dependencies as complex index terms in an attempt to solve the problems deriving from both syntactic and morpho-syntactic variation and, in this way, to obtain more precise index terms. Such dependencies are obtained through a shallow parser based on cascades of finite-state transducers in order to reduce as far as possible the overhead due to this parsing process. The use of different sources of syntactic information, queries or documents, has been also studied, as has the restriction of the dependencies applied to those obtained from noun phrases. Our approaches have been tested using the CLEF corpus, obtaining consistent improvements with regard to classical word-level non-linguistic techniques. Results show, on the one hand, that syntactic information extracted from documents is more useful than that from queries. On the other hand, it has been demonstrated that by restricting dependencies to those corresponding to noun phrases, important reductions of storage and management costs can be achieved, albeit at the expense of a slight reduction in performance. © 2007 Elsevier Ltd. All rights reserved.","Finite-state transducers; Information retrieval; Linguistic variation; Natural language processing; Shallow parsing","Computational linguistics; Information retrieval systems; Natural language processing systems; Problem solving; Syntactics; Text processing; Finite-state transducers; Linguistic variation; Shallow parsing; Information retrieval",2-s2.0-44449174059
"Werner C., Buschmann C., Brandt Y., Fischer S.","XML compression for web services on resource-constrained devices",2008,"International Journal of Web Services Research",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949106999&partnerID=40&md5=2da8726c903dc16c438405fcc71534db","Compared to other middleware approaches like CORBA or Java RMI the protocol overhead of SOAP is very high. This fact is not only disadvantageous for several performance-critical applications, but especially in environments with limited network bandwidth or resource-constrained computing, devices. Although recent research work concentrated on more compact, binary representations of XML data only very few approaches account for the special characteristics of SOAP communication. In this article we will discuss the most relevant state-of-the-art technologies for compressing XML data. Furthermore, we will present a novel solution for compacting SOAP messages. In order to achieve significantly better compression rates than current approaches, our compressor utilizes structure information from an XML Schema or WSDL document. With this additional knowledge on the ""grammar"" of the exchanged messages, our compressor generates a single custom pushdown automaton, which can be used as a highly efficient validating parser as well as a highly efficient compressor. The main idea is to tag the transitions of the automaton with short binary identifiers that are then used to encode the path trough the automaton during parsing. Our approach leads to extremely compact data representations and is also usable in environments with very limited CPU and memory resources. Copyright © 2008, IGI Global.","Binary XML; Data compression; Efficient XML processing; Grammar-specific compression; Grammar-specific XML parsing; Middlewarm; Pushdown automata; SOAP; Web services","Automata theory; Common object request broker architecture (CORBA); Compressors; Computational linguistics; Computer networks; Data storage equipment; Formal languages; Internet protocols; Markup languages; Middleware; Powders; Robots; Translation (languages); Web services; World Wide Web; XML; Binary XML; Efficient XML processing; Grammar-specific compression; Grammar-specific XML parsing; Middlewarm; Pushdown automata; SOAP; Data compression",2-s2.0-55949106999
"Sarikaya R.","Rapid bootstrapping of statistical spoken dialogue systems",2008,"Speech Communication",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849117229&doi=10.1016%2fj.specom.2008.03.011&partnerID=40&md5=74c63a7e0814b7cd875936d3d50f71b8","Rapid deployment of statistical spoken dialogue systems poses portability challenges for building new applications. We discuss the challenges that arise and focus on two main problems: (i) fast semantic annotation for statistical speech understanding and (ii) reliable and efficient statistical language modeling using limited in-domain resources. We address the first problem by presenting a new bootstrapping framework that uses a majority-voting based combination of three methods for the semantic annotation of a ""mini-corpus"" that is usually manually annotated. The three methods are a statistical decision tree based parser, a similarity measure and a support vector machine classifier. The bootstrapping framework results in an overall cost reduction of about a factor of two in the annotation effort compared to the baseline method. We address the second problem by devising a method to efficiently build reliable statistical language models for new spoken dialog systems, given limited in-domain data. This method exploits external text resources that are collected for other speech recognition tasks as well as dynamic text resources acquired from the World Wide Web. The proposed method is applied to a spoken dialog system in a financial transaction domain and a natural language call-routing task in a package shipment domain. The experiments demonstrate that language models built using external resources, when used jointly with the limited in-domain language model, result in relative word error rate reductions of 9-18%. Alternatively, the proposed method can be used to produce a 3-to-10 fold reduction for the in-domain data requirement to achieve a given performance level. © 2008 Elsevier B.V. All rights reserved.","Rapid deployment; Semantic annotation; Spoken dialog systems; Web-based language modeling","Computational linguistics; Computer networks; Cost reduction; Costs; Data reduction; Decision making; Decision theory; Decision trees; Dynamical systems; Error analysis; Finance; Game theory; Industrial economics; Information theory; Learning systems; Linguistics; Mathematical models; Profitability; Reduction; Semantics; Speech; Speech analysis; Speech processing; Speech recognition; Statistics; Support vector machines; World Wide Web; Applied (CO); Dialog systems; domain language; Dynamic text; Elsevier (CO); External resources; Language models (LM); natural languages; new applications; Performance level (PL); recognition tasks; Semantic annotations; Similarity measuring; speech understanding; Spoken dialog system; Spoken dialogue systems (SDS); Statistical decision; Statistical language modeling; Word-error rate (WRD); Statistical methods",2-s2.0-45849117229
"Guan Y., Wang X., Wang Q.","A new measurement of systematic similarity",2008,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649098889&doi=10.1109%2fTSMCA.2008.918611&partnerID=40&md5=46b7f8bb6aa1b879d45aef1a9fd12334","The relationship of similarity may be the most universal relationship that exists between every two objects in either the material world or the mental world. Although similarity modeling has been the focus of cognitive science for decades, many theoretical and realistic issues are still under controversy. In this paper, a new theoretical framework that conforms to the nature of similarity and incorporates the current similarity models into a universal model is presented. The new model, i.e., the systematic similarity model, which is inspired by the contrast model of similarity and structure mapping theory in cognitive psychology, is the universal similarity measurement that has many potential applications in text, image, or video retrieval. The text relevance ranking experiments undertaken in this research tentatively show the validity of the new model. © 2008 IEEE.","Matching objects pair; Structure mapping theory; Systematic similarity; Systematic similarity measurement criterion; Systematic similarity model (SSM)","Experiments; (e ,2e) theory; Cognitive psychology; Cognitive sciences; contrast model; material world; new model; potential applications; Relevance rankings; similarity measurements; Similarity models; Systematic (CO); Universal models; Video retrieval; Research",2-s2.0-46649098889
"Erozel G., Cicekli N.K., Cicekli I.","Natural language querying for video databases",2008,"Information Sciences",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41749119145&doi=10.1016%2fj.ins.2008.02.001&partnerID=40&md5=ceeb080655c8a1cacea8b8c7ebe23f55","The video databases have become popular in various areas due to the recent advances in technology. Video archive systems need user-friendly interfaces to retrieve video frames. In this paper, a user interface based on natural language processing (NLP) to a video database system is described. The video database is based on a content-based spatio-temporal video data model. The data model is focused on the semantic content which includes objects, activities, and spatial properties of objects. Spatio-temporal relationships between video objects and also trajectories of moving objects can be queried with this data model. In this video database system, a natural language interface enables flexible querying. The queries, which are given as English sentences, are parsed using link parser. The semantic representations of the queries are extracted from their syntactic structures using information extraction techniques. The extracted semantic representations are used to call the related parts of the underlying video database system to return the results of the queries. Not only exact matches but similar objects and activities are also returned from the database with the help of the conceptual ontology module. This module is implemented using a distance-based method of semantic similarity search on the semantic domain-independent ontology, WordNet. © 2008 Elsevier Inc. All rights reserved.","Conceptual ontology; Content-based querying in video databases; Information extraction; Link parser; Natural language querying","Information retrieval; Ontology; Query languages; Query processing; Conceptual ontology; Information extraction; Natural language querying; Video databases; Natural language processing systems",2-s2.0-41749119145
"Reddy C.C.S., Shameer K., Offmann B.O., Sowdhamini R.","PURE: A webserver for the prediction of domains in unassigned regions in proteins",2008,"BMC Bioinformatics",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749135262&doi=10.1186%2f1471-2105-9-281&partnerID=40&md5=eba366f8e4a3c7c3096e1154fa39cfb4","Background: Protein domains are the structural and functional units of proteins. The ability to parse proteins into different domains is important for effective classification, understanding of protein structure, function, and evolution and is hence biologically relevant. Several computational methods are available to identify domains in the sequence. Domain finding algorithms often employ stringent thresholds to recognize sequence domains. Identification of additional domains can be tedious involving intense computation and manual intervention but can lead to better understanding of overall biological function. In this context, the problem of identifying new domains in the unassigned regions of a protein sequence assumes a crucial importance. Results: Wehad earlier demonstrated that accumulation of domain information of sequence homologues can substantially aid prediction of new domains. In this paper, we propose a computationally intensive, multi-step bioinformatics protocol as a web server named as PURE (Prediction of Unassigned REgions in proteins) for the detailed examination of stretches of unassigned regions in proteins. Query sequence is processed using different automated filtering steps based on length, presence of coiled-coil regions, transmembrane regions, homologous sequences and percentage of secondary structure content. Later, the filtered sequence segments and their sequence homologues are fed to PSI-BLAST, cd-hit and Hmmpfam. Data from the various programs are integrated and information regarding the probable domains predicted from the sequence is reported. Conclusion: We have implemented PURE protocolas a web server for rapid and comprehensive analysis of unassigned regions in the proteins. This server integrates data from different programs and provides information about the domains encoded in the unassigned regions. © 2008 Reddy et al; licensee BioMed Central Ltd.",,"Biological functions; Comprehensive analysis; Domain informations; Homologous sequences; Manual intervention; Protein structures; Secondary structures; Transmembrane regions; Bioinformatics; Forecasting; Web services; Proteins; bacterial protein; adenylate cyclase; amino acid sequence; article; automation; bioinformatics; computer program; Mycoplasma genitalium; nonhuman; prediction; protein analysis; protein domain; protein secondary structure; sequence homology; animal; automated pattern recognition; biology; cluster analysis; genetics; human; methodology; Mycoplasma gallisepticum; physiology; protein database; protein motif; protein tertiary structure; sequence analysis; structural homology; ultrastructure; Adenylate Cyclase; Amino Acid Motifs; Animals; Cluster Analysis; Computational Biology; Databases, Protein; Humans; Mycoplasma gallisepticum; Pattern Recognition, Automated; Protein Structure, Tertiary; Sequence Analysis, Protein; Software; Structural Homology, Protein",2-s2.0-47749135262
"Bethard S., Lu Z., Martin J.H., Hunter L.","Semantic role labeling for protein transport predicates",2008,"BMC Bioinformatics",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749117601&doi=10.1186%2f1471-2105-9-277&partnerID=40&md5=89e986b31043f0219f6ca30287ab69da","Background: Automatic semantic role labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations. This technique has been widely studied in the recent years, but mostly with data in newswire domains. Here, we report on a SRL model for identifying the semantic roles of biomedical predicates describing protein transport in GeneRIFs - manually curated sentences focusing on gene functions. To avoid the computational cost of syntactic parsing, and because the boundaries of our protein transport roles often did not match up with syntactic phrase boundaries, we approached this problem with a word-chunking paradigm and trained support vector machine classifiers to classify words as being at the beginning, inside or outside of a protein transport role. Results: We collected a set of 837GeneRIFs describing movements of proteins between cellular components, whose predicates were annotated for the semantic roles AGENT, PATIENT, ORIGIN and DESTINATION. We trained these models with the features of previous word-chunking models, features adapted from phrase-chunking models, and features derived from an analysis of our data. Our models were able to label protein transport semantic roles with 87.6% precision and 79.0% recall when using manually annotated protein boundaries, and 87.0% precision and 74.5% recall when using automatically identified ones. Conclusion: We successfully adapted the word-chunking classification paradigm to semantic role labeling, applying it to a new domain with predicates completely absent from any previous studies. By combining the traditional word and phrasal role labeling features with biomedical features like protein boundaries and MEDPOST part of speech tags, we were able to address the challenges posed by the new domain data and subsequently build robust models that achieved F-measures as high as 83.1. This system for extracting protein transport information from GeneRIFs performs well even with proteins identified automatically, and is therefore more robust than the rule-based methods previously used to extract protein transport roles. © 2008 Bethard et al; licensee BioMed Central Ltd.",,"Cellular components; Computational costs; NAtural language processing; Origin and destinations; Part-of-speech tags; Semantic representation; Semantic role labeling; Support vector machine classifiers; Natural language processing systems; Semantics; Support vector machines; Syntactics; Proteins; protein; accuracy; analytical error; article; classification; computer model; data analysis; machine learning; natural language processing; protein analysis; protein transport; support vector machine; animal; automated pattern recognition; biology; documentation; gene; genetics; human; information retrieval; information science; linguistics; methodology; nomenclature; physiology; semantics; Abstracting and Indexing as Topic; Animals; Classification; Computational Biology; Genes; Humans; Information Storage and Retrieval; Information Theory; Natural Language Processing; Pattern Recognition, Automated; Protein Transport; Semantics; Terminology as Topic; Vocabulary, Controlled",2-s2.0-47749117601
"Guo Q., Zhang M.","Question answering system based on ontology and semantic web",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649152420&doi=10.1007%2f978-3-540-79721-0_87&partnerID=40&md5=0db096a700795148ade2a218c975d5dd","Semantic web and ontology are the key technologies of Question Answering system. Ontology is becoming the pivotal methodology to represent domain-specific conceptual knowledge in order to promote the semantic capability of a QA system. In this paper we present a QA system in which the domain knowledge is represented by means of Ontology. In addition, a Chinese Natural Language human-machine interface is implemented mainly through a NL parser in this system. An initial evaluation result shows the feasibility to build such a semantic QA system based on Ontology, the effectivity of personalized semantic QA, the extensibility of ontology and knowledge base, and the possibility of self-produced knowledge based on semantic relations in the ontology.And experiments do prove that it is feasible to use the method to develop a QA System, which is valuable for further study in more depth. © 2008 Springer-Verlag Berlin Heidelberg.","Ontology; Question Answering; Semantic Web; WWW","Human computer interaction; Information retrieval; Knowledge representation; Natural language processing systems; Ontology; Semantic Web; Domain knowledge; Question Answering; Semantic relations; Query processing",2-s2.0-44649152420
"Marinov M., Stefanova S.","Localization of distributed data in a CORBA-based environment",2008,"WSEAS Transactions on Information Science and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955647277&partnerID=40&md5=313ba6b82097347e25298e3defe20c86","Query processing over distributed and fragmented databases is more challenging than doing so in a centralized environment. In a distributed environment, the DBMS needs to know where each node is located. The main role of data localization layer is to localize the query's data using data distribution information. We propose an approach to incorporate the artificial intelligence techniques into a distributed database management system (DBMS), namely to extend the core of a distributed CORBA-based environment with deductive functionalities of the query and view services during the process of data localization. The basic principles and the architecture of the software tool are considered. The implementation and class hierarchy of the objectoriented theorem prover which is built in the core of distributed CORBA-based system are also discussed.","CORBA-based architecture; Data localization; Distributed systems; Theorem prover","Artificial intelligence techniques; Basic principles; Class hierarchies; CORBA-based architecture; Data distribution; Data localization; Distributed data; Distributed database; Distributed environments; Distributed systems; Object oriented; Software tool; Theorem prover; Theorem provers; Artificial intelligence; Common object request broker architecture (CORBA); Computer architecture; Distributed database systems; Middleware; Search engines; Information management",2-s2.0-79955647277
"Zhang Y., Cao Y., Li X.","A decision procedure for XPath satisfiability in the presence of DTD containing choice",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749097436&doi=10.1007%2f978-3-540-78849-2_22&partnerID=40&md5=39fda4bdb8adbebd9f59db50193bee75","XPath satisfiability is one of the most basic problems of XML query optimization. A satisfiability decision framework, named SAT∈-∈DTD, is proposed to determine, given a set of XPath queries P and a DTD τ , which subset of P are satisfiable by an XML tree conforming to DTD τ. In the framework, an indexed NFA is constructed from the set of XPath queries P, and then the NFA is driven by simple API for DTD (SAD, something like SAX) events, derived from DTD τ, to evaluate the predicates in P and to decide the satisfiability of P. Especially, DTD choice (i.e. '|' operator) is taken into consideration, and an algorithm, named SAT∈-∈DTD_C, which bases on SAT∈-∈DTD, is put forward to determine the unsatisfiability caused by DTD choice. At last, the complexity of the algorithms is analyzed, and the correctness of the algorithms is tested by experiments. © 2008 Springer-Verlag Berlin Heidelberg.","Automaton; DTD choice; XPath satisfiability","Algorithms; Automatic programming; Computational complexity; Decision trees; Optimization; Query languages; DTD choice; Query optimization; XPath satisfiability; XML",2-s2.0-43749097436
"Delpratt O., Raman R., Rahman N.","Engineering succinct DOM",2008,"Advances in Database Technology - EDBT 2008 - 11th International Conference on Extending Database Technology, Proceedings",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-43349096828&doi=10.1145%2f1353343.1353354&partnerID=40&md5=918038a9923af81e6b9fbfc6eafa2ffb","We describe the engineering of Succinct DOM (SDOM), a DOM implementation, written in C++, which is suitable for in-memory representation of large static XML documents. SDOM avoids the use of pointers, and is based upon succinct data structures, which use an information-theoretically minimum amount of space to represent an object. SDOM gives a space-efficient in-memory representation, with stable and predictable memory usage. The space used by SDOM is an order of magnitude less than that used by a standard C++ DOM representation such as Xerces, but SDOM is extremely fast: navigation is in some cases faster than for a pointer-based representation such as Xerces (even for moderate-sized documents which can comfortably be loaded into main memory by Xerces). A variant, SDOM-CT, applies bzip-based compression to textual and attribute data, and its space usage is comparable with ""queryable"" XML compressors. Some of these compressors support navigation and/or querying (e.g. subpath queries) of the compressed file. SDOM-CT does not support querying directly, but remains extremely fast: it is several orders of magnitude faster for navigation than queryable XML compressors that support navigation (and only a few times slower than say Xerces). Copyright 2008 ACM.",,"Data compression; Data structures; Navigation; Storage allocation (computer); XML; Succinct DOM (SDOM); XML compressors; Computer programming languages",2-s2.0-43349096828
"Nederhof M.-J., Satta G.","Computation of distances for regular and context-free probabilistic languages",2008,"Theoretical Computer Science",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-45849119469&doi=10.1016%2fj.tcs.2008.01.010&partnerID=40&md5=0c1ca8edc8ec8c766e7c817b6beac19f","Several mathematical distances between probabilistic languages have been investigated in the literature, motivated by applications in language modeling, computational biology, syntactic pattern matching and machine learning. In most cases, only pairs of probabilistic regular languages were considered. In this paper we extend the previous results to pairs of languages generated by a probabilistic context-free grammar and a probabilistic finite automaton. © 2008 Elsevier Ltd. All rights reserved.","Kullback-Leibler divergence; Language entropy; Probabilistic context-free languages; Probabilistic finite automata; Probabilistic language distances","Artificial intelligence; Bioinformatics; Computational linguistics; Learning systems; Linguistics; Pattern matching; Probability; Query languages; Computational biology; Context free; Context-free grammar (CFG); Elsevier (CO); Language modeling (LM); Languages (traditional); machine-learning; Context free languages",2-s2.0-45849119469
"Curry G.B., Connor R.C.H.","Automated extraction of data from text using an XML parser: An earth science example using fossil descriptions",2008,"Geosphere",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41949097911&doi=10.1130%2fGES00140.1&partnerID=40&md5=b6dc69a1a0878b3d7f3444c2c7d0511d","Many valuable earth science data are not available in a digital format. Manual entry of such information into databases is time consuming, unrewarding, and prone to introducing errors. Taxonomic descriptions of fossils are a good example of valuable data that are overwhelming and available only in printed volumes and journals, some of which are increasingly rare and inaccessible. The highly structured nature of taxonomic procedures and nomenclature means that many previously published data remain equally valid to the present day, and contain information that is currently not available on the World Wide Web; these data would be of great use to a wide variety of scientists and other end users in government, industry, academia and the general public. This paper describes an XML (extensible markup language) parsing technique that allows taxonomic descriptions to be fully digitized much more rapidly than would be possible by manual entry of the data into a database. The technique exploits the high degree of structure in taxonomic descriptions, which are written in a standardized format, to automate the processing of tagging separate sections of the text. Once tagged using XML, the data can be subjected to complex searches using queries written in any of the XML query standards. The XML-tagged data can potentially be imported into existing databases, in effect removing the necessity to manually enter the information, and hence overcoming the main bottleneck in generating digital data from printed material. Individual parsers can be tailored precisely to the nature of the text being analyzed, and once the underlying concepts and procedures are understood, those interested in acquiring and using digital data will be able to generate XML parsers dedicated to text with different styles of standardized formatting. © 2008 Geological Society of America.","Data acquistion; Databased; Geoinformatics; Taxonomy; XML parsing","data acquisition; database; Earth science; nomenclature; taxonomy; World Wide Web",2-s2.0-41949097911
"Yang H., Nenadic G., Keane J.A.","Identification of transcription factor contexts in literature using machine learning approaches",2008,"BMC Bioinformatics",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649134672&doi=10.1186%2f1471-2105-9-S3-S11&partnerID=40&md5=8e82730b43bb1b088b936131001dd1d4","Background: Availability of information about transcription factors (TFs) is crucial for genome biology, as TFs play a central role in the regulation of gene expression. While manual literature curation is expensive and labour intensive, the development of semi-automated text mining support is hindered by unavailability of training data. There have been no studies on how existing data sources (e.g. TF-related data from the MeSH thesaurus and GO ontology) or potentially noisy example data (e.g. protein-protein interaction, PPI) could be used to provide training data for identification of TF-contexts in literature. Results: In this paper we describe a text-classification system designed to automatically recognise contexts related to transcription factors in literature. A learning model is based on a set of biological features (e.g. protein and gene names, interaction words, other biological terms) that are deemed relevant for the task. We have exploited background knowledge from existing biological resources (MeSH and GO) to engineer such features. Weak and noisy training datasets have been collected from descriptions of TF-related concepts in MeSH and GO, PPI data and data representing non-protein-function descriptions. Three machine-learning methods are investigated, along with a vote-based merging of individual approaches and/or different training datasets. The system achieved highly encouraging results, with most classifiers achieving an F-measure above 90%. Conclusions: The experimental results have shown that the proposed model can be used for identification of TF-related contexts (i.e. sentences) with high accuracy, with a significantly reduced set of features when compared to traditional bag-of-words approach. The results of considering existing PPI data suggest that there is not as high similarity between TF and PPI contexts as we have expected. We have also shown that existing knowledge sources are useful both for feature engineering and for obtaining noisy positive training data. © 2008 Yang et al.; licensee BioMed Central Ltd.",,"Back-ground knowledge; Biological features; Biological resources; Feature engineerings; Machine learning approaches; Noisy training datasets; Protein-protein interactions; Training data sets; Character recognition; Data mining; Information retrieval; Text processing; Transcription factors; Learning systems; transcription factor; Bayesian learning; biological model; biomedicine; conference paper; controlled study; data mining; information processing; information retrieval; machine learning; maximum entropy; protein analysis; protein database; protein protein interaction; support vector machine; transcription regulation; algorithm; article; artificial intelligence; automated pattern recognition; classification; linguistics; metabolism; methodology; natural language processing; nomenclature; Algorithms; Artificial Intelligence; Natural Language Processing; Pattern Recognition, Automated; Terminology as Topic; Transcription Factors; Vocabulary, Controlled",2-s2.0-44649134672
"Kalleberg K.T., Visser E.","Fusing a Transformation Language with an Open Compiler",2008,"Electronic Notes in Theoretical Computer Science",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41349113807&doi=10.1016%2fj.entcs.2008.03.042&partnerID=40&md5=c33e79005f5bac250986b7104194e31b","Program transformation systems provide powerful analysis and transformation frameworks as well as concise languages for language processing, but instantiating them for every subject language is an arduous task, most often resulting in half-completed frontends. Compilers provide mature frontends with robust parsers and type checkers, but solving language processing problems in general-purpose languages without transformation libraries is tedious. Reusing these frontends with existing transformation systems is therefore attractive. However, for this reuse to be optimal, the functional logic found in the frontend should be exposed to the transformation system - simple data serialization of the abstract syntax tree is not enough, since this fails to expose important compiler functionality, such as import graphs, symbol tables and the type checker. In this paper, we introduce a novel and general technique for combining term-based transformation systems with existing language frontends. The technique is presented in the context of a scriptable analysis and transformation framework for Java built on top of the Eclipse Java compiler. The framework consists of an adapter automatically extracted from the abstract syntax tree of the compiler and an interpreter for the Stratego program transformation language. The adapter allows the Stratego interpreter to rewrite directly on the compiler AST. We illustrate the applicability of our system with scripts written in Stratego that perform framework and library-specific analyses and transformations. © 2008 Elsevier B.V. All rights reserved.","compiler scripting; program transformation; strategic programming","Data acquisition; Digital libraries; Graph theory; Java programming language; Problem solving; Program compilers; Compiler scripting; Program transformation; Program transformation systems; Strategic programming; Computer programming languages",2-s2.0-41349113807
"Baldridge J., Osborne M.","Active learning and logarithmic opinion pools for HPSG parse selection",2008,"Natural Language Engineering",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949091034&doi=10.1017%2fS1351324906004396&partnerID=40&md5=3ea61544a32cfba73015206abc3d0900","For complex tasks such as parse selection, the creation of labelled training sets can be extremely costly. Resource-efficient schemes for creating informative labelled material must therefore be considered. We investigate the relationship between two broad strategies for reducing the amount of manual labelling necessary to train accurate parse selection models: ensemble models and active learning. We show that popular active learning methods for reducing annotation costs can be outperformed by instead using a model class which uses the available labelled data more efficiently. For this, we use a simple type of ensemble model called the Logarithmic Opinion Pool (LOP). We furthermore show that LOPs themselves can benefit from active learning. As predicted by a theoretical explanation of the predictive power of LOPs, a detailed analysis of active learning using LOPs shows that component model diversity is a strong predictor of successful LOP performance. Other contributions include a novel active learning method, a justification of our simulation studies using timing information, and cross-domain verification of our main ideas using text classification. © 2006 Cambridge University Press.",,"Linguistics; Text processing; Active learning; Ensemble models; Logarithmic opinion pools; Manual labelling; Parse selection; Natural language processing systems",2-s2.0-40949091034
"Haw S.-C., Lee C.-S.","TwigINLAB: A decomposition-matching-merging approach to improving XML query processing",2008,"American Journal of Applied Sciences",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149089195&partnerID=40&md5=d1f9dc1016349ea7ea911ead9a0420bb","The emergence of the Web has increased significant interests in querying XML data. Current methods for XML query processing still suffers from producing large intermediate results and are not efficient in supporting query with mixed types of relationships. We propose the TwigINLAB algorithm to process and optimize the query evaluation. Our TwigINLAB adopts the decomposition-matching-merging approach and focuses on optimizing all three sub-processes; introducing a novel compact labeling scheme, optimizing the matching phase and reducing the number of inspection required in the merging phase. Experimental results indicate that TwigINLAB can process both path queries and twig queries better than the TwigStack algorithm on an average of 21.7% and 18.7% respectively in terms of execution time using the SwissProt dataset. © 2008 Science Publications.","Decomposition-matching-merging; Query optimization; Structural query; Twig query; XML",,2-s2.0-41149089195
"Zhen L., Jiang Z.-H.","Innovation-oriented knowledge query in knowledge grid",2008,"Journal of Information Science and Engineering",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549126360&partnerID=40&md5=6e2b96e1ba2a6063350849e5a3859853","The knowledge grid, based on the Semantic Web, aims at providing the new platform for effectively sharing and managing versatile Web resources including information, knowledge and services. This paper is mainly concerned with the implementation of innovation-oriented knowledge query based on knowledge grid. The architecture of a knowledge query platform based on Ontology is proposed at first. Then we analyze the key techniques: knowledge ontology building, ontology reasoning, semantic register, and semantic parser. At last, a working scenario is employed to illustrate the process of knowledge query on the platform.","Grid; Innovation; Knowledge grid; Knowledge management; Knowledge query","Information services; Knowledge based systems; Knowledge management; Ontology; Query languages; Semantics; Knowledge grids; Knowledge query; Knowledge query platforms; Semantic registers; Grid computing",2-s2.0-41549126360
"Cox R., Bergan T., Clements A.T., Kaashoek F., Kohler E.","Xoc, an extension-oriented compiler for systems programming",2008,"ACM SIGPLAN Notices",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650046443&partnerID=40&md5=f12ebcb68ed2ea5415ecdafdd73b2e5e","Today's system programmers go to great lengths to extend the languages in which they program. For instance, system-specific compilers find errors in Linux and other systems, and add support for specialized control flow to Qt and event-based programs. These compilers are dificult to build and cannot always understand each other's language changes. However, they can greatly improve code understandability and correctness, advantages that should be accessible to all programmers. We describe an extension-oriented compiler for C called xoc. An extension-oriented compiler, unlike a conventional extensible compiler, implements new features via many small extensions that are loaded together as needed. Xoc gives extension writers full control over program syntax and semantics while hiding many compiler internals. Xoc programmers concisely define powerful compiler extensions that, by construction, can be combined; even some parts of the base compiler, such as GNU C compatibility, are structured as extensions. Xoc is based on two key interfaces. Syntax patterns allow extension writers to manipulate language fragments using concrete syntax. Lazy computation of attributes allows extension writers to use the results of analyses by other extensions or the core without needing to worry about pass scheduling. Extensions built using xoc include xsparse, a 345-line extension that mimics Sparse, Linux's C front end, and xlambda, a 170-line extension that adds function expressions to C. An evaluation of xoc using these and 13 other extensions shows that xoc extensions are typically more concise than equivalent extensions written for conventional extensible compilers and that it is possible to compose extensions. © 2008 ACM.","Extension-oriented compilers","Concrete syntax; Control flows; Event-based; Extension-oriented compilers; Front end; Full control; Line extension; Understandability; Computer operating systems; Computer programming; Electromigration; Linguistics; Query languages; Syntactics; Program compilers",2-s2.0-67650046443
"Carl M., Melero M., Badia T., Vandeghinste V., Dirix P., Schuurman I., Markantonatou S., Sofianopoulos S., Vassiliou M., Yannoutsou O.","METIS-II: Low resource machine translation",2008,"Machine Translation",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349156539&doi=10.1007%2fs10590-008-9048-z&partnerID=40&md5=40123eb34098a4f122a1bfa16014992e","METIS-II was an EU-FET MT project running from October 2004 to September 2007, which aimed at translating free text input without resorting to parallel corpora. The idea was to use ""basic"" linguistic tools and representations and to link them with patterns and statistics from the monolingual target-language corpus. The METIS-II project has four partners, translating from their ""home"" languages Greek, Dutch, German, and Spanish into English. The paper outlines the basic ideas of the project, their implementation, the resources used, and the results obtained. It also gives examples of how METIS-II has continued beyond its lifetime and the original scope of the project. On the basis of the results and experiences obtained, we believe that the approach is promising and offers the potential for development in various directions. © 2008 Springer Science+Business Media B.V.","Low resource MT; Pattern-based MT; Shallow linguistic processing for MT; Statistical MT","Computer aided language translation; Information theory; Linguistics; Query languages; Software agents; Speech transmission; Basic ideas; Free texts; Life-times; Low resource MT; Machine translations; Parallel corpora; Pattern-based MT; Shallow linguistic processing for MT; Spanishs; Statistical MT; Telluric prospecting",2-s2.0-57349156539
"Kamel M., Nagi K., El-Makky N.","Adaptive storage model for XML in object-relational databases- an extended version",2008,"AEJ - Alexandria Engineering Journal",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849127748&partnerID=40&md5=b7acbb7411f21fb4e84c56dc4d185d64","Object Relational DataBase Management Systems (ORDBMS) are becoming more popular in storing and retrieving XML than native XML DBMS. In most ORDBMS, XML is stored as CLOB inside the relation. Efficient XML parsers and indexing techniques are used to retrieve the desired XML nodes. However, less attention is given to XML updates queries. With the upcoming standardization of XML updates queries, the current implementation of the lock granularity imposes a great limitation on the concurrency of parallel transactions. This motivated several experimental ORDBMS to shred the XML nodes across internal relations. This approach has also several drawbacks. In this paper, we propose an adaptive technique for selective shredding. It is based on existing database engines and takes the changes in the workload pattern into consideration. We analyze the performance of our approach and compare it to the CLOB and the complete shredding approaches. © Faculty of Engineering Alexandria University.","Ordbms; Performance analysis; Shredding techniques; Xml storage models",,2-s2.0-56849127748
"De Raedt L., Kersting K., Kimmig A., Revoredo K., Toivonen H.","Compressing probabilistic Prolog programs",2008,"Machine Learning",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849007683&doi=10.1007%2fs10994-007-5030-x&partnerID=40&md5=f2e523cab1763d6e1a8efbc2f05df687","ProbLog is a recently introduced probabilistic extension of Prolog (De Raedt, et al. in Proceedings of the 20th international joint conference on artificial intelligence, pp. 2468-2473, 2007). A ProbLog program defines a distribution over logic programs by specifying for each clause the probability that it belongs to a randomly sampled program, and these probabilities are mutually independent. The semantics of ProbLog is then defined by the success probability of a query in a randomly sampled program. This paper introduces the theory compression task for ProbLog, which consists of selecting that subset of clauses of a given ProbLog program that maximizes the likelihood w.r.t. a set of positive and negative examples. Experiments in the context of discovering links in real biological networks demonstrate the practical applicability of the approach. © 2007 Springer Science+Business Media, LLC.","Biological applications; Compression; Inductive logic programming; Network mining; Probabilistic logic; Statistical relational learning; Theory revision","Data compression; Data mining; Optimization; Probability; Semantics; Statistical methods; Biological applications; Inductive logic programming; Network mining; Statistical relational learning; Theory revision; Probabilistic logics",2-s2.0-37849007683
"Pardede E., Rahayu J.W., Taniar D.","XML data update management in XML-enabled database",2008,"Journal of Computer and System Sciences",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37548999046&doi=10.1016%2fj.jcss.2007.04.008&partnerID=40&md5=f30c607beb4899a136e6b930862a481c","With the increasing demand for a proper and efficient XML data storage, XML-Enabled Database (XEnDB) has emerged as one of the popular solutions. It claims to combine the pros and limit the cons of the traditional Database Management Systems (DBMS) and Native XML Database (NXD). In this paper, we focus on XML data update management in XEnDB. Our aim is to preserve the conceptual semantic constraints and to avoid inconsistencies in XML data during update operations. In this current era when XML data interchange mostly occurs in a commercial setting, it is highly critical that data exchanged be correct at all times, and hence data integrity in XML data is paramount. To achieve our goal, we firstly classify different constraints in XML documents. Secondly, we transform these constraints into XML Schema with embedded SQL annotations. Thirdly, we propose a generic update methodology that utilizes the proposed schema. We then implement the method in one of the current XEnDB products. Since XEnDB has a Relational Model as the underlying data model, our update method uses the SQL/XML as a standard language. Finally, we also analyze the processing performance. © 2007 Elsevier Inc. All rights reserved.","Constraints; Schema; SQL/XML; XML data storage; XML update; XML-enabled database","Data processing; Data storage equipment; Database systems; Semantic Web; Constraints; Schema; XML data storage; XML update; XML-enabled database; XML",2-s2.0-37548999046
"Hunter L., Lu Z., Firby J., Baumgartner Jr. W.A., Johnson H.L., Ogren P.V., Cohen K.B.","OpenDMAP: An open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression",2008,"BMC Bioinformatics",89,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-41349107398&doi=10.1186%2f1471-2105-9-78&partnerID=40&md5=b37ba1c78439705cfb5bb9900c3e69a0","Background: Information extraction (IE) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge, particularly in areas where important factual information is published in a diverse literature. Here we report on the design, implementation and several evaluations of OpenDMAP, an ontology-driven, integrated concept analysis system. It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources, integrating diverse text processing applications, and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering. Results: OpenDMAP information extraction systems were produced for extracting protein transport assertions (transport), protein-protein interaction assertions (interaction) and assertions that a gene is expressed in a cell type (expression). Evaluations were performed on each system, resulting in F-scores ranging from .26 - .72 (precision .39 - .85, recall .16 - .85). Additionally, each of these systems was run over all abstracts in MEDLINE, producing a total of 72,460 transport instances, 265,795 interaction instances and 176,153 expression instances. Conclusion: OpenDMAP advances the performance standards for extracting protein-protein interaction predications from the full texts of biomedical research articles. Furthermore, this level of performance appears to generalize to other information extraction tasks, including extracting information about predicates of more than two arguments. The output of the information extraction system is always constructed from elements of an ontology, ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality. The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction. The open source OpenDMAP code library is freely available at http://bionlp.sourceforge.net/. © 2008 Hunter et al; licensee BioMed Central Ltd.",,"Biomedical research; Extracting information; Factual information; Information extraction systems; Performance standards; Processing applications; Protein interaction; Protein-protein interactions; Gene expression; Information retrieval; Information retrieval systems; Knowledge management; Knowledge representation; Proteins; Semantics; Text processing; Open systems; accuracy; analytical error; article; automation; bioinformatics; cell type; computer system; gene expression; information retrieval; medical research; MEDLINE; protein interaction; protein transport; scoring system; semantics; algorithm; attitude to health; biological model; cell function; computer program; information retrieval; methodology; natural language processing; physiology; protein analysis; publication; Algorithms; Cell Physiology; Gene Expression; Health Knowledge, Attitudes, Practice; Information Storage and Retrieval; Models, Biological; Natural Language Processing; Periodicals as Topic; Protein Interaction Mapping; Protein Transport; Software",2-s2.0-41349107398
"Burns G.A.P.C., Feng D., Hovy E.","Intelligent approaches to mining the primary research literature: Techniques, systems, and examples",2008,"Studies in Computational Intelligence",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349161601&doi=10.1007%2f978-3-540-75767-2_2&partnerID=40&md5=0b18ff77d817bc92866b9864b4f4cde0","In this chapter, we describe how creating knowledge bases from the primary biomedical literature is formally equivalent to the process of performing a literature review or a 'research synthesis'. We describe a principled approach to partitioning the research literature according to the different types of experiments performed by researchers and how knowledge engineering approaches must be carefully employed to model knowledge from different types of experiment. The main body of the chapter is concerned with the use of text mining approaches to populate knowledge representations for different types of experiment. We provide a detailed example from neuroscience (based on anatomical tract-tracing experiments) and provide a detailed description of the methodology used to perform the text mining itself (based on the Conditional Random Fields model). Finally, we present data from textmining experiments that illustrate the use of these methods in a real example. This chapter is designed to act as an introduction to the field of biomedical text-mining for computer scientists who are unfamiliar with the way that biomedical research uses the literature. © 2008 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-38349161601
"Lin M., Zhang Z.","Question-driven segmentation of lecture speech text: Towards intelligent E-learning systems",2008,"Journal of the American Society for Information Science and Technology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849165235&doi=10.1002%2fasi.20685&partnerID=40&md5=bc16d63e935430ea02471f69dab0ff22","Recently, lecture videos have been widely used in e-learning systems. Envisioning intelligent e-learning systems, this article addresses the challenge of information seeking in lecture videos by retrieving relevant video segments based on user queries, through dynamic segmentation of lecture speech text. In the proposed approach, shallow parsing such as part of-speech tagging and noun phrase chunking are used to parse both questions and Automated Speech Recognition (ASR) transcripts. A sliding-window algorithm is proposed to identify the start and ending boundaries of returned segments. Phonetic and partial matching is utilized to correct the errors from automated speech recognition and noun phrase chunking. Furthermore, extra knowledge such as lecture slides is used to facilitate the ASR transcript error correction. The approach also makes use of proximity to approximate the deep parsing and structure match between question and sentences in ASR transcripts. The experimental results showed that both phonetic and partial matching improved the segmentation performance, slides-based ASR transcript correction improves information coverage, and proximity is also effective in improving the overall performance.",,"Automated Speech Recognition (ASR); Lecture videos; User queries; E-learning; Information analysis; Pattern matching; Query languages; Speech recognition; Image segmentation",2-s2.0-38849165235
"Clegg A.B., Shepherd A.J.","Text mining",2008,"Methods in Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934435260&doi=10.1007%2f978-1-60327-429-6_25&partnerID=40&md5=cb669455d33657d3ee36b6a9c0299356","One of the fastest-growing fields in bioinformatics is text mining: the application of natural language processing techniques to problems of knowledge management and discovery, using large collections of biological or biomedical text such as MEDLINE. The techniques used in text mining range from the very simple (e.g., the inference of relationships between genes from frequent proximity in documents) to the complex and computationally intensive (e.g., the analysis of sentence structures with parsers in order to extract facts about protein -protein interactions from statements in the text). This chapter presents a general introduction to some of the key principles and challenges of natural language processing, and introduces some of the tools available to end-users and developers. A case study describes the construction and testing of a simple tool designed to tackle a task that is crucial to almost any application of text mining in bioinformatics -identifying gene/protein names in text and mapping them onto records in an external database. © 2008 Humana Press, a part of Springer Science+Business Media, LLC.","Information extraction; Information retrieval; Named entity recognition; Natural language processing; Parsing; Part-of-speech tagging; Text mining",,2-s2.0-84934435260
"King B., Reinold K.","Finding the Concept, Not Just the Word: A Librarian's Guide to Ontologies and Semantics",2008,"Finding the Concept, Not Just the Word: A Librarian's Guide to Ontologies and Semantics",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-66249112445&doi=10.1533%2f9781780631721&partnerID=40&md5=15295ce2903956c49f951d5ebb64bbf6","Aimed at students and professionals within Library and Information Services (LIS), this book is about the power and potential of ontologies to enhance the electronic search process. The book will compare search strategies and results in the current search environment and demonstrate how these could be transformed using ontologies and concept searching. Simple descriptions, visual representations, and examples of ontologies will bring a full understanding of how these concept maps are constructed to enhance retrieval through natural language queries. Readers will gain a sense of how ontologies are currently being used and how they could be applied in the future, encouraging them to think about how their own work and their users' search experiences could be enhanced by the creation of a customized ontology. © 2008 B.E. King and K. Reinold. All rights reserved.",,,2-s2.0-66249112445
"Kim N., Lee C.","Bioinformatics detection of alternative splicing",2008,"Methods in Molecular Biology",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934437741&doi=10.1007%2f978-1-60327-159-2_9&partnerID=40&md5=35a4ed7a08d8e1f2099a2bdba2abf2e0","In recent years, genome-wide detection of alternative splicing based on Expressed Sequence Tag (EST) sequence alignments with mRNA and genomic sequences has dramatically expanded our understanding of the role of alternative splicing in functional regulation. This chapter reviews the data, methodology, and technical challenges of these genome-wide analyses of alternative splicing, and briefly surveys some of the uses to which such alternative splicing databases have been put. For example, with proper alternative splicing database schema design, it is possible to query genome-wide for alternative splicing patterns that are specific to particular tissues, disease states (e.g., cancer), gender, or developmental stages. EST alignments can be used to estimate exon inclusion or exclusion level of alternatively spliced exons and evolutionary changes for various species can be inferred from exon inclusion level. Such databases can also help automate design of probes for RT-PCR and microarrays, enabling high throughput experimental measurement of alternative splicing. © 2008 Humana Press, a part of Springer Science+Business Media, LLC.","Alternative acceptor; Alternative donor; Alternative splicing; Cancer-specific; Exon exclusion; Exon inclusion; Exon skipping; Genome annotation; Intron retention; Microarray; RT-PCR; Tissue-specific",,2-s2.0-84934437741
"Martins A.L., Pinto H.S., Oliveira A.L.","Using grammatical inference techniques to learn ontologies that describe the structure of domain instances",2008,"Applied Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-40149109469&doi=10.1080%2f08839510701853309&partnerID=40&md5=c0a1eb64ef50c4efdca17ebb041cea57","Information produced by people usually has an implicit agreed-upon structure. However, this structure is not usually available to computer programs, where it could be used, for example, to aid in answering search queries. For example, when considering technical articles, one could ask for the occurrence of a keyword in a particular part of the article, such as the reference section. This implicit structure could be used, in the form of an ontology, to further the efforts of improving search in the semantic web. We propose a method to build ontologies encoding this structure information by the application of grammar inference techniques. This results in a semi-automatic approach to the inference of such ontologies. Our approach has two main components: (1) the inference of a grammatical description of the implicit structure of the supplied examples, and (2) the transformation of that description into an ontology. We present the application of the method to the inference of an ontology describing the structure of technical articles.",,"Automatic programming; Computer program listings; Data structures; Encoding (symbols); Inference engines; Ontology; Grammatical description; Grammatical inference techniques; Implicit structure; Computational grammars",2-s2.0-40149109469
"Imamura M., Takayama Y., Akiyoshi M., Komoda N.","An acquisition method on term knowledge from operating manuals for information equipments by using the structure of headline sentences",2008,"IEEJ Transactions on Electronics, Information and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-72549095501&doi=10.1541%2fieejeiss.128.1833&partnerID=40&md5=e8b8d6612177516401a137e1b62f6f7b","This paper proposes a method for automatically extracting term knowledge such as case relations and IS-A relations between words in the headline sentences of the operating manuals for information equipments. The proposed method acquires term knowledge by the following iterative processing: the case relation extraction using correspondence relations between the surface cases and the deep cases; the case and IS-A relation extraction using the compound word structures; the IS-A relation extraction using correspondence between the case structures in the hierarchical headline sentences. The distinctive feature of our method is to extract new case relations and IS-A relations by comparison and matching the case relations extracting from the super and sub headline sentences using the headline hierarchy. We have confirmed that the proposed method to achieve approximately 90% recall and precision for extracting case relations and IS-A relations from operating manuals of a car navigation system and a mobile phone. © 2008 The Institute of Electrical Engineers of Japan.","Case frame; Document structure; Linguistic knowledge acquisition; Operating manual","Knowledge acquisition; Linguistics; Mergers and acquisitions; Telecommunication equipment; Telephone systems; Car navigation systems; Case relations; Case structures; Distinctive features; Document structure; Iterative processing; Linguistic knowledge acquisition; Recall and precision; Relation extraction; Word structures; Feature extraction",2-s2.0-72549095501
"Robison R.","Google: A chronology of innovations, acquisitions, and growth",2008,"Journal of Library Administration",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011438647&doi=10.1300%2fJ111v46n03_02&partnerID=40&md5=68590a40b0ae0eacc981e489dee5a9d3","This article presents a chronology of major events, initiatives, and milestones of the search engine giant Google and commentary on their impact on libraries, higher education, advertising, and access to information. By placing Google's many milestones, initiatives, and acquisitions into a historical context, this article seeks to illustrate Google's aggressive growth strategy, highlight how rapidly Google has transformed the information landscape, and add to the discussion of how these moves continue to affect the world of libraries, higher education, and access to information in general. © 2008 Taylor and Francis Group, LLC. All Rights Reserved.","Chronology; Google; History; Strategy",,2-s2.0-85011438647
"Skiena S.S.","The algorithm design manual: Second edition",2008,"The Algorithm Design Manual: Second Edition",52,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892193314&doi=10.1007%2f978-1-84800-070-4&partnerID=40&md5=d48efb5b04b03f8f6662bba8d016e4fa","....The most comprehensive guide to designing practical and efficient algorithms!.... The Algorithm Design Manual, Second Edition ""...the book is an algorithm-implementation treasure trove, and putting all of these implementations in one place was no small feat. The list of implementations [and] extensive bibliography make the book an invaluable resource for everyone interested in the subject."" -ACM Computing Reviews ""It has all the right ingredients: rich contents, friendly, personal language, subtle humor, the right references, and a plethora of pointers to resources."" - P. Takis Metaxas, Wellesley College ""This is the most approachable book on algorithms I have."" - Megan Squire, Elon University, USA. This newly expanded and updated second edition of the best-selling classic continues to take the ""mystery"" out of designing algorithms, and analyzing their efficacy and efficiency. Expanding on the first edition, the book now serves as the primary textbook of choice for algorithm design courses while maintaining its status as the premier practical reference guide to algorithms for programmers, researchers, and students. The reader-friendly Algorithm Design Manual provides straightforward access to combinatorial algorithms technology, stressing design over analysis. The first part, Techniques, provides accessible instruction on methods for designing and analyzing computer algorithms. The second part, Resources, is intended for browsing and reference, and comprises the catalog of algorithmic resources, implementations and an extensive bibliography. NEW to the second edition: • Doubles the tutorial material and exercises over the first edition • Provides full online support for lecturers, and a completely updated and improved website component with lecture slides, audio and video • Contains a unique catalog identifying the 75 algorithmic problems that arise most often in practice, leading the reader down the right path to solve them • Includes several NEW ""war stories"" relating experiences from real-world applications • Provides up-to-date links leading to the very best algorithm implementations available in C, C++, and Java ADDITIONAL Learning Tools: • Exercises include ""job interview problems"" from major software companies • Highlighted take-home lesson boxes emphasize essential concepts • Provides comprehensive references to both survey articles and the primary literature • Exercises point to relevant programming contest challenge problems • Many algorithms presented with actual code (written in C) as well as pseudo-code • A full set of lecture slides and additional material available at www.algorist.com. Written by a well-known algorithms researcher who received the IEEE Computer Science and Engineering Teaching Award, this new edition of The Algorithm Design Manual is an essential learning tool for students needing a solid grounding in algorithms, as well as a special text/reference for professionals who need an authoritative and insightful guide. Professor Skiena is also author of the popular Springer text, Programming Challenges: The Programming Contest Training Manual. © Springer-Verlag London Limited 2008.",,,2-s2.0-84892193314
"Kundu S.R., Pal S., Schuba C.L., Das S.K.","Fast and scalable classification of structured data in the network",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249004272&partnerID=40&md5=99b714d1dc509379fc467c9afcdb8daf","For many network services, such as firewalling, load balancing, or cryptographic acceleration, data packets need to be classified (or filtered) before network appliances can apply any action processing on them. Typical actions are header manipulations, discarding packets, or tagging packets with additional information required for later processing. Structured data, such as XML, is independent from any particular presentation format and is an ideal information exchange format for a variety of heterogeneous sources. In this paper, we propose a new algorithm for fast and efficient classification of structured data in the network. In our approach, packet processing and classification is performed on structured payload data rather than only packet header information. Using a combination of hash functions, Bloom filter, and set intersection theory our algorithm builds a hierarchical and layered data element tree over the input grammar that requires logarithmic time and tractable space complexity. © IFIP International Federation for Information Processing 2007.",,"Computational complexity; Cryptography; Packet networks; Resource allocation; Scalability; XML; Complex networks; Computer system firewalls; Data handling; Hash functions; Network management; Next generation networks; Packet networks; Sensor networks; Trees (mathematics); Wireless ad hoc networks; Wireless sensor networks; Discarding packets; Firewalling; Header manipulations; Network services; Structured data; Classification (of information); Classification (of information); Heterogeneous sources; Information exchanges; Network appliances; Network services; Packet processing; Presentation formats; Set intersection; Space complexity",2-s2.0-37249004272
"Shi Z., Melli G., Wang Y., Liu Y., Gu B., Kashani M.M., Sarkar A., Popowich F.","Question answering summarization of multiple biomedical documents",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249046579&partnerID=40&md5=5890fe3fd8d9cc129fe6f470c4ced92d","In this paper we introduce a system that automatically summarizes multiple biomedical documents relevant to a question. The system extracts biomedical and general concepts by utilizing concept-level knowledge from domain-specific and domain-independent sources. Semantic role labeling, semantic subgraph-based sentence selection and automatic post-editing are involved in the process of finding the information need. Due to the absence of expert-written summaries of biomedical documents, we propose an approximate evaluation by taking MEDLINE abstracts as expert-written summaries. Evaluation results indicate that our system does help in answering questions and the automatically generated summaries are comparable to abstracts of biomedical articles, as evaluated using the ROUGE measure. © Springer-Verlag Berlin Heidelberg 2007.",,"Abstracting; Biomedical engineering; Graph theory; Knowledge engineering; Semantics; Biomedical documents; MEDLINE abstracts; Subgraph-based sentence selection; Data mining",2-s2.0-37249046579
"Roulland F., Kaplan A., Castellani S., Roux C., Grasso A., Pettersson K., O'Neill J.","Query reformulation and refinement using NLP-based sentence clustering",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149026026&partnerID=40&md5=da1f537316069e349d635eaa3e918146","We have developed an interactive query refinement tool that helps users search a knowledge base for solutions to problems with electronic equipment. The system is targeted towards non-technical users, who are often unable to formulate precise problem descriptions on their own. Two distinct but interrelated functionalities support the refinement of a vague, non-technical initial query into a more precise problem description: a synonymy mechanism that allows the system to match non-technical words in the query with corresponding technical terms in the knowledge base, and a novel refinement mechanism that helps the user build up successively longer and more precise problem descriptions starting from the seed of the initial query. A natural language parser is used both in the application of context-sensitive synonymy rules and the construction of the refinement tree. © Springer-Verlag Berlin Heidelberg 2007.",,"Cluster analysis; Computer aided software engineering; Interactive computer systems; Knowledge based systems; Search engines; User interfaces; Interactive query refinement tools; NLP-based sentence clustering; Non-technical users; Query reformulation; Query processing",2-s2.0-37149026026
"Rus V., Ravi S., Lintean M.C., McCarthy P.M.","Unsupervised method for parsing coordinated base noun phrases",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149034039&partnerID=40&md5=b690435a5bbedb95266eb6da10f0642c","Syntactic parsing is an important processing step for various language processing applications including Information Extraction, Question Answering, and Machine Translation. Parsing base Noun Phrases is one particular parsing issue that is not handled by current state-of-the-art syntactic parsers. In this paper we present research that investigates the base Noun Phrase parsing problem. We develop a base Noun Phrase parser based on several statistical models that provide promising results on a test set of 538 base Noun Phrases. The parameters of the models are estimated from the web in the form of web counts. This makes our method unsupervised with no training data being needed. © Springer-Verlag Berlin Heidelberg 2007.",,"Mathematical models; Natural language processing systems; Problem solving; Query processing; Syntactics; World Wide Web; Information Extraction; Machine Translation; Question Answering; Computational linguistics",2-s2.0-37149034039
"Mooney R.J.","Learning for semantic parsing",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149055335&partnerID=40&md5=994718f117e2e3d55a3b1fe932ef0cea","Semantic parsing is the task of mapping a natural language sentence into a complete, formal meaning representation. Over the past decade, we have developed a number of machine learning methods for inducing semantic parsers by training on a corpus of sentences paired with their meaning representations in a specified formal language. We have demonstrated these methods on the automated construction of naturallanguage interfaces to databases and robot command languages. This paper reviews our prior work on this topic and discusses directions for future research. © Springer-Verlag Berlin Heidelberg 2007.",,"Database systems; Formal languages; Interfaces (computer); Learning systems; Semantics; Robot command languages; Semantic parsing; Natural language processing systems",2-s2.0-37149055335
"Bajpai A., Sridhar S., Reddy H.M., Jesudasan R.A.","BRM-Parser: A tool for comprehensive analysis of BLAST and RepeatMasker results",2007,"In Silico Biology",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949139804&partnerID=40&md5=04a79c49a4ea68b65d068e6aea1610b9","BLAST and Repeat Masker Parser (BRM-Parser) is a service that provides users a unified platform for easy analysis of relatively large outputs of BLAST (Basic Local Alignment Search Tool) and RepeatMasker programs. BLAST Summary feature of BRM-Parser summarizes BLAST outputs, which can be filtered using user defined thresholds for hit length, percentage identity and E-value and can be sorted by query or subject coordinates and length of the hit. It also provides a tool that merges BLAST hits which satisfy user-defined criteria for hit length and gap between hits. The RepeatMasker Summary feature uses the RepeatMasker alignment as an input file and calculates the frequency and proportion of mutations in copies of repeat elements, as identified by the RepeatMasker. Both features can be run through a GUI or can be executed via command line using the standalone version. © 2007 IOS Press. All rights reserved.","BLAST; Filter; Merge; Mutation; Parse; RepeatMasker; Repeats","access to information; analytic method; article; computer interface; computer program; gene frequency; gene mutation; gene sequence; genetic database; information processing; mutation rate; sequence alignment; sequence analysis; Algorithms; Animals; Computational Biology; Computing Methodologies; Male; Mice; Repetitive Sequences, Nucleic Acid; Sequence Alignment; Software; Y Chromosome",2-s2.0-38949139804
"Anderson G., Asare S.D., Ayalew Y., Garg D., Gopolang B., Masizana-Katongo A., Mogotlhwane O., Mpoeleng D., Nyongesa H.O.","Towards a bilingual SMS parser for HIV and AIDS information retrieval in botswana",2007,"2007 International Conference on Information and Communication Technologies and Development, ICTD 2007",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650567909&doi=10.1109%2fICTD.2007.4937420&partnerID=40&md5=56e3cdfe30afbd96b1bddb8f267b3892","Integrated Health Information System through Mobile Telephony (IHISM) is a project carried out by Computer Science department at the University of Botswana. The main objective of the research project is to develop an integrated health care information service which will provide access to a variety of HIV AND AIDS related information for different users. The aim of this paper is to analyze the use of SMSs as an information access technology in a multilingual environment such as Botswana. The main focus is the analysis of the users' language in constructing SMSs to retrieve information from the IHISM information service. The results acquired in this work give a good lead towards designing the most appropriate bilingual natural language parser. The analysis, carried out on a sample of 1,600 SMS queries (800 English, 800 Setswana), would support the way parsing techniques could best hypothesize as accurately as possible the SMS question being asked.","HIV and AIDS; Multilingualism; Natural language parser; SMS technology","Botswana; Health information systems; HIV and AIDS; Information access; Mobile telephony; Multilingual environments; Multilingualism; Natural language parser; Natural languages; SMS technology; Health care; Information retrieval; Information services; Linguistics; Technology",2-s2.0-67650567909
"Daniel T.E., Mount S.N.I., Newman R.M., Gaura E.I.","Towards a trusted compiler for a query language for wireless sensor networks",2007,"Proceedings - ISoLA 2006: 2nd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956446924&doi=10.1109%2fISoLA.2006.24&partnerID=40&md5=deda0815e43b560f2fd545ca8e4e002a","Efficient and reliable information extraction in wireless sensor networks continues to be an area of abundant research. Current investigations follow two general themes: agent-based systems and query-based systems. The ASQue Sensor Query language (ASQue) was put forward by the Cogent Computing Research Centre as an applicative query language specifically designed to address the particular constraints imposed by sensor networks. A parser for the language and its formal specification were seen as an essential first step in being able to issue correct, purposeful queries to the network. A simple specification model that could be incrementally refined through to implementation was considered essential in accomplishing this goal. The purpose of this paper, therefore, is two-fold. First, to report the motivation for the formal specification of the ASQue parser, and second, to describe the construction of the parser from specification to implementation using the B-method. © 2007 IEEE.","Formal specification; Query language; Wireless sensor networks","Agent-based systems; B-method; Computing research; Formal Specification; Information Extraction; Query-based; Specification models; Wireless sensor; Formal methods; Linguistics; Program compilers; Query languages; Sensor networks; Specifications; Wireless sensor networks",2-s2.0-77956446924
"Cardoso N., Silva M.J.","Query expansion through geographical feature types",2007,"International Conference on Information and Knowledge Management, Proceedings",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954321471&doi=10.1145%2f1316948.1316963&partnerID=40&md5=31249561476a8b3cf6e34e6b3597d1da","This paper introduces a new approach for expansion of queries with geographical context. The proposed strategy is based on a query parser that captures geonames and spatial relationships, and maps geographical features and feature types into concepts of a geographical ontology. Different strategies for query expansion, according to the geographical restrictions given by the user, are compared. The proposed method allows a more versatile and focused expansion towards the geographical information need of the user.","Feature types; Features; Geographical IR; Geographical relevance; Ontologies; Query expansion; Query parsing","Feature types; Geographical features; Geographical information; Geographical ontology; New approaches; Query expansion; Spatial relationships; Information retrieval; Knowledge management; Ontology; Expansion",2-s2.0-77954321471
"Juárez-González A., Téllez-Valero A., Denicia-Carral C., Montes-y-Gómez M., Villaseñor-Pineda L.","Using machine learning and text mining in question answering",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049132855&partnerID=40&md5=0d8ca142e72bbdde91f2ad47b18d89d4","This paper describes a QA system centered in a full data-driven architecture. It applies machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing resources such as named entity classifiers, parsers and ontologies. Experimental results on the Spanish Question Answering task at CLEF 2006 show that the proposed architecture can be a practical solution for monolingual question answering by reaching a precision as high as 51%. © Springer-Verlag Berlin Heidelberg 2007.",,"Computer architecture; Data mining; Ontology; Query processing; Text processing; Complex language processing; Parsers; Question answering; Text mining; Learning systems",2-s2.0-38049132855
"Grau B.","Finding an answer to a question",2007,"Proceedings of the International Workshop on Research Issues in Digital Libraries, IWRIDL-2006, in Association with ACM SIGIR",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954477128&doi=10.1145%2f1364742.1364751&partnerID=40&md5=6c9136794fabe5228225dc170033be2c","The huge quantity of available electronic information leads to a growing need for users to have tools able to be precise and selective. These kinds of tools have to provide answers to requests quite rapidly without requiring the user to explore each document, to reformulate her request or to seek for the answer inside documents. From that viewpoint, finding an answer consists not only in finding relevant documents but also in extracting relevant parts. This leads us to express the question-answering problem in terms of an information retrieval problem that can be solved using natural language processing (NLP) approaches. In my talk, I will focus on defining what a ""good"" answer is, and how a system can find it. A good answer has to give the required piece of information. However, it is not sufficient; it also has both to be presented within its context of interpretation and to be justified in order to give a user means to evaluate if the answer fits her needs and is appropriate. One can view searching an answer to a question as a reformulation problem: according to what is asked, find one of the different linguistic expressions of the answer in all candidate sentences. Within this framework, interlingual question-answering can also be seen as another kind of linguistic variation. The answer phrasing can be considered as an affirmative reformulation of the question, partly or totally, which entails the definition of models that match with sentences containing the answer. According to the different approaches, the kinds of model and the matching criteria greatly differ. It can consist in building a structured representation that makes explicit the semantic relations between the concepts of the question and that is compared to a similar representation of sentences. As this approach requires a syntactic parser and a semantic knowledge base, which are not always available in all the languages, systems often apply a less formal approach based on a similarity measure between a passage and the question and answers are extracted from highest scored passages. Similarity involves different criteria: question terms and their linguistic variations in passages, syntactic proximity, answer type. We will see that, in such an approach, justifications can be envisioned by using text themselves, considered as depositories of semantic knowledge. I will focus on the approach the LIR group of LIMSI has taken for its monolingual and bilingual systems. Copyright 2007 ACM.","answer extraction; computational linguistic; natural language processing; passage retrieval; question answering","answer extraction; Bilingual systems; Electronic information; Formal approach; In-buildings; Information retrieval problems; Linguistic expressions; NAtural language processing; Passage retrieval; Question Answering; Relevant documents; Semantic knowledge; Semantic relations; Similarity measure; Syntactic parsers; Computational linguistics; Digital libraries; Knowledge based systems; Learning algorithms; Query languages; Semantics; Syntactics; Natural language processing systems",2-s2.0-77954477128
"Oliveira F., Wong F., Leong K.-S., Tong C.-K., Dong M.-C.","Query translation for cross-language information retrieval by parsing constraint synchronous grammar",2007,"Proceedings of the Sixth International Conference on Machine Learning and Cybernetics, ICMLC 2007",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049020529&doi=10.1109%2fICMLC.2007.4370846&partnerID=40&md5=73c1106045b3bb5e71cb270fe032ce42","With the availability of large amounts of multilingual documents, Cross-Language Information Retrieval (CLIR) has become an active research area in recent years. However, researchers often face with the problem of inherent ambiguities involved in natural languages. Moreover, this task is even more challenging for processing the Chinese language because word boundaries are not defined in the sentence. This paper presents a Chinese-Portuguese query translation for CLIR based on a Machine Translation (MT) system that parses Constraint Synchronous Grammar (CSG). Unlike traditional transfer-based MT architectures, this model only requires a set of CSG rules for modeling syntactic structures of two languages simultaneously to perform the translation. Moreover, CSG can be used to remove different levels of disambiguation as the parsing processes in order to generate a translation with quality. © 2007 IEEE.","Constraint synchronous grammar; Cross-language information retrieval; Machine translation","Formal languages; Learning systems; Logic programming; Query processing; Software architecture; Constraint synchronous grammar; Cross-language information retrieval; Machine translation; Natural language processing systems",2-s2.0-38049020529
"Zhang H., Zhou X., Yang Z., Wu X., Yu Z.","The research of an XML-based grid information modelling and retrieval system",2007,"3rd International Conference on Semantics, Knowledge, and Grid, SKG 2007",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50149087127&doi=10.1109%2fSKG.2007.61&partnerID=40&md5=522f1401e82b3512cf8b7c52bb36a063","To achieve the goal of grid computing, scalable and effective mechanisms for representing, managing, and retrieving diversiform grid information are required. This paper presents an XML-based Grid Information Management System (XGIMS), which implements grid information modelling and retrieval. With GLUE specification, an extensible schema service is proposed, which supports user-defined resource schemas. Different from lots of existing approaches, an index-based XML parser is used to build indexes of grid resources based on the model. Furthermore an index-based XPath engine is presented for efficient and flexible query. XGIMS has been applied to the Information Service in ChinaGrid and the experiment results proved its efficiency. © 2007 IEEE.",,"Grid computing; Information retrieval; Information science; Information services; Information theory; Management information systems; Markup languages; Semantics; XML; Effective mechanisms; Grid information; Grid resources; International conferences; Schemas; XML parser; Information management",2-s2.0-50149087127
"Groppe S., Groppe J., Kukulenz D., Linnemann V.","A SPARQL engine for streaming RDF data",2007,"Proceedings - International Conference on Signal Image Technologies and Internet Based Systems, SITIS 2007",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849128901&doi=10.1109%2fSITIS.2007.22&partnerID=40&md5=12637a3a19f5b509cf4ce1e2d2ceb95d","The basic data format of the Semantic Web is RDF. SPARQL, which has been developed by the W3C, is the upcoming standard for RDF query languages. Typical engines for processing SPARQL queries on RDF data first read all RDF data, may build indices of the complete read data and afterwards evaluate SPARQL queries. Such engines cannot operate on streaming RDF data. Streaming query engines operating on streams of data can (a) discard irrelevant input as early as possible, and thus save processing costs and space costs, (b) build indices only on those parts of the data, which are needed for the evaluation of the query, and (c) determine partial results of a query as early as possible, and thus evaluate queries more efficiently. We propose such a streaming SPARQL engine, which is the first streaming SPARQL engine to the best of our knowledge. ©2008 IEEE.",,"Engines; Information theory; Internet; Operating costs; Data formats; On streams; Partial results; Processing costs; Query engines; RDF datums; RDF query languages; SPARQL queries; Query languages",2-s2.0-57849128901
"Zhang W., Liu S., Yu C., Sun C., Liu F., Meng W.","Recognition and classification of noun pharses in queries for effective retrieval",2007,"International Conference on Information and Knowledge Management, Proceedings",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-63449138078&doi=10.1145%2f1321440.1321540&partnerID=40&md5=6e93e6221f4d719892f326840771320f","It has been shown that using phrases properly in the document retrieval leads to higher retrieval effectiveness. In this paper, we define four types of noun phrases and present an algorithm for recognizing these phrases in queries. The strengths of several existing tools are combined for phrase recognition. Our algorithm is tested using a set of 500 web queries from a query log, and a set of 238 TREC queries. Experimental results show that our algorithm yields high phrase recognition accuracy. We also use a baseline noun phrase recognition algorithm to recognize phrases from the TREC queries. A document retrieval experiment is conducted using the TREC queries (1) without any phrases, (2) with the phrases recognized from a baseline noun phrase recognition algorithm, and (3) with the phrases recognized from our algorithm respectively. The retrieval effectiveness of (3) is better than that of (2), which is better than that of (1). This demonstrates that utilizing phrases in queries does improve the retrieval effectiveness, and better noun phrase recognition yields higher retrieval performance. Copyright 2007 ACM ACM.","Complex phrase; Dictionary phrase; Feedback; Information retrieval; Noun phrases; Proper noun; Simple phrase; Verification","Complex phrase; Dictionary phrase; Noun phrases; Proper noun; Simple phrase; Information retrieval; Information services; Knowledge management; Algorithms",2-s2.0-63449138078
"Wong Y.W., Mooney R.J.","Learning synchronous grammars for semantic parsing with lambda calculus",2007,"ACL 2007 - Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics",112,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449083629&partnerID=40&md5=64d1a7c4bb1888a2e567c89904406363","This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with λ-operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the best-performing system so far in a database query domain. © 2007 Association for Computational Linguistics.",,"Database queries; Lambda calculus; Logical forms; Semantic parsing; Statistical machine translation; Synchronous grammars; Computational linguistics; Differentiation (calculus); Personnel training; Semantics",2-s2.0-56449083629
"Wang F., Agrawal G., Jin R., Piontkivska H.","SNPMiner: A domain-specific deep web mining tool",2007,"Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, BIBE",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649100510&doi=10.1109%2fBIBE.2007.4375564&partnerID=40&md5=7067c82ff4b04ed5be44a4a71849fa94","In this paper, we propose a novel query-oriented, mediator-based biological data querying tool, SNPMiner. The system searches and queries Single Nucleotide Polymorphisms (SNPs) data from eight widely used web accessible databases. The system provides a domain-specific search utility, which can access and collect data from the deep web. This is a web-based system, so any user can use the system by accessing our server from their own computers. The system includes three important components, which are the web server interface, the dynamic query planner, and the web page parser. The web server interface can provide end users a unified and friendly interface. The dynamic query planner can automatically schedule an efficient query order on all available databases according to user's query request. The web page parser analyzes the layout of HTML files and extracts desired data from those files. The final results of the query are organized in a tabular format, which can be reviewed by a biological researcher. ©2007 IEEE.",,"Bioinformatics; Database systems; Image retrieval; Markup languages; Nucleic acids; Web services; World Wide Web; Biological datums; Deep webs; Do-mains; Dynamic queries; End users; HTML files; Web pages; WEB servers; Web-based systems; Servers",2-s2.0-47649100510
"Prince V., Labadié A.","Text segmentation based on document understanding for information retrieval",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149074211&partnerID=40&md5=266661084ac5113b30a7bbaf1c644b02","Information retrieval needs to match relevant texts with a given query. Selecting appropriate parts is useful when documents are long, and only portions are interesting to the user. In this paper, we describe a method that extensively uses natural language techniques for text segmentation based on topic change detection. The method requires a NLP-parser and a semantic representation in Roget-based vectors. We have run the experiment on French documents, for which we have the appropriate tools, but the method could be transposed to any other language with the same requirements. The article sketches an overview of the NL understanding environment functionalities, and the algorithms related to our text segmentation method. An experiment in text segmentation is also presented and its result in an information retrieval task is shown. © Springer-Verlag Berlin Heidelberg 2007.",,"Algorithms; Natural language processing systems; Query processing; User interfaces; Vectors; Roget-based vectors; Text segmentation; Information retrieval systems",2-s2.0-38149074211
"Deruelle L., Basson H., Bouneffa M., Hattat J.","An eclipse platform extension for analysis and manipulation of multi-language software code",2007,"20th International Conference on Computer Applications in Industry and Engineering 2007, CAINE 2007",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883262847&partnerID=40&md5=da47db8ad82448df9415641d4c483f90","In this paper, we propose formal model and a plat- form, based on eclipse plugins, for analysis of multi- language software. These provide a graph representa- Tion of the software source codes, database schemas, resource files, integrated within formal models which are implemented in an eclipse platform. The models are based on graphs rewriting, and represent software components and their corresponding various relation- ships which are extracted from the source codes files of an eclipse project. The implementation uses javacc tool allowing to generate parsers based on grammars specifications files, which include features permitting to produce a graph representation of the software compo- nents. The JBoss Rule expert system performs rules on obtained graphs such as to deduce knowledge which is not provided by the current eclipse platform. Ob- Tained knowledge concerns relationships between soft- ware components such as database tables and their use by Java Query Statements. Furthermore, this knowl- edge is used by a software change propagation plugins in order to evaluate the impact of changed database ta- bles.","Change propagation; Eclipse plugins; Program transformation; Source code analysis","Change propagation; Database schemas; Eclipse plug-ins; Graph representation; Program transformations; Software component; Software source codes; Source code analysis; Computer applications; Computer programming languages; Database systems; Expert systems; Formal methods; Query processing",2-s2.0-84883262847
"Cai K., Chen C., Bu J., Qiu G., Huang P.","A multi-dependency language modeling approach to information retrieval",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549128642&partnerID=40&md5=29977336ec14cd7d3ab709bc69ffe8cf","This paper presents a multi-dependency language modeling approach to information retrieval. The approach extends the basic KL-divergence retrieval approach by introducing the hybrid dependency structure, which includes syntactic dependency, syntactic proximity dependency and co-occurrence dependency, to describe dependencies between terms. Term and dependency language models are constructed for both document and query. The relevant between a document and a query is then evaluated by using the KL-divergence between their corresponding models. The new dependency retrieval model has been compared with other traditional retrieval models. Experiment results indicate that it produces significant improvements in retrieval effectiveness. © Springer-Verlag Berlin Heidelberg 2007.","Information retrieval; Language model; Term dependency","Computer simulation; Data mining; Data structures; Query languages; Language models; Multi-dependency language modeling; Term dependency; Information retrieval",2-s2.0-38549128642
"Scherzinger S.","Bulk data in main memory-based XQuery evaluation",2007,"4th International Workshop on XQuery Implementation, Experience and Perspectives, XIME-P 2007, Co-located with ACM SIGMOD 2007",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954591047&doi=10.1145%2f1328158.1328159&partnerID=40&md5=5f4a5652828aea6920c0bed670bbcc09","XQuery processors that load the input into main memory suffer from huge memory demands. Yet for the evaluation of many queries, large parts of the input are actually irrelevant. In XML document projection, this data is recognized and not loaded in the first place. However, there are also queries where little can be gained by projection. We have observed that these queries tend to require large parts of the input only for generating output. This suggests that such ""bulk"" data may be stored and treated differently from data that is actually traversed in query evaluation. In this paper, we present a technique to recognize bulk data while loading XML documents for the evaluation of composition-free XQuery. Our approach is coupled with XML document projection, and utilizes a finite automaton that is expressly suited for matching path expressions. We show in an exploratory analysis that bulk data arises in practice, and discuss ongoing work along the line of bulk-bypassing in main memory-based XQuery engines. Copyright 2007 ACM.",,"Exploratory analysis; Large parts; Main memory; Path expressions; Query evaluation; XQuery evaluation; Markup languages; XML",2-s2.0-77954591047
"Van Wyk E., Krishnan L., Bodin D., Schwerdfeger A.","Attribute grammar-based language extensions for java",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",54,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149076049&partnerID=40&md5=606019447a308e094819be662c366d2a","This paper describes the ableJ extensible language framework, a tool that allows one to create new domain-adapted languages by importing domain-specific language extensions into an extensible implementation of Java 1.4. Language extensions may define the syntax, semantic analysis, and optimizations of new language constructs. Java and the language extensions are specified as higher-order attribute grammars. We describe several language extensions and their implementation in the framework. For example, one extension embeds the SQL database query language into Java and statically checks for syntax and type errors in SQL queries. The tool supports the modular specification of composable language extensions so that programmers can import into Java the unique set of extensions that they desire. When extensions follow certain restrictions, they can be composed without requiring any implementation-level knowledge of the language extensions. The tools automatically compose the selected extensions and the Java host language specification. © Springer-Verlag Berlin Heidelberg 2007.",,"Computational grammars; Optimization; Query languages; Semantics; Java host language specifications; Language extensions; SQL database; Java programming language",2-s2.0-38149076049
"Sonntag D., Engel R., Herzog G., Pfalzgraf A., Pfleger N., Romanelli M., Reithinger N.","SmartWeb handheld - Multimodal interaction with ontological knowledge bases and semantic web services",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-49949090696&doi=10.1007%2f978-3-540-72348-6_14&partnerID=40&md5=3aa0e1d8b2d2c6b5a64623ec138fe404","SmartWeb aims to provide intuitive multimodal access to a rich selection of Web-based information services. We report on the current prototype with a smartphone client interface to the Semantic Web. An advanced ontology-based representation of facts and media structures serves as the central description for rich media content. Underlying content is accessed through conventional web service middleware to connect the ontological knowledge base and an intelligent web service composition module for external web services, which is able to translate between ordinary XML-based data structures and explicit semantic representations for user queries and system responses. The presentation module renders the media content and the results generated from the services and provides a detailed description of the content and its layout to the fusion module. The user is then able to employ multiple modalities, like speech and gestures, to interact with the presented multimedia material in a multimodal way. © 2007 Springer-Verlag Berlin Heidelberg.",,"Artificial intelligence; Computer software; File organization; Information services; Information theory; Knowledge based systems; Markup languages; Middleware; Ontology; Semantic Web; Semantics; Web services; World Wide Web; Client interfaces; External-; Handheld; Human computing; Hyderabad , India; Intelligent web; International workshops; Joint conference; Media content; Multi-modal; Multi-Modal Interactions; Multimodal access; Multiple modalities; Ontological knowledge; Ontology-based representation; Semantic representations; Semantic Web Services; Smart phones; System responses; User queries; Web-based information; Data structures",2-s2.0-49949090696
"Ruiz A.T., Puşcaşu G., Monteagudo L.M., Beviá R.I., Boró E.S.","University of Alicante at WiQA 2006",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049098593&partnerID=40&md5=a0bd76cf2a4373155f8999155dc1a89d","This paper presents the participation of University of Alicante at the WiQA pilot task organized as part of the CLEF 2006 campaign. For a given set of topics, this task presupposes the discovery of important novel information distributed across different Wikipedia entries. The approach we adopted for solving this task uses Information Retrieval, query expansion by feedback, novelty re-ranking, as well as temporal ordering. Our system has participated both in the Spanish and English monolingual tasks. For each of the two participations the results are promising because, by employing a language independent approach, we obtain scores above the average. Moreover, in the case of Spanish, our result is very close to the best achieved score. Apart from introducing our system, the present paper also provides an in-depth result analysis, and proposes future lines of research, as well as follow-up experiments. © Springer-Verlag Berlin Heidelberg 2007.","Information retrieval","Decision support systems; Information retrieval; Natural language processing systems; Query processing; English monolingual tasks; Language independent approach; Distributed computer systems",2-s2.0-38049098593
"Bos J., Nissim M.","Answer translation: An alternative approach to cross-lingual question answering",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049158333&partnerID=40&md5=b7ea347a187fc0f6dec235e996860ce9","We approach cross-lingual question answering by using a mono-lingual QA system for the source language and by translating resulting answers into the target language. As far as we are aware, this is the first cross-lingual QA system in the history of CLEF that uses this method - almost without exception, cross-lingual QA systems use translation of the question or query terms instead. We demonstrate the feasibility of our alternative approach by using a mono-lingual QA system for English, and translating answers and finding appropriate documents in Italian and Dutch. For factoid and definition questions, we achieve overall accuracy scores ranging from 13% (EN→NL) to 17% (EN→IT) and lenient accuracy figures from 19% (EN→NL) to 25% (EN→IT). The advantage of this strategy to cross-lingual QA is that translation of answers is easier than translating questions - the disadvantage is that answers might be missing from the source corpus and additional effort is required for finding supporting documents of the target language. © Springer-Verlag Berlin Heidelberg 2007.",,"Artificial intelligence; Formal languages; Information retrieval; Query processing; Answer translation; Lenient accuracy; Translation (languages)",2-s2.0-38049158333
"Maisonnasse L., Chevallet J.P., Berrut C.","Incomplete and fuzzy conceptual graphs to automatically index medical reports",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149036014&partnerID=40&md5=ab43dac372ed2e8caad211d4824fd890","Most of Information Retrieval (IR) systems are still based on bag of word paradigm. This is a strong limitation if one needs high precision answers. For example, in restricted domain, like medicine, user builds short and precise query, like ""Show me chest CT images with emphysema."", and expects from the system precise answers. In such a case, the use of natural language processing to model document content is the only way to improve IR precision. This paper presents a model for text IR that index documents with Fuzzy Conceptual Graphs (FCG). Building automatically a complete and relevant conceptual structure is known to be a difficult task. To overcome this problem and keeping automatic graph building, we promote the use of incomplete FCG. We show how to deal with this incompleteness by using confidence. This confidence is attached to concepts and conceptual relations. As we use FCG as index, the matching process is based on a fuzzy graph matching. Finally, our experiments show that this outperforms classical word based indexing. © Springer-Verlag Berlin Heidelberg 2007.",,"Fuzzy Conceptual Graphs (FCG); Fuzzy graph matching; Graph theory; Indexing (of information); Information retrieval; Natural language processing systems; Query processing; Fuzzy logic",2-s2.0-38149036014
"Hirzel M., Grimm R.","Jeannie: Granting java native interface developers their wishes",2007,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149088252&doi=10.1145%2f1297027.1297030&partnerID=40&md5=561c241e1e3a034abc9a6f19048346b3","Higher-level languages interface with lower-level languages such as C to access platform functionality, reuse legacy libraries, or improve performance. This raises the issue of how to best integrate different languages while also reconciling productivity, safety, portability, and efficiency. This paper presents Jeannie, a new language design for integrating Java with C. In Jeannie, both Java and C code are nested within each other in the same file and compile down to JNI, the Java platform's standard foreign function interface. By combining the two languages' syntax and semantics, Jeannie eliminates verbose boiler-plate code, enables static error detection across the language boundary, and simplifies dynamic resource management. We describe the Jeannie language and its compiler, while also highlighting lessons from composing two mature programming languages. Copyright © 2007 ACM.","C; Foreign function interface; Java; JNI; Modular syntax; Programming language composition; Rats; Xtc","Dynamic resource management; Higher-level languages; international conferences; Java native interface (JNI); Java platforms; Language boundary (LB); language designs; Languages (traditional); Lower level languages; Object-oriented programming; Static error detection; C; Foreign function interface; Java; JNI; Modular syntax; Programming language composition; Xtc; Codes (standards); Codes (symbols); Computational linguistics; Computer programming; Computer programming languages; Computer software; Computer systems programming; Error detection; Information management; Information theory; Java programming language; Knowledge management; Linguistics; Management; Neodymium; Product design; Query languages; Resource allocation; Standards; C (programming language); Rats; Syntactics; Object oriented programming",2-s2.0-42149088252
"Puchol-Blasco M., Saquete E., Martínez-Barco P.","Multilingual extension of temporal expression recognition using parallel corpora",2007,"Proceedings of the International Workshop on Temporal Representation and Reasoning",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349092690&doi=10.1109%2fTIME.2007.54&partnerID=40&md5=c9f5fef19344e546c377bc1e39d34526","This paper presents the automatic extension of TERSEO to other languages, a knowledge-based system for the recognition and normalization of temporal expressions, originally developed for Spanish. TERSEO was extended to English and Italian through the automatic translation of the temporal expressions, and it was presented in previous works (see Saquete et al. [12]), but a new methodology has been designed with the purpose of obtaining better results in this issue. This new methodology is based on the use of parallel corpora for extending the TERSEO temporal model to other languages. In this case, two different methods have been tested: (1) automatic translation of TERSEO patterns to other languages and (2) automatic corpora annotation in the target side of parallel corpora. The main idea is focused on annotating the Spanish side of a parallel corpora, projecting the analysis to the second language, and then obtaining new TERSEO patterns (1) and new annotated corpus (2). The set of new patterns will be used to improve the current TERSEO language independent modules. Whereas the new annotated corpus will be used to train a ML system. This system will annotate new temporal expressions in the new language. © 2007 IEEE.",,"Knowledge based systems; Linguistics; Query languages; Syntactics; Translation (languages); Automatic translations; Corpora (CO); International symposium; Knowledge based systems; Languages (traditional); ML System (CO); Parallel corpora; Second language; Spanish; Temporal expression (TE); Temporal modeling; Two different methods; Software agents",2-s2.0-47349092690
"Orǎsan C., Puşcaşu G.","A high precision information retrieval method for WiQA",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049101187&partnerID=40&md5=27c26eecf45fad351c506c09cc717aaa","This paper presents Wolverhampton University's participation in the WiQA competition. The method chosen for this task combines a high precision, but low recall information retrieval approach with a greedy sentence ranking algorithm. The high precision retrieval is ensured by querying the search engine with the exact topic, in this way obtaining only sentences which contain the topic. In one of the runs, the set of retrieved sentences is expanded using coreferential relations between sentences. The greedy algorithm used for ranking selects one sentence at a time, always the one which adds most information to the set of sentences without repeating the existing information too much. The evaluation revealed that it achieves a performance similar to other systems participating in the competition and that the run which uses coreference obtains the highest MRR score among all the participants. © Springer-Verlag Berlin Heidelberg 2007.",,"Algorithms; Information theory; Precision engineering; Query processing; Greedy algorithms; WiQA competition; Information retrieval",2-s2.0-38049101187
"Pérez-Coutiño M., Montes-y-Gómez M., López-López A., Villaseñor-Pineda L., Pancardo-Rodríguez A.","Applying dependency trees and term density for answer selection reinforcement",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049158326&partnerID=40&md5=750ae30037b435009a4d6253820fab45","This paper describes the experiments performed for the QA@CLEF-2006 within the joint participation of the eLing Division at VEng and the Language Technologies Laboratory at INAOE. The aim of these experiments was to observe and quantify the improvements in the final step of the Question Answering prototype when some syntactic features were included into the decision process. In order to reach this goal, a shallow approach to answer ranking based on the term density measure has been integrated into the weighting schema. This approach has shown an interesting improvement against the same prototype without this module. The paper discusses the results achieved, the conclusions and further directions within this research. © Springer-Verlag Berlin Heidelberg 2007.",,"Artificial intelligence; Decision making; Query processing; Software prototyping; Answer ranking; Answer selection reinforcement; Decision processes; Trees (mathematics)",2-s2.0-38049158326
"Thomas S., Williams L.","Using automated fix generation to secure SQL statements",2007,"Proceedings - ICSE 2007 Workshops: Third International Workshop on Software Engineering for Secure Systems, SESS'07",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549128867&doi=10.1109%2fSESS.2007.12&partnerID=40&md5=889692e671305d7bcacea86537341b1e","Since 2002, over 10% of total cyber vulnerabilities were SQL injection vulnerabilities. Since most developers are not experienced software security practitioners, a solution for correctly fixing SQL injection vulnerabilities that does not require security expertise is desirable. In this paper, we propose an automated method for removing SQL injection vulnerabilities from Java code by converting plain text SQL statements into prepared statements. Prepared statements restrict the way that input can affect the execution of the statement. An automated solution allows developers to remove SQL injection vulnerabilities by replacing vulnerable code with generated secure code. In a formative case study, we tested our automated fix generation algorithm on five toy Java programs which contained seeded SQL injection vulnerabilities and a set of object traceability issues. The results of our case study show that our technique was able remove SQL injection vulnerabilities in five different statement configurations. © 2007 IEEE.",,"Algorithms; Codes (symbols); Java programming language; Security of data; Object traceability; Software security; Query languages",2-s2.0-38549128867
"Dubois M., Aboulhamid E.M., Rousseau F.","Acceleration for heterogeneous systems cosimulation",2007,"Proceedings of the IEEE International Conference on Electronics, Circuits, and Systems",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50649119901&doi=10.1109%2fICECS.2007.4510988&partnerID=40&md5=09183d4f72cdaa7b61a1e8e999fda117","Heterogeneous systems simulation requires executing models using different simulators. One of the main problems consists in synchronizing all the kernels together, which significantly increases the simulation time. In this work we are interested in two currently prevailing abstractions levels: Transaction Level Modeling (TLM) and Register Transfer Level (RTL). They both require different semantics for simulating model. Acceleration for both TLM and RTL simulation models can be achieved by using transformations to build a new efficient simulation model respecting the semantics of the initial multi-language model. Our objective is to create an internal representation from descriptions in different languages, and then generate a more efficient model. To highlight the effectiveness of our approach, we show how to cosimulate ESys.Net and SystemC models with a performance gain over a shared memory cosimulation environment. ©2007 IEEE.",,"Computational linguistics; Electronic equipment; Information theory; Linguistics; Query languages; Semantics; Co-simulation; Heterogeneous systems; International conferences; Simulation modelling; Computer simulation languages",2-s2.0-50649119901
"Baral C., Dzifcak J., Tari L.","Towards overcoming the knowledge acquisition bottleneck in answer set prolog applications: Embracing natural language inputs",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149139604&partnerID=40&md5=40dba95f11e761c202f62c06d37ac9a1","Answer set Prolog, or AnsProlog in short, is one of the leading knowledge representation (KR) languages with a large body of theoretical and building block results, several implementations and reasoning and declarative problem solving applications. But it shares the problem associated with knowledge acquisition with all other KR languages; most knowledge is entered manually by people and that is a bottleneck. Recent advances in natural language processing have led to some systems that convert natural language sentences to a logical form. Although these systems are in their infancy, they suggest a direction to overcome the above mentioned knowledge acquisition bottleneck. In this paper we discuss some recent work by us on developing applications that process logical forms of natural language text and use the processed result together with AnsProlog rules to do reasoning and problem solving. © Springer-Verlag Berlin Heidelberg 2007.",,"(e ,3e) process; Answer sets; Building blocks; Heidelberg (CO); international conferences; Knowledge Representation (KR); Languages (traditional); Logical forms; Natural Language Processing (NLP); Natural language texts; natural languages; Springer (CO); Artificial intelligence; Computational linguistics; Computer programming; Decision making; Fuzzy logic; Information theory; Knowledge acquisition; Knowledge representation; Krypton; Linguistics; Logic programming; Mergers and acquisitions; Problem solving; PROLOG (programming language); Query languages; Natural language processing systems",2-s2.0-38149139604
"Liu C., Wang H., McClean S., Liu J., Wu S.","Syntactic information retrieval",2007,"Proceedings - 2007 IEEE International Conference on Granular Computing, GrC 2007",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749132338&doi=10.1109%2fGRC.2007.4403191&partnerID=40&md5=878c28d3f5ba60e0aab3ab2cbbb9581b","Natural language processing (NLP) techniques are believed to have the potential to aid information retrieval (IR) in terms of retrieval accuracy. In this paper we report a proof of concept study on a new approach to NLP-based IR that we propose. Documents and queries are represented as syntactic parse trees, which are generated by a natural language parser. Based on this tree structured representation of documents and queries, the matching between a document and a query is executed on their tree representations, with tree comparison as the key operation. An IR experiment is designed to test if this approach is feasible. Experimental results show that this approach is promising and has the potential to outperform the standard bag of words approach to Information retrieval, especially in response to long queries. © 2007 IEEE.","Information retrieval; Natural language processing; Syntactic parse tree; Tree comparison","Artificial intelligence; Computational linguistics; Information analysis; Information retrieval; Information science; Information services; Linguistics; Nonlinear programming; Search engines; Standards; Syntactics; (+ mod 2N) operation; Bag of words; Experimental results; Granular computing (GrC); international conferences; Natural Language Processing (NLP); natural languages; new approaches; Parse trees; Proof-of-concept (POC); Retrieval (MIR); retrieval accuracy; Syntactic information; Tree-structured representation; Natural language processing systems",2-s2.0-46749132338
"Farfán F., Hristidis V., Rangaswami R.","Beyond lazy XML parsing",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049051137&partnerID=40&md5=60b0d813bebd606e998b0e02ceeb38b0","XML has become the standard format for data representation and exchange in domains ranging from Web to desktop applications. However, wide adoption of XML is hindered by inefficient document-parsing methods. Recent work on lazy parsing is a major step towards alleviating this problem. However, lazy parsers must still read the entire XML document in order to extract the overall document structure, due to the lack of internal navigation pointers inside XML documents. Further, these parsers must load and parse the entire virtual document tree into memory during XML query processing. These overheads significantly degrade the performance of navigation operations. We have developed a framework for efficient XML parsing based on the idea of placing internal physical pointers within the document, which allows skipping large portions of the document during parsing. The internal pointers are generated in a way that optimizes parsing for common navigation patterns. A double-Lazy Parser (2LP) is then used to parse the document that exploits the internal pointers. To create the internal pointers, we use constructs supported by the current W3C XML standard. We study our pointer generation and parsing algorithms both theoretically and experimentally, and show that they perform considerably better than existing approaches. © Springer-Verlag Berlin Heidelberg 2007.","Deferred expansion; Document object model; Double lazy parsing; XML; XPath","Deferred expansion; Document object models; Double lazy parsing; XPath; Electronic data interchange; Knowledge representation; Mathematical models; Personal computers; World Wide Web; XML",2-s2.0-38049051137
"Van Wyk E.R., Schwerdfeger A.C.","Context-aware scanning for parsing extensible languages",2007,"GPCE'07 - Proceedings of the Sixth International Conference on Generative Programming and Component Engineering",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849168071&doi=10.1145%2f1289971.1289983&partnerID=40&md5=e9218645bc008ddfa61afe06bc750f70","This paper introduces new parsing and context-aware scanning algorithms in which the scanner uses contextual information to disambiguate lexical syntax. The parser uses a slightly modified LR-style algorithm that passes to the scanner the set of valid symbols that the scanner may return at that point in parsing. This set is those terminals whose entries in the parse table for the current parse state are shift, reduce, or accept, but not error. The scanner then only returns tokens in this set. An analysis is given that can statically verify that the scanner will never return more than one token for a single input. Context-aware scanning is especially useful when parsing and scanning extensible languages in which domain specific languages can be embedded. It has been used in extensible versions of Java 1.4 and ANSI C. We illustrate this approach with a declarative specification of a subset of Java and extensions that embed SQL queries and Boolean expression tables into Java. Copyright © 2007 ACM.","Context-aware scanning; Extensible languages","Algorithms; Boolean algebra; Embedded systems; Java programming language; Context-aware scanning; Extensible languages; Scanning algorithms; XML",2-s2.0-38849168071
"Purver M., Dowding J., Niekrasz J., Ehlen P., Noorbaloochi S., Peters S.","Detecting and summarizing action items in multi-party dialogue",2007,"Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue",35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857768632&partnerID=40&md5=e17e696dfaf6e5c0362cf8155fa22b5d","This paper addresses the problem of identifying action items discussed in open-domain conversational speech, and does so in two stages: firstly, detecting the subdialogues in which action items are proposed, discussed and committed to; and secondly, extracting the phrases that accurately capture or summarize the tasks they involve. While the detection problem is hard, we show that we can improve accuracy by taking account of dialogue structure. We then describe a semantic parser that identifies potential summarizing phrases, and show that for some task properties these can be more informative than plain utterance transcriptions. © 2007 Association for Computational Linguistics.",,"Detection problems; Multi-party dialogues; Two stage; Semantics",2-s2.0-84857768632
"Lee B.-S., Hwang B.-Y.","X-binder: Path combining system of XML documents based on RDBMS",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-39649090954&partnerID=40&md5=5f7fb1cba34da5afb4385276657ab1a2","With the increasing use of XML, considerable research is being conducted on the XML document management systems for more efficient storage and searching of XML documents. Depending on the base systems, these researches can be classified into object-oriented DBMS (OODBMS) and relational DBMS (RDBMS). OODBMS-based systems are better suited to reflect the structure of XML-documents than RDBMS-based ones. However, using an XML parser to map the contents of documents to relational tables is a better way to construct a stable and effective XML document management system. The proposed X-Binder system uses an RDBMS-based inverted index; this guarantees high searching speed but wastes considerable storage space. To avoid this, the proposed system incorporates a path combining module agent that combines paths with sibling relations, and stores them in a single row. Performance evaluation revealed that the proposed system reduces storage wastage and search time. © Springer-Verlag Berlin Heidelberg 2007.",,"Performance evaluation; RDBMS; XML document management systems; Decision tables; Information management; Information systems; Relational database systems; XML",2-s2.0-39649090954
"Merlet J.-P.","A local motion planner for closed-loop robots",2007,"IEEE International Conference on Intelligent Robots and Systems",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51349125756&doi=10.1109%2fIROS.2007.4398984&partnerID=40&md5=5314276151c71db0051447497f32c979","Global motion planners have been proposed forclosed-loop robot based on the same paradigm than has been proposed for serial chains: a sparse representation of the configuration space of the robot is constructed as a set of nodes and a motion planning query consists simply in connecting the start and goal points through an appropriate set of nodes (usually minimizing the length of the trajectory). But such motion planner should be complemented by a local motion planner that addresses the following issues: 1) ensure that two successive nodes belong to the same robot kinematic branch (otherwise connecting these nodes will require to disassemble the robot) 2) verify that all poses between nodes satisfy the robot constraints (if possible taking into account the uncertainties in the robot modeling) 3) eventually try to shorten the trajectory length We present such a local motion planner that addresses all three issues and illustrates its use on a Gough parallel robot. ©2007 IEEE.","Interval analysis; Motion planning; Parallel robots","Intelligent robots; Intelligent systems; Motion planning; Planning; Trajectories; Wave functions; Interval analysis; Local motions; Parallel robots; Robots",2-s2.0-51349125756
"Sarawagi S.","Information extraction",2007,"Foundations and Trends in Databases",301,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868288681&doi=10.1561%2f1500000003&partnerID=40&md5=7e0097f31343581913c68d7e17950d29","The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem. © 2008 S. Sarawagi.",,"Automatic extraction; Fact-finding; Information Extraction; Named entities; NAtural language processing; New applications; News articles; On-line access; Scientific publications; Structure extraction; Structured database; Unstructured data; Computational linguistics; Database systems; Learning algorithms; Natural language processing systems; Semantics; Information retrieval",2-s2.0-84868288681
"Erdogmus M., Sezerman O.U.","Application of automatic mutation-gene pair extraction to diseases",2007,"Journal of Bioinformatics and Computational Biology",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049005041&doi=10.1142%2fS021972000700317X&partnerID=40&md5=99c4628dca8122714caaf221c0704f55","To have a better understanding of the mechanisms of disease development, knowledge of mutations and the genes on which the mutations occur is of crucial importance. Information on disease-related mutations can be accessed through public databases or biomedical literature sources. However, information retrieval from such resources can be problematic because of two reasons: manually created databases are usually incomplete and not up to date, and reading through a vast amount of publicly available biomedical documents is very time-consuming. In this paper, we describe an automated system, MuGeX (Mutation Gene eXtractor), that automatically extracts mutation-gene pairs from Medline abstracts for a disease query. Our system is tested on a corpus that consists of 231 Medline abstracts. While recall for mutation detection alone is 85.9%, precision is 95.9%. For extraction of mutation-gene pairs, we focus on Alzheimer's disease. The recall for mutation-gene pair identification is estimated at 91.3%, and precision is estimated at 88.9%. With automatic extraction techniques, MuGeX overcomes the problems of information retrieval from public resources and reduces the time required to access relevant information, while preserving the accuracy of retrieved information. © 2007 Imperial College Press.","Disease; Gene; Information extraction; Mutation","accuracy; Alzheimer disease; amino acid sequence; article; computer system; data base; gene identification; gene mutation; information retrieval; Internet; medical informatics; MEDLINE; Mutation Gene Extractor; protein analysis; protein expression; protein interaction; Abstracting and Indexing as Topic; Algorithms; Alzheimer Disease; Bayes Theorem; Computational Biology; Databases, Genetic; Disease; Humans; Information Storage and Retrieval; MEDLINE; Mutation; Software",2-s2.0-38049005041
"Tomanek K., Wermter J., Hahn U.","An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data",2007,"EMNLP-CoNLL 2007 - Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",35,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649102769&partnerID=40&md5=bb8db653c2bbc6e6002327240e2e79c9","We consider the impact Active Learning (AL) has on effective and efficient text corpus annotation, and report on reduction rates for annotation efforts ranging up until 72%. We also address the issue whether a corpus annotated by means of AL - using a particular classifier and a particular feature set - can be re-used to train classifiers different from the ones employed by AL, supplying alternative feature sets as well. We, finally, report on our experience with the AL paradigm under real-world conditions, i.e., the annotation of large-scale document corpora for the life sciences. © 2007 Association for Computational Linguistics.",,"Active learning; Feature sets; Life-sciences; Reduction rate; Text corpora; Active Learning; Computational linguistics; Reusability; Software agents; Natural language processing systems",2-s2.0-56649102769
"Zettlemoyer L.S., Collins M.","Online learning of relaxed CCG grammars for parsing to logical form",2007,"EMNLP-CoNLL 2007 - Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",133,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053356596&partnerID=40&md5=a4422c5b6d140fc5808454f227210987","We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar-for example allowing flexible word order, or insertion of lexical items- with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006). © 2007 Association for Computational Linguistics.",,"Combinators; Combinatory categorial grammar; F-measure; Lambda-calculus; Lexical items; Logical forms; On-line algorithms; Online learning; Semantic analysis; Word orders; Algorithms; Calculations; Computational linguistics; Differentiation (calculus); Formal languages; Semantics; Natural language processing systems",2-s2.0-80053356596
"Ling M.H.T., Lefevre C., Nicholas K.R., Lin F.","Reconstruction of protein-protein interaction pathways by mining subject-verb-objects intermediates",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349003165&partnerID=40&md5=12d4e5d2cb76b6f0903e5244f11ecd3f","The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was genetically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting proteinprotein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks. © Springer-Verlag Berlin Heidelberg 2007.","Biomedical literature analysis; Monty lingua; Protein-protein interaction","Biological systems; Data acquisition; Gene expression; Information analysis; Biomedical literature analysis; Data extraction techniques; Monty lingua; Protein-protein interaction; Proteins",2-s2.0-38349003165
"Müller M., Markó K., Daumke P., Paetzold J., Roesner A., Klar R.","Biomedical data mining in clinical routine: Expanding the impact of hospital information systems",2007,"Studies in Health Technology and Informatics",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-44349138545&partnerID=40&md5=35c3f4d838605693bb6388c6bd663819","In this paper we want to describe how the promising technology of biomedical data mining can improve the use of hospital information systems: a large set of unstructured, narrative clinical data from a dermatological university hospital like discharge letters or other dermatological reports were processed through a morpho-semantic text retrieval engine ('MorphoSaurus') and integrated with other clinical data using a web-based interface and brought into daily clinical routine. The user evaluation showed a very high user acceptance-this system seems to meet the clinicians' requirements for a vertical data mining in the electronic patient records. What emerges is the need for integration of biomedical data mining into hospital information systems for clinical, scientific, educational and economic reasons. © 2007 The authors. All rights reserved.","computerized; hospital information systems; information storage and retrieval; medical records systems; natural language processing",,2-s2.0-44349138545
"Budi I., Bressan S.","Application of association rules mining to Named Entity Recognition and co-reference resolution for the Indonesian language",2007,"International Journal of Business Intelligence and Data Mining",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649098556&doi=10.1504%2fIJBIDM.2007.016382&partnerID=40&md5=23a5acb73dd7ae45f9aff0bfc63f5634","In this paper, we propose a new method, association rules mining for Named Entity Recognition (NER) and co-reference resolution. The method uses several morphological and lexical features such as Pronoun Class (PC) and Name Class (NC), String Similarity (SP) and Position (P) in the text, into a vector of attributes. Applied to a corpus of newspaper in the Indonesian language, the method outperforms state-of-the-art maximum entropy method in name entity recognition and is comparable with state-of-the-art machine learning methods, decision tree, for co-reference resolution. © 2007, Inderscience Publishers.","Association rules; Co-reference resolution; Entity equivalence; Named Entity Recognition; NER",,2-s2.0-48649098556
[No author name available],"Chapter 8 Research directions of L-systems",2007,"Mathematics in Science and Engineering",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-40649123028&doi=10.1016%2fS0076-5392%2807%2980110-X&partnerID=40&md5=751bff7b93e0c2ef30971723b7f042c4","Fractal images generated using L-System concept, is relatively new and has been proved challenging. The field of research is far from being exhausted since there are many directions that have not yet been fully investigated (e.g., the use of non-affine transformations, image compression using L-System, use of 3D L-Systems, fractal encoding and the use of hybrid fractals discussed in this book etc.). We hope the directions highlighted in this chapter will definitely help many researchers to add the basic outlook in their interest in the field of L-System. All most every fractal can be regenerated using L-System concept. Particularly the growth phenomenon can be easily simulated by this concept with less mathematical complexity with least computational resources. © 2007 Elsevier B.V. All rights reserved.",,,2-s2.0-40649123028
"Clavel M., Duran F., Eker S., Lincoln P., Martí-Oliet N., Meseguer J., Talcott C.","All about maude - A high-performance logical framework how to specify, program and verify systems in rewriting logic",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",114,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049179424&partnerID=40&md5=abc1621ecc9fa6c002ba768fa0c4c6b7",[No abstract available],,,2-s2.0-38049179424
"Kate R.J., Mooney R.J.","Learning language semantics from ambiguous supervision",2007,"Proceedings of the National Conference on Artificial Intelligence",29,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36349001158&partnerID=40&md5=a7f66d0759626b5ad548e0082a30fd60","This paper presents a method for learning a semantic parser from ambiguous supervision. Training data consists of natural language sentences annotated with multiple potential meaning representations, only one of which is correct. Such ambiguous supervision models the type of supervision that can be more naturally available to language-learning systems. Given such weak supervision, our approach produces a semantic parser that maps sentences into meaning representations. An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision. Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers. Copyright © 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Data reduction; Learning systems; Mapping; Mathematical models; Language-learning systems; Semantic parsers; Semantics",2-s2.0-36349001158
"Forbus K.D., Riesbeck C., Birnbaum L., Livingston K., Sharma A., Ureel L.","Integrating natural language, knowledge representation and reasoning, and analogical processing to learn by reading",2007,"Proceedings of the National Conference on Artificial Intelligence",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348957836&partnerID=40&md5=2dc4e12c983739d81b65a03219496630","Learning by reading requires integrating several strands of AI research. We describe a prototype system, Learning Reader, which combines natural language processing, a large-scale knowledge base, and analogical processing to learn by reading simplified language texts. We outline the architecture of Learning Reader and some of system-level results, then explain how these results arise from the components. Specifically, we describe the design, implementation, and performance characteristics of a natural language understanding model (DMAP) that is tightly coupled to a knowledge base three orders of magnitude larger than previous attempts. We show that knowing the kinds of questions being asked and what might be learned can help provide more relevant, efficient reasoning. Finally, we show that analogical processing provides a means of generating useful new questions and conjectures when the system ruminates off-line about what it has read. Copyright © 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Conjectures; Natural language understanding model (DMAP); Reasoning; Knowledge based systems; Knowledge representation; Learning systems; Mathematical models; Query processing; Natural language processing systems",2-s2.0-36348957836
"Yin D., Chen B., Fang Y., Huang Z.","Fully distributed R-tree for efficient range query dissemination in peer-to-peer spatial data grid",2007,"Proceedings of SPIE - The International Society for Optical Engineering",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36249021023&doi=10.1117%2f12.761369&partnerID=40&md5=f1d5d6d861e8867a116a1831c0cd16f6","To make the diverse organizations access the massive distributed heterogeneous spatial data easily, we build a Spatial Data Grid in peer-to-peer way, which composed of several spatial datacenters distributed in different agencies. Given a query, which often a range query in spatial application, the query parser will decompose the global query into several sub-queries and disseminate them to the peers who take responsibility for storing data of that area. In order to accelerate the query dissemination process, we utilize distributed index to locate object more quickly and accurately. We propose an adaptive indexing mechanism, Fully Distributed R-Tree Index, which is appropriate for efficient range query dissemination. It composes of two layers, on top of which is the spatial range of each peer. The under layer stores part of the R-Tree of each peer's neighbors, which can be adjusted according to the capacity of peer. There is no centralized control on top of the entire system, and it could adaptive to the environment change.","Distributed GIS; Query dissemination; R-Tree Index; Spatial Data Grid","Climate change; Database systems; Environmental impact; Geographic information systems; Indexing (of information); Distributed GIS; Query dissemination; R-Tree Index; Spatial Data Grid; Distributed computer systems",2-s2.0-36249021023
"Li Y., Yang H., Jagadish H.V.","NaLIX: A generic natural language search environment for XML data",2007,"ACM Transactions on Database Systems",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949038999&doi=10.1145%2f1292609.1292620&partnerID=40&md5=74ac0550c89b2aced127b0cee446c629","We describe the construction of a generic natural language query interface to an XML database. Our interface can accept a large class of English sentences as a query, which can be quite complex and include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression. The translation is based on mapping grammatical proximity of natural language parsed tokens in the parse tree of the query sentence to proximity of corresponding elements in the XML data to be retrieved. Iterative search in the form of followup queries is also supported. Our experimental assessment, through a user study, demonstrates that this type of natural language interface is good enough to be usable now, with no restrictions on the application domain. © 2007 ACM.","Dialog system; Iterative search; Natural language interface; XML; XQuery","Dialog systems; Iterative search; Natural language interfaces; XQuery expression; Data reduction; Interfaces (computer); Iterative methods; Natural language processing systems; Query languages; XML; Tabu search",2-s2.0-36949038999
"Dunlavy D.M., O'Leary D.P., Conroy J.M., Schlesinger J.D.","QCS: A system for querying, clustering and summarizing documents",2007,"Information Processing and Management",36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547946738&doi=10.1016%2fj.ipm.2007.01.003&partnerID=40&md5=e36e7dcedd4c8117f0a1a4f34d8c336e","Information retrieval systems consist of many complicated components. Research and development of such systems is often hampered by the difficulty in evaluating how each particular component would behave across multiple systems. We present a novel integrated information retrieval system-the Query, Cluster, Summarize (QCS) system-which is portable, modular, and permits experimentation with different instantiations of each of the constituent text analysis components. Most importantly, the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic. We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences (DUC) as measured by the best known automatic metric for summarization system evaluation, ROUGE. Although the DUC data and evaluations were originally designed to test multidocument summarization, we developed a framework to extend it to the task of evaluation for each of the three components: query, clustering, and summarization. Under this framework, we then demonstrate that the QCS system (end-to-end) achieves performance as good as or better than the best summarization engines. Given a query, QCS retrieves relevant documents, separates the retrieved documents into topic clusters, and creates a single summary for each cluster. In the current implementation, Latent Semantic Indexing is used for retrieval, generalized spherical k-means is used for the document clustering, and a method coupling sentence ""trimming"" and a hidden Markov model, followed by a pivoted QR decomposition, is used to create a single extract summary for each cluster. The user interface is designed to provide access to detailed information in a compact and useful format. Our system demonstrates the feasibility of assembling an effective IR system from existing software libraries, the usefulness of the modularity of the design, and the value of this particular combination of modules. © 2007.","Clustering; Information retrieval; Latent semantic indexing; Sentence trimming; Summarization; Text processing","Data processing; Information retrieval; Query languages; Semantics; Text processing; Latent semantic indexing; Sentence trimming; Summarization; Data reduction",2-s2.0-34547946738
"Redfern O.C., Harrison A., Dallman T., Pearl F.M.G., Orengo C.A.","CATHEDRAL: A fast and effective algorithm to predict folds and domain boundaries from multidomain protein structures",2007,"PLoS Computational Biology",50,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36949008379&doi=10.1371%2fjournal.pcbi.0030232&partnerID=40&md5=1ceec4833b3e544415856476acde0e1d","We present CATHEDRAL, an iterative protocol for determining the location of previously observed protein folds in novel multidomain protein structures. CATHEDRAL builds on the features of a fast secondary-structure-based method (using graph theory) to locate known folds within a multidomain context and a residue-based, double-dynamic programming algorithm, which is used to align members of the target fold groups against the query protein structure to identify the closest relative and assign domain boundaries. To increase the fidelity of the assignments, a support vector machine is used to provide an optimal scoring scheme. Once a domain is verified, it is excised, and the search protocol is repeated in an iterative fashion until all recognisable domains have been identified. We have performed an initial benchmark of CATHEDRAL against other publicly available structure comparison methods using a consensus dataset of domains derived from the CATH and SCOP domain classifications. CATHEDRAL shows superior performance in fold recognition and alignment accuracy when compared with many equivalent methods. If a novel multidomain structure contains a known fold, CATHEDRAL will locate it in 90% of cases, with <1% false positives. For nearly 80% of assigned domains in a manually validated test set, the boundaries were correctly delineated within a tolerance of ten residues. For the remaining cases, previously classified domains were very remotely related to the query chain so that embellishments to the core of the fold caused significant differences in domain sizes and manual refinement of the boundaries was necessary. To put this performance in context, a well-established sequence method based on hidden Markov models was only able to detect 65% of domains, with 33% of the subsequent boundaries assigned within ten residues. Since, on average, 50% of newly determined protein structures contain more than one domain unit, and typically 90% or more of these domains are already classified in CATH, CATHEDRAL will considerably facilitate the automation of protein structure classification. © 2007 Redfern et al.",,"algorithm; amino acid sequence; article; cathedral; consensus; data base; methodology; protein analysis; protein domain; protein folding; protein structure; chemical model; chemical structure; chemistry; computer program; computer simulation; molecular genetics; protein conformation; protein folding; protein tertiary structure; sequence alignment; sequence analysis; ultrastructure; protein; Algorithms; Amino Acid Sequence; Computer Simulation; Models, Chemical; Models, Molecular; Molecular Sequence Data; Protein Conformation; Protein Folding; Protein Structure, Tertiary; Proteins; Sequence Alignment; Sequence Analysis, Protein; Software",2-s2.0-36949008379
"Bassi S.","A primer on python for life science researchers",2007,"PLoS Computational Biology",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36948999784&doi=10.1371%2fjournal.pcbi.0030199&partnerID=40&md5=f4eb0ac46bd861cf06e19df40eab1982",[No abstract available],,"biomedicine; computer language; computer program; computer system; human computer interaction; information processing; molecular biology; python computer language; review; biological model; biomedicine; computer program; computer simulation; methodology; Biological Science Disciplines; Computer Simulation; Models, Biological; Programming Languages; Research Design; Software; Software Design",2-s2.0-36948999784
"Li M., Zhou Z.-H.","Improve computer-aided diagnosis with machine learning techniques using undiagnosed samples",2007,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",167,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36249007597&doi=10.1109%2fTSMCA.2007.904745&partnerID=40&md5=7495e1878e7cb9414484b46dd8076bd3","In computer-aided diagnosis (CAD), machine learning techniques have been widely applied to learn a hypothesis from diagnosed samples to assist the medical experts in making a diagnosis. To learn a well-performed hypothesis, a large amount of diagnosed samples are required. Although the samples can be easily collected from routine medical examinations, it is usually impossible for medical experts to make a diagnosis for each of the collected samples. If a hypothesis could be learned in the presence of a large amount of undiagnosed samples, the heavy burden on the medical experts could be released. In this paper, a new semisupervised learning algorithm named Co-Forest is proposed. It extends the co-training paradigm by using a well-known ensemble method named Random Forest, which enables Co-Forest to estimate the labeling confidence of undiagnosed samples and easily produce the final hypothesis. Experiments on benchmark data sets verify the effectiveness of the proposed algorithm. Case studies on three medical data sets and a successful application to microcalcification detection for breast cancer diagnosis show that undiagnosed samples are helpful in building CAD systems, and Co-Forest is able to enhance the performance of the hypothesis that is learned on only a small amount of diagnosed samples by utilizing the available undiagnosed samples. © 2007 IEEE.","CAD; Co-training; Computer-aided diagnosis (CAD); Ensemble learning; Learning (artificial intelligence); Machine learning; Microcalcification cluster detection; Patient diagnosis; Random forest; Semisupervised learning","Artificial intelligence; Computer aided diagnosis; Learning algorithms; Learning systems; Oncology; Random processes; Ensemble learning; Microcalcification cluster detection; Patient diagnosis; Semisupervised learning; Biomedical engineering",2-s2.0-36249007597
"Zhou Z.-H., Li M.","Semisupervised regression with cotraining-style algorithms",2007,"IEEE Transactions on Knowledge and Data Engineering",93,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348881683&doi=10.1109%2fTKDE.2007.190644&partnerID=40&md5=c4c1412f38e632c5af30f472949d1a06","The traditional setting of supervised learning requires a large amount of labeled training examples in order to achieve good generalization. However, in many practical applications, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semisupervised learning has attracted much attention. Previous research on semisupervlsed learning mainly focuses on semisupervised classification. Although regression is almost as important as classification, semisupervised regression is largely understudied. In particular, although cotraining is a main paradigm in semisupervised learning, few works has been devoted to cotraining-style semisupervised regression algorithms. In this paper, a cotraining-style semisupervised regression algorithm, that is, COREG, is proposed. This algorithm uses two regressors, each labels the unlabeled data for the other regressor, where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean squared error over the labeled neighborhood of that example. Analysis and experiments show that COREG can effectively exploit unlabeled data to improve regression estimates. © 2007 IEEE.","Cotraining; Data mining; Learning with unlabeled data; Machine learning; Semisupervised learning; Semisupervised regression","Learning with unlabeled data; Semisupervised learning; Semisupervised regression; Algorithms; Classification (of information); Data mining; Data reduction; Mean square error; Regression analysis; Supervised learning",2-s2.0-35348881683
"Chellapilla K., Maykov A.","A taxonomy of JavaScript redirection spam",2007,"ACM International Conference Proceeding Series",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35548952721&doi=10.1145%2f1244408.1244423&partnerID=40&md5=b1895baf6d460fb7c013a1aee76c123a","Redirection spam presents a web page with false content to a crawler for indexing, but automatically redirects the browser to a different web page. Redirection is usually immediate (on page load) but may also be triggered by a timer or a harmless user event such as a mouse move. JavaScript redirection is the most notorious of redirection techniques and is hard to detect as many of the prevalent crawlers are script-agnostic. In this paper, we study common JavaScript redirection spam techniques on the web. Our findings indicate that obfuscation techniques are very prevalent among JavaScript redirection spam pages. These obfuscation techniques limit the effectiveness of static analysis and static feature based systems. Based on our findings, we recommend a robust counter measure using a light weight JavaScript parser and engine. Copyright 2007 ACM.","JavaScript; Redirection spam; Web search; Web spam","JavaScript; Redirection spams; Web spam; Automata theory; Feature extraction; Java programming language; Search engines; Taxonomies; Web browsers; Spamming",2-s2.0-35548952721
"Gou G., Chirkova R.","Efficient algorithms for evaluating xpath over streams",2007,"Proceedings of the ACM SIGMOD International Conference on Management of Data",43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448980001&doi=10.1145%2f1247480.1247512&partnerID=40&md5=c93424a41a157efa98ea31daec5e5365","In this paper we address the problem of evaluating XPath queries over streaming XML data. We consider a practical XPath fragment called Univariate XPath, which includes the commonly used '/' and '//' axes and allows*-node tests and arbitrarily nested predicates. It is well known that this XPath fragment can be efficiently evaluated in O(|D||Q|) time in the non-streaming environment, where |D| is the document size and |Q| is the query size. However, this is not necessarily true in the streaming environment, since streaming algorithms have to satisfy stricter requirement than non-streaming algorithms, in that all data must be read sequentially in one pass. Therefore, it is not surprising that state-of-the-art stream-querying algorithms have higher time complexity than O(|D||Q|). In this paper we revisit the XPath stream-querying problem, and show that Univariate XPath can be efficiently evaluated in O|D||Q|) time in the streaming environment. Specifically, we propose two O(|D||Q|)-time stream-querying algorithms, LQ and EQ, which are based on the lazy strategy and on the eager strategy, respectively. To the best of our knowledge, LQ and EQ are the first XPath stream-querying algorithms that achieve O(|D||Q|) time performance. Further, our algorithms achieve O(|D||Q|) time performance without trading off space performance. Instead, they have better buffering-space performance than state-of-the-art stream-querying algorithms. In particular, EQ achieves optimal buffering-space performance. Our experimental results show that our algorithms have not only good theoretical complexity but also considerable practical performance advantages over existing algorithms. Copyright 2007 ACM.","Query processing; Streams; XML; XPath","Algorithms; Data reduction; Motion planning; Optimal systems; Problem solving; XML; Non-streaming environment; Querying algorithms; XPath queries; Query processing",2-s2.0-35448980001
"Tang L.R.","Learning an ensemble of semantic parsers for building dialog-based natural language interfaces",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448956022&partnerID=40&md5=5e50c4d9db7d01b9596d1bc7e98e3526","Building or learning semantic parsers has been an interesting approach for creating natural language interfaces (NLI's) for databases. Recently, the problem of imperfect precision in an NLI has been brought up as an NLI that might answer a question incorrectly can render it unstable, if not useless. In this paper, an approach based on ensemble learning is proposed to trivially address the problem of unreliability in an NLI due to imperfect precision in the semantic parser in a way that also allows the recall of the NLI to be improved. Experimental results in two real world domains suggested that such an approach can be promising. © Springer-Verlag Berlin Heidelberg 2007.",,"Database systems; Learning systems; Precision engineering; Problem solving; Semantics; User interfaces; Ensemble learning; Natural language interfaces (NLI); Real world domains; Semantic parsers; Natural language processing systems",2-s2.0-35448956022
[No author name available],"KI2006: Advances in Artificial Intelligence",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35449002767&partnerID=40&md5=622e04e0fe2612b91d50f959894bd6da","The proceedings contain 38 papers. The topics discussed include: techniques for fast query relaxation in content-based recommender systems; building robots with analogy-based anticipation; classification of skewed and homogenous document corpora with class-based and corpus-based keywords; learning an ensemble of semantic parsers for building dialog-based natural language interfaces; towards the computation of stable probabilistic model semantics; applications of automated reasoning; on the scalability of description logic instance retrieval; relation instantiation for ontology population using the web; a robot learns to know people - first contacts of a robot; a framework for quasi-exact optimization using relaxed best-first search; a unifying framework for hybrid planning and scheduling; and a hybrid time management approach to agent-based simulation.",,"Computational methods; Computer simulation languages; Ontology; Query processing; Robots; Semantics; Language interfaces; Query relaxation; Recommender systems; Relation instantiation; Artificial intelligence",2-s2.0-35449002767
"Ohlbach H.J.","GeTS - A specification language for geo-temporal notions",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35448945956&partnerID=40&md5=494d02fc869517d47ff81689dd175578","his paper contains a brief overview of the 'Geo-Temporal' specification language GeTS. The objects which can be described and manipulated with this language are time points, crisp and fuzzy time intervals and labeled partitionings of the time axis. The partitionings are used to represent periodic temporal notions like months, semesters etc. and also whole calendar systems. GeTS is essentially a typed functional language with a few imperative constructs and many built-ins. GeTS can be used to specify and compute with many different kinds of temporal notions, from simple arithmetic operations on time points up to complex fuzzy relations between fuzzy time intervals. A parser, a compiler and an abstract machine for GeTS is implemented. © Springer-Verlag Berlin Heidelberg 2007.",,"Abstract machines; Fuzzy time intervals; GeTS; Periodic temporal notions; Abstracting; Digital arithmetic; Fuzzy logic; Program compilers; Specification languages",2-s2.0-35448945956
"Witte R., Kappler T., Baker C.J.O.","Enhanced semantic access to the protein engineering literature using ontologies populated by text mining",2007,"International Journal of Bioinformatics Research and Applications",16,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35349006936&doi=10.1504%2fIJBRA.2007.015009&partnerID=40&md5=24e86650268c6ce409db960cbf027d2b","The biomedical literature is growing at an ever-increasing rate, which pronounces the need to support scientists with advanced, automated means of accessing knowledge. We investigate a novel approach employing description logics (DL)-based queries made to formal ontologies that have been created using the results of text mining full-text research papers. In this paradigm, an OWL-DL ontology becomes populated with instances detected through natural language processing (NLP). The generated ontology can be queried by biologists using DL reasoners or integrated into bioinformatics workflows for further automated analyses. We demonstrate the feasibility of this approach with a system targeting the protein mutation literature. Copyright © 2007 Inderscience Enterprises Ltd.","Automated reasoning in bioinformatics; Description logics; Ontological NLP; Protein mutations; Querying OWL-DL ontologies; Semantic web; Text mining","automation; mutation; natural language processing; protein engineering; review; scientific literature; Computational Biology; Databases, Protein; Mutation; Natural Language Processing; Protein Engineering; Publications; Semantics",2-s2.0-35349006936
"Wang Y.-H., Wang W.-N., Huang C.-C.","Enhanced semantic question answering system for e-learning environment",2007,"Proceedings - 21st International Conference on Advanced Information Networking and Applications Workshops/Symposia, AINAW'07",19,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248878778&doi=10.1109%2fAINAW.2007.174&partnerID=40&md5=5d1eb0657cbf1f30620fcfd2448be5e5","To support an automatic assistant learning and self-paced learning mechanism is the objective in today's e-learning environment. Researches of QA system focus on the following characteristics: understand questioners' questions in the form of nature language; enhance the accuracy of search result, and establish automatic learning mechanism. Base these characteristics, we develop a Semantic English QA system to analyze learners' questions and find the relevant answer from the target course ontology. Firstly, this research uses Link Grammar Parser to analyze the syntactic information from the input sentence. According to the syntactic information, secondly, the following process queries the similar word lists generated from WordNet to extend relevant meaning. Thirdly, the two kinds of information can be used to form a semantic tree. Lastly, the semantic tree will map the Data Structure course ontology and find the relevant contents in order to answer learners. © 2007 IEEE.","e-learning; Link grammar; Semantic question answering","Data structures; Information retrieval; Learning systems; Natural language processing systems; Ontology; Semantics; Automatic assistant learning; Link grammars; Semantic question answering; E-learning",2-s2.0-35248878778
"Datcu D., Yang Z., Rothkrantz L.","Multimodal workbench for automatic surveillance applications",2007,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948911534&doi=10.1109%2fCVPR.2007.383529&partnerID=40&md5=1c252c0331023ea31dd36a64beac96b9",[No abstract available],,,2-s2.0-34948911534
"Binkley D.","Source code analysis: A road map",2007,"FoSE 2007: Future of Software Engineering",96,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748895724&doi=10.1109%2fFOSE.2007.27&partnerID=40&md5=db5c457ce2ada409de9cb93124ac2ffa","The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically, The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years. © 2007 IEEE.",,"Algorithms; Software engineering; Road map; Semi-automated analysis; Source-code analysis; Automation",2-s2.0-34748895724
"Henrlksson J., Johannes J., Zschaler S., Aßmann U.","Reuseware - Adding modularity to your language of choice",2007,"Journal of Object Technology",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36049037352&partnerID=40&md5=b23aa911376ba963d0cf6342d16c8523","The trend towards domain-specific languages leads to an ever-growing plethora of highly specialized languages. Developers of such languages focus on their specific domains rather than on technical challenges of language design. Generic features of languages are rarely Included In special-purpose languages. One very Important feature Is modularization, the ability to formulate partial programs In separate entitles, composable Into a complete program In a defined manner. This paper presents a generic approach for adding modularity to arbitrary languages, discussing the underlying concepts and presenting the Reuseware Composition Framework. We walk through an example based on Xcerpt, a Semantic Web query language.",,,2-s2.0-36049037352
"Schein A.I., Ungar L.H.","Active learning for logistic regression: An evaluation",2007,"Machine Learning",84,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548168342&doi=10.1007%2fs10994-007-5019-5&partnerID=40&md5=c221043beb5e3f5634f1d86deb4e12a9","Which active learning methods can we expect to yield good performance in learning binary and multi-category logistic regression classifiers? Addressing this question is a natural first step in providing robust solutions for active learning across a wide variety of exponential models including maximum entropy, generalized linear, log-linear, and conditional random field models. For the logistic regression model we re-derive the variance reduction method known in experimental design circles as 'A-optimality.' We then run comparisons against different variations of the most widely used heuristic schemes: query by committee and uncertainty sampling, to discover which methods work best for different classes of problems and why. We find that among the strategies tested, the experimental design methods are most likely to match or beat a random sample baseline. The heuristic alternatives produced mixed results, with an uncertainty sampling variant called margin sampling and a derivative method called QBB-MM providing the most promising performance at very low computational cost. Computational running times of the experimental design methods were a bottleneck to the evaluations. Meanwhile, evaluation of the heuristic methods lead to an accumulation of negative results. We explore alternative evaluation design parameters to test whether these negative results are merely an artifact of settings where experimental design methods can be applied. The results demonstrate a need for improved active learning methods that will provide reliable performance at a reasonable computational cost. © 2007 Springer Science+Business Media, LLC.","Active learning; Experimental design; Generalized linear models; Logistic regression","Binary codes; Computational methods; Linear equations; Regression analysis; Active learning; Experimental design; Generalized linear models; Logistic regression models; Learning systems",2-s2.0-34548168342
"Lin J., Wilbur W.J.","Syntactic sentence compression in the biomedical domain: Facilitating access to related articles",2007,"Information Retrieval",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848922191&doi=10.1007%2fs10791-007-9029-5&partnerID=40&md5=9a1298d2878e01a9c001f519a6046da6","We explore a syntactic approach to sentence compression in the biomedical domain, grounded in the context of result presentation for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles, a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a corpus of manually compressed examples from MEDLINE: an automatic evaluation using Bleu and a summative evaluation involving human assessors. Experiments show that a syntactic approach to sentence compression is effective in the biomedical domain and that the presentation of compressed article titles supports accurate ""interest judgments"", decisions by users as to whether an article is worth examining in more detail. © 2007 Springer Science+Business Media, LLC.","Extrinsic evaluation; Genomics IR; MEDLINE; PubMed; Sentence compression",,2-s2.0-34848922191
"Fernau H.","Learning tree languages from text",2007,"RAIRO - Theoretical Informatics and Applications",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36749047923&doi=10.1051%2fita%3a2007030&partnerID=40&md5=4e969d7cced995838e6381d975eabb0c","We study the problem of learning regular tree languages from text. We show that the framework of function distinguishability, as introduced by the author in [Theoret. Comput. Sci. 290 (2003) 1679-1711], can be generalized from the case of string languages towards tree languages. This provides a large source of identifiable classes of regular tree languages. Each of these classes can be characterized in various ways. Moreover, we present a generic inference algorithm with polynomial update time and prove its correctness. In this way, we generalize previous works of Angluin, Sakakibara and ourselves. Moreover, we show that this way all regular tree languages can be approximately identified. © 2007 EDP Sciences.",,"Algorithms; Computer programming languages; Polynomial approximation; String theory; Learning tree languages; Regular tree languages; String languages; Information analysis",2-s2.0-36749047923
"Hsu J.","Extracting the knowledge entangled in the web: Technologies, applications and developments",2007,"International Journal of Innovation and Learning",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548761507&doi=10.1504%2fIJIL.2007.013451&partnerID=40&md5=0475f3b69edb741411b6362ac2837826","Web mining is an area that has garnered much attention in the past few years, with the ability to 'mine' new knowledge and information from the vast expanse of the World Wide Web. This paper starts with an introduction to web mining research and applications, and then focuses on the key new technologies and methods in the field. This is then followed by a discussion of the important applications in the field, including those in marketing, e-commerce, crime, and web communities. The benefits and future of web mining, together with important areas for research, conclude the paper. Copyright © 2007 Inderscience Enterprises Ltd.","Data mining; E-commerce; Innovation; Internet; Knowledge; Learning; Web mining; World Wide Web","e-commerce; innovation; learning; World Wide Web",2-s2.0-34548761507
"Baghi H., Barouni-Ebrahimi M., Ghorbani A.A., Zafarani R.","ConnectA!: An intelligent search engine based on authors' connectivity",2007,"Proceedings - CNSR 2007: Fifth Annual Conference on Communication Networks and Services Research",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548349094&doi=10.1109%2fCNSR.2007.26&partnerID=40&md5=946dc3f8698f368062795fdcf84a0d44","Searching documents about a specific subject written by a group of closely related authors may sometimes come in use for the researchers. This task is usually overwhelming, since the surfer is not familiar with all the authors in that community. In this paper, we propose a semantic web search engine called ConnectA! that helps the users to find the publications of a community of authors on a specific subject, while they only know a very limited number of the authors in that community. The primary live evaluation of the implemented model shows the efficiency of the method1. © 2007 IEEE.",,"Electronic publishing; Intelligent systems; Mathematical models; Online searching; Document sharing; Intelligent search engines; Primary live evaluation; Search engines",2-s2.0-34548349094
"Mandelbaum Y., Fisher K., Walker D., Fernandez M., Gleyzer A.","PADS/ML: A functional data description language",2007,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548253441&doi=10.1145%2f1190216.1190231&partnerID=40&md5=17ade10ea253e80851dfb52f68d6aa4b","Massive amounts of useful data are stored and processed in ad hoc formats for which common tools like parsers, printers, query engines and format converters are not readily available. In this paper, we explain the design and implementation of PADS/ML , a new language and system that facilitates the generation of data processing tools for ad hoc formats. The PADS/ML design includes features such as dependent, polymorphic and recursive datatypes, which allow programmers to describe the syntax and semantics of ad hoc data in a concise, easy-to-read notation. The PADS/ML implementation compiles these descriptions into ml structures and functors that include types for parsed data, functions for parsing and printing, and auxiliary support for user-specified, format-dependent and format-independent tool generation. Copyright © 2007 ACM.","Data description languages; Dependent types; Domain-specific languages; Functional programming; ML; Modules; Parsing; Printing","Ad hoc networks; Computer programming languages; Functional programming; Printing; Search engines; Data description languages; Dependent types; Domain-specific languages; Parsing; Data description; Computer Programing; Printers",2-s2.0-34548253441
"Quinlan D.J., Vuduc R.W., Misherghi G.","Techniques for specifying bug patterns",2007,"Proceedings of the 2007 ACM Workshop on Parallel and Distributed Systems: Testing and Debugging, PADTAD'07",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548263662&doi=10.1145%2f1273647.1273654&partnerID=40&md5=09c69abb018bef12ae75941fd65d2ea8","We present our on-going work to develop techniques for specifying source code signatures of bug patterns. Specifically, we discuss two approaches. The first approach directly analyzes a program in the intermediate representation (IR) of the ROSE compiler infrastructure using ROSE's API. The second analyzes the program using the bddbddb system of Lam, Whaley, et al.. In this approach, we store the IR produced by ROSE as a relational database, express patterns as declarative inference rules on relations in the language Datalog, and bddbddb implements the Datalog programs using binary decision diagram (BDD) techniques. Both approaches readily apply to large-scale applications, since ROSE provides full type analysis, control flow, and other available analysis information. In this paper, we primarily consider bug patterns expressed with respect to the structure of the source code or the control flow, or both. More complex techniques to specify patterns that are functions of data flow properties may be addressed by either of the above approaches, but are not directly treated here. Our Datalog-based work includes explicit support for expressing patterns on the use of the Message Passing Interface (MPI) in parallel distributed memory programs. We show examples of this on-going work as well. Copyright 2007 ACM.","Bug patterns; Datalog; Message passing interface; Static analysis","Binary decision diagrams; Computer programming languages; Message passing; Program compilers; Software engineering; Static analysis; Bug patterns; Datalog; Message passing interfaces; Software testing",2-s2.0-34548263662
"Badawi M., Donikian S.","The generic description and management of interaction between autonomous agents and objects in an informed virtual environment",2007,"Computer Animation and Virtual Worlds",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348995524&doi=10.1002%2fcav.204&partnerID=40&md5=1c987b02749f596da5a3b3eb699e9240","Autonomous agents cannot exist in an environment without interacting with the world surrounding them. In this paper we propose an informed environment based on synoptic objects which contain a synopsis of the interactions they can undergo. Through the use of interactive surfaces (ISs) we manage to describe the surfaces of interest on the object itself and the space affected by the object during interaction. Then, by defining a set of seven basic actions, the objects can describe the interaction process through these actions to any agent implementing them. The description of the interaction process is done through complex actions which indicate the order in which the basic actions need to be accomplished and what to do depending on the result of the undertaken action. Copyright © 2007 John Wiley & Sons, Ltd.","Computer animation; Informed environment; Interactive objects","Autonomous agents; Interactive computer systems; Object recognition; Virtual reality; Computer animation; Informed environment; Interactive objects; Interactive surfaces (ISs); Human computer interaction",2-s2.0-35348995524
"Quiñones K.D., Su H., Marshall B., Eggers S., Chen H.","User-centered evaluation of Arizona BioPathway: An information extraction, integration, and visualization system",2007,"IEEE Transactions on Information Technology in Biomedicine",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548680543&doi=10.1109%2fTITB.2006.889706&partnerID=40&md5=825dcb3a611864f043b2fd8543a4be93","Explosive growth in biomedical research has made automated information extraction, knowledge integration, and visualization increasingly important and critically needed. The Arizona BioPathway (ABP) system extracts and displays biological regulatory pathway information from the abstracts of journal articles. This study uses relations extracted from more than 200 PubMed abstracts presented in a tabular and graphical user interface with built-in search and aggregation functionality. This paper presents a task-centered assessment of the usefulness and usability of the ABP system focusing on its relation aggregation and visualization functionalities. Results suggest that our graph-based visualization is more efficient in supporting pathway analysis tasks and is perceived as more useful and easier to use as compared to a text-based literature-viewing method. Relation aggregation significantly contributes to knowledge-acquisition efficiency. Together, the graphic and tabular views in the ABP Visualizer provide a flexible and effective interface for pathway relation browsing and analysis. Our study contributes to pathway-related research and biological information extraction by assessing the value of a multiview, relation-based interface that supports user-controlled exploration of pathway information across multiple granularities. © 2007 IEEE.","Biomedical information extraction; Pathway analysis; System evaluation; Visualization","Computer graphics; Computer vision; Data processing; Graphical user interfaces; Knowledge acquisition; Biomedical information; Biomedical research; Information extraction; Bioinformatics; proteome; article; biological model; classification; computer interface; data base; evaluation; factual database; information retrieval; metabolism; methodology; natural language processing; physiology; publication; signal transduction; system analysis; Database Management Systems; Databases, Factual; Information Storage and Retrieval; Models, Biological; Natural Language Processing; Periodicals; Proteome; Signal Transduction; Systems Integration; User-Computer Interface",2-s2.0-34548680543
"Fagge T.J., Robin G.R., Colin G.C., Stove G., Robinson M.J., Head M.W., Ironside J.W., Turner M.L.","Application of Atomic Dielectric Resonance Spectroscopy for the screening of blood samples from patients with clinical variant and sporadic CJD",2007,"Journal of Translational Medicine",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348877877&doi=10.1186%2f1479-5876-5-41&partnerID=40&md5=753d2249e42cfa33f5ef71bc37253590","Background: Sub-clinical variant Creutzfeldt-Jakob disease (vCJD) infection and reports of vCJD transmission through blood transfusion emphasise the need for blood screening assays to ensure the safety of blood and transplanted tissues. Most assays aim to detect abnormal prion protein (PrPSc), although achieving required sensitivity is a challenge. Methods: We have used innovative Atomic Dielectric Resonance Spectroscopy (ADRS), which determines dielectric properties of materials which are established by reflectivity and penetration of radio/micro waves, to analyse blood samples from patients and controls to identify characteristic ADR signatures unique to blood from vCJD and to sCJD patients. Initial sets of blood samples from vCJD, sCJD, non-CJD neurological diseases and normal healthy adults (blood donors) were screened as training samples to determine group-specific ADR characteristics, and provided a basis for classification of blinded sets of samples. Results: Blood sample groups from vCJD, sCJD, non-CJD neurological diseases and normal healthy adults (blood donors) screened by ADRS were classified with 100% specificity and sensitivity, discriminating these by a co-variance expert analysis system. Conclusion: ADRS appears capable of recognising and discriminating serum samples from vCJD, sCJD, non-CJD neurological diseases, and normal healthy adults, and might be developed to provide a system for primary screening or confirmatory assay complementary to other screening systems. © 2007 Fagge et al; licensee BioMed Central Ltd.",,"prion protein; adult; article; blood analysis; blood donor; blood sampling; controlled study; Creutzfeldt Jakob disease; diagnostic accuracy; diagnostic value; disease classification; expert system; human; microwave radiation; neurologic disease; protein determination; sensitivity and specificity; spectroscopy; variant Creutzfeldt Jakob disease; blood; Fourier analysis; mass screening; methodology; Creutzfeldt-Jakob Syndrome; Fourier Analysis; Humans; Mass Screening; Spectrum Analysis",2-s2.0-35348877877
"Immaneni T., Thirunarayan K.","A unified approach to retrieving web documents and semantic web data",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548137391&partnerID=40&md5=d7f812a6cdcf7c13f5e9c02b7f57ad3c","The Semantic Web seems to be evolving into a property-linked web of RDF data, conceptually divorced from (but physically housed in) the hyperlinked web of HTML documents. We discuss the Unified Web model that integrates the two webs and formalizes the structure and the semantics of interconnections between them. We also discuss the Hybrid Query Language which combines the Data and Information Retrieval techniques to provide a convenient and uniform way to retrieve data and documents from the Unified Web. We present the retrieval system SITAR and some preliminary results. © Springer-Verlag Berlin Heidelberg 2007.","Data retrieval; Hybrid query language; Hybrid retrieval; Information retrieval; Semantic web; Unified web","HTML; Query languages; RSS; Semantic Web; Semantics; Data retrieval; SITAR; Unified web; Information retrieval systems",2-s2.0-34548137391
"Wang C., Xiong M., Zhou Q., Yu Y.","PANTO: A portable natural language interface to ontologies",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",74,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548075069&partnerID=40&md5=3eb758722b5e0bd2c8afd641077d0339","Providing a natural language interface to ontologies will not only offer ordinary users the convenience of acquiring needed information from ontologies, but also expand the influence of ontologies and the semantic web consequently. This paper presents PANTO, a Portable nAtural laNguage inTerface to Ontologies, which accepts generic natural language queries and outputs SPARQL queries. Based on a special consideration on nominal phrases, it adopts a triple-based data model to interpret the parse trees output by an off-the-shelf parser. Complex modifications in natural language queries such as negations, superlative and comparative are investigated. The experiments have shown that PANTO provides state-of-the-art results. © Springer-Verlag Berlin Heidelberg 2007.",,"Data structures; Interfaces (computer); Ontology; Semantic Web; Trees (mathematics); Off-the-shelf parser; Phrases; SPARQL queries; Natural language processing systems",2-s2.0-34548075069
"McCarty L.T.","Deep semantic interpretations of legal texts",2007,"Proceedings of the International Conference on Artificial Intelligence and Law",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548027022&doi=10.1145%2f1276318.1276361&partnerID=40&md5=e9b1b6c391926ff94c1c81e4cd65f390","One of the main obstacles to progress in the field of artificial intelligence and law is the natural language barrier, but the technology of natural language processing has advanced recently. In this paper, we will show that a state-of-the-art statistical parser can handle even the complex syntactic constructions of an appellate court judge, and that a deep semantic interpretation of the full text of a judicial opinion can be computed automatically from the output of the parser. Our ultimate goal is to use this semantic interpretation to extract from a judicial opinion precisely the information that a lawyer wants to know about a case. Copyright 2007 ACM.","Computational semantics; Information extraction; Legal texts; Natural language processing","Artificial intelligence; Information dissemination; Laws and legislation; Natural language processing systems; Syntactics; Text processing; Complex syntactic constructions; Computational semantics; Information extraction; Legal texts; Semantics",2-s2.0-34548027022
"Ou S., Khoo C.S.G., Goh D.H.","Automatic multidocument summarization of research abstracts: Design and user evaluation",2007,"Journal of the American Society for Information Science and Technology",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547609251&doi=10.1002%2fasi.20618&partnerID=40&md5=059d945678b517c17674e267afb769f8","The purpose of this study was to develop a method for automatic construction of multidocument summaries of sets of research abstracts that may be retrieved by a digital library or search engine in response to a user query. Sociology dissertation abstracts were selected as the sample domain in this study. A variable-based framework was proposed for integrating and organizing research concepts and relationships as well as research methods and contextual relations extracted from different dissertation abstracts. Based on the framework, a new summarization method was developed, which parses the discourse structure of abstracts, extracts research concepts and relationships, integrates the information across different abstracts, and organizes and presents them in a Web-based interface. The focus of this article is on the user evaluation that was performed to assess the overall quality and usefulness of the summaries. Two types of variable-based summaries generated using the summarization method-with or without the use of a taxonomy-were compared against a sentence-based summary that lists only the research-objective sentences extracted from each abstract and another sentencebased summary generated using the MEAD system that extracts important sentences. The evaluation results indicate that the majority of sociological researchers (70%) and general users (64%) preferred the variable-based summaries generated with the use of the taxonomy.",,"Automatic programming; Engineering research; Feature extraction; Query languages; User interfaces; World Wide Web; Automatic multidocument summarization; Research abstracts; User evaluation; User queries; Abstracting",2-s2.0-34547609251
"Karpouzis K., Caridakis G., Fotinea S.-E., Efthimiou E.","Educational resources and implementation of a Greek sign language synthesis architecture",2007,"Computers and Education",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846542491&doi=10.1016%2fj.compedu.2005.06.004&partnerID=40&md5=1914b0bc36f70f571b6b00f0e94ed931","In this paper, we present how creation and dynamic synthesis of linguistic resources of Greek Sign Language (GSL) may serve to support development and provide content to an educational multitask platform for the teaching of GSL in early elementary school classes. The presented system utilizes standard virtual character (VC) animation technologies for the synthesis of sign sequences/streams, exploiting digital linguistic resources of both lexicon and grammar of GSL. Input to the system is written Greek text, which is transformed into GSL and animated on screen. To achieve this, a syntactic parser decodes the structural patterns of written Greek and matches them into equivalent patterns of GSL, which are then signed by a VC. The adopted notation system for the representation of GSL phonology incorporated in the system's lexical knowledge database, is Hamburg Notation System (HamNoSys). For the implementation of the virtual signer tool, the definition of the VC follows the h-anim standard and is implemented in a web browser using a standard VRML plug-in. © 2005 Elsevier Ltd. All rights reserved.","Distance education and telelearning; Human-computer interface; Interactive learning environments; Multimedia/hypermedia systems; Virtual reality","Animation; Computational grammars; Distance education; Formal languages; Human computer interaction; Multimedia systems; Multitasking; Virtual reality; Interactive learning environments; Lexical knowledge databases; Multimedia/hypermedia systems; Telelearning; Education",2-s2.0-33846542491
"Drake T.A., Braun J., Marchevsky A., Kohane I.S., Fletcher C., Chueh H., Beckwith B., Berkowicz D., Kuo F., Zeng Q.T., Balis U., Holzbach A., McMurry A., Gee C.E., McDonald C.J., Schadow G., Davis M., Hattab E.M., Blevins L., Hook J., Becich M., Crowley R.S., Taube S.E., Berman J.","A system for sharing routine surgical pathology specimens across institutions: the Shared Pathology Informatics Network",2007,"Human Pathology",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250757721&doi=10.1016%2fj.humpath.2007.01.007&partnerID=40&md5=4a67eb5ca6d1714a83d10116c0318cb1","This report presents an overview for pathologists of the development and potential applications of a novel Web enabled system allowing indexing and retrieval of pathology specimens across multiple institutions. The system was developed through the National Cancer Institute's Shared Pathology Informatics Network program with the goal of creating a prototype system to find existing pathology specimens derived from routine surgical and autopsy procedures (""paraffin blocks"") that may be relevant to cancer research. To reach this goal, a number of challenges needed to be met. A central aspect was the development of an informatics system that supported Web-based searching while retaining local control of data. Additional aspects included the development of an eXtensible Markup Language schema, representation of tissue specimen annotation, methods for deidentifying pathology reports, tools for autocoding critical data from these reports using the Unified Medical Language System, and hierarchies of confidentiality and consent that met or exceeded federal requirements. The prototype system supported Web-based querying of millions of pathology reports from 6 participating institutions across the country in a matter of seconds to minutes and the ability of bona fide researchers to identify and potentially to request specific paraffin blocks from the participating institutions. With the addition of associated clinical and outcome information, this system could vastly expand the pool of annotated tissues available for cancer research as well as other diseases. © 2007 Elsevier Inc. All rights reserved.","Database; Informatics; Internet; Pathology; Tissue bank","paraffin; article; autopsy; cancer center; cancer research; computer system; confidentiality; data base; Internet; medical informatics; patient identification; Humans; Medical Informatics; Pathology, Surgical; Specimen Handling; Tissue Banks; United States",2-s2.0-34250757721
"Zdun U.","Systematic pattern selection using pattern language grammars and design space analysis",2007,"Software - Practice and Experience",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547174850&doi=10.1002%2fspe.799&partnerID=40&md5=b6181b91ca84bbc86bd120eee9b02770","Software patterns provide reusable solutions to recurring design problems in a particular context. The software architect or designer must find the relevant patterns and pattern languages that need to be considered, and select the appropriate patterns, as well as the best order to apply them. If the patterns and pattern languages are written by multiple pattern authors, it might be necessary to identify interdependencies and overlaps between these patterns and pattern languages first. Out of the possible multitude of patterns and pattern combinations that might provide a solution to a particular design problem, one fitting solution must be selected. This can only be mastered with a sufficient expertise for both the relevant patterns and the domain in which they are applied. To remedy these issues we provide an approach to support the selection of patterns based on desired quality attributes and systematic design decisions based on patterns. We propose to formalize the pattern relationships in a pattern language grammar and to annotate the grammar with effects on quality goals. In a second step, complex design decisions are analyzed further using the design spaces covered by a set of related software patterns. This approach helps to systematically find and categorize the appropriate software patterns-possibly even from different sources. As a case study of our approach, we analyze patterns from a pattern language for distributed object middleware. Copyright © 2006 John Wiley & Sons, Ltd.","Design space analysis; Pattern languages; Pattern selection; Software design; Software patterns","Computer programming languages; Middleware; Software architecture; Software design; Complex design decisions; Design space analysis; Pattern languages; Pattern selection; Software patterns; Pattern recognition",2-s2.0-34547174850
"Haw S.C., Rao G.S.V.R.K.","A comparative study and benchmarking on XML parsers",2007,"International Conference on Advanced Communication Technology, ICACT",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34347235852&doi=10.1109%2fICACT.2007.358364&partnerID=40&md5=ac15b92f599052ee6991465b29f99315","Due to its flexibility and efficiency in transmission of data, XML has become the emerging standard of data transfer and data exchange across the Internet. XML document must always be checked for well formedness before data transfer and exchange can take place. To choose the right parser for an organization respective system is crucial and critical; since improper parser will lead to degradation in performance and decrease in productivity. In this paper, we will do an extensive comparative study and benchmarking on the popular XML parsers found in the market today. In addition, we also propose a non-validating SAX based XML parser, xParse. We implemented our technique and present the performance results, which prove the viability of our approach.","Benchmark test; Comparative study; XML; XML parser","Benchmark test; SAX based XML parser; XML parser; Benchmarking; Codes (standards); Data communication systems; Data transfer; Electronic data interchange; Internet; XML",2-s2.0-34347235852
"Ferrè F., Ponty Y., Lorenz W.A., Clote P.","DIAL: A web server for the pairwise alignment of two RNA three-dimensional structures using nucleotide, dihedral angle and base-pairing similarities",2007,"Nucleic Acids Research",38,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547585553&doi=10.1093%2fnar%2fgkm334&partnerID=40&md5=11400d8acb4d9936cfe9461710197584","DIAL (dihedral alignment) is a web server that provides public access to a new dynamic programming algorithm for pairwise 3D structural alignment of RNA. DIAL achieves quadratic time by performing an alignment that accounts for (i) pseudo-dihedral and/or dihedral angle similarity, (ii) nucleotide sequence similarity and (iii) nucleotide base-pairing similarity. DIAL provides access to three alignment algorithms: global (Needleman-Wunsch), local (Smith-Waterman) and semiglobal (modified to yield motif search). Suboptimal alignments are optionally returned, and also Boltzmann pair probabilities Prai; bj for aligned positions ai, bj from the optimal alignment. If a non-zero suboptimal alignment score ratio is entered, then the semiglobal alignment algorithm may be used to detect structurally similar occurrences of a user-specified 3D motif. The query motif may be contiguous in the linear chain or fragmented in a number of noncontiguous regions. The DIAL web server provides graphical output which allows the user to view, rotate and enlarge the 3D superposition for the optimal (and suboptimal) alignment of query to target. Although graphical output is available for all three algorithms, the semiglobal motif search may be of most interest in attempts to identify RNA motifs. DIAL is available at http://bioinformatics.bc.edu/clotelab/DIAL. © 2007 The Author(s).",,"RNA; algorithm; article; base pairing; biology; chemistry; computer graphics; computer interface; computer language; computer program; conformation; Internet; methodology; protein database; sequence alignment; sequence analysis; statistical model; Algorithms; Base Pairing; Computational Biology; Computer Graphics; Databases, Protein; Internet; Models, Statistical; Nucleic Acid Conformation; Programming Languages; RNA; Sequence Alignment; Sequence Analysis, RNA; Software; User-Computer Interface",2-s2.0-34547585553
"Raybould A., Stacey D., Vlachos D., Graser G., Li X., Joseph R.","Non-target organism risk assessment of MIR604 maize expressing mCry3A for control of corn rootworm",2007,"Journal of Applied Entomology",44,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34447133125&doi=10.1111%2fj.1439-0418.2007.01200.x&partnerID=40&md5=54617014c6f6dd1fccc9883275b12918","Event MIR604 maize expresses a modified Cry3A protein (mCry3A), for control of corn rootworm. As part of the environmental safety assessment of MIR604 maize, risks to non-target organisms of mCry3A were assessed. The potential exposure of non-target organisms to mCry3A following cultivation of MIR604 maize was determined, and the hypothesis that such exposure is not harmful was tested. The hypothesis was tested rigorously by making worst-case or highly conservative assumptions about exposure, along with laboratory testing for hazards using species taxonomically related to the target pest and species expected to have high exposure to mCry3A, or both. Further rigour was introduced by study designs incorporating long exposures and measurements of sensitive endpoints. No adverse effects were observed in any study, and in most cases exposure to mCry3A in the study was higher than the worst-case expected exposure. In all cases, exposure in the study was higher than realistic, but still conservative, estimates of exposure. These results indicate minimal risk of MIR604 maize to non-target organisms. © 2007 The Authors.","Environmental safety; Exposure; Hazard; Hypothesis testing; Transgenic plants","beetle; crop pest; environmental assessment; gene expression; hypothesis testing; laboratory method; maize; pest control; pesticide; safety; transgenic plant; Zea mays",2-s2.0-34447133125
"Wierzbicki A.P., Nakamori Y.","Summary and conclusions",2007,"Studies in Computational Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250183768&doi=10.1007%2f978-3-540-71562-7_18&partnerID=40&md5=4e96e6d0ba2e6e0b469417bde83b6958","This chapter is the concluding part of this book and is organised in three substantive sections. We deal first with a summary of the diverse contributions to the theory of knowledge creation processes and to the development of creative environments supporting such processes presented in Parts I, II, III and IV of the book. Then we turn to the basic epistemological issue arising from these contributions and from the discussions in Part IV: The divergence of the episteme of the three cultural spheres of hard and natural sciences, technology and social sciences with humanities implies the need to create a new, integrated episteme in the era of knowledge civilisation. Finally, we add some concluding remarks. © 2007 Springer-Verlag Berlin Heidelberg.",,,2-s2.0-34250183768
"Choi R.H., Wong R.K.","Efficient processing of branch queries for high-performance XML filtering",2007,"ACM International Conference Proceeding Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994716170&partnerID=40&md5=ac22796ba26e551d303d366910eb0307","In this paper, we consider the problem of filtering a continuous stream of XML data efficiently against a large number of branch XPath queries. Several approaches have been proposed, and many of them improve their run-time efficiencies by sharing some paths between branch queries. This paper further improves the run-time efficiencies by classifying and grouping semantically equivalent twig patterns, and identifying the common paths that are shared between these groups. Query structure matching is done at index compilation phase, and the paths shared between these groups of queries are processed once. Experiments show that our proposal is efficient and scalable compared to previous work. Copyright 2007 ACM.",,"Efficiency; Information systems; Common path; Number of branches; Query structures; Run-time efficiency; Twig pattern; XML data; XML filtering; Xpath queries; XML",2-s2.0-84994716170
"Guo Q.-L., Fan X.-Z.","Question answer system based on natural language understanding",2007,"Journal of Harbin Institute of Technology (New Series)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547320429&partnerID=40&md5=57eb62ee71bf845f3ece5d8565cf847e","Automatic Question Answer System (QAS) is a kind of high-powered software system based on Internet. Its key technology is the interrelated technology based on natural language understanding, including the construction of knowledge base and corpus, the Word Segmentation and POS Tagging of text, the Grammatical Analysis and Semantic Analysis of sentences etc. This thesis dissertated mainly the denotation of knowledge-information based on semantic network in QAS, the stochastic syntax-parse model named LSF of knowledge-information in QAS, the structure and constitution of QAS. And the LSF models parameters were exercised, which proved that they were feasible. At the same time, through the limited-domain QAS which was exploited for banks by us, these technologies were proved effective and propagable.","LSF model; Predicate logic; Question answer system; Semantic network","Formal languages; Formal logic; Internet; Semantics; LSF model; Predicate logic; Question answer system; Semantic network; Query languages",2-s2.0-34547320429
"Ren H., Tian J., Nakamori Y., Wierzbicki A.P.","Electronic support for knowledge creation in a research institute",2007,"Journal of Systems Science and Systems Engineering",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34347242145&doi=10.1007%2fs11518-007-5046-6&partnerID=40&md5=49aaff179714734b43a4af991bf418fd","This paper focuses on the question how to build an electronic support environment for knowledge creation in a research institute (JAIST). In order to assess the importance of diverse conditions of scientific creativity, we performed a survey in JAIST, and extracted useful knowledge from the database of survey results. Following the analysis of the theory of academic processes of knowledge creation and the survey findings in JAIST, a computer-based integrated system is proposed. In the aspect of the system design, we postulate that an electronic support environment for academic creativity can be achieved through a seamless integration with Internet, Application Server, Middle Ware, Database and Data Warehouse. The paper addresses issues of knowledge representation in the Electronic Support System for academic research, testing and evaluation issues and conclusions. © Systems Engineering Society of China and Springer 2007.","Electronic support; Knowledge creation; Software system","Data warehouses; Internet; Knowledge acquisition; Middleware; Systems analysis; Computer based integrated system; Electronic support; Knowledge creation; Software system; Systems engineering",2-s2.0-34347242145
"Cai K.-K., Bu J.-J., Chen C., Qiu G.","A novel dependency language model for information retrieval",2007,"Journal of Zhejiang University: Science A",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547264137&doi=10.1631%2fjzus.2007.A0871&partnerID=40&md5=5fa1181c41c85958dd5b266f9cb8b907","This paper explores the application of term dependency in information retrieval (IR) and proposes a novel dependency retrieval model. This retrieval model suggests an extension to the existing language modeling (LM) approach to IR by introducing dependency models for both query and document. Relevance between document and query is then evaluated by reference to the Kullback-Leibler divergence between their dependency models. This paper introduces a novel hybrid dependency structure, which allows integration of various forms of dependency within a single framework. A pseudo relevance feedback based method is also introduced for constructing query dependency model. The basic idea is to use query-relevant top-ranking sentences extracted from the top documents at retrieval time as the augmented representation of query, from which the relationships between query terms are identified. A Markov Random Field (MRF) based approach is presented to ensure the relevance of the extracted sentences, which utilizes the association features between query terms within a sentence to evaluate the relevance of each sentence. This dependency retrieval model was compared with other traditional retrieval models. Experiments indicated that it produces significant improvements in retrieval effectiveness. © 2007 Springer-Verlag.","Language modeling (LM); Retrieval model; Sentence retrieval; Term dependency","Feedback; Information retrieval; Markov processes; Query processing; Language modeling (LM); Retrieval model; Sentence retrieval; Term dependency; Natural language processing systems",2-s2.0-34547264137
"Gajendran V.K., Lin J.-R., Fyhrie D.P.","An application of bioinformatics and text mining to the discovery of novel genes related to bone biology",2007,"Bone",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047269282&doi=10.1016%2fj.bone.2006.12.067&partnerID=40&md5=be07ec34c7a60e719a4f0e32bff0ae92","The treatment and management of complex genetic diseases such as osteoporosis can greatly benefit from the integration of relevant research across many different disciplines. We created a text mining tool that analyzes the PubMed literature database and integrates the available genomic information to provide a detailed mapping of the genes and their interrelationships within a particular network such as osteoporosis. The results obtained from our text mining program show that existing genomic data within the PubMed database can effectively be used to predict potentially novel target genes for osteoporosis research that have not previously been reported in the literature. To filter the most significant findings, we developed a ranking system to rate our predicted novel genes. Some of our predicted genes ranked higher than those currently studied, suggesting that they may be of particular interest from a therapeutic standpoint. A preliminary analysis of the current biomedical literature in our research area using our tool suggests that S100A12, as well as a group of SMAD genes previously unstudied in relation to osteoporosis, may be highly relevant to the mechanism of action of bisphosphonates, that the function of osteocytes may be influenced by a family of important interleukins and interleukin-related molecules, and that the FYN oncogene may play an important role in regulating the apoptosis of bone cells in the context of degenerative bone diseases. An evaluation of our tool's predictive ability with an analysis of PubMed literature published before the year 2000 in the area of osteoporosis research shows that many of its top-rated novel target genes from that analysis were later studied and shown to be relevant to osteoporosis in the period between 2000 and 2006. We believe that our tool will be beneficial to researchers in the field of orthopaedics seeking to identify novel target genes in their research area, and it will allow them to delve deeper into the complex interplay between genes, biological systems and diseases. © 2007 Elsevier Inc. All rights reserved.","Bisphosphonate; Bone apoptosis; Osteocyte; Osteoporosis; Text mining","bisphosphonic acid derivative; cytokine; protein kinase Fyn; protein S 100; protein s100a12; Smad protein; unclassified drug; apoptosis; article; bioinformatics; bone cell; bone disease; cell function; computer program; drug mechanism; gene identification; gene mapping; genomics; medical literature; MEDLINE; oncogene; osteocyte; osteoporosis; prediction; protein family; Bone and Bones; Computational Biology; Diphosphonates; Gene Regulatory Networks; Humans; Osteocytes; Osteoporosis",2-s2.0-34047269282
"Yang W.-D., Wang Q.-M., Shi B.-L.","Complex twig pattern query processing over XML streams",2007,"Ruan Jian Xue Bao/Journal of Software",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249313384&doi=10.1360%2fjos180893&partnerID=40&md5=320183036bf4627a72d9a54831b1a82d","The problem of processing streaming XML data is gaining widespread attention from the research community. In this paper, a novel approach for processing complex Twig Pattern with OR-predicates and AND-predicates over XML documents stream is presented. For the improvement of the processing performance of Twig Patterns, all the Twig Patterns are combined into a single prefix query tree that represents such queries by sharing their common prefixes. Its OR-predicates and AND-predicates of a node are represented as a separate abstract syntax tree associated with the node. Consequently, all the Twig Patterns are evaluated in a single, document-order pass over the input document stream for avoiding the interim results produced by the post-processing nested paths of YFilter. Compared with the existing approach, experimental results show that it can significantly improve the performance for matching complex Twig Patterns over XML document stream, especially for large size XML documents. Based on the prior works, the optimization of twig patters under DTD (document type definition) by using structural and constraint information of DTD is also addressed, which is static, namely, it is processed before the runtime of stream processing.","DTD (document type definition); Query tree; Twig Pattern; XML document stream; Xpath","Optimization; Query processing; Document stream; Document type definition; Twig Pattern; Xpath; XML",2-s2.0-34249313384
"Raghavachari M., Shmueli O.","Efficient revalidation of XML documents",2007,"IEEE Transactions on Knowledge and Data Engineering",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-64149087868&doi=10.1109%2fTKDE.2007.1004&partnerID=40&md5=83478a1680d3ea570331bc8971c06682","We study the problem of schema revalidation where XML data known to conform to one schema must be validated with respect to another schema. Such revalidation algorithms have applications in schema evolution, query processing, XML-based programming languages, and other domains. We describe how knowledge of conformance to an XML Schema may be used to determine conformance to another XML Schema efficiently. We examine both the situation where an XML document is modified before it is revalidated and the situation where it is unmodified © 2007 IEEE.","Subtyping; Updates; Validation; XML; XML Schema","Revalidation; Schema evolutions; Subtyping; Updates; Validation; XML datum; XML Schema; XML-based programming languages; Query languages; Query processing; XML; Markup languages",2-s2.0-64149087868
"Baswana S., Hariharan R., Sen S.","Improved decremental algorithms for maintaining transitive closure and all-pairs shortest paths",2007,"Journal of Algorithms",18,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247553345&doi=10.1016%2fj.jalgor.2004.08.004&partnerID=40&md5=653a75058f97249453ccb918f3dabb15","This paper presents improved algorithms for the following problem: given an unweighted directed graph G (V, E) and a sequence of on-line shortest-path/reachability queries interspersed with edge-deletions, develop a data-structure that can answer each query in optimal time, and can be updated efficiently after each edge-deletion. The central idea underlying our algorithms is a scheme for implicitly storing all-pairs reachability/shortest-path information, and an efficient way to maintain this information. Our algorithms are randomized and have one-sided inverse polynomial error for query. © 2004 Elsevier Inc. All rights reserved.","BFS tree; Dynamic; Graph; Shortest paths; Transitive closure","Edge detection; Error analysis; Polynomials; Problem solving; Trees (mathematics); BFS tree; Shortest paths; Transitive closure; Algorithms",2-s2.0-34247553345
"Zhou L., Hripcsak G.","Temporal reasoning with medical data-A review with emphasis on medical natural language processing",2007,"Journal of Biomedical Informatics",69,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847640080&doi=10.1016%2fj.jbi.2006.12.009&partnerID=40&md5=13ef8becf3437f28385fa743c75d16c2","Temporal information is crucial in electronic medical records and biomedical information systems. Processing temporal information in medical narrative data is a very challenging area. It lies at the intersection of temporal representation and reasoning (TRR) in artificial intelligence and medical natural language processing (MLP). Some fundamental concepts and important issues in relation to TRR have previously been discussed, mainly in the context of processing structured data in biomedical informatics; however, it is important that these concepts be re-examined in the context of processing narrative data using MLP. Theoretical and methodological TRR studies in biomedical informatics can be classified into three main categories: category 1 applies theories and models from temporal reasoning in AI; category 2 defines frameworks that meet needs from clinical applications; category 3 resolves issues such as temporal granularity and uncertainty. Currently, most MLP systems are not designed with a formal representation of time, and their ability to reason about temporal relations among medical events is limited. Previous work in processing time with clinical narrative data includes processing time in clinical reports, modeling textual temporal expressions in clinical databases, processing time in clinical guidelines, and building time standards for data exchange and integration. In addition to common problems in MLP, there are challenges specific to TRR in medical text, which occur at each level of linguistic structure and analysis. Despite advances in temporal reasoning in biomedical informatics, processing time in medical text deserves more attention. Besides the need for more research in temporal granularity, fuzzy time, temporal contradiction, intermittent events and uncertainty, broad areas for future research include enhancing functions of current MLP systems on processing temporal information, incorporating medical knowledge into temporal reasoning systems, resolving coreference, integrating narrative data with structured data and evaluating these systems.","Medical narrative data; Natural language processing; Temporal reasoning; Temporal representation","Artificial intelligence; Data reduction; Data structures; Fuzzy sets; Information systems; Natural language processing systems; Medical narrative data; Temporal reasoning; Temporal representation; Bioinformatics; coding; content analysis; data analysis; data base; electronic data interchange; electronic medical record; information storage; information technology; linguistics; medical informatics; natural language processing; practice guideline; priority journal; review; semantics; standard; time; Artificial Intelligence; Database Management Systems; Databases, Factual; Information Storage and Retrieval; Medical History Taking; Medical Records Systems, Computerized; Narration; Natural Language Processing; Pattern Recognition, Automated; Terminology; Time Factors",2-s2.0-33847640080
"Mitkov R., Pekar V., Blagoev D., Mulloni A.","Methods for extracting and classifying pairs of cognates and false friends",2007,"Machine Translation",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749126533&doi=10.1007%2fs10590-008-9034-5&partnerID=40&md5=c1ac94ee00f90387e9df39e0ded859b2","The identification of cognates has attracted the attention of researchers working in the area of Natural Language Processing, but the identification of false friends is still an under-researched area. This paper proposes novel methods for the automatic identification of both cognates and false friends from comparable bilingual corpora. The methods are not dependent on the existence of parallel texts, and make use of only monolingual corpora and a bilingual dictionary necessary for the mapping of co-occurrence data across languages. In addition, the methods do not require that the newly discovered cognates or false friends are present in the dictionary and hence are capable of operating on out-of-vocabulary expressions. These methods are evaluated on English, French, German and Spanish corpora in order to identify English-French, English-German, English-Spanish and French-Spanish pairs of cognates or false friends. The experiments were performed in two settings: (i) assuming 'ideal' extraction of cognates and false friends from plain-text corpora, i.e. when the evaluation data contains only cognates and false friends, and (ii) a real-world extraction scenario where cognates and false friends have to first be identified among words found in two comparable corpora in different languages. The evaluation results show that the developed methods identify cognates and false friends with very satisfactory results for both recall and precision, with methods that incorporate background semantic knowledge, in addition to co-occurrence data obtained from the corpora, delivering the best results. © 2008 Springer Science+Business Media B.V.","Cognates; Distributonal similarity; Faux amis; Orthographic similarity; Semantic similarity; Translational equivalence","Addition reactions; Artificial intelligence; Automation; Computational linguistics; Electronic data interchange; Extraction; Information theory; Learning algorithms; Linguistics; Natural language processing systems; Query languages; automatic identification; Bilingual corpora; Bilingual dictionaries; Business media; Co-occurrence; Comparable corpora; Corpora (CO); Evaluation data; Evaluation results; In order; Languages (traditional); Natural Language Processing (NLP); Novel methods; Out-of-vocabulary (OOV); Parallel texts; Real world; Semantic knowledge; Spanish; Spanish corpora; Springer (CO); text corpora; Knowledge based systems",2-s2.0-47749126533
"De Boni M.","Using logical relevance for question answering",2007,"Journal of Applied Logic",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846192781&doi=10.1016%2fj.jal.2005.12.003&partnerID=40&md5=68ab6de319627617f3657fd6c8a45b44","We propose a novel method of determining the appropriateness of an answer to a question through a proof of logical relevance rather than a logical proof of truth. We define logical relevance as the idea that answers should not be considered as absolutely true or false in relation to a question, but should be considered true more flexibly in a sliding scale of aptness. This enables us to reason rigorously about the appropriateness of an answer even in cases where the sources we are getting answers from are incomplete or inconsistent or contain errors. We show how logical relevance can be implemented through the use of measured simplification, a form of constraint relaxation, in order to seek a logical proof than an answer is in fact an answer to a particular question. We then give an example of such an implementation providing a set of specific rules for this purpose. © 2005 Elsevier B.V. All rights reserved.","Logical relevance; Question answering; Relevance","Error analysis; Numerical methods; Constraint relaxation; Logical relevance; Question answering; Formal logic",2-s2.0-33846192781
"Lu C.-T., Dos Santos Jr. R.F., Sripada L.N., Kou Y.","Advances in GML for geospatial applications",2007,"GeoInformatica",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847683494&doi=10.1007%2fs10707-006-0013-9&partnerID=40&md5=0a9d8a79238958763ed9c3fcb707be85","This paper presents a study of Geography Markup Language (GML), the issues that arise from using GML for spatial applications, including storage, parsing, querying and visualization, as well as the use of GML for mobile devices and web services. GML is a modeling language developed by the Open Geospatial Consortium (OGC) as a medium of uniform geographic data storage and exchange among diverse applications. Many new XML-based languages are being developed as open standards in various areas of application. It would be beneficial to integrate such languages with GML during the developmental stages, taking full advantage of a non-proprietary universal standard. As GML is a relatively new language still in development, data processing techniques need to be refined further in order for GML to become a more efficient medium for geospatial applications. © Springer Science+Business Media, LLC 2007.","Geospatial data processing; Geospatial data storage; Geospatial database; GML; XML","Data processing; Database systems; Geographic information systems; Open systems; Geography markup language; Geospatial data processing; Open geospatial consortium; Markup languages; data processing; database; GIS; language; software; spatial data; visualization",2-s2.0-33847683494
"Clegg A.B., Shepherd A.J.","Benchmarking natural-language parsers for biological applications using dependency graphs",2007,"BMC Bioinformatics",52,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847095048&doi=10.1186%2f1471-2105-8-24&partnerID=40&md5=52e4e151781873b191e1f7f903ae8767","Background: Interest is growing in the application of syntactic parsers to natural language processing problems in biology, but assessing their performance is difficult because differences in linguistic convention can falsely appear to be errors. We present a method for evaluating their accuracy using an intermediate representation based on dependency graphs, in which the semantic relationships important in most information extraction tasks are closer to the surface. We also demonstrate how this method can be easily tailored to various application-driven criteria. Results: Using the GENIA corpus as a gold standard, we tested four open-source parsers which have been used in bioinformatics projects. We first present overall performance measures, and test the two leading tools, the Charniak-Lease and Bikel parsers, on subtasks tailored to reflect the requirements of a system for extracting gene expression relationships. These two tools clearly outperform the other parsers in the evaluation, and achieve accuracy levels comparable to or exceeding native dependency parsers on similar tasks in previous biological evaluations. Conclusion: Evaluating using dependency graphs allows parsers to be tested easily on criteria chosen according to the semantics of particular biological applications, drawing attention to important mistakes and soaking up many insignificant differences that would otherwise be reported as errors. Generating high-accuracy dependency graphs from the output of phrase-structure parsers also provides access to the more detailed syntax trees that are used in several natural-language processing techniques. © 2007 Clegg and Shepherd; licensee BioMed Central Ltd.",,"Biological applications; Biological evaluation; Dependency graphs; Intermediate representations; NAtural language processing; Performance measure; Processing technique; Semantic relationships; Bioinformatics; Errors; Gene expression; Natural language processing systems; Tools; Trees (mathematics); Semantics; transcription factor GATA 1; access to information; accuracy; analytical error; article; bioinformatics; gene expression; human; information processing; macrophage; monocyte; natural language processing; nonhuman; semantics; algorithm; animal; biology; computer language; computer program; data base; devices; information retrieval; language; procedures; quality control; reproducibility; standards; statistical model; Algorithms; Animals; Benchmarking; Computational Biology; Database Management Systems; Humans; Information Storage and Retrieval; Language; Models, Statistical; Natural Language Processing; Programming Languages; Reproducibility of Results; Software",2-s2.0-33847095048
"Horng-Jyh P.W., Jin-Cheon N., Soo-Guan C.K.","A hybrid approach to fuzzy name search incorporating language-based and text-based principles",2007,"Journal of Information Science",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846606041&doi=10.1177%2f0165551506068146&partnerID=40&md5=042cbad825f25b84399679ce49b95ddf","Name Search is an important search function in various types of information retrieval systems, such as online library catalogs and electronic yellow pages. It is also difficult, because of the high degree of fuzziness required in matching name variants. Previous approaches to name search systems use ad hoc combinations of search heuristics. This paper first discusses two approaches to name modeling - the natural language processing (NLP) and information retrieval (IR) models - and proposes a hybrid approach. The approach demonstrates a critical combination of complementary NLP and IR features that produces more effective fuzzy name matching. Two principles, position-as-attribute and position-transition-likelihood, are introduced as the principles for integrating the advantageous aspects of both approaches. They have been implemented in an NLP- and IR-hybrid model system called Friendly Name Search (FNS) for real world applications in multilingual directory searches on the Singapore Yellow pages website.","Fuzzy name search; Hybrid system; Information retrieval; Language and text; Natural language processing","Database systems; Fuzzy sets; Heuristic methods; Information retrieval; Natural language processing systems; Search engines; Fuzzy name search; Hybrid systems; Language and text; Computer programming languages",2-s2.0-33846606041
"Jiang T., Tan A.-H., Wang K.","Mining generalized associations of semantic relations from textual web content",2007,"IEEE Transactions on Knowledge and Data Engineering",36,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846239955&doi=10.1109%2fTKDE.2007.36&partnerID=40&md5=f561e005df18224a0743ece9986c1033","Traditional text mining techniques transform free text into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. In this paper, we present a two-step procedure to mine generalized associations of semantic relations conveyed by the textual content of Web documents. First, RDF (Resource Description Framework) metadata representing semantic relations are extracted from raw text using a myriad of natural language processing techniques. The relation extraction process also creates a term taxonomy in the form of a sense hierarchy inferred from WordNet. Then, a novel generalized association pattern mining algorithm (GP-Close) is applied to discover the underlying relation association patterns on RDF metadata. For pruning the large number of redundant overgeneralized patterns in relation pattern search space, the GP-Close algorithm adopts the notion of generalization closure for systematic overgeneralization reduction. The efficacy of our approach is demonstrated through empirical experiments conducted on an online database of terrorist activities. © 2007 IEEE.","Association rule mining; RDF mining; Relation association; Text mining","Association rule mining; Relation association; Systematic overgeneralization reduction; Text mining; Algorithms; Data mining; Knowledge acquisition; Metadata; Natural language processing systems; Online searching; Semantics",2-s2.0-33846239955
"Lana-Serrano S., Villena-Román J., Goñi-Menoyo J.M.","MIRACLE at GeoCLEF query parsing 2007: Extraction and classification of geographical information",2007,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921978599&partnerID=40&md5=5150ee5e68258da0562a19a9db47def3","This paper describes the participation of MIRACLE research consortium at the Query Parsing task of GeoCLEF 2007. Our system is composed of three main modules. First, the Named Geo-entity Identifier, whose objective is to perform the geo-entity identification and tagging, i.e., to extract the ""where"" component of the geographical query, should there be any. This module is based on a gazetteer built up from the Geonames geographical database and carries out a sequential process in three steps that consist on geo-entity recognition, geo-entity selection and query tagging. Then, the Query Analyzer parses this tagged query to identify the ""what"" and ""geo-relation"" components by means of a rule-based grammar. Finally, a two-level multiclassifier first decides whether the query is indeed a geographical query and, should it be positive, then determines the query type according to the type of information that the user is supposed to be looking for: map, yellow page or information. According to a strict evaluation criterion where a match should have all fields correct, our system reaches a precision value of 42.8% and a recall of 56.6% and our submission is ranked 1st out of 6 participants in the task. A detailed evaluation of the confusion matrixes reveal that some extra effort must be invested in ""user-oriented"" disambiguation techniques to improve the first level binary classifier for detecting geographical queries, as it is a key component to eliminate many false-positives.","Classification; Gazetteer; Geographic entity recognition; Geographical IR; Linguistic engineering; Semantic expansion; Wordnet","Digital libraries; Digital libraries; Formal languages; Formal languages; Query processing; Query processing; Semantics; Semantics; User interfaces; User interfaces; Entity recognition; Entity recognition; Gazetteer; Gazetteer; Linguistic engineering; Linguistic engineering; Semantic expansion; Semantic expansion; Wordnet; Wordnet; Classification (of information); Classification (of information)",2-s2.0-84921978599
"Cabral L.M., Costa L.F., Santos D.","Esfinge at CLEF 2007: First steps in a multiple question and multiple answer approach",2007,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921959665&partnerID=40&md5=5817d3d8507f3033564ed0c0bbac0a20","Esfinge is a general domain Portuguese question answering system which uses the information available on the Web as an additional resource when searching for answers. Other external resources and tools used are a broad coverage parser, a morphological analyzer, a named entity recognizer and a Web-based database of word co-occurrences. In this fourth participation in CLEF, in addition to the new challenges posed by the organization (topics and anaphors in questions and the use of Wikipedia to search and support answers), we experimented with a multiple question and multiple answer approach in QA. Although the official results were severely compromised by a series of bugs, later experiments showed that the hardest-and so far mostly unsuccessful-subtask for Esfinge with several competing answers was to effect a principled choice among them. Anyway, access to Wikipedia managed to achieve better results on last year's questions, and, based on a satisfactory evaluation of the anaphoric reference module, we can conclude that Esfmge's current results are mainly due to a increase in the question's difficulty.","Anaphoric reference; Named entity recognition; Parser evaluation; Portuguese; Question answering; Wikipedia","Artificial intelligence; Natural language processing systems; Search engines; Social networking (online); Anaphoric reference; Named entity recognition; Parser evaluation; Portuguese; Question Answering; Wikipedia; Digital libraries",2-s2.0-84921959665
"Kalpathy-Cramer J., Hersh W.","Medical image retrieval and automatic annotation: OHSU at ImageCLEF 2007",2007,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921941105&partnerID=40&md5=be90e69688d34af1b0dc73f357aee5ee","Oregon Health & Science University participated in the medical retrieval and medical annotation tasks of ImageCLEF 2007. In the medical retrieval task, we created a webbased retrieval system for the collection built on a full-text index of both image and case annotations. The text-based search engine was implemented in Ruby using Ferret, a port of Lucene, and a custom query parser. In addition to this textual index of annotations, supervised machine learning techniques using visual features were used to classify the images based on image acquisition modality. All images were annotated with the purported modality. Purely textual runs as well as mixed runs using the purported modality were submitted. Our runs performed moderately well using the MAP metric and better for the early precision (P10) metric. In the automatic annotation task, we used the 'gist' technique to create the feature vectors. Using statistics derived from a set of multi-scale oriented filters, we created a 512 dimensional vector. PCA was then used to create a 100-dimensional vector. This feature vector was fed into a two layer neural network. Our error rate on the 1000 test images was 67.8 using the hierarchical error calculations.","Image modality classification; Neural networks; Query parsing; Text retrieval","Artificial intelligence; Content based retrieval; Digital libraries; Image classification; Image retrieval; Information retrieval; Learning systems; Medical imaging; Network layers; Neural networks; Supervised learning; Text processing; Vectors; Automatic annotation; Dimensional vectors; Image modality; Medical retrieval tasks; Query parsing; Supervised machine learning; Text retrieval; Web-based retrieval systems; Search engines",2-s2.0-84921941105
"Mandelbaum Y., Fisher K., Walker D., Fernandez M., Gleyzer A.","PADS/ML: A functional data description language",2007,"ACM SIGPLAN Notices",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846528587&partnerID=40&md5=043d5f2ce17fcbf18b9e751ed13cb5ed","Massive amounts of useful data are stored and processed in ad hoc formats for which common tools like parsers, printers, query engines and format converters are not readily available. In this paper, we explain the design and implementation of PADS/ML, a new language and system that facilitates the generation of data processing tools for ad hoc formats. The PADS/ML design includes features such as dependent, polymorphic and recursive datatypes, which allow programmers to describe the syntax and semantics of ad hoc data in a concise, easy-to-read notation. The PADS/ML implementation compiles these descriptions into ML structures and functors that include types for parsed data, functions for parsing and printing, and auxiliary support for user-specified, format-dependent and format-independent tool generation. Copyright © 2007 ACM.","Data description languages; Dependent types; Domain-specific languages; Functional programming; ML; Modules; Parsing; Printing","Data description languages; Dependent types; Domain-specific languages; Format-independent tool generation; Functional programming; Data processing; Printers (computer); Semantics; Syntactics; Query languages",2-s2.0-33846528587
"Jorwekar S., Fekete A., Ramamritham K., Sudarshan S.","Automating the detection of snapshot isolation anomalies",2007,"33rd International Conference on Very Large Data Bases, VLDB 2007 - Conference Proceedings",32,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011061639&partnerID=40&md5=bf4e8a7259f941fa302fd04cdf7fcd5d","Snapshot isolation (SI) provides significantly improved concurrency over 2PL, allowing reads to be non-blocking. Unfortunately, it can also lead to non-serializable executions in general. Despite this, it is widely used, supported in many commercial databases, and is in fact the highest available level of consistency in Oracle and PostgreSQL. Sufficient conditions for detecting whether SI anomalies could occur in a given set of transactions were presented recently, and extended to necessary conditions for transactions without predicate reads. In this paper we address several issues in extending the earlier theory to practical detection/correction of anomalies. We first show how to mechanically find a set of programs which is large enough so that we ensure that all executions will be free of SI anomalies, by modifying these programs appropriately. We then address the problem of false positives, i.e., transaction programs wrongly identified as possibly leading to anomalies, and present techniques that can significantly reduce such false positives. Unlike earlier work, our techniques are designed to be automated, rather than manually carried out. We describe a tool which we are developing to carry out this task. The tool operates on descriptions of the programs either taken from the application code itself, or taken from SQL query traces. It can be used with any database system. We have used our tool on two real world applications in production use at IIT Bombay, and detected several anomalies, some of which have caused real world problems. We believe such a tool will be invaluable for ensuring safe execution of the large number of applications which are already running under SI. Copyright 2007 VLDB Endowment, ACM.",,"Query processing; Application codes; False positive; Level of consistencies; Non-blocking; PostgreSQL; Real-world; Real-world problem; Snapshot isolation; Application programs",2-s2.0-85011061639
"Haddad C., Desai B.C.","Cross lingual question answering using CINDI-QA for QA@CLEF 2007",2007,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921967218&partnerID=40&md5=637465786bcd784e65ec6442924faabd","This article presents the first participation of the CINDI group in the Multiple Language Question Answering Cross Language Evaluation Forum (QA@CLEF). We participated in a track using French as the source language and English as the target language. CINDI-QA first uses an online translation tool to convert the French input question into an English sentence. Second, a Natural Language Parser extracts keywords such as verbs, nouns, adjectives and capitalized entites from the query. Third, synonyms of those keywords are generated thanks to a Lexical Reference module. Fourth, our integrated Searching and Indexing component localises the answer candidates from the QA@CLEF data collection provided to us. Finally, the candidates are matched against our existing set of templates to decide on the best answer to return to the user. Two runs were submitted. Having missed using a large part of the corpora, CINDI-QA was only able to generate correct and exact answers for 13% of the questions it received.","Bilingual; English; French; Question answering; Questions beyond factoids","Computational linguistics; Digital libraries; Syntactics; Bilingual; English; French; Question Answering; Questions beyond factoids; Natural language processing systems",2-s2.0-84921967218
"Angioni M., Demontis R., Deriu M., Tuveri F.","SemanticNet: A WordNet-based tool for the navigation of semantic information",2007,"GWC 2008: 4th Global WordNet Conference, Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904623721&partnerID=40&md5=fde12cae0b7ae1343f6f27f74d6e8693","The main aim of the DART search engine is to index and retrieve information both in a generic and in a specific context where documents can be mapped or not on ontologies, vocabularies and thesauri. To achieve this goal, a semantic analysis process on structured and unstructured parts of documents is performed. While the unstructured parts need a linguistic analysis and a semantic interpretation performed by means of Natural Language Processing (NLP) techniques, the structured parts need a specific parser. In this paper we illustrate how semantic keys are extracted from documents starting from WordNet and used by an automatic tool in order to define a new semantic net called SemanticNet build enriching the WordNet semantic net with new nodes, links and attributes. Formulating the query through the search engine, the user can move through the SemanticNet and extracts the concepts which really interest him, limiting the search field and obtaining a more specific result by means of a dedicated tool called 3DUI4SemanticNet. © University of Szeged, Department of Informatics, 2007. All rights are reserved.","3D user interface; NLP; Ontologies; Semantic net","Natural language processing systems; Search engines; Semantics; User interfaces; 3D user interface; Linguistic analysis; NAtural language processing; NLP; Semantic analysis; Semantic information; Semantic interpretation; Semantic net; Ontology",2-s2.0-84904623721
"Minock M.J.","A STEP towards realizing Codd's vision of rendezvous with the casual user",2007,"33rd International Conference on Very Large Data Bases, VLDB 2007 - Conference Proceedings",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449251472&partnerID=40&md5=f2e6ec88bc93706252473835e2c51909","This demonstration showcases the STEP system for natural language access to relational databases. In STEP an administrator authors a highly structured semantic grammar through coupling phrasal patterns to elementary expressions within a decidable fragment of tuple relational calculus. The resulting phrasal lexicon serves as a bi-directional grammar, enabling the generation of natural language from tuple relational calculus and the inverse parsing of natural language to tuple calculus. This ability to both understand and generate natural language enables STEP to engage the user in clarification dialogs when the parse of their query is of questionable quality. The STEP system is nearing completion and will soon be field tested in several domains. Copyright 2007 VLDB Endowment, ACM.",,"Calculations; Formal languages; Semantics; Bi-directional; Natural languages; Phrasal patterns; Relational Database; Semantic grammars; STEP system; Tuple relational calculus; Natural language processing systems",2-s2.0-78449251472
"Li J., Brew C.","Disambiguating Levin verbs using untagged data",2007,"International Conference Recent Advances in Natural Language Processing, RANLP",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959051339&partnerID=40&md5=3b42a7802ecefa770663f2f73934d41d","Lapata and Brew [8] (hereafter LB04) obtain from untagged texts a statistical prior model that is able to generate class preferences for ambiguous Levin [9] verbs. They also show that their informative priors, incorporated into a Naive Bayesian classifier deduced from handtagged data, can aid in verb class disambiguation. We re-examine the parameter estimation of LB04's prior model and identify the only parameter in LB04's prior model that determines the predominant class for a particular verb in a particular frame. In addition, we propose a method for training our classifier without using hand-tagged data. Our experiments suggest that although our verb class disambiguator does not match the performance of the ones that make use of hand-tagged data, it consistently outperforms the random baseline model. Our experiments also demonstrate that the informative priors derived from untagged texts help improve the performance of the classifier trained on untagged data.","Informative priors; Levin verb class; Lexical semantics; Naive Bayesian classifier; Untagged corpus; Verb class disambiguation","Computational linguistics; Natural language processing systems; Semantics; Informative Priors; Levin verb class; Lexical semantics; Naive Bayesian Classifier; Untagged corpus; Verb class disambiguation; Classification (of information)",2-s2.0-84959051339
"Burger A.","Agent technologies in the life sciences",2007,"Semantic Web: Revolutionizing Knowledge Discovery in the Life Sciences",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919974347&doi=10.1007%2f978-0-387-48438-9_16&partnerID=40&md5=1528466085cc56bf8f1d33a760fbd7c0","Software systems for the Life Sciences in the context of the Semantic Web will typically be driven by domain knowledge and distributed across the Internet, suggesting that software agent technology should play a key part in the development of such applications. This chapter introduces the reader to the areas of intelligent agents and multiagent systems, describes various Life Science applications that were built following the agent paradigm and reviews future trends in agent technology and their relevance to the development of bioinformatics systems. Some of the obstacles that need to be overcome in this context are discussed. © 2007 Springer Science+Business Media, LLC. All rights reserved.","Agent Roadmap; Architecture; Distributed Problem Solving; Intelligent Agents; Interaction Protocols; Multiagent Systems; Planning","Application programs; Architecture; Intelligent agents; Multi agent systems; Network architecture; Planning; Agent paradigm; Agent technology; Distributed problem solving; Domain knowledge; Inter-action protocols; Roadmap; Software agent technology; Software systems; Software agents",2-s2.0-84919974347
"Lindenberg J., Pasman W., Kranenborg K., Stegeman J., Neerincx M.A.","Improving service matching and selection in ubiquitous computing environments: A user study",2007,"Personal and Ubiquitous Computing",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750298232&doi=10.1007%2fs00779-006-0066-7&partnerID=40&md5=b566114effdc0c50952138c66b31a500","In large ubiquitous computing environments it is hard for users to identify and activate the electronic services that match their needs. This user study compares the newly developed service matcher system with a conventional system for identifying and selecting appropriate services. The study addresses human factors issues such as usability, trust and service awareness. With the conventional system users have to browse a hierarchical list of currently available services and activate the service that they think satisfies their current needs. With the service matcher users just enter their current need using natural language, after which a wizard, emulating an existing service matcher algorithm, searches for and activates a matching service based on the given need and the users' location and gaze direction. This study shows that with the hierarchical list, only 66% of the tasks are solved correctly, and females score significantly worse than males. With the service matcher, the performance increases significantly to 84% correctly performed tasks and the gender difference disappears. © Springer-Verlag London Limited 2006.","Agents; Ambient intelligence; Gender differences; Service awareness; Service matching; User study","Algorithms; Human engineering; Intelligent agents; Natural language processing systems; Ambient intelligence; Service awareness; Service matching; User study; Embedded systems",2-s2.0-33750298232
"Ding S., Huang M., Wang H., Zhu X.","Profile-feature based protein interaction extraction from full-text articles",2007,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908269692&partnerID=40&md5=81b96bd68b98be4e6b32fa1f928ed802","Various methods have been proposed to extract genetic protein-protein interactions from abstracts. These methods are unable to specify the interactions in which molecules are physically related and fail to explore the abundant evidence all over the articles. In this paper, we present a method of mining physical protein-protein interactions by exploiting profile feature from full-text articles during our participation in the second task of BioCreAtIvE Challenge 2006. This method synthesizes the features from the whole article as the protein pair's profile to extract the physical interactions, and specifies the SwissProt AC of the molecules involved in the interaction to help biologists make use of the information of the molecules, such as the sequence and cross reference. Compared with the other methods' performance released in BioCreAtIvE 2006, our method has shown very promising results.","Information extraction; Protein-protein interaction; Text mining","Bioinformatics; Information retrieval; Molecules; Proteins; Feature-based; Physical interactions; Profile features; Protein interaction; Protein-protein interactions; SWISS-PROT; Text mining; Data mining",2-s2.0-84908269692
"Palakal M., Bright J., Sebastian T., Hartanto S.","A comparative study of cells in inflammation, EAE and MS using biomedical literature data mining",2007,"Journal of Biomedical Science",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847112465&doi=10.1007%2fs11373-006-9120-8&partnerID=40&md5=b74f0099bae5639854b420401c2a22fa","Biomedical literature and database annotations, available in electronic forms, contain a vast amount of knowledge resulting from global research. Users, attempting to utilize the current state-of-the-art research results are frequently overwhelmed by the volume of such information, making it difficult and time-consuming to locate the relevant knowledge. Literature mining, data mining, and domain specific knowledge integration techniques can be effectively used to provide a user-centric view of the information in a real-world biological problem setting. Bioinformatics tools that are based on real-world problems can provide varying levels of information content, bridging the gap between biomedical and bioinformatics research. We have developed a user-centric bioinformatics research tool, called BioMap, that can provide a customized, adaptive view of the information and knowledge space. BioMap was validated by using inflammatory diseases as a problem domain to identify and elucidate the associations among cells and cellular components involved in multiple sclerosis (MS) and its animal model, experimental allergic encephalomyelitis (EAE). The BioMap system was able to demonstrate the associations between cells directly excavated from biomedical literature for inflammation, EAE and MS. These association graphs followed the scale-free network behavior (average γ = 2.1) that are commonly found in biological networks. © 2006 National Science Council.","Association discovery; Association graph; Biomedical literature mining; Experimental allergic encephalomyelitis (EAE); Inflammation; Multiple sclerosis (MS)","allergic encephalomyelitis; article; bioinformatics; biomedicine; cell composition; comparative study; cytology; data mining; human; medical information system; multiple sclerosis; nonhuman; priority journal; Animals; Biomedical Research; Encephalomyelitis, Autoimmune, Experimental; Humans; Inflammation; Multiple Sclerosis; Natural Language Processing; Pattern Recognition, Automated; PubMed; Animalia",2-s2.0-33847112465
"Koike A., Takagi T.","Knowledge discovery based on an implicit and explicit conceptual network",2007,"Journal of the American Society for Information Science and Technology",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845976670&doi=10.1002%2fasi.20421&partnerID=40&md5=97a1aa7a0fe1091392af292dbd2584e7","The amount of knowledge accumulated in published scientific papers has increased due to the continuing progress being made in scientific research. Since numerous papers have only reported fragments of scientific facts, there are possibilities for discovering new knowledge by connecting these facts. We therefore developed a system called BioTermNet to draft a conceptual network with hybrid methods of information extraction and information retrieval. Two concepts are regarded as related in this system if (a) their relationship is clearly described in MEDLINE abstracts or (b) they have distinctively co-occurred in abstracts. PRIME data, including protein interactions and functions extracted by NLP techniques, are used in the former, and the Singhal-measure for information retrieval is used in the latter. Relationships that are not clearly or directly described in an abstract can be extracted by connecting multiple concepts. To evaluate how well this system performs, Swanson's association between Raynaud's disease and fish oil and that between migraine and magnesium were tested with abstracts that had been published before the discovery of these associations. The result was that when start and end concepts were given, plausible and understandable intermediate concepts connecting them could be detected. When only the start concept was given, not only the focused concept (magnesium and fish oil) but also other probable concepts could be detected as related concept candidates. Finally, this system was applied to find diseases related to the BRCA1 gene. Some other new potentially related diseases were detected along with diseases whose relations to BRCA1 were already known. The BioTermNet is available at http://btn.ontology. ims.u-tokyo.ac.jp.",,"Abstracting; Data reduction; Genes; Information retrieval; Knowledge engineering; Telecommunication networks; Conceptual network; Hybrid methods; Information extraction; Knowledge discovery; Knowledge based systems",2-s2.0-33845976670
"Lussier Y.A., Liu Y.","Computational approaches to phenotyping: High-throughput phenomics",2007,"Proceedings of the American Thoracic Society",39,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846080223&doi=10.1513%2fpats.200607-142JG&partnerID=40&md5=6d973ffb4553aa3b50a0fe41b13ac00a","The recent completion of the Human Genome Project has made possible a high-throughput ""systems approach"" for accelerating the elucidation of molecular underpinnings of human diseases, and subsequent derivation of molecular-based strategies to more effectively prevent, diagnose, and treat these diseases. Although altered phenotypes are among the most reliable manifestations of altered gene functions, research using systematic analysis of phenotype relationships to study human biology is still in its infancy. This article focuses on the emerging field of high-throughput phenotyping (HTP) phenomics research, which aims to capitalize on novel high-throughput computation and informatics technology developments to derive genomewide molecular networks of genotype-phenotype associations,or ""phenomic associations."" The HTP phenomics research field faces the challenge of technological research and development to generate novel tools in computation and informatics that will allow researchers to amass, access, integrate, organize, and manage phenotypic databases across species and enable genomewide analysis to associate phenotypic information with genomic data at different scales of biology. Key state-of-the-art technological advancements critical for HTP phenomics research are covered in this review. In particular, we highlight the power of computational approaches to conduct large-scale phenomics studies.","Computational genomics; Gene-disease associations; Phenomics; Phenotype","gene function; genetic analysis; genetic database; genetic linkage; genotype; human; information processing; information science; mathematical model; nonhuman; phenotype; proteomics; review; Computational Biology; Databases, Genetic; Genomics; Humans; Phenotype",2-s2.0-33846080223
"Deek F.P., McHugh J.A.M.","Open source: Technology and policy",2007,"Open Source: Technology and Policy",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928362136&doi=10.1017%2fCBO9780511619526&partnerID=40&md5=6b857f663efd1c2e61f6663d651e9eac","The open source movement is a worldwide effort to promote an open style of software development more aligned with the accepted intellectual style of science than the proprietary modes of invention that have been characteristic of modern business. The idea is to keep the scientific advances created by software development openly available for everyone to use, understand, and improve. The very process of open source creation is highly transparent. This book addresses prominent projects in the open source movement, along with its enabling technologies, social characteristics, legal issues, business venues, and public and educational roles. © Fadi P. Deek and James A. M. McHugh 2008 and Cambridge University Press, 2010.",,"Open systems; Software design; Enabling technologies; Legal issues; Open source movement; Open sources; Scientific advances; Open source software",2-s2.0-84928362136
"Frost R.A.","Realization of natural language interfaces using lazy functional programming",2006,"ACM Computing Surveys",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845987528&doi=10.1145%2f1177352.1177353&partnerID=40&md5=d3d22bde47548bac93b955ab2e5df25d","The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. This is to be expected as the task is extremely difficult, involving syntactic and semantic analysis of potentially ambiguous input. The use of LISP and Prolog in this area is well documented. However, research involving the relatively new lazy functional programming paradigm is less well known. This paper provides a comprehensive survey of that research. © 2006 ACM.","Computational linguistics; Higher-order functions; Lazy functional programming; Montague grammar; Natural-language interfaces","Higher-order functions; Lazy functional programming; Montague grammar; Natural-language interfaces; Computational linguistics; Computer programming; Interfaces (computer); Semantics; Syntactics; Natural language processing systems",2-s2.0-33845987528
"Gao J., Nie J.-Y., Zhou M.","Statistical query translation models for cross-language information retrieval",2006,"ACM Transactions on Asian Language Information Processing",15,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147207726&doi=10.1145%2f1236181.1236184&partnerID=40&md5=a1688edd38a1b5e8cf6835bb1f4ba01b","Query translation is an important task in cross-language information retrieval (CLIR), which aims to determine the best translation words and weights for a query. This article presents three statistical query translation models that focus on the resolution of query translation ambiguities. All the models assume that the selection of the translation of a query term depends on the translations of other terms in the query. They differ in the way linguistic structures are detected and exploited. The co-occurrence model treats a query as a bag of words and uses all the other terms in the query as the context for translation disambiguation. The other two models exploit linguistic dependencies among terms. The noun phrase (NP) translation model detects NPs in a query, and translates each NP as a unit by assuming that the translation of a term only depends on other terms within the same NP. Similarly, the dependency translation model detects and translates dependency triples, such as verb-object, as units. The evaluations show that linguistic structures always lead to more precise translations. The experiments of CLIR on TREC Chinese collections show that all three models have a positive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best results are obtained by combining the three models. © 2006 ACM.","CLIR; Linguistic structures; Query translation; Statistical models","Cross language information retrieval (CLIR); Linguistic dependencies; Linguistic structures; Query translation; Statistical models; Computational linguistics; Query languages; Query processing; Statistical methods; Information retrieval",2-s2.0-34147207726
"Völkel M.","SemWiki: A RESTful distributed Wiki architecture",2006,"Proceedings of WikiSym'06 - 2006 International Symposium on Wikis",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247329529&doi=10.1145%2f1149453.1149485&partnerID=40&md5=98a79c74c2477b0e011da50529362ace","Current Wiki engines are mostly monolithic applications which intermingle parser, user interface and data management backend. In this paper we show how these three components can be realised as lightweight, REST-style web services. We explain why this separation is useful and how the wiki community benefits from such an approach. Additionally, the presented wiki allows semantic statements and queries over the model.","REST; Semantic web; Semantic wiki; System architecture; Web service; Wiki","Computer supported cooperative work; Information management; Mathematical models; Query languages; User interfaces; Web services; Distributed Wiki architecture; Intermingle parser; Websites",2-s2.0-34247329529
"Eryiǧit G., Nivre J., Oflazer K.","The incremental use of morphological information and lexicalization in data-driven dependency parsing",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049122042&doi=10.1007%2f11940098_53&partnerID=40&md5=9abe9fc5a37a4643e43f270377d524a8","Typological diversity among the natural languages of the world poses interesting challenges for the models and algorithms used in syntactic parsing. In this paper, we apply a data-driven dependency parser to Turkish, a language characterized by rich morphology and flexible constituent order, and study the effect of employing varying amounts of morpholexical information on parsing performance. The investigations show that accuracy can be improved by using representations based on inflectional groups rather than word forms, confirming earlier studies. In addition, lexicalization and the use of rich morphological features are found to have a positive effect. By combining all these techniques, we obtain the highest reported accuracy for parsing the Turkish Treebank. © 2006 Springer-Verlag.",,"Data-driven; Dependency parser; Dependency parsing; Lexicalization; Morphological features; Morphological information; Natural languages; Positive effects; Syntactic parsing; Treebanks; Turkishs; Industrial research; Query languages; Linguistics",2-s2.0-77049122042
"Meij E., Jansen M., De Rijke M.","Expanding queries using multiple resources the AID group at TREC 2006: Genomics track",2006,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535407&partnerID=40&md5=952cecbd0c64d9a9cc49955260a59c64","We describe our participation in the TREC 2006 Genomics track, in which our main focus was on query expansion. We hypothesized that applying query expansion techniques would help us both to identify and retrieve synonymous terms, and to cope with ambiguity. To this end, we developed several collection-specific as well as web-based strategies. We also performed post-submission experiments, in which we compare various retrieval engines, such as Lucene, Indri, and Lemur, using a simple baseline topicset. When indexing entire paragraphs as pseudodocuments, we find that Lemur is able to achieve the highest document-, passage-, and aspect-level scores, using the KL-divergence method and its default settings. Additionally, we index the collection at a lower level of granularity, by creating pseudo-documents comprising of individual sentences. When we search these instead of paragraphs in Lucene, the passage-level scores improve considerably. Finally we note that stemming improves overall scores by at least 10%.",,"Default setting; Genomics; KL-divergence; Multiple resources; Query expansion; Query expansion techniques; Retrieval engines; Information retrieval",2-s2.0-84873535407
"Kate R.J., Mooney R.J.","Using string-kernels for learning semantic parsers",2006,"COLING/ACL 2006 - 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",105,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860540911&partnerID=40&md5=4375f976665df3b38fca965e3a18c35e","We present a new approach for mapping natural language sentences to their formal meaning representations using string-kernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our experiments on two real-world data sets show that this approach compares favorably to other existing systems and is particularly robust to noise. © 2006 Association for Computational Linguistics.",,"Existing systems; Language grammar; Learning semantics; Natural languages; Real world data; Formal languages; Semantics; Virtual reality; Computational linguistics",2-s2.0-84860540911
"Yadav S., Bellah J.","An improved method for automatically determining webpage cohesiveness for quality information retrieval from world wide web",2006,"Proceedings of the 2006 International Conference on Information Quality, ICIQ 2006",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871587438&partnerID=40&md5=d0fbce7b924803db7b3cc9b4364ab482","Cohesiveness plays a major role in evaluating the overall quality of information contained in a webpage. Research has shown that it is an important dimension of quality, specifically for webpages that provide specialized information. This paper presents a new method for automatically measuring the cohesiveness of a webpage by using the semantic similarity score between sentences. Part of an overall research project for retrieving quality medical information from the World Wide Web, in this paper we describe this new measure of webpage cohesiveness, a system that automatically computes the webpage cohesiveness score. In addition, we evaluate the new method and find it to be effective in determining the cohesiveness of a webpage.","HTML Parser; Quality Information Retrieval; Quality Webpage; Semantic Similarity Score; Webpage Cohesiveness","Medical information; Overall quality; Quality information; Semantic similarity; Web-page; Information retrieval; Research; Websites",2-s2.0-84871587438
"Kosseim L., Siblini R., Baker C.J.O., Bergler S.","Using selectional restrictions to query an OWL ontology",2006,"Frontiers in Artificial Intelligence and Applications",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876757890&partnerID=40&md5=f6e4639c6053c8419d01b601b39a7ffe","This paper discusses the linguistic module of an Ontology Natural Language Interaction System that is based on semantic restrictions. The system, called ONLI, takes as input questions in unrestricted natural language, translates them into nRQL, an extension to the RACER ontology query language, then generates answers as returned by the RACER ontology reasoning server. Translation into nRQL is done through a syntactic analysis (with Minipar), followed by the use of semantic restrictions imposed by the roles stored in the ontology to map terms in the question to concepts and roles in the ontology. The system was evaluated on the FungalWeb ontology using the mean reciprocal rank (MRR) measure used in question-answering. With a test set of 36 questions, the systems achieved an MRR of 0.72. © 2006 The authors. All rights reserved.","Natural language; Ontology interface; Selectional restrictions; Semantic web",,2-s2.0-84876757890
"Corzo J.M.S., Portillo D.D., Zamora J.A.C.","Natural language database query system [Sistema de consultas en lenguaje natural para bases de datos]",2006,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884757156&partnerID=40&md5=554e2fb3828f84200c67ce9fd1ff0ae3",[No abstract available],,,2-s2.0-84884757156
"Homayounfar H., Wang F.","A parsing cache for highly efficient XML data processing",2006,"Proceedings of the Second IASTED International Conference on Web Technologies, Applications, and Services, WTAS 2006",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049178404&partnerID=40&md5=829aa2ac52f30d0ba3ec8a81a2743baa","XML (extensible Markup Language) documents are relatively large. Therefore processing such documents can be very costly. Meanwhile traditional filesystem buffering and caching mechanisms are not designed particularly to support XML parsing. Hence, redundant parsing problem causes the current XML parsers and query processors not to be efficient when processing large documents. Also, due to the data structure and algorithms used in the current XML tools, parsing is not fast enough or even providing required resource (i.e. memory) may not be feasible. Furthermore, XML tools are not designed to provide a sharable parse tree for multiple users, and users have to limit themselves to specific tools to access the parse tree. To overcome these limitations, we introduce a ""Parsing Cache"", which uses a well-structured parse tree, an efficient parsing algorithm and a user-cache interface. It provides a resident parsing storage specially for XML documents. Due to small memory usage, parse trees can be resident for further processing. The user-cache interface enables multiple users to share the parse tree. The parsing and query time are significantly improved compared with the current XML parsers and query managers.","Caching; Databases; Filesystems; Kernel; Operating systems; Parsing; Querying; XML","Computer operating systems; Data processing; Data structures; Database systems; Problem solving; Redundancy; User interfaces; Buffering; Filesystems; Query time; Sharable parse tree; XML tools; XML",2-s2.0-38049178404
"Huang C.-H., Chuang T.-R., Deng D.-P., Lee H.-M.","Efficient GML-native processors for web-based GIS: Techniques and tools",2006,"GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547492280&doi=10.1145%2f1183471.1183488&partnerID=40&md5=88b5dfb3d49bebca4fc67473323c2609","Geography Markup Language (GML) is an XML-based language for the markup, storage, and exchange of geospatial data. It provides a rich geospatial vocabulary and allows flexible document structure. However, GML documents are usually large and complicated in structure. Existing techniques for XML document processing, either streaming-based or memory-based, may not deal with such GML documents efficiently. There is an urgent need to adapt existing XML techniques to support the processing of large XML/GML documents, as well as to express GML-native geospatial operations.In this paper, we propose and implement an efficient GML query processor, GPXQuery, and a GML-aware streaming parser, GPSAX, by extending an XQuery processor and a SAX parser, respectively, to support GML-native geospatial functionalities. In addition to these tools, an XML prefiltering technique is applied to the processors to speed up geospatial operations over large GML documents. Our experiment results show that the XML prefiltering technique significantly improves the performance of both the GPXQuery and GPSAX processors by reducing either the query execution time or the memory space consumption. Depending on the nature of user queries, the enhanced query processors can achieve a ten-fold performance improvement. These efficient GML-native processors have been used to develop a GML-based Web GIS with a geospatial query interface and a Scalable Vector Graphics (SVG) map navigator. Copyright 2006 ACM.","DOM; GIS; GML; SAX; Web; XML prefiltering; XPath; XQuery","Interfaces (computer); Query processing; World Wide Web; XML; Geospatial data; Prefiltering; XML prefiltering; Geographic information systems",2-s2.0-34547492280
"Thurston A.D., Cordy J.R.","A backtracking LR algorithm for parsing ambiguous context-dependent languages",2006,"Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research, CASCON '06",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952411466&doi=10.1145%2f1188966.1188972&partnerID=40&md5=d88fd8126beba4fb0adf9a42aeee31f9","Parsing context-dependent computer languages requires an ability to maintain and query data structures while parsing for the purpose of influencing the parse. Parsing ambiguous computer languages requires an ability to generate a parser for arbitrary context-free grammars. In both cases we have tools for generating parsers from a grammar. However, languages that have both of these properties simultaneously are much more difficult to parse. Consequently, we have fewer techniques. One approach to parsing such languages is to endow traditional LR systems with backtracking. This is a step towards a working solution, however there are number of problems. In this work we present two enhancements to a basic backtracking LR approach which enable the parsing of computer languages that are both context-dependent and ambiguous. Using our system we have produced a fast parser for C++ that is composed of strictly a scanner, a name lookup stage and parser generated from a grammar augmented with semantic actions and semantic 'undo' actions. Language ambiguities are resolved by prioritizing grammar declarations. © 2006 Adrian D. Thurston and James R. Cordy.",,"Computer language; Context dependent; Lookups; Semantic action; Data structures; Linguistics; Query languages; Semantics; Context free languages",2-s2.0-77952411466
"Ho B.Q., Dong T.B.T., Chevallet J.-P., Bruandet M.-F.","A structured indexing model based on noun phrases",2006,"Proceedings of the 4th IEEE International Conference on Research, Innovation and Vision for the Future, RIVF'06",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250776164&doi=10.1109%2fRIVF.2006.1696423&partnerID=40&md5=993588be1a452700ae2407dfa98d3771","Most of the indexing models are based on simple independent words, also known as key words. This approach does not take account of the context as well as the relations between the words. Therefore, the precision of system is limited. In this article, we present a structured indexing model based on noun phrases to increase the precision of an Information Retrieval System (IRS). In this model, we used a grammatical parser to extract and structure a noun phrase in determining the various roles of the words of a noun phrase and their syntactic relations. We represent the set of the index terms of query in the form of Bayesian networks which enables us to calculate the matching function between a query and a document. We carried out experiments to test this model. That the positive results obtained encourages us to continue in this direction. © 2006 IEEE.","Bayesian network; Indexing model; Information retrieval; Natural language processing","Bayesian networks; Indexing (of information); Natural language processing systems; Query processing; Syntactics; Indexing model; Key words; Noun phrases; Information retrieval systems",2-s2.0-34250776164
"Li X., Agrawal G.","Parallelizing XQuery in a cluster environment",2006,"Proceedings of the International Database Engineering and Applications Symposium, IDEAS",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-39749095002&doi=10.1109%2fIDEAS.2006.35&partnerID=40&md5=febb0ad2138fef885af1634896348412","In this paper, we report on a parallel implementation of XQuery. As XQuery is being used for processing large datasets, and/or for computeintensive applications, efficiency of XQuery implementations is becoming an important issue. Our work has specifically focused on scientific data processing and data mining applications. Parallelization of this class of XQuery queries involves a number of challenges, which include data distribution, parallelization of generalized reductions, and translation to an imperative language like C/C++, so as to invoke efficient parallel communication libraries. In this paper, we report our solutions towards the above problems. By implementing the techniques in a compiler and generating code based on a C++ SAX parser and the Message Passing Interface (MPI), we are able to achieve efficient parallel execution on a cluster of machines. © 2006 IEEE.",,"Data distribution; Message Passing Interface (MPI); Parallel communication libraries; Computer programming languages; Data mining; Data processing; Message passing; Query processing",2-s2.0-39749095002
"Chanev A.","Statistical dependency parsing of four treebanks",2006,"Informatica (Ljubljana)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846419776&partnerID=40&md5=e27d7ed5b3914e30e15aaaf297c6920f","Multilingual dependency parsing is gaining popularity in recent years for several reasons. Dependency structures are more adequate for languages with freer word order than the traditional constituency notion. There is a growing availability of dependency treebanks for new languages. Broad coverage statistical dependency parsers are available and easily portable to new languages. Dependency parsing can provide useful contributions in areas such as information extraction, machine translation and question answering, among others. In addition, syntactic head-dependent pairs are a good interface between the traditional phrase structures and semantic theta roles. In this paper we present the learning curves of a statistical dependency parser for four languages: Arabic, Bulgarian, Italian and Slovene. We discuss issues that mostly concern the employed annotation scheme for each treebank with an emphasis on coordinated structures.","Coordination; Dependency parsing; Human language technologies","Formal languages; Information retrieval; Query languages; Statistical methods; Syntactics; Coordination; Dependency parsing; Human language technologies; Information extraction; Computational linguistics",2-s2.0-33846419776
"Koulouris A., Andronikos T., Pavlatos C., Dimopoulos A., Panagopoulos I., Papakonstantinou G.","Efficient signal processing using syntactic pattern recognition methods",2006,"Proceedings of the 8th IASTED International Conference on Signal and Image Processing, SIP 2006",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-56549130309&partnerID=40&md5=95491567e687a1df7cd847e174bedddc","This paper presents an optimal architecture for hardware implementation of Context-Free Grammar (CFG) parsers, which can be used to accelerate the performance of applications where response to real time signal processing is a crucial aspect, such as Electrocardiogram (ECG) analysis. Our architecture increases the performance by a factor of approximately two orders of magnitude compared to the pure software implementation, depending on the CFG. This speed up derives mainly from the hardware nature of the implementation, the innovative combinatorial nature of the circuit that implements the fundamental operation of the parsing algorithm and the underlying data representation. We further propose an automated synthesis tool that, given the specification of an arbitrary CFG and using the aforementioned hardware architecture in a template form, generates the HDL (Hardware Design Language) synthesizable source code of the hardware parser for the given grammar. The proposed architecture may be used for real time applications, e.g. natural languages interfaces. The generated source has been simulated for validation, synthesized and tested on a Xilinx FPGA (Field Programmable Gate Array) board.","FPGA; Signal processing; Syntactic pattern recognition","Applications; Combinatorial mathematics; Computer systems; Digital image storage; Feature extraction; Field programmable gate arrays (FPGA); Hardware; Image processing; Imaging systems; Internet protocols; Linguistics; Multimedia systems; Pattern recognition; Query languages; Signal processing; Syntactics; Trees (mathematics); Automated syntheses; Data representations; FPGA; Fundamental operations; Hardware architectures; Hardware designs; Hardware implementations; Natural languages; Optimal architectures; Orders of magnitudes; Parsing algorithms; Proposed architectures; Real-time applications; Software implementations; Source codes; Speed-up; Syntactic pattern recognition; Syntactic pattern recognitions; Xilinx fpga; Computer hardware description languages",2-s2.0-56549130309
"Sheremetyeva S.","Understanding long sentences",2006,"Proceedings of the 3rd International Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2006, in Conjunction with ICEIS 2006",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954139990&partnerID=40&md5=dbdd25968a9ac751097695f9d6fad703","This paper describes a natural language understanding component for parsing long sentences. The NLU component includes a generation module so that the results of understanding can be displayed to the user in a natural language and interactively corrected before the final parse is sent to a subsequent module of a particular application. Parsing proper is divided into a phrase level and a level of individual clauses included in a sentence. The output of the parser is an interlingual representation that captures the content of a whole sentence. The load of detecting the sentence clause hierarchy level is shifted to the generator. The methodology is universal in the sense that it could be used for different domains, languages and applications. We illustrate it on the example of parsing a patent claim, - an extreme case of a long sentence.",,"Different domains; Extreme case; Natural language understanding; Natural languages; Patent claims; Linguistics; Query languages",2-s2.0-77954139990
"Nguyen T.P., Shimazu A.","A syntactic transformation model for statistical machine translation",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049124284&doi=10.1007%2f11940098_7&partnerID=40&md5=f27db40a13a5f7b0f47320016e16e3be","We present a phrase-based SMT approach in which the word-order problem is solved using syntactic transformation in the preprocessing phase (There is no reordering in the decoding phase.) We describe a syntactic transformation model based on the probabilistic context-free grammar. This model is trained by using bilingual corpus and a broad coverage parser of the source language. This phrase-based SMT approach is applicable to language pairs in which the target language is poor in resources. We considered translation from English to Vietnamese and from English to French. Our experiments showed significant BLEU-score improvements in comparison with Pharaoh, a state-of-the-art phrase-based SMT system. © 2006 Springer-Verlag.",,"Bilingual corpora; Language pairs; Order problems; Preprocessing phase; Probabilistic context free grammars; SMT systems; Source language; Statistical machine translation; Syntactic transformations; Target language; Industrial research; Information theory; Linguistics; Query languages; Speech transmission; Syntactics; Translation (languages)",2-s2.0-77049124284
"Karanastasi A., Zotos A., Christodoulakis S.","User interactions with multimedia repositories using natural language interfaces - OntoNL: An architectural framework and its implementation",2006,"Journal of Digital Information Management",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845925229&partnerID=40&md5=78be53ce109a805416d2c49e78d10de5","We present a generalized architectural framework for constructing and using natural language interfaces for interactions with multimedia repositories. The system allows the users to specify natural language queries about the multimedia content with rich semantics. It uses an extensive set of methodologies and tools for linguistic processing, and utilizes the MPEG-7 and the domain ontologies to reduce the ambiguities in the natural language and to rank the results. We describe the implementation of this framework for supporting interactions with a multimedia repository, described with the MPEG-7 MDS (Multimedia Description Schemes) structures, that also uses User Profile information for better ranking of the result queries.","MPEG 7; Natural language interface; Ontology; Semantics","Multimedia systems; Natural language processing systems; Query languages; Semantics; User interfaces; Natural language interface; Ontology; Information management",2-s2.0-33845925229
"Fragos K., Skourlas C.","An N-gram based distributional test for authorship identification",2006,"Proceedings of the 3rd International Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2006, in Conjunction with ICEIS 2006",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954117086&partnerID=40&md5=4c96c1aa6d3c1a103b488cb44069bd7f","In this paper, a novel method for the authorship identification problem is presented. Based on character level text segmentation we study the disputed text's N-grams distributions within the authors' text collections. The distribution that behaves most abnormally is identified using the Kolmogorov -Smirnov test and the corresponding Author is selected as the correct one. Our method is evaluated using the test sets of the 2004 ALLC/ACH Ad-hoc Authorship Attribution Competition and its performance is comparable with the best performances of the participants in the competition. The main advantage of our method is that it is a simple, not parametric way for authorship attribution without the necessity of building authors' profiles from training data. Moreover, the method is language independent and does not require segmentation for languages such as Chinese or Thai. There is also no need for any text preprocessing or higher level processing, avoiding thus the use of taggers, parsers, feature selection strategies, or the use of other language dependent NLP tools.",,"Authorship attribution; Authorship identification; Character level; Feature selection; Kolmogorov-Smirnov test; N-grams; NLP tools; Novel methods; Test sets; Text collection; Text preprocessing; Text segmentation; Training data; Competition; Feature extraction; Query languages; Linguistics",2-s2.0-77954117086
"Xu R., Wong K.-F., Lu Q., Li W.","An improved method for finding bilingual collocation correspondences from monolingual corpora",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049092066&doi=10.1007%2f11940098_6&partnerID=40&md5=57638731aca0ef635b31816f565f40f3","Bilingual collocation correspondence is helpful to machine translation and second language learning. Existing techniques for identifying Chinese-English collocation correspondence suffer from two major problems. They are sensitive to the coverage of the bilingual dictionary and the insensitive to semantic and contextual information. This paper presents the ICT (Improved Collocation Translation) method to overcome these problems. For a given Chinese collocation, the word translation candidates extracted from a bilingual dictionary are expanded to improve the coverage. A new translation model, which incorporates statistics extracted from monolingual corpora, word semantic similarities from monolingual thesaurus and bilingual context similarities, is employed to estimate and rank the probabilities of the collocation correspondence candidates. Experiments show that ICT is robust to the coverage of bilingual dictionary. It achieves 50.1% accuracy for the first candidate and 73.1% accuracy for the top-3 candidates. © 2006 Springer-Verlag.","Bilingual collocation correspondence; Monolingual corpora","Bilingual dictionary; Contextual information; Improved methods; Machine translations; Second language; Semantic similarity; Translation models; Word translation; Industrial research; Information technology; Information theory; Linguistics; Query languages; Semantics; Speech transmission; Translation (languages)",2-s2.0-77049092066
"Dubois M., Aboulhamid E.M., Rousseau F.","Towards an efficient simulation of multi-language descriptions of heterogeneous systems",2006,"IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249164280&doi=10.1109%2fAPCCAS.2006.342527&partnerID=40&md5=732c9f8e7fdf353e7b92f6f24503c4e2","Heterogeneous systems design requires executing models using different simulators. Usually, simulators are connected and synchronized together by using mechanisms such as shared memory or TCP/IP. Current approaches have many drawbacks: they are not adapted to distributed environments, the simulation performance may be very disappointing, and each simulator has its own paradigm. We propose a new approach to simulate models described with different languages such as SystemC or ESyS.NET on a distributed environment. All the models are translated into a unique internal format in order to be able to optimize a distributed and multi-paradigm simulation of the models. Optimizations include different mechanisms such as regrouping of threads in a single one while respecting the semantics of the models or the splitting of a model in order to execute it on several computer resources. ©2006 IEEE.",,"Computer simulation languages; Information theory; Linguistics; Mechanisms; Optimization; Query languages; Asia-Pacific; Computer resources; Different mechanisms; Distributed environments; Esys.Net; Heterogeneous systems; Language descriptions; Multi-paradigm simulation; New approaches; Shared memories; Simulation performance; SystemC; Simulators",2-s2.0-50249164280
"Shriram R., Sugumaran V., Kapetanios E.","Cross-lingual information retrieval and delivery using community mobile networks",2006,"2006 1st International Conference on Digital Information Management, ICDIM",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849135545&doi=10.1109%2fICDIM.2007.369217&partnerID=40&md5=bbcf9286356f9712fed70f8f68bbd2af","Much of the Web content is in English and accessing this content is difficult for non-English speaking users because of the language barrier. Hence, there is a great need for providing applications and interfaces in one's own language to tap into this vast knowledge reserve. In addition, access to the Internet is still a major problem in developing countries because of the ""digital divide"" and hand held devices such as PDAs and Mobile Phones are seen as enablers in bridging this gap. However, displaying cross-lingual content on these mobile devices is a non trivial task and there is a great need for robust mechanisms and infrastructure for content delivery in different languages on the fly. This paper presents an overall approach for cross-lingual content specification and delivery for computing/mobile devices. It helps mitigate the language barrier by providing cross-lingual search and retrieval capabilities for accessing the Web content. © 2006 IEEE.",,"Content deliveries; Content specifications; Cross linguals; Cross-lingual information retrievals; Digital divides; Hand held devices; Language barriers; Mobile networks; Non-trivial tasks; On the flies; Search and retrievals; Web contents; Developing countries; Information management; Information services; Linguistics; Mobile devices; Query languages; Wireless networks; World Wide Web; Digital devices",2-s2.0-51849135545
"Reiter T., Kapsammer E., Retschitzegger W., Schwinger W., Stumptner M.","A generator framework for domain-specific model transformation languages",2006,"ICEIS 2006 - 8th International Conference on Enterprise Information Systems, Proceedings",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953884428&partnerID=40&md5=1f2ee3313ffecc0efc063b852d5d6838","Domain specific languages play an important role in model driven development, as they allow to model a system using modeling constructs carrying implicit semantics specific to a domain. Consequently, possibly many reusable, domain specific languages will emerge. Thereby, certain application areas, such as business process engineering, can be jointly covered by a number of conceptually related DSLs, that are similar in a sense of sharing semantically equal concepts. Although, a crucial role in being able to use, manage and integrate all these DSLs comes to model transformation languages with QVT as one of their most prominent representatives, existing approaches have not aimed at reaping benefit of these semantically overlapping DSLs in terms of providing abstraction mechanisms for shared concepts. Therefore, as opposed to a general-purpose model transformation language sought after with the QVT-RFP, this work discusses the possibility of employing domain-specific model transformation languages. These are specifically tailored for defining transformations between metamodels sharing certain characteristics. In this context, the paper introduces a basic framework which allows generating the necessary tools to define and execute transformations written in such a domain-specific transformation language. To illustrate the approach, an example language will be introduced and its realization within the framework is shown.","Domain-specific languages; MDA; Model transformation; QVT; Workflow patterns","Abstraction mechanism; Application area; Business process engineerings; Domain specific; Domain specific languages; Implicit semantics; Meta model; Model driven development; Model transformation; Modeling construct; Workflow patterns; Computer software reusability; Information systems; Linguistics; Management; Problem oriented languages; Software architecture; Query languages",2-s2.0-77953884428
"Wu D.-S., Liang T.","A case-based reasoning approach to zero anaphora resolution in Chinese texts",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049125048&doi=10.1007%2f11940098_55&partnerID=40&md5=8a191436703bc7d38a5e48760e3561bd","Anaphora is a common phenomenon in discourses as well as an important research issue in the applications of natural language processing. In this paper, both intra-sentential and inter-sentential zero anaphora in Chinese texts are addressed. Unlike general rule-based approaches, our resolution method is embedded with a case-based reasoning mechanism which has the benefit of knowledge acquisition if the case size varies. In addition, the presented approach employs informative features with the help of two outer knowledge resources. Compared to rule-based approaches, our resolution to 1047 zero anaphora instances achieved 82% recall and 77% precision. © 2006 Springer-Verlag.",,"Case-based reasoning approaches; Chinese text; Knowledge resource; NAtural language processing; Research issues; Resolution methods; Rule-based approach; Zero anaphora resolution; Computational linguistics; Industrial research; Knowledge acquisition; Natural language processing systems; Query languages; Case based reasoning",2-s2.0-77049125048
"Liu F., Ackerman M., Fontelo P.","BabelMeSH: development of a cross-language tool for MEDLINE/PubMed.",2006,"AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748881780&partnerID=40&md5=ba7c87fd4dcf759b1d19a9ff65e5b38f","BabelMeSH is a cross-language tool for searching MEDLINE/PubMed. Queries can be submitted as single terms or complex phrases in French, Spanish and Portuguese. Citations will be sent to the user in English. It uses a smart parser interface with a medical terms database in MySQL. Preliminary evaluation using compound key words in foreign language medical journals showed an accuracy of 68%, 60% and 51% for French, Spanish and Portuguese, respectively. Development is continuing.",,"article; computer interface; information retrieval; language; MEDLINE; MeSH heading; Information Storage and Retrieval; Language; Medical Subject Headings; MEDLINE; PubMed; Translating; User-Computer Interface",2-s2.0-34748881780
"Jung S.-H., Jung T.-S., Kim T.-K., Kim K.-R., Yoo J.-S., Cho W.-S.","An efficient storage model for the SBML documents using object databases",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547454384&partnerID=40&md5=0ccd70e4501b8238719bec44f83ef81e","As SBML is regarded as a de-facto standard to express the biological network data in systems biology, the amount of the SBML documents is exponentially increasing. We propose an SBML data management system (SMS) on top of an object database. Since the object database supports abundant data types like multi-valued attributes and object references, mapping from the SBML documents into the object database is straightforward. We adopt the event-based SAX parser instead of the DOM parser for dealing with huge SBML documents. Note that DOM parser suffers from excessive memory overhead for the document parsing. For high quality data, SMS supports data cleansing function by using gene ontology. Finally, SMS generates user query results in an SBML format (for data exchange) or in a visual graphs (for intuitive understanding). Real experiments show that our approach is superior to the one using conventional relational databases in the aspects of the modeling capability, storage requirements, and data quality. © Springer-Verlag Berlin Heidelberg 2006.","Bioinformatics; Object database; Ontology; Pathway; SBML; System biology","Data reduction; Information retrieval systems; Object recognition; Object database; System biology; Database systems",2-s2.0-34547454384
"Miyao Y., Ohta T., Masuda K., Tsuruoka Y., Yoshida K., Ninomiya T., Tsujii J.","Semantic retrieval for the accurate identification of relational concepts in massive textbases",2006,"COLING/ACL 2006 - 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",53,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860536484&partnerID=40&md5=70244f069dd6e1941d5fe8e06bb70682","This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts. Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer. During the run time, user requests are converted into queries of region algebra on these annotations. Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts. This framework was applied to a text retrieval system for MEDLINE. Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved. © 2006 Association for Computational Linguistics.",,"Argument structures; Medline; Real-time application; Runtimes; Semantic annotations; Semantic retrieval; Text retrieval system; Computational linguistics; Semantics; Information retrieval",2-s2.0-84860536484
"Werner C., Buschmann C., Brandt Y., Fischer S.","Compressing SOAP messages by using pushdown automata",2006,"Proceedings - ICWS 2006: 2006 IEEE International Conference on Web Services",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949138763&doi=10.1109%2fICWS.2006.46&partnerID=40&md5=02d1cc03cedd34aada5b8e46bda6dd44","In environments with limited network bandwidth or resource-constrained computing devices the high amount of protocol overhead caused by SOAP is disadvantageous. Therefore, recent research work concentrated on more compact, binary representations of XML data. However, due to the special characteristics of SOAP communication most of these approaches are not applicable in the field of web services. First, we give a detailed overview of the latest developments in the field of XML data compression. Then we will introduce a new approach for compressing SOAP data which utilizes information on the structure of the data from an XML Schema or WSDL document to generate a single custom pushdown automaton. This cannot only be used as a highly efficient validating parser but also as a compressor: its transitions are tagged with short binary identifiers which replace XML tags during compression. This approach leads to extremely compact data representations as well as low memory and CPU utilization. © 2006 IEEE.",,"Data structures; Web services; XML; Binary representations; Custom pushdown automaton; SOAP messages; Data compression",2-s2.0-38949138763
"Verberne S., Boves L., Oostdijk N., Coppen P.-A.","Exploring the use of linguistic analysis for answering why-questions",2006,"Proceedings of the 16th Computational Linguistics in the Netherlands",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870559273&partnerID=40&md5=bd6bc1872dbb3d8ce72a4dd4fb2b19fa","In the current project, we aim at developing an approach for automatically answering why-questions (why-QA). In the present paper, we investigate the relevance of linguistic analysis for why-QA. We focus on two tasks: the use of syntactic information for answer type determination and the use of discourse structure for the extraction of possible answers from retrieved documents. For answer type determination, syntactic analysis appears to be of significance: we obtain 77.5% performance using a method based on syntactic parses by the TOSCA parser? compared to 58.1% using a comparable approach without syntactic analysis. Discourse analysis appears to be very relevant for extraction of potential answers to why-questions. We performed a manual analysis of 336 question-answer pairs and the corresponding RST annotated texts. We found that for 58.9% of why-questions, the RST analysis of the source text can lead to a correct answer to the question. Copyright © 2007 by the individual authors.",,"Current projects; Discourse analysis; Discourse structure; Linguistic analysis; Manual analysis; Question-answer pairs; Retrieved documents; Source text; Syntactic analysis; Syntactic information; Computational linguistics; Text processing; Syntactics",2-s2.0-84870559273
"Wong Y.W., Mooney R.J.","Learning for semantic parsing with statistical machine translation",2006,"HLT-NAACL 2006 - Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings of the Main Conference",97,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858394994&partnerID=40&md5=889b8da28032ba7448df420fa636f1b9","We present a novel statistical approach to semantic parsing, WASP, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning representations. The main innovation of WASP is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order. © 2006 Association for Computational Linguistics.",,"Learning methods; Lexical acquisition; Semantic parsing; Statistical approach; Statistical machine translation; Task complexity; Translation models; Word alignment; Word orders; Computational linguistics; Information theory; Semantics; Context free grammars",2-s2.0-84858394994
"Penev A., Wong R.","Shallow NLP techniques for internet search",2006,"Conferences in Research and Practice in Information Technology Series",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868696664&partnerID=40&md5=2f422318a4cf9432291bc886b5fc0814","Information Retrieval (IR) is a major component in many of our daily activities, with perhaps its most prominent role manifested in search engines. Today's most advanced engines use the keyword-based (""bag of words"") paradigm, which concedes some inherent disadvantages. We believe that natural language (NL) is a more user-oriented, context-preservative and intuitive mechanism for web search. In this paper, we explore shallow NLP techniques to support a range of NL queries over an existing keyword-based engine. We present JASE, a web application enveloping the Google search engine, which performs web searches by decomposing input NL queries and generating new queries that are more suitable for the search engine. By using some of Google's syntactic operators and filters, it creates ""clever"" queries to improve precision. A preliminary evaluation was conducted to test JASE's accuracy, and results have been encouraging. We conclude that the NL model has potential to not only rival the keyword-based paradigm, but substantially surpass it. Copyright © 2006, Australian Computer Society, Inc.","Google; Information retrieval; Natural language processing","Bag of words; Daily activity; Google; Google search engine; Internet searches; NAtural language processing; Natural languages; Shallow NLP; User oriented; WEB application; Web searches; Computer science; Information retrieval; Natural language processing systems; Websites; Search engines",2-s2.0-84868696664
"Caporaso J.G., Baumgartner Jr. W.A., Kim H., Lu Z., Johnson H.L., Medvedeva O., Lindemann A., Fox L.M., White E.K., Cohen K.B., Hunter L.","Concept recognition, information retrieval, and machine learning in genomics question-answering",2006,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873539048&partnerID=40&md5=e946967d133a0cd746981a44f677a2f2","TREC Genomics 2006 presented a genomics question-answering challenge with questions on twenty-seven topics, and a corpus of 162,259 full-text biomedical journal articles from which to derive answers. Questions were formulated from actual information needs of biomedical researchers, and performance was based on human evaluation of the answers. The University of Colorado approach to this task involved three key components: semantic analysis, document zoning, and a promiscuous retrieval approach followed by pruning by classifiers trained to identify near-misses. We began by parsing the document HTML, splitting it into paragraph-length passages and classifying each passage with respect to a model of the sections (zones) of scientific publications. We filtered out certain sections, and built a search index for these passages using the Lemur system. Next, for each query, we semi-automatically created a set of expansions using ontological resources, including MeSH and the Gene Ontology. This expansion included not only synonyms, but terms related to concepts that were both more specific and (in some cases) more general than the query. We searched the passage collection for these expanded queries using the Indri search engine from the Lemur package, with pseudo-relevance feedback. We also tried expanding the retrieved passages by adding passages that had a small cosine distance to the initial retrievals in an LSA-defined vector space. Our final step was to filter this expanded retrieval set with document classifiers whose input features included word stems and recognized concepts. Three separate runs were constructed using varying components of the above set, allowing us to explore the utility of each. The system produced the best result for at least one query in each of the three evaluations (document, passage and aspect diversity).",,"Biomedical journal; Concept recognition; Gene ontology; Genomics; Human evaluation; Information need; Input features; Near-misses; Pseudo relevance feedback; Question Answering; Scientific publications; Semantic analysis; University of Colorado; Search engines; Semantics; Information retrieval",2-s2.0-84873539048
"Attardi G., Simi M.","Blog mining through opinionated words",2006,"NIST Special Publication",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873564876&partnerID=40&md5=0fe994502eabbcef0fd6887b40c5cbb7","Intent mining is a special kind of document analysis whose goal is to assess the attitude of the document author with respect to a given subject. Opinion mining is a kind of intent mining where the attitude is a positive or negative opinion. Most systems tackle the problem with a two step approach, an information retrieval followed by a post process or filter phase to identify opinionated blogs. We explored a single stage approach to opinion mining, retrieving opinionated documents ranked with a special ranking function which exploits an index enriched with opinion tags. A set of subjective words are used as tags for identifying opinionated sentences. Subjective words are marked as ""opinionated"" and are used in the retrieval phase to boost the rank of documents containing them. In indexing the collection, we recovered the relevant content from the blog permalink pages, exploiting HTML metadata about the generator and heuristics to remove irrelevant parts from the body. The index also contains information about the occurrence of opinionated words, extracted from an analysis of WordNet glosses. The experiments compared the precision of normal queries with respect to queries which included as constraint the proximity to an opinionated word. The results show a significant improvement in precision for both topic relevance and opinion relevance.",,"Blog mining; Document analysis; Opinion mining; Post process; Ranking functions; Single stage; Topic relevance; Two-step approach; Wordnet; Data mining; Filtration; Internet; Metadata; Information retrieval",2-s2.0-84873564876
"Jang H., Lim J., Lim J.-H., Park S.-J., Lee K.-C.","BioProber: Software system for biomedical relation discovery from PubMed",2006,"Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047115266&doi=10.1109%2fIEMBS.2006.259838&partnerID=40&md5=7453871a8f49156ce03ba517f9bdb892","The numbers of articles and journals that are published are increasing at a considerable rate, and the published information is growing continuously and fast. Because of this, researches to acquire knowledge automatically have been carried out in the areas of information retrieval, information extraction and text mining. Information retrieval approaches are good for specific topics that the number of related articles is small. But, if the number is bigger, searching skill and knowledge acquisition ability are useless. Though many efforts have been made to extract information from literature, many approaches have concentrated on specific entities, such as proteins, genes and their interactions, and much information is still remained in unstructured text. So, we have developed a system that discovers relations between various categories of biomedical entities. Our system collects abstracts from PubMed by queries representing a topic and visualizes relationship from the collection by automatic information extraction. © 2006 IEEE.",,"Data mining; Electronic publishing; Genes; Information retrieval; Knowledge acquisition; Proteins; Text processing; BioProbers; Information extraction; Software systems; Text mining; Medical computing",2-s2.0-34047115266
"Moens M.-F.","Information extraction: Algorithms and prospects in a retrieval context",2006,"Information Extraction: Algorithms and Prospects in a Retrieval Context",94,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892039609&doi=10.1007%2f978-1-4020-4993-4&partnerID=40&md5=6908a7fffb93e601ac6c0b275f85ac5c","Information extraction regards the processes of structuring and combining content that is explicitly stated or implied in one or multiple unstructured information sources. It involves a semantic classification and linking of certain pieces of information and is considered as a light form of content understanding by the machine. Currently, there is a considerable interest in integrating the results of information extraction in retrieval systems, because of the growing demand for search engines that return precise answers to flexible information queries. Advanced retrieval models satisfy that need and they rely on tools that automatically build a probabilistic model of the content of a (multi-media) document. The book focuses on content recognition in text. It elaborates on the past and current most successful algorithms and their application in a variety of domains (e.g., news filtering, mining of biomedical text, intelligence gathering, competitive intelligence, legal information searching, and processing of informal text). An important part discusses current statistical and machine learning algorithms for information detection and classification and integrates their results in probabilistic retrieval models. The book also reveals a number of ideas towards an advanced understanding and synthesis of textual content. The book is aimed at researchers and software developers interested in information extraction and retrieval, but the many illustrations and real world examples make it also suitable as a handbook for students. © 2006 Springer. All Rights Reserved.",,,2-s2.0-84892039609
"Delmonte R.","Building domain ontologies from text analysis: An application for question answering",2006,"Proceedings of the 3rd International Workshop on Natural Language Understanding and Cognitive Science, NLUCS 2006, in Conjunction with ICEIS 2006",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954120240&partnerID=40&md5=bb6a9b561920d504732788b47413cc67","In the field of information extraction and automatic question answering access to a domain ontology may be of great help. But the main problem is building such an ontology, a difficult and time consuming task. We propose an approach in which the domain ontology is learned from the linguistic analysis of a number of texts which represent the domain itself. NLP analysis is done with GETARUNS system. GETARUNS can build a Discourse Model and is able to assign a relevance score to each entity. From Discourse Model we extract best candidates to become concepts in the domain ontology. To arrange concepts in the correct hierarchy we use WordNet taxonomy. Once the domain ontology is built we reconsider the texts to extract information. In this phase the entities recognized at discourse level are used to create instances of the concepts. The predicate-argument structure of the verb is used to construct instance slots for concepts. Eventually, the question answering task is performed by translating the natural language question in a suitable form and use that to query the Discourse Model enriched by the ontology.",,"Argument structures; Discourse model; Domain ontologies; Information Extraction; Linguistic analysis; Natural language questions; Question Answering; Question Answering Task; Relevance score; Text analysis; Time-consuming tasks; Wordnet; Linguistics; Ontology",2-s2.0-77954120240
"Zhao Y., Xu Z., Li P., Guan Y.","InsunQA06 on QA track of TREC2006",2006,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873530873&partnerID=40&md5=0746e80693ae149eada5ffc58d2510e3",[No abstract available],,,2-s2.0-84873530873
"Höfig E.","Template matching on XML streams",2006,"Proceedings of the IASTED International Conference on Software Engineering, as part of the 24th IASTED International Multi-Conference on APPLIED INFORMATICS",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047095399&partnerID=40&md5=d07cd94fc3e220ed00a6ca008a841f19","This paper describes the employment of template matching principles for analysis of XML streams. We introduce TMPL, a template specification language, argue why it is better suited for stream analysis than conventional technologies, that are adopted to the requirements of data stream processing. We discuss the language's constructs and their implications to runtime behaviour. Feasability of our processing approach is shown by presenting the application of TMPL to two real-world scenarios and examining the differences to conventional processing methods.","Programming tools and languages; Stream processing; Template matching; XML","Computer programming languages; Data processing; Information technology; Specification languages; Template matching; Data stream processing; Programming tools; Stream processing; XML streams; XML",2-s2.0-34047095399
"Blažević M.","Streaming component combinators",2006,"Proceedings of Extreme Markup Languages 2006 Conference",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871737229&partnerID=40&md5=b3fb5aecc468823728eee9de763f0ac1","We present a small framework for building and reusing components for processing of markup streams. The core of the framework is built around two concepts that, as far as we know, have not been used in this area so far: the concept of splitter components that follow a common interface and contract, and the concept of generic component combinators that can be used to combine arbitrary splitter and filter components into higher-level components. Copyright © 2006 Mario Blaževic.","Processing","Combinators; Filter components; Generic components; Processing; Markup languages",2-s2.0-84871737229
"Rinaldi F., Schneider G., Kaljurand K., Hess M., Romacker M.","An environment for relation mining over richly annotated corpora: The case of GENIA",2006,"CEUR Workshop Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874273066&doi=10.1186%2f1471-2105-7-S3-S3&partnerID=40&md5=76f77af310739588851ccd0e913c3220","Background: The biomedical domain is witnessing a rapid growth of the amount of published scientific results, which makes it increasingly difficult to filter the core information. There is a real need for support tools that 'digest' the published results and extract the most important information. Results: We describe and evaluate an environment supporting the extraction of domain-specific relations, such as protein-protein interactions, from a richly-annotated corpus. We use full, deeplinguistic parsing and manually created, versatile patterns, expressing a large set of syntactic alternations, plus semantic ontology information. Conclusion: The experiments show that our approach described is capable of delivering highprecision results, while maintaining sufficient levels of recall. The high level of abstraction of the rules used by the system, which are considerably more powerful and versatile than finite-state approaches, allows speedy interactive development and validation. © 2006 Rinaldi et al; licensee BioMed Central Ltd.",,"Biomedical domain; Domain specific; Finite-state; High level of abstraction; High-precision; Interactive development; Protein-protein interactions; Rapid growth; Relation mining; Scientific results; Semantic ontology; Support tool; Proteins; Semantics; Filtration",2-s2.0-84874273066
"Zhao Y.-M., Xu Z.-M., Guan Y., Wang X.-L.","An open domain question answering system based on improved system similarity model",2006,"Proceedings of the 2006 International Conference on Machine Learning and Cybernetics",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947286254&doi=10.1109%2fICMLC.2006.259170&partnerID=40&md5=43ebeca8e3e6b377daf643df16d895d0","Question-answering has recently received more and more attention from researchers. It is widely regarded as the advanced stage of information retrieval. This paper provides a novel domain-independent question-answering system which is based on information retrieval in a large-scale collection of texts, and an improved system similarity model is developed and applied in it which improves the performance of the system. Many natural language processing technologies are adopted to increase the accuracy of the system. Several useful tools are incorporates as external auxiliary resources. In addition, some external knowledge such as knowledge from Internet is also widely used in this system. Test data collection and evaluation methodology from 2006 Text Retrieval Conference's Question Answering Track are used to evaluate the system, and the results are comparatively satisfying. © 2006 IEEE.","Information retrieval; Question-answering; Semantic similarity","Data acquisition; Internet; Knowledge acquisition; Natural language processing systems; Resource allocation; Semantics; External auxiliary resources; Question answering; Semantic similarity; System similarity models; Information retrieval systems",2-s2.0-33947286254
"Cogliati A., Vuorimaa P.","Optimized CSS engine",2006,"WEBIST 2006 - 2nd International Conference on Web Information Systems and Technologies, Proceedings",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749103429&partnerID=40&md5=7ca663aebd447735db3728a9a04c9d19","Future Web applications will be based on XML platform. CSS is a tool used to create different XML presentations' layouts in the heterogeneous set of client devices, which often have limited resources. In this paper, design and implementation of an optimized CSS engine are described. At first, the optimization algorithm is explained, and then the implementation of the CSS engine and its integration within an XML browser are described. Measurements taken with real Web XML documents styled with CSS style sheets show performance improvements of the optimization. © 2010.","Browser; CSS; Optimization; XML","Browser; Client devices; CSS; Optimization algorithms; Performance improvements; Style sheets; WEB application; XML browsers; Information systems; Optimization; World Wide Web; XML; Markup languages",2-s2.0-48749103429
"Jalabert F., Ranwez S., Crampes M., Derozier V.","I2DEE : Intégrer et visualiser des données biologiques pour concevoir une ressource termino-ontologique",2006,"17es Journees Francophones d'Ingenierie des Connaissances, IC 2006",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875787354&partnerID=40&md5=15b6205d104841eee750a1d6f04568e0",[No abstract available],,,2-s2.0-84875787354
"Wang F., Jusoh S., Yang S.X.","A collaborative behavior-based approach for handling ambiguity, uncertainty, and vagueness in robot natural language interfaces",2006,"Engineering Applications of Artificial Intelligence",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750503083&doi=10.1016%2fj.engappai.2006.02.003&partnerID=40&md5=352c4f59cf0071792237feb809b071c4","Service robots are used by ordinary people in their houses and offices. For such users, a desirable way to communicate with robots is through natural language interfaces. So far, techniques for developing robot natural language interfaces are far from mature. A major challenge is handling ambiguity, uncertainty, and vagueness in parsing, object resolution, and vague natural language words. In this research, we develop a new approach called the collaborative behavior-based approach, in which behaviors of robots and behaviors of human users, as well as the changes of object states caused by the behaviors, are taken into consideration integratedly in processing natural language user instructions. In this paper, we analyze the special features of a human-robot interface that may affect language understanding, describe our approach that is designed based on the features, and present the implementation and some experimental results. © 2006 Elsevier Ltd. All rights reserved.","Natural language interface; Possibility theory; Robot","Feature extraction; Formal languages; Man machine systems; Object recognition; Product design; Robotics; User interfaces; Natural language interfaces; Object resolution; Possibility theory; Computer supported cooperative work",2-s2.0-33750503083
"Chernik K., Lánský J., Galamboš L.","Syllable-based compression for XML documents",2006,"CEUR Workshop Proceedings",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868710856&partnerID=40&md5=82ab19989bff5c3304f09846fe73a792","Syllable-based compression achieves sufficiently good results on text documents of a medium size. Since the majority of XML documents are of that size, we suppose that the syllable-based method can give good results on XML documents, especially on documents that have a simple structure (small amount of elements and attributes) and relatively long character data content. In this paper we propose two syllable-based compression methods for XML documents. The first method, XMLSyl, replaces XML tokens (element tags and attributes) by special codes in input document and then compresses this document using a syllable-based method. The second method, XMillSyl, incorporates syllable-based compression into the existing method for XML compression XMill. XMLSyl and XMillSyl are compared with a non-XML syllable-based method and with other existing method for XML compression.",,"Compression methods; Data contents; Medium size; Simple structures; Text document; XML compression; Specifications; XML",2-s2.0-84868710856
"Nakano K., Mu S.-C.","A pushdown machine for recursive XML processing",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845919052&partnerID=40&md5=6dc123f3ed8cb3b648cb41fec3561d17","XML transformations are most naturally defined as recursive functions on trees. A naive implementation, however, would load the entire input XML tree into memory before processing. In contrast, programs in stream processing style minimise memory usage since it may release the memory occupied by the processed prefix of the input, but they are harder to write because the programmer is left with the burden to maintain a state. In this paper, we propose a model for XML stream processing and show that all programs written in a particular style of recursive functions on XML trees, the macro forest transducer, can be automatically translated to our stream processors. The stream processor is declarative in style, but can be implemented efficiently by a pushdown machine. We thus get the best of both worlds - program clarity, and efficiency in execution. © Springer-Verlag Berlin Heidelberg 2006.",,"Functions; Mathematical transformations; Multiprocessing systems; Transducers; Trees (mathematics); Memory usage; XML stream processing; XML transformations; XML trees; XML",2-s2.0-33845919052
"Li Q., Wu Y.-F.B.","Identifying important concepts from medical documents",2006,"Journal of Biomedical Informatics",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750840050&doi=10.1016%2fj.jbi.2006.02.001&partnerID=40&md5=e12eb321cebad857fd8e2f82ecaff93c","Automated medical concept recognition is important for medical informatics such as medical document retrieval and text mining research. In this paper, we present a software tool called keyphrase identification program (KIP) for identifying topical concepts from medical documents. KIP combines two functions: noun phrase extraction and keyphrase identification. The former automatically extracts noun phrases from medical literature as keyphrase candidates. The latter assigns weights to extracted noun phrases for a medical document based on how important they are to that document and how domain specific they are in the medical domain. The experimental results show that our noun phrase extractor is effective in identifying noun phrases from medical documents, so is the keyphrase extractor in identifying important medical conceptual terms. They both performed better than the systems they were compared to. © 2006 Elsevier Inc. All rights reserved.","Document keyphrase; Keyphrase extraction; Medical concepts; Medical documents; Noun phrase extraction; Text mining","Biomedical engineering; Data mining; Data reduction; Information retrieval; Document keyphrase; Keyphrase extraction; Medical concepts; Medical documents; Noun phrase extraction; Text mining; Information retrieval systems; algorithm; article; computer program; data base; medical documentation; medical literature; nomenclature; priority journal; Algorithms; Concept Formation; Humans; Information Storage and Retrieval; Information Systems; Language; Linguistics; Medical Records Systems, Computerized; Models, Statistical; Pattern Recognition, Automated; Terminology; Vocabulary, Controlled",2-s2.0-33750840050
"Gubała T., Herȩzlak D., Bubak M., Malawski M.","Semantic composition of scientific workflows based on the petri nets formalism",2006,"e-Science 2006 - Second IEEE International Conference on e-Science and Grid Computing",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38449117817&doi=10.1109%2fE-SCIENCE.2006.261096&partnerID=40&md5=f4a1ca700703bc6f02b5c56108d90347","The idea of an application described through its workflow is becoming popular in the Grid community as a natural method of functional decomposition of an application. It shows all the important dependencies as a set of connections of data flow and/or control flow. As scientific workflows grow in size and complexity, a tool to assist end users is becoming necessary. In this paper we describe the formal basis, design and implementation of such a tool - an assistant which analyzes user requirements regarding application results and works with information registries that provide information on resources available in the Grid. The Workflow Composition Tool (WCT) provides the functionality of automatic workflow construction based on the process of semantic service discovery and matchmaking. It uses a well-designed construction algorithm together with specific heuristics in order to provide useful solutions for application users. © 2006 IEEE.",,"Algorithms; Data flow analysis; Grid computing; Internet; User interfaces; Construction algorithm; Scientific workflows; Workflow Composition Tool (WCT); Semantics",2-s2.0-38449117817
"Levine R.D., Meurers W.D.","Head-Driven Phrase Structure Grammar",2006,"Encyclopedia of Language & Linguistics",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954730494&doi=10.1016%2fB0-08-044854-2%2f02040-X&partnerID=40&md5=5545c21e4ad4d3f08202a8e7fb572e9c","We provide a tripartite overview of HPSG, a constraint-based model-theoretic framework for grammatical representation widely used for both theoretical research and computational implementations of natural language grammars. We begin with a condensed survey of the descriptive machinery assumed in the theory, focusing on the feature-based encoding of grammatical properties and the form of constraints regulating the specification of values for these features in complex syntatic structures, with illustrations from various local and nonlocal dependencies exhibited in a range of grammatical constructions. The following section explores in some detail the logical and formal foundations of the descriptive apparatus, centered on the crucial ontological distinction between, on the one hand, formal objects embodying the properties of linguistic expressions, and on the other, descriptions of those objects. We conclude with a review of HPSG-based processing, including the motivations behind such work, the different computational approaches and systems, as well as current strands of research. © 2006 Elsevier Ltd. All rights reserved.","Constraint-based; Constraint-resolution; Declarative; Feature logic; Formal foundations of linguistics; Head-Driven Phrase Structure Grammar; HPSG; Information-structure; Logic programming parsing; Model theory; Semantics; Syntax",,2-s2.0-77954730494
"Ananiadou S., Kell D.B., Tsujii J.-i.","Text mining and its potential applications in systems biology",2006,"Trends in Biotechnology",191,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751111173&doi=10.1016%2fj.tibtech.2006.10.002&partnerID=40&md5=b766c5fa6d417e7dd3443614f1e61c32","With biomedical literature increasing at a rate of several thousand papers per week, it is impossible to keep abreast of all developments; therefore, automated means to manage the information overload are required. Text mining techniques, which involve the processes of information retrieval, information extraction and data mining, provide a means of solving this. By adding meaning to text, these techniques produce a more structured analysis of textual knowledge than simple word searches, and can provide powerful tools for the production and analysis of systems biology models. © 2006 Elsevier Ltd. All rights reserved.",,"Information management; Information retrieval; Knowledge acquisition; Mathematical models; Online searching; Text processing; Biology models; Systems biology; Text mining; Data mining; artificial intelligence; automation; data base; information processing; information retrieval; Internet; medical literature; MEDLINE; priority journal; review; search engine; systems biology; Artificial Intelligence; Information Storage and Retrieval; Models, Biological; Natural Language Processing; Publications; Research; Systems Biology",2-s2.0-33751111173
"Kogan J., Nicholas C., Teboulle M.","Grouping multidimensional data: Recent advances in clustering",2006,"Grouping Multidimensional Data: Recent Advances in Clustering",26,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892013223&doi=10.1007%2f3-540-28349-8&partnerID=40&md5=b70d8b59c7b447398a127fa4ded505a5","Clustering is one of the most fundamental and essential data analysis techniques. Clustering can be used as an independent data mining task to discern intrinsic characteristics of data, or as a preprocessing step with the clustering results then used for classification, correlation analysis, or anomaly detection. Kogan and his co-editors have put together recent advances in clustering large and high-dimension data. Their volume addresses new topics and methods which are central to modern data analysis, with particular emphasis on linear algebra tools, opimization methods and statistical techniques. The contributions, written by leading researchers from both academia and industry, cover theoretical basics as well as application and evaluation of algorithms, and thus provide an excellent state-of-the-art overview. The level of detail, the breadth of coverage, and the comprehensive bibliography make this book a perfect fit for researchers and graduate students in data mining and in many other important related application areas. © Springer-Verlag Berlin Heidelberg 2006.",,,2-s2.0-84892013223
"Cimiano P.","Ontology learning and population from text: Algorithms, evaluation and applications",2006,"Ontology Learning and Population from Text: Algorithms, Evaluation and Applications",266,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892008824&doi=10.1007%2f978-0-387-39252-3&partnerID=40&md5=b571a084399be774e96194639347114a","Standard formalisms for knowledge representation such as RDFS or OWL have been recently developed by the semantic web community and are now in place. However, the crucial question still remains: how will we acquire all the knowledge available in people's heads to feed our machines? Natural language is THE means of communication for humans, and consequently texts are massively available on the Web. Terabytes and terabytes of texts containing opinions, ideas, facts and information of all sorts are waiting to be mined for interesting patterns and relationships, or used to annotate documents to facilitate their retrieval. A semantic web which ignores the massive amount of information encoded in text, might actually be a semantic, but not a very useful, web. Knowledge acquisition, and in particular ontology learning from text, actually has to be regarded as a crucial step within the vision of a semantic web. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications presents approaches for ontology learning from text and will be relevant for researchers working on text mining, natural language processing, information retrieval, semantic web and ontologies. Containing introductory material and a quantity of related work on the one hand, but also detailed descriptions of algorithms, evaluation procedures etc. on the other, this book is suitable for novices, and experts in the field, as well as lecturers. Datasets, algorithms and course material can be downloaded at http://www.cimiano.de/olp. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications is designed for practitioners in industry, as well researchers and graduate-level students in computer science. © 2006 Springer Science+Business Media, LLC. All rights reserved.",,,2-s2.0-84892008824
"Rinaldi F., Schneider G., Kaljurand K., Hess M., Romacker M.","An environment for relation mining over richly annotated corpora: The case of GENIA",2006,"BMC Bioinformatics",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947368247&doi=10.1186%2f1471-2105-7-S3-S3&partnerID=40&md5=2be4b850f5b1315a0db32c612285538b","Background: The biomedical domain is witnessing a rapid growth of the amount of published scientific results, which makes it increasingly difficult to filter the core information. There is a real need for support tools that 'digest' the published results and extract the most important information. Results: We describe and evaluate an environment supporting the extraction of domain-specific relations, such as protein-protein interactions, from a richly-annotated corpus. We use full, deep-linguistic parsing and manually created, versatile patterns, expressing a large set of syntactic alternations, plus semantic ontology information. Conclusion: The experiments show that our approach described is capable of delivering highprecision results, while maintaining sufficient levels of recall. The high level of abstraction of the rules used by the system, which are considerably more powerful and versatile than finite-state approaches, allows speedy interactive development and validation. © 2006 Rinaldi et al; licensee BioMed Central Ltd.",,"Biomedical domain; Domain specific; High level of abstraction; Interactive development; Protein-protein interactions; Relation mining; Scientific results; Semantic ontology; Proteins; Semantics; Filtration; accuracy; article; automation; bioinformatics; biomedicine; data analysis; data mining; information processing; information system; linguistics; medical information; protein protein interaction; semantics; algorithm; artificial intelligence; computer program; controlled vocabulary; documentation; evaluation study; factual database; information retrieval; natural language processing; nomenclature; procedures; publication; Abstracting and Indexing as Topic; Algorithms; Artificial Intelligence; Databases, Factual; Information Storage and Retrieval; Natural Language Processing; Periodicals as Topic; Semantics; Software; Terminology as Topic; Vocabulary, Controlled",2-s2.0-33947368247
"Abbott J., Bell J., Clark A., De Vel O., Mohay G.","Automated recognition of event scenarios for digital forensics",2006,"Proceedings of the ACM Symposium on Applied Computing",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751045850&partnerID=40&md5=810e127740e6eac1279d8dbfd4f99837","The authors have previously developed the ECF (Event Correlation for Forensics) framework for scenario matching in the forensic investigation of activity manifested in digital transactional logs. ECF incorporated a suite of log parsers to reduce event records from heterogeneous logs to a canonical form for lodging in an SQL database. This paper presents work since then, the Auto-ECF system, which represents significant advances on ECF. The paper reports on the development and implementation of the new event abstraction and scenario specification methodology and on the development of the Auto-ECF system which builds on that to achieve the automated recognition of event scenarios. The paper also reports on the evaluation of Auto-ECF using three scenarios including one from the well known DARPA test data. Copyright 2006 ACM.","Computer forensics; Event correlation; Events; Heterogeneous event logs; Logs","Computer crime; Computer software; Database systems; Query languages; Computer forensics; Event correlation; Events; Heterogeneous event logs; Logs; Pattern recognition",2-s2.0-33751045850
"Mark Pettovello P., Fotouhi F.","MTree: An XML XPath graph index",2006,"Proceedings of the ACM Symposium on Applied Computing",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751035247&partnerID=40&md5=866e0900e03db4f256a744c33a928808","This paper introduces the MTree index algorithm, a special purpose XML XPath index designed to meet the needs of the hierarchical XPath query language. With the increasing importance of XML, XPath, and XQuery, several methods have been proposed for creating XML structure indexes and many variants using relational technology have been proposed. This work proposes a new XML structure index, called MTree, which is designed to be optimal for traversing all XPath axes. The primary feature of MTree lies in its ability to provide the next subtree root node in document order, for all axes, to each context node in O(1). MTree is a special purpose XPath index structure that matches the special purpose query requirements for XPath. This approach is in contrast to other approaches that map the problem domain into general purpose index structures such as B-Tree that must reconstruct the XML tree from those structures for every query. MTree supports modification operations such as insert and delete. MTree has been implemented both in memory and on disk, and performance results using XMark benchmark data are presented showing up to two orders of magnitude improvement over other well-known implementations. Copyright 2006 ACM.","Graph; Index; Threaded paths; XML; XPath","Data structures; Query languages; Relational database systems; Trees (mathematics); XML; MTree; XML tree; XML XPath graph index; Algorithms",2-s2.0-33751035247
"Lumb L.I., Aldridge K.D.","Grid-enabling the global geodynamics project: Automatic RDF extraction from the ESML data description and representation via GRDDL",2006,"20th International Symposium on High-Performance Computing in an Advanced Collaborative Environment, 2006. HPCS 2006",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751049917&doi=10.1109%2fHPCS.2006.26&partnerID=40&md5=d06696a4df22a9d86aa58ee0b890ce4a","An eXtensible Markup Language (XML) based data model for the Global Geodynamics Project (GGP) has been previously developed. Mindful of the need to incorporate metadata into the description and representation, a Resource Description Framework (RDF) based approach is introduced that extends the previous data model. Specifically, use of RDF allows relationships to be described and represented, and will eventually result in an 'informal ontology'. The bottom-up approach makes use of GRDDL (Gleaning Resource Descriptions from Dialects of Languages) - a vehicle that allows for the extraction of RDF from XML according to rules. Because there exists some latitude in such extractions, complimentary top-down approaches will be required - especially when reconciling with formal ontologies. From this 'information science' perspective, GGP data has the potential to factor in the broader context being defined by the 'new geoinformatics'. © 2006 IEEE.",,"Data reduction; Data structures; Information science; Metadata; Project management; XML; Formal ontologies; Gleaning Resource Descriptions from Dialects of Languages (GRDDL); Global Geodynamics Project (GGP); Resource Description Framework (RDF); Geophysics",2-s2.0-33751049917
"Cohen F.","Fast SOA: The way to use native XML technology to achieve Service Oriented Architecture governance, scalability, and performance",2006,"Fast SOA: The way to use native XML technology to achieve Service Oriented Architecture governance, scalability, and performance",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941623304&doi=10.1016%2fB978-0-12-369513-0.X5000-9&partnerID=40&md5=1560bf5ecc613aeac2f2b8fec21abc81","Without the right controls to govern SOA development, the right set of tools to build SOA, and the right support of exciting new protocols and patterns, your SOA efforts can result in software that delivers only 1.5 transactions per second (TPS) on expensive modern servers. This is a disaster enterprises, organizations, or institutions avoid by using Frank Cohens FastSOA patterns, test methodology, and architecture. In FastSOA you will learn how to apply native XML technology to SOA for: * Data mediation using mid-tier data and service caching to handle the explosion of new schemas and new devices in an ever changing environment * Data aggregation in the SOA middle-tier for off-line browsing, service acceleration through mid-tier caching and transformation, and bandwidth-needs reduction * Increased service and application scalability and performance * Successful evaluations of application server, XML parser, relational and native XML database, Enterprise Service Bus, Business Integration server, workflow server, and Web Service tools for performance, scalability, and developer productivity * Improved service governance through XML persistence in SOA registries and repositories * Composite data services (CDS) to provide maximum reuse of software components and data, accelerate performance, and reduce development time and maintenance in your SOA © 2007 Elsevier Inc. All rights reserved.",,"Computer software reusability; Disaster prevention; Information services; Metadata; Scalability; Web services; XML; Application scalability; Application Servers; Business integration; Changing environment; Data aggregation; Enterprise service bus; Native xml database; Software component; Service oriented architecture (SOA)",2-s2.0-84941623304
"Yang C.G., Granite S.J., Van Eyk J.E., Winslow R.L.","MASCOT HTML and XML parser: An implementation of a novel object model for protein identification data",2006,"Proteomics",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751077285&doi=10.1002%2fpmic.200600157&partnerID=40&md5=35eefefbe13ffb1c0d80c9fe330b33db","Protein identification using MS is an important technique in proteomics as well as a major generator of proteomics data. We have designed the protein identification data object model (PDOM) and developed a parser based on this model to facilitate the analysis and storage of these data. The parser works with HTML or XML files saved or exported from MASCOT MS/MS ions search in peptide summary report or MASCOT PMF search in protein summary report. The program creates PDOM objects, eliminates redundancy in the input file, and has the capability to output any PDOM object to a relational database. This program facilitates additional analysis of MASCOT search results and aids the storage of protein identification information. The implementation is extensible and can serve as a template to develop parsers for other search engines. The parser can be used as a stand-alone application or can be driven by other Java programs. It is currently being used as the front end for a system that loads HTML and XML result files of MASCOT searches into a relational database. The source code is freely available at http:// www.ccbm.jhu.edu and the program uses only free and open-source Java libraries. © 2006 WILEY-VCH Verlag GmbH & Co. KGaA.","HTML parser; Java; MASCOT parser; Protein identification data object model; XML parser","article; computer program; information system; mass spectrometry; priority journal; protein analysis; protein database; Databases, Protein; Hypermedia; Information Storage and Retrieval; Mass Spectrometry; Programming Languages; Proteins; Proteomics; Software; User-Computer Interface",2-s2.0-33751077285
"Higgins J.","Method of finding similar log messages in numerous log files",2006,"Research Disclosure",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845516305&partnerID=40&md5=0cfdc666b7ec0bdc8b613701d536d8ce","An algorithmic approach is proposed to find similar messages in various log files and to identify a single log message that acts as a root cause of the problems in large computer systems. The approach uses a target log message to find similar messages in the database. The proposed algorithm parses the message and uses parsed data to construct a query. This query string is issued into the search engines as a result set. The software components used in the process include search engine, document index, document indexer and message parser, and hardware specific information. The result set contains all messages that match the target log message, which is presented to the user in rank order. This method allows the users to efficiently find related log messages from an overwhelming number of log files.",,"Computer software; Computer systems; Indexing (of information); Problem solving; Query languages; Search engines; Log files; Log messages; Message parser; Database systems",2-s2.0-33845516305
"Ng W., Lam W.-Y., Wood P.T., Levene M.","XCQ: A queriable XML compression system",2006,"Knowledge and Information Systems",43,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750537792&doi=10.1007%2fs10115-006-0012-z&partnerID=40&md5=3d7e5387d0324b16d76b68fbd2fb4593","XML has already become the de facto standard for specifying and exchanging data on the Web. However, XML is by nature verbose and thus XML documents are usually large in size, a factor that hinders its practical usage, since it substantially increases the costs of storing, processing, and exchanging data. In order to tackle this problem, many XML-specific compression systems, such as XMill, XGrind, XMLPPM, and Millau, have recently been proposed. However, these systems usually suffer from the following two inadequacies: They either sacrifice performance in terms of compression ratio and execution time in order to support a limited range of queries, or perform full decompression prior to processing queries over compressed documents. In this paper, we address the above problems by exploiting the information provided by a Document Type Definition (DTD) associated with an XML document. We show that a DTD is able to facilitate better compression as well as generate more usable compressed data to support querying. We present the architecture of the XCQ, which is a compression and querying tool for handling XML data. XCQ is based on a novel technique we have developed called DTD Tree and SAX Event Stream Parsing (DSP). The documents compressed by XCQ are stored in Partitioned Path-Based Grouping (PPG) data streams, which are equipped with a Block Statistics Signature (BSS) indexing scheme. The indexed PPG data streams support the processing of XML queries that involve selection and aggregation, without the need for full decompression. In order to study the compression performance of XCQ, we carry out comprehensive experiments over a set of XML benchmark datasets. © Springer-Verlag London Limited 2006.","Compression algorithms; Document type definitions; Performance; Query processing; XML",,2-s2.0-33750537792
"Mayfield J., McNamee P.","Improving QA retrieval using document priors",2006,"Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750312814&partnerID=40&md5=ecd56a5a0306fc5c70967910d895c862","We present a simple way to improve document retrieval for question answering systems. The method biases the retrieval system toward documents that contain words that have appeared in other documents containing answers to the same type of question. The method works with virtually any retrieval system, and exhibits a statistically significant performance improvement over a strong baseline.","Document priors; Document retrieval; Question answering","Online searching; Query languages; Data structures; Search engines; Statistical methods; Document priors; Document retrieval; Question answering; Question answering systems; Retrieval system; Information retrieval systems",2-s2.0-33750312814
"Wei K., Muthuprasanna M., Kothari S.","Preventing SQL injection attacks in stored procedures",2006,"Proceedings of the Australian Software Engineering Conference, ASWEC",33,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750143286&doi=10.1109%2fASWEC.2006.40&partnerID=40&md5=578c56c4b3be7144ca5a00307667680f","An SQL injection attack targets interactive web applications that employ database services. These applications accept user inputs and use them to form SQL statements at runtime. During an SQL injection attack, an attacker might provide malicious SQL query segments as user input which could result in a different database request. By using SQL injection attacks, an attacker could thus obtain and/or modify confidential/sensitive information. An attacker could even use a SQL injection vulnerability as a rudimentary IP/Port scanner of the internal corporate network. Several papers in literature have proposed ways to prevent SQL injection attacks in the application layer by examining dynamic SQL query semantics at runtime. However, very little emphasis is laid on securing stored procedures in the database layer which could also suffer from SQL injection attacks. Some papers in literature even refer to stored procedures as a remedy against SQL injection attacks. As stored procedures reside on the database front, the methods proposed by them cannot be applied to secure stored procedures themselves. In this paper, we propose a novel technique to defend against the attacks targeted at stored procedures. This technique combines static application code analysis with run-time validation to eliminate the occurrence of such attacks. In the static part, we design a stored procedure parser, and for any SQL statement which depends on user inputs, we use this parser to instrument the necessary statements in order to compare the original SQL statement structure to that including user inputs. The deployment of this technique can be automated and used on a need-only basis. We also provide a preliminary evaluation of the results of the technique proposed, as performed on several stored procedures in the SQL Server 2005 database. © 2006 IEEE.",,"Database services; Internal corporate networks; Secure stored procedures; Automata theory; Codes (symbols); Computer viruses; Database systems; Query languages; Security systems; Semantics; Computer programming languages",2-s2.0-33750143286
"Chen Y., Davidson S.B., Zheng Y.","An efficient XPath query processor for XML streams",2006,"Proceedings - International Conference on Data Engineering",74,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749612519&doi=10.1109%2fICDE.2006.18&partnerID=40&md5=1034d0ff0fe85ff13eeb0b9582de3973","Streaming XPath evaluation algorithms must record a potentially exponential number of pattern matches when both predicates and descendant axes are present in queries, and the XML data is recursive. In this paper, we use a compact data structure to encode these pattern matches rather than storing them explicitly. We then propose a polynomial time streaming algorithm to evaluate XPath queries by probing the data structure in a lazy fashion. Extensive experiments show that our approach not only has a good theoretical complexity bound but is also efficient in practice. © 2006 IEEE.",,"Pattern matches; Query processor; Streaming XPath evaluation algorithms; Theoretical complexity; Algorithms; Data processing; Data storage equipment; Data structures; Pattern recognition; Polynomial approximation; XML; Query languages",2-s2.0-33749612519
"Yu H., Kim W., Hatzivassiloglou V., Wilbur J.","A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations",2006,"ACM Transactions on Information Systems",17,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749588452&doi=10.1145%2f1165774.1165778&partnerID=40&md5=3ff12fd8b0fa56ef6e72ae94a2066f34","Abbreviations and acronyms are widely used in the biomedical literature and many of them represent important biomedical concepts. Because many abbreviations are ambiguous (e.g., CAT denotes both chloramphenicol acetyl transferase and computed axial tomography, depending on the context), recognizing the full form associated with each abbreviation is in most cases equivalent to identifying the meaning of the abbreviation. This, in turn, allows us to perform more accurate natural language processing, information extraction, and retrieval. In this study, we have developed supervised approaches to identifying the full forms of ambiguous abbreviations within the context they appear. We first automatically assigned multiple possible full forms for each abbreviation; we then treated the in-context full-form prediction for each specific abbreviation occurrence as a case of word-sense disambiguation. We generated automatically a dictionary of all possible full forms for each abbreviation. We applied supervised machine-learning algorithms for disambiguation. Because some of the links between abbreviations and their corresponding full forms are explicitly given in the text and can be recovered automatically, we can use these explicit links to automatically provide training data for disambiguating the abbreviations that are not linked to a full form within a text. We evaluated our methods on over 150 thousand abstracts and obtain for coverage and precision results of 82% and 92%, respectively, when performed as tenfold cross-validation, and 79% and 80%, respectively, when evaluated against an external set of abstracts in which the abbreviations are not defined. © 2006 ACM.","Data mining; Information retrieval; Machine learning; Word-sense disambiguation","Acetyl transferase; Biomedical abbreviations; Information extraction; Word sense disambiguation; Abstracting; Data mining; Information retrieval; Learning systems; Medical computing; Natural language processing systems; Text processing; Character recognition",2-s2.0-33749588452
"Nystrom N., Qi X., Myers A.C.","J&: Nested intersection for scalable software composition",2006,"ACM SIGPLAN Notices",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750421100&doi=10.1145%2f1167515.1167476&partnerID=40&md5=e5d26fb6f87f8f3422403df3a56195d8","This paper introduces a programming language that makes it convenient to compose large software systems, combining their features in a modular way. J& supports nested intersection, building on earlier work on nested inheritance in the language Jx. Nested inheritance permits modular, type-safe extension of a package (including nested packages and classes), while preserving existing type relationships. Nested intersection enables composition and extension of two or more packages, combining their types and behavior while resolving conflicts with a relatively small amount of code. The utility of J& is demonstrated by using it to construct two composable, extensible frameworks: a compiler framework for Java, and a peer-to-peer networking system. Both frameworks support composition of extensions. For example, two compilers adding different, domain-specific features to Java can be composed to obtain a compiler for a language that supports both sets of features. Copyright © 2006 ACM.","Compilers; Nested inheritance; Nested intersection","Nested inheritance; Nested intersection; Computer networks; Object oriented programming; Program compilers; Software engineering; Computer programming languages",2-s2.0-33750421100
"Ceccarelli M., Musacchia F., Petrosino A.","Content-based image retrieval by a fuzzy scale-space approach",2006,"International Journal of Pattern Recognition and Artificial Intelligence",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749039495&doi=10.1142%2fS0218001406005009&partnerID=40&md5=d4bd1fd97ab9b9166ff21d057fbc7f80","Image descriptions aimed at the realization of content-based image retrieval (CBIR) should include the vagueness of both data representations and user queries. Here we show how multiscale textural gradient can be used as an efficient visual cue for image description. This feature has been already efficiently used in problems of image segmentation and texture separation. Our main idea is based on the assumption that, for image description, shape and textures should be considered together within a unified model. We report an efficient image description algorithm where the multiscale analysis is modeled by a differential morphological filter. Experiments with large image databases and comparisons with classical methods are reported. © World Scientific Publishing Company.","Content-based image retrieval (CBIR); Morphological gradient; Rough fuzzy sets; Scale-space analysis","Algorithms; Database systems; Fuzzy sets; Image segmentation; Mathematical models; Problem solving; Textures; Image databases; Morphological gradients; Rough fuzzy sets; Scale space analysis; Image retrieval",2-s2.0-33749039495
"Li S., Zhao T.-J.","Chinese information processing and its prospects",2006,"Journal of Computer Science and Technology",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750070922&doi=10.1007%2fs11390-006-0838-6&partnerID=40&md5=e56a357d2f42aff7fc1b80ad9addd702","The paper presents some main progresses and achievements in Chinese information processing. It focuses on six aspects, i.e., Chinese syntactic analysis, Chinese semantic analysis, machine translation, information retrieval, information extraction, and speech recognition and synthesis. The important techniques and possible key problems of the respective branch in the near future are discussed as well. © Springer Science + Business Media, Inc. 2006.","Chinese information processing; Computational linguistics; Natural language processing","Computational linguistics; Information analysis; Information retrieval; Learning systems; Natural language processing systems; Problem solving; Semantics; Speech recognition; Syntactics; Translation (languages); Information extraction; Machine translation; Semantic analysis; Data processing",2-s2.0-33750070922
"Oard D.W.","Towards analysis tools for a multilingual blogsphere",2006,"AAAI Spring Symposium - Technical Report",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747201605&partnerID=40&md5=239e1ed63f53abea5860ef8799123514","Many approaches to Weblog analysis draw on words found in the postings and comments, and the meaning of those words is inherently bound to a language. Moving beyond single-language analysis will require tools that leverage translation resources in ways that are well matched to the task. In this paper, we describe how state of the art techniques developed originally for machine translation and cross-language information retrieval could provide a foundation for extending existing blog analysis tools to learn from many languages things would te harder to see in just one. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,"Cross-language; Machine translation; Multilingual blogsphere; Weblog analysis; Information retrieval; Linguistics; Resource allocation; Translation (languages); World Wide Web",2-s2.0-33747201605
"Holland T.A., Veretnik S., Shindyalov I.N., Bourne P.E.","Partitioning Protein Structures into Domains: Why Is it so Difficult?",2006,"Journal of Molecular Biology",76,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746338567&doi=10.1016%2fj.jmb.2006.05.060&partnerID=40&md5=1eaa58d1dd6bad1380b321b43d179dd8","This analysis takes an in-depth look into the difficulties encountered by automatic methods for domain decomposition from three-dimensional structure. The analysis involves a multi-faceted set of criteria including the integrity of secondary structure elements, the tendency toward fragmentation of domains, domain boundary consistency and topology. The strength of the analysis comes from the use of a new comprehensive benchmark dataset, which is based on consensus among experts (CATH, SCOP and AUTHORS of the 3D structures) and covers 30 distinct architectures and 211 distinct topologies as defined by CATH. Furthermore, over 66% of the structures are multi-domain proteins; each domain combination occurring once per dataset. The performance of four automatic domain assignment methods, DomainParser, NCBI, PDP and PUU, is carefully analyzed using this broad spectrum of topology combinations and knowledge of rules and assumptions built into each algorithm. We conclude that it is practically impossible for an automatic method to achieve the level of performance of human experts. However, we propose specific improvements to automatic methods as well as broadening the concept of a structural domain. Such work is prerequisite for establishing improved approaches to domain recognition. (The benchmark dataset is available from http://pdomains.sdsc.edu). © 2006 Elsevier Ltd. All rights reserved.","benchmark dataset; integrity of secondary structures; performance evaluation; structural domains; topological assessment","article; priority journal; protein analysis; protein domain; protein localization; protein structure; structure analysis; Computational Biology; Computer Simulation; Models, Molecular; Protein Structure, Secondary; Protein Structure, Tertiary",2-s2.0-33746338567
"Zhou Z.-H., Chen K.E.-J., Dai H.-B.","Enhancing relevance feedback in image retrieval using unlabeled data",2006,"ACM Transactions on Information Systems",116,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746834100&doi=10.1145%2f1148020.1148023&partnerID=40&md5=0c616abc7c5cc6fc00915742100e6a04","Relevance feedback is an effective scheme bridging the gap between high-level semantics and low-level features in content-based image retrieval (CBIR). In contrast to previous methods which rely on labeled images provided by the user, this article attempts to enhance the performance of relevance feedback by exploiting unlabeled images existing in the database. Concretely, this article integrates the merits of semisupervised learning and active learning into the relevance feedback process. In detail, in each round of relevance feedback two simple learners are trained from the labeled data, that is, images from user query and user feedback. Each learner then labels some unlabeled images in the database for the other learner. After retraining with the additional labeled data, the learners reclassify the images in the database and then their classifications are merged. Images judged to be positive with high confidence are returned as the retrieval result, while those judged with low confidence are put into the pool which is used in the next round of relevance feedback. Experiments show that using semisupervised learning and active learning simultaneously in CBIR is beneficial, and the proposed method achieves better performance than some existing methods. © 2006 ACM.","Active learning; Content-based image retrieval machine learning; Learning with unlabeled data; Relevance feedback; Semisupervised learning","Active learning; Content-based image retrieval machine learning; Learning with unlabeled data; Relevance feedback; Semisupervised learning; Classification (of information); Database systems; Learning systems; Semantics; User interfaces; Image retrieval",2-s2.0-33746834100
"Begel A., Graham S.L.","XGLR-an algorithm for ambiguity in programming languages",2006,"Science of Computer Programming",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646849285&doi=10.1016%2fj.scico.2006.04.003&partnerID=40&md5=749470bcc0a1eece9ff254fd4b942ff4","Automatically generated lexers and parsers for programming languages have a long history. Although they are well suited for many languages, many widely used generators, among them Flex and Bison, fail to handle input stream ambiguities that arise in embedded languages, in legacy languages, and in programming by voice. We have developed Blender, a combined lexer and parser generator that enables designers to describe many classes of embedded languages and to handle ambiguities in spoken input and in legacy languages. We have enhanced the incremental lexing and parsing algorithms in our Harmonia framework to analyse lexical, syntactic and semantic ambiguities. The combination of better language description and enhanced analysis provides a powerful platform on which to build the next generation of language analysis tools. © 2006 Elsevier B.V. All rights reserved.","Embedded languages; GLR; Harmonia; Programming-by-voice; XGLR","Computer programming languages; Computer science; Laws and legislation; Semantics; Embedded languages; GLR; Harmonia; Programming-by-voice; XGLR; Algorithms",2-s2.0-33646849285
"Su Z., Wassermann G.","The essence of command injection attacks in web applications",2006,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",169,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745811685&partnerID=40&md5=9b6a0fe091a86dfd13d28394d3623e5c","Web applications typically interact with a back-end database to retrieve persistent data and then present the data to the user as dynamically generated output, such as HTML web pages. However, this interaction is commonly done through a low-level API by dynamically constructing query strings within a general-purpose programming language, such as Java. This low-level interaction is ad hoc because it does not take into account the structure of the output language. Accordingly, user inputs are treated as isolated lexical entities which, if not properly sanitized, can cause the web application to generate unintended output. This is called a command injection attack, which poses a serious threat to web application security. This paper presents the first formal definition of command injection attacks in the context of web applications, and gives a sound and complete algorithm for preventing them based on context-free grammars and compiler parsing techniques. Our key observation is that, for an attack to succeed, the input that gets propagated into the database query or the output document must change the intended syntactic structure of the query or document. Our definition and algorithm are general and apply to many forms of command injection attacks. We validate our approach with SQLCHECK, an implementation for the setting of SQL command injection attacks. We evaluated SQLCHECK on real-world web applications with systematically compiled real-world attack data as input. SQLCHECK produced no false positives or false negatives, incurred low runtime overhead, and applied straightforwardly to web applications written in different languages. Copyright © 2006 ACM.","Command injection attacks; Grammars; Parsing; Runtime verification; Web applications","Command injection attacks; Parsing; Runtime verification; Web applications; Computer programming languages; Context free grammars; HTML; Information retrieval; Query languages; World Wide Web; Database systems",2-s2.0-33745811685
"Böhm A., Brantner M., Kanne C.-C., May N., Moerkotte G.","Natix visual interfaces",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745606028&partnerID=40&md5=e0856762dacf3bd101eaa834a038b118","We present the architecture of Natix V2. Among the features of this native XML Data Store are an optimizing XPath query compiler and a powerful API. In our demonstration we explain this API and present XPath evaluation in Natix using its visual explain facilities. © Springer-Verlag Berlin Heidelberg 2006.",,"Computer architecture; Optimization; Program compilers; Storage allocation (computer); XML; Data store; Natix; Queries; Query compilers; Graphical user interfaces",2-s2.0-33745606028
"Zhou X., Hu X., Lin X., Han H., Zhang X.","Relation-based document retrieval for biomedical literature databases",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745572295&doi=10.1007%2f11733836_48&partnerID=40&md5=2c202b7a9ae10a615f0b793a69e638a9","In this paper, we explore the direct use of relations in information retrieval for precision-focused biomedical literature search. A relation is defined as a pair of two concepts which are semantically and syntactically related to each other. Unlike the traditional term-based IR models, our model represents a document by a set of controlled concepts and their binary relations. Since document level co-occurrence of two concepts, in many cases, does not mean this document really addresses their relationships, the direct use of relation may improve the precision of very specific search, e.g. searching documents that mention genes regulated by Smad4. For this purpose, we develop a generic ontology-based approach to extract concepts and their relations; a prototyped IR system supporting relation-based search is then built for Medline abstract search. We then use this novel IR system to improve the retrieval result of all official runs in TREC-2004 Genomics Track. The experiment shows promising performance of relation-based IR. The mean of P@100 (the precision of top 100 documents) for all 50 topics is raised from 26.37 %( the P@100 of the best run is 42.10%) to 53.69% while the recall is kept at an acceptable level of 44.31%. The experiment also demonstrates the expressiveness of relations for the representation of genomic information needs. © Springer-Verlag Berlin Heidelberg 2006.",,"Biomedical literature databases; Document retrieval; Genomic information; Genomics Track; Biomedical engineering; Computer science; Database systems; Genes; Information technology; Semantics; Information retrieval",2-s2.0-33745572295
"Mcdonald D.M., Chen H.","Summary in context: Searching versus browsing",2006,"ACM Transactions on Information Systems",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745194096&partnerID=40&md5=41f94d1aa4c6eb9d6e6b7c18744680e6","The use of text summaries in information-seeking research has focused on query-based summaries. Extracting content that resembles the query alone, however, ignores the greater context of the document. Such context may be central to the purpose and meaning of the document. We developed a generic, a query-based, and a hybrid summarizer, each with differing amounts of document context. The generic summarizer used a blend of discourse information and information obtained through traditional surface-level analysis. The query-based summarizer used only query-term information, and the hybrid summarizer used some discourse information along with query-term information. The validity of the generic summarizer was shown through an intrinsic evaluation using a well-established corpus of human-generated summaries. All three summarizers were then compared in an information-seeking experiment involving 297 subjects. Results from the information-seeking experiment showed that the generic summaries outperformed all others in the browse tasks, while the query-based and hybrid summaries outperformed the generic summary in the search tasks. Thus, the document context of generic summaries helped users browse, while such context was not helpful in search tasks. Such results are interesting given that generic summaries have not been studied in search tasks and the that majority of Internet search engines rely solely on query-based summaries. © 2006 ACM.","Browse; Generic summaries; Indicative summaries; Information seeking; Natural language processing; Search; Summarization; Text processing","Browse; Generic summaries; Indicative summaries; Information seeking; Natural language processing; Search; Summarization; Information analysis; Internet; Query languages; Text processing; Web browsers; Search engines",2-s2.0-33745194096
"Su Z., Wassermann G.","The essence of command injection attacks in web applications",2006,"ACM SIGPLAN Notices",65,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745216482&doi=10.1145%2f1111320.1111070&partnerID=40&md5=1abc4ec2d39e1efc032fc72ff1852692","Web applications typically interact with a back-end database to retrieve persistent data and then present the data to the user as dynamically generated output, such as HTML web pages. However, this interaction is commonly done through a low-level API by dynamically constructing query strings within a general-purpose programming language, such as Java. This low-level interaction is ad hoc because it does not take into account the structure of the output language. Accordingly, user inputs are treated as isolated lexical entities which, if not properly sanitized, can cause the web application to generate unintended output. This is called a command injection attack, which poses a serious threat to web application security. This paper presents the first formal definition of command injection attacks in the context of web applications, and gives a sound and complete algorithm for preventing them based on context-free grammars and compiler parsing techniques. Our key observation is that, for an attack to succeed, the input that gets propagated into the database query or the output document must change the intended syntactic structure of the query or document. Our definition and algorithm are general and apply to many forms of command injection attacks. We validate our approach with SQLCHECK, an implementation for the setting of SQL command injection attacks. We evaluated SQLCHECK on real-world web applications with systematically compiled real-world attack data as input. SQLCHECK produced no false positives or false negatives, incurred low run-time overhead, and applied straightforwardly to web applications written in different languages. Copyright © 2006 ACM.","Command injection attacks; Grammars; Parsing; Runtime verification; Web applications","Command injection attacks; Parsing; Runtime verification; Web applications; Computer programming languages; Context free grammars; Database systems; Information retrieval; Query languages; Websites; World Wide Web; Computer crime",2-s2.0-33745216482
"Kong J., Zhang K., Zeng X.","Spatial graph grammars for graphical user interfaces",2006,"ACM Transactions on Computer-Human Interaction",42,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749637218&doi=10.1145%2f1165734.1165739&partnerID=40&md5=b73a9847f1033126c365090092e74248","In a graphical user interface, physical layout and abstract structure are two important aspects of a graph. This article proposes a new graph grammar formalism which integrates both the spatial and structural specification mechanisms in a single framework. This formalism is equipped with a parser that performs in polynomial time with an improved parsing complexity over its nonspatial predecessor, that is, the Reserved Graph Grammar. With the extended expressive power, the formalism is suitable for many user interface applications. The article presents its application in adaptive Web design and presentation. © 2006 ACM.","Diagram parsing; Graph grammars; Spatial specification; Visual languages; Visual programming","Diagram parsing; Graph grammars; Spatial specification; Visual languages; Visual programming; Computational complexity; Computer graphics; Graphical user interfaces; Logic design; World Wide Web; Computational grammars",2-s2.0-33749637218
"Russomanno D.J.","A plausible inference prototype for the Semantic Web",2006,"Journal of Intelligent Information Systems",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747036465&doi=10.1007%2fs10844-006-0369-1&partnerID=40&md5=b9068da474b53ada1684481394c85b12","This paper presents a prototype that is capable of drawing plausible inferences from Resource Description Framework (RDF) assertions that are constituents of a distributed, Semantic Web knowledge system. The approach taken to build the prototype can be viewed as the extension and adaptation of a classical approach to plausible inference to exploit the evolving infrastructure being developed to represent declarative knowledge on the Semantic Web. The approach includes a knowledge representation formalism that supports meta properties, which define precise semantics, enabling subsequent plausible inferences via extended composition of RDF properties. Most research and development in the context of the Semantic Web has been devoted to representational infrastructure and accompanying query and logical deduction formalisms to evolve the Web from a document repository to a set of distributed knowledge bases. The work presented in this paper provides a functioning Semantic Web application in which the generation of new inferences is not contained within the deductive closure of the knowledge and data expressed by a collection of information sources represented using RDF. Moreover, the paper provides a concrete example of an RDF schema and a working system built around it which demonstrates one potential use of meta data on the Web. © Springer Science + Business Media, LLC 2006.","Composition of relations; Plausible inference; RDF; Semantic Web","Distributed computer systems; Inference engines; Information analysis; Knowledge based systems; Metadata; Query languages; Semantics; Software prototyping; Composition of relations; Plausible inference; Resource Description Framework (RDF); Semantic Web; World Wide Web",2-s2.0-33747036465
"Chang T.-K., Hwang G.-H.","To secure XML documents with the extension function of XSLT",2006,"Software - Practice and Experience",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646233224&doi=10.1002%2fspe.710&partnerID=40&md5=0512adc298d67f0785e052c5fc1a819a","XSLT is a very popular and flexible language for transforming XML documents which provides a powerful implementation of a tree-oriented transformation language for transmuting instances of XML using a single vocabulary into a desired output. In this paper, we propose a processing model that enables the XSLT processor to encrypt and decrypt XML documents. The details of the implementation are presented. Our model supports a more general encryption scope than previous models. The implementation and experimental results demonstrate the practicality of the proposed model. Copyright © 2006 John Wiley & Sons, Ltd.","DSL; Security; XML; XSLT","Computer programming languages; Cryptography; Formal languages; Information technology; Mathematical models; Mathematical transformations; Security of data; DSL; Processing models; Single vocabulary; XSLT; XML",2-s2.0-33646233224
"Groppe S., Böttcher S., Birkenheuer G., Höing A.","Reformulating XPath queries and XSLT queries on XSLT views",2006,"Data and Knowledge Engineering",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-31844456859&doi=10.1016%2fj.datak.2005.04.002&partnerID=40&md5=a922c56223a5bc2b011108f65ad890d2","Applications using XML for data representation very often use different XML formats and thus require the transformation of XML data. The common approach transforms entire XML documents from one format into another, e.g. by using an XSLT stylesheet. Different from this approach, we use an XSLT stylesheet in order to transform a given XPath query or a given XSLT query so that we retrieve and transform only that part of the XML document, which is sufficient to answer the given query. Among other things, our approach avoids problems of replication, saves processing time, and in distributed scenarios, transportation costs. © 2005 Elsevier B.V. All rights reserved.","Query optimization; Query reformulation; Query transformation; Semi-structured data; XML; XPath; XSLT","Query optimization; Query reformulation; Semi-structred data; Costs; Mathematical transformations; Optimization; Problem solving; Transportation charges; XML; Query languages",2-s2.0-31844456859
"Lam C.-W., James J.T., McCluskey R., Arepalli S., Hunter R.L.","A review of carbon nanotube toxicity and assessment of potential occupational and environmental health risks",2006,"Critical Reviews in Toxicology",773,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645744010&doi=10.1080%2f10408440600570233&partnerID=40&md5=f24d102715e7d1c68fe89256813aae9f","Nanotechnology has emerged at the forefront of science research and technology development. Carbon nanotubes (CNTs) are major building blocks of this new technology. They possess unique electrical, mechanical, and thermal properties, with potential wide applications in the electronics, computer, aerospace, and other industries. CNTs exist in two forms, single-wall (SWCNTs) and multi-wall (MWCNTs). They are manufactured predominately by electrical arc discharge, laser ablation and chemical vapor deposition processes; these processes involve thermally stripping carbon atoms off from carbon-bearing compounds. SWCNT formation requires catalytic metals. There has been a great concern that if CNTs, which are very light, enter the working environment as suspended particulate matter (PM) of respirable sizes, they could pose an occupational inhalation exposure hazard. Very recently, MWCNTs and other carbonaceous nanoparticles in fine (<2.5 μm) PM aggregates have been found in combustion streams of methane, propane, and natural-gas flames of typical stoves; indoor and outdoor fine PM samples were reported to contain significant fractions of MWCNTs. Here we review several rodent studies in which test dusts were administered intratracheally or intrapharyngeally to assess the pulmonary toxicity of manufactured CNTs, and a few in vitro studies to assess biomarkers of toxicity released in CNT-treated skin cell cultures. The results of the rodent studies collectively showed that regardless of the process by which CNTs were synthesized and the types and amounts of metals they contained, CNTs were capable of producing inflammation, epithelioid granulomas (microscopic nodules), fibrosis, and biochemical/toxicological changes in the lungs. Comparative toxicity studies in which mice were given equal weights of test materials showed that SWCNTs were more toxic than quartz, which is considered a serious occupational health hazard if it is chronically inhaled; ultrafine carbon black was shown to produce minimal lung responses. The differences in opinions of the investigators about the potential hazards of exposures to CNTs are discussed here. Presented here are also the possible mechanisms of CNT pathogenesis in the lung and the impact of residual metals and other impurities on the toxicological manifestations. The toxicological hazard assessment of potential human exposures to airborne CNTs and occupational exposure limits for these novel compounds are discussed in detail. Environmental fine PM is known to form mainly from combustion of fuels, and has been reported to be a major contributor to the induction of cardiopulmonary diseases by pollutants. Given that manufactured SWCNTs and MWCNTs were found to elicit pathological changes in the lungs, and SWCNTs (administered to the lungs of mice) were further shown to produce respiratory function impairments, retard bacterial clearance after bacterial inoculation, damage the mitochondrial DNA in aorta, increase the percent of aortic plaque, and induce atherosclerotic lesions in the brachiocephalic artery of the heart, it is speculated that exposure to combustion-generated MWCNTs in fine PM may play a significant role in air pollution-related cardiopulmonary diseases. Therefore, CNTs from manufactured and combustion sources in the environment could have adverse effects on human health. Copyright © Taylor and Francis Group, LLC.","Cardiopulmonary Diseases; Fibrosis; Fullerenes; Granulomas; Intratracheal Instillation; Multi-Wall Carbon Nanotubes; Nanomaterials; Nanotechnology; Natural Gas Combustion; Particulate Matter; PM2.5; Pulmonary Toxicity; Risk Assessment","biological marker; carbon nanotube; fuel; multiwall carbon nanotube; silicon dioxide; single wall carbon nanotube; unclassified drug; airborne particle; aorta atherosclerosis; atherosclerosis; brachiocephalic trunk; cell culture; combustion; comparative toxicology; dose response; electric potential; environmental health; epithelioid cell; exposure; fibrosis; granuloma; guinea pig; health hazard; human; inflammation; inoculation; lung toxicity; mitochondrial DNA disorder; nonhuman; occupational hazard; oxidation reduction potential; particle size; particulate matter; pathogenesis; pollutant; public health; respiratory tract disease; review; risk assessment; skin cell; synthesis; transmission electron microscopy; Air Pollutants; Animals; Environmental Health; Granuloma, Respiratory Tract; Heart; Humans; Inhalation Exposure; Lung; Nanotubes, Carbon; Occupational Exposure; Particle Size; Pneumonia; Pulmonary Fibrosis; Risk Assessment; Skin; Bacteria (microorganisms); Cavia porcellus; Rodentia",2-s2.0-33645744010
"Catanho M., Mascarenhas D., Degrave W., De Miranda A.B.","BioParser: A tool for processing of sequence similarity analysis reports",2006,"Applied Bioinformatics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645033905&doi=10.2165%2f00822942-200605010-00007&partnerID=40&md5=3d9ddb7c01e6a13bea8813c9d8bdb698","The widely used programs BLAST (in this article, 'BLAST' includes both the National Center for Biotechnology Information [NCBI] BLAST® and the Washington University version WU BLAST) and FASTA for similarity searches in nucleotide and protein databases usually result in copious output. However, when large query sets are used, human inspection rapidly becomes impractical. BioParser is a Perl program for parsing BLAST and FASTA reports. Making extensive use of the BioPerl toolkit, the program filters, stores and returns components of these reports in either ASCII or HTML format. BioParser is also capable of automatically feeding a local MySQL® database with the parsed information, allowing subsequent filtering of hits and/or alignments with specific attributes. For this reason, BioParser is a valuable tool for large-scale similarity analyses by improving the access to the information present in BLAST or FASTA reports, facilitating extraction of useful information of large sets of sequence alignments, and allowing for easy handling and processing of the data. Availability: BioParser is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 2.0 license terms (http://creativecommons.org/licenses/by-nc-nd/2.0/) and is available upon request. Additional information can be found at the BioParser website (http://www.dbbm.fiocruz.br/BioParser.html). Contact: Bioinformatics Team at Fiocruz (bioinfoteam@fiocruz.br). © 2006 Adis Data Information BV. All rights reserved.",,"access to information; amino acid sequence; article; computer interface; computer program; data base; information processing; nucleotide sequence; sequence alignment; sequence homology; Amino Acid Sequence; Computer Graphics; Database Management Systems; Databases, Protein; Information Storage and Retrieval; Molecular Sequence Data; Sequence Alignment; Sequence Analysis, Protein; Software; User-Computer Interface",2-s2.0-33645033905
"Moukdad H.","Stemming and root-based approaches to the retrieval of Arabic documents on the Web",2006,"Webology",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646592500&partnerID=40&md5=8f740ca61f6eac73456aa9724f21522a","Using information retrieval systems to gain access to documents in languages other than English is becoming an increasingly significant problem. Rules, theories, algorithms, and retrieval methods designed and developed for English and other morphological1y similar languages may or may not apply in the linguistic environments of other languages. The problem is particularly acute in languages that differ radically from English on account of morphological rules. This paper compares the effects stemming and root retrieval on information retrieval in Arabic through an exploratory study of the handling of Arabic words by an English-language search engine (ELSE). Search experiments, using 2000 Arabic documents and 40 Arabic search terms (nouns), were conducted in a Web search engine developed for English (Alta Vista) and in an Arabic search engine (al-Idrisi) to compare the performances of stemming and root retrieval and to investigate the possibility of adapting Alta Vista for use with Arabic text. The results of the experiments show that more effective retrieval can be accomplished through stemming, and that it is possible to adapt an ELSE for use with Arabic without the need to develop root-retrieval features. Copyright © 2006, Haidar Moukdad.","Arabic language; Search engines; World Wide Web",,2-s2.0-33646592500
"Higashinaka R., Sudoh K., Nakano M.","Incorporating discourse features into confidence scoring of intention recognition results in spoken dialogue systems",2006,"Speech Communication",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-32144463810&doi=10.1016%2fj.specom.2005.06.011&partnerID=40&md5=0c4f0b82a38a73d86e3981feda8be9fe","This paper proposes a method for the confidence scoring of intention recognition results in spoken dialogue systems. To achieve tasks, a spoken dialogue system has to recognize user intentions. However, because of speech recognition errors and ambiguity in user utterances, it sometimes has difficulty recognizing them correctly. Confidence scoring allows errors to be detected in intention recognition results and has proved useful for dialogue management. Conventional methods use the features obtained from the speech recognition/understanding results for single utterances for confidence scoring. However, this may be insufficient since the intention recognition result is a result of discourse processing. We propose incorporating discourse features for a more accurate confidence scoring of intention recognition results. Experimental results show that incorporating discourse features significantly improves the confidence scoring. © 2005 Elsevier B.V. All rights reserved.","Confidence scoring; Discourse understanding; Speech understanding; Spoken dialogue systems","Speech recognition; Confidence scoring; Speech understanding; Spoken dialog system; Speech communication",2-s2.0-32144463810
"Grootjen F.A., Van Der Weide Th.P.","Conceptual query expansion",2006,"Data and Knowledge Engineering",47,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844592269&doi=10.1016%2fj.datak.2005.03.006&partnerID=40&md5=0299905c5c803a9c24fec660ddc15d34","This article presents a new, hybrid approach that projects an initial query result onto global information, yielding a local conceptual overview. Concepts found this way are candidates for query refinement. We show that the resulting conceptual structure after a typical short query of 2 terms, contains refinements that perform just as well as a most accurate query formulation. Subsequently we illustrate that query by navigation is an effective mechanism which in most cases finds the optimal concept in a small number of steps. When an optimal concept is not found, the navigation process still finds an acceptable sub-optimum. © 2005 Elsevier B.V. All rights reserved.","Expansion; Feedback; Query; Relevance","Query; Relevance; Expansion; Feedback; Information analysis; Knowledge engineering; Query languages",2-s2.0-27844592269
"Wang W.-Y., Cheng G.-W., Shiu F.-S.","Design and implementation of an XML e-mail client for medical referrals",2006,"Journal on Information Technology in Healthcare",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33144470218&partnerID=40&md5=23c7b2cee900acfbd21be4042bf996a9","Objective: To develop an XML e-mail client (XEC) that can be used by ph ysicians to send, receive and process electronic referrals. Design: Developmental process Setting: The system can be used with any operating system and any database platform. Methods: XML documents are prepared from data stored in a standard medical database when a physician queries the database using SQL. The generated XML documents are sent as an e-mail attachment using standard e-mail protocols. The receiving physician is able to read the attachment as an XML document and can also convert it into data that can be stored in a relational database. To assist the process of converting data into XML documents and to confirm the validity of XML documents received as e-mail attachments. a document type definition parser is used. To prevent the generation of duplicate XML documents when a SQL is run for the same patient a rule-based optimizer is employed. Results: We have demonstrated that the system can be use to generate XML documents from any database, that the documents can be sent as e-mail attachments using standard email protocols, and that the receiving physician can read these XML documents and convert them into data to be stored in a relational database. Conclusion: The XEC system can be used by physicians to send, receive and process electronic medical referrals conveniently and efficiently. As the system integrates e-mail referrals with data stored in databases it can aid physicians in patient management. © The Journal on Information Technology in Healthcare 2006.",,"article; computer system; data base; e-mail; electronic medical record; human; medical decision making; patient care; patient referral; physician; validation process",2-s2.0-33144470218
"Wang C.-B., Chen Y.-J., Chen Y.-M., Chu H.-C.","Development of technology for customer requirement-based reference design retrieval",2006,"Robotics and Computer-Integrated Manufacturing",7,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144485386&doi=10.1016%2fj.rcim.2005.02.002&partnerID=40&md5=c79651482ee7ff33c5c8c2a36634268c","Engineering design is a knowledge-intensive process, and includes conceptual design, detailed design, engineering analysis, assembly design, process design, and performance evaluation. Each task involves various aspects of knowledge and experience. Whether this knowledge and experience can be effectively shared is key to increasing product development capability and quality, and also to reducing the duration and cost of the development cycle. Therefore, offering engineering designers various query methods for retrieving engineering knowledge is one of the most important tasks in engineering knowledge management. The study develops a technology for customer requirement-based reference design retrieval to provide engineering designers with easy access to relevant design and associated knowledge. The tasks involved in this research include (i) designing a customer requirement-based reference design retrieval process, (ii) developing techniques related to the technology for customer requirement-based reference design retrieval, and (iii) implementing a customer requirement-based reference design retrieval mechanism. The retrieval process comprises the steps of customer requirement-based query, case searching and matching, and case ranking. The technology involves (1) a structured query model for customer requirement, (2) an index structure for historical design cases, (3) customer requirement-based case searching and matching mechanisms, (4) a customer requirement-based case ranking mechanism, and (5) a case-based representation of designed entities. © 2005 Elsevier Ltd. All rights reserved.","Engineering design; Knowledge management; Knowledge retrieval","Customer satisfaction; Knowledge engineering; Product development; Quality assurance; Case ranking; Engineering design; Knowledge-intensive process; Product design",2-s2.0-27144485386
"Durand P., Labarre L., Meil A., Divol J.-L., Vandenbrouck Y., Viari A., Wojcik J.","GenoLink: A graph-based querying and browsing system for investigating the function of genes and proteins",2006,"BMC Bioinformatics",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644631181&doi=10.1186%2f1471-2105-7-21&partnerID=40&md5=724dedb1bd567358d6d0f914e3f347af","Background: A large variety of biological data can be represented by graphs. These graphs can be constructed from heterogeneous data coming from genomic and post-genomic technologies, but there is still need for tools aiming at exploring and analysing such graphs. This paper describes GenoLink, a software platform for the graphical querying and exploration of graphs. Results: GenoLink provides a generic framework for representing and querying data graphs. This framework provides a graph data structure, a graph query engine, allowing to retrieve sub-graphs from the entire data graph, and several graphical interfaces to express such queries and to further explore their results. A query consists in a graph pattern with constraints attached to the vertices and edges. A query result is the set of all sub-graphs of the entire data graph that are isomorphic to the pattern and satisfy the constraints. The graph data structure does not rely upon any particular data model but can dynamically accommodate for any user-supplied data model. However, for genomic and post-genomic applications, we provide a default data model and several parsers for the most popular data sources. GenoLink does not require any programming skill since all operations on graphs and the analysis of the results can be carried out graphically through several dedicated graphical interfaces. Conclusion: GenoLink is a generic and interactive tool allowing biologists to graphically explore various sources of information. GenoLink is distributed either as a standalone application or as a component of the Genostar/logma platform. Both distributions are free for academic research and teaching purposes and can be requested at academy@genostar.com. A commercial licence form can be obtained for profit company at info@genostar.com. See also http://www.genostar.org. © 2006 Durand et al; licensee BioMed Central Ltd.",,"Academic research; Generic frameworks; Graphical interface; Heterogeneous data; Programming skills; Software platforms; Sources of informations; Standalone applications; Genes; Graphic methods; Models; Search engines; Tools; Graph theory; article; computer graphics; computer interface; computer model; computer program; controlled study; gene function; information processing; information retrieval; information system; Internet; molecular biology; protein function; skill; structural genomics; structure analysis; computer graphics; computer interface; computer simulation; data base; gene; gene expression; genetic database; physiology; signal transduction; protein; Computer Graphics; Computer Simulation; Database Management Systems; Databases, Genetic; Gene Expression; Genes; Proteins; Signal Transduction; User-Computer Interface",2-s2.0-33644631181
"Minock M.","Natural language updates to databases through dialogue",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746667192&doi=10.1007%2f11765448_19&partnerID=40&md5=87782af02d40a0cd8adb117053bdaf97","This paper reopens the long dormant topic of natural language updates to databases. A protocol to handle database updates of the IDM (Insert-Delete-Modify) class is proposed and implemented. This protocol exploits modern relational update facilities and constraints and structures update dialogues using DAMSL dialogue acts. The protocol may be used with any natural language parser that maps to relational queries. © Springer-Verlag Berlin Heidelberg 2006.",,"Computer science; Constraint theory; Data processing; Data structures; Database systems; Network protocols; Query languages; Dialogue; Insert-Delete-Modify (IDM); Natural language; Parser; Relational queries; Linguistics",2-s2.0-33746667192
"Kiss A., Anh V.L.","Efficient processing SAPE queries using the dynamic labelling structural indexes",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750074442&partnerID=40&md5=6c8c2a8c6a6244f28a78b18fb6ec1367","There are a variety of structural indexes which have been proposed to speed up path expression queries over XML data. They usually work by partitioning nodes in the data graph into equivalence classes and storing equivalence classes as index nodes. In most of current structural indexes, the nodes in the same partition have the same label. They are not flexible with queries containing the wild- or alternation cards, and sometimes their size is bigger than the necessity. In this paper, we introduce the dynamic labelling structural indexes. These structural indexes only support a set of frequently used simple alternation path expressions (SAPE for short), where expressions may contain wild- or alternation cards. The labels of data nodes in the same partition may be different. The dynamic labelling not only decreases the size of the structural index, but also supports SAPE's better. Every static labelling structural index can be improved by using dynamic labelling. Because of the limitation, in this paper we just study the DL-1-index improved from the 1-index, and the DL-A *(k)-index improved from the A(k)-index. The construction and refinement of these indexes are based on our results from the properties of partitions and the split operation. Our experiments show that the size of the improved dynamic labelling structural indexes is smaller and the query processing on these indexes is more efficient comparing to the naive ones. © Springer-Verlag Berlin Heidelberg 2006.",,"Automatic indexing; Data reduction; Data structures; Dynamic programming; Equivalence classes; Graph theory; XML; Data graph; Path expression queries; Query processing; Structural indexes; Query languages",2-s2.0-33750074442
"Psaila G.","Loosely coupling Java algorithms and XML parsers: A Performance-Oriented study",2006,"ICDEW 2006 - Proceedings of the 22nd International Conference on Data Engineering Workshops",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990955124&doi=10.1109%2fICDEW.2006.73&partnerID=40&md5=c1321fd92de0fe0c37cb4dc8ce09d63d","The adoption of XML to represent any kind of data and documents, even complex and huge, is becoming a matter of fact. However, interfacing algorithms and applications with XML Parsers requires to adapt algorithms and applications: Event-based SAX Parsers need algorithms that react to events generated by the parser. But parsing/loading XML documents provides poor performance (if compared to reading flat files): Therefore, several researches are trying to address this problem by improving the parsing phase, e.g., by adopting condensed or binary representations of XML documents. This paper deals with the other side of the coin, i.e., the problem of coupling algorithms with XML Parsers, in a way that does not require to change the active (polling-based) nature of many algorithms and provides acceptable performance during execution; this problem becomes even more important when we consider Java algorithms, that usually are less efficient than C or C++ algorithms. This paper presents a study about the problem of loosely coupling Java algorithms with XML Parsers. The coupling is loose because the algorithm should be unaware of the particular interface provided by parsers. We consider several coupling techniques, and we compare them by analyzing their performance. The evaluation leads us to identify the coupling techniques that perform better, depending on the specific algorithm's needs and application scenario. © 2006 IEEE.",,"Technical presentations; Acceptable performance; Application scenario; Binary representations; Coupling algorithms; Coupling techniques; Loosely coupling; Performance-oriented; Poor performance; XML",2-s2.0-84990955124
"Karlgren J., Sahlgren M., Cöster R.","Weighting query terms based on distributional statistics",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749599701&partnerID=40&md5=792aeef30375751f81b6aa9754dc3035","This year, the SICS team has concentrated on query processing and on the internal topical structure of the query, specifically compound translation. Compound translation is non-trivial due to dependencies between compound elements. This year, we have investigated topical dependencies between query terms: if a query term happens to be non-topical or noise, it should be discarded or given a low weight when ranking retrieved documents; if a query term shows high topicality its weight should be boosted. The two experiments described here are based on the analysis of the distributional character of query terms: one using similarity of occurrence context between query terms globally across the entire collection; the other using the likelihood of individual terms to appear topically in individual texts. Both - complementary - boosting schemes tested delivered improved results. © Springer-Verlag Berlin Heidelberg 2006.",,"Data reduction; Data structures; Distributed computer systems; Information retrieval systems; Statistics; Distributional statistics; Topical dependencies; Topical structure; Query languages",2-s2.0-33749599701
"Lonsdale D., Tustison C., Parker C., Embley D.W.","Formulating queries for assessing clinical trial eligibility",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746698345&doi=10.1007%2f11765448_8&partnerID=40&md5=0ba7725d0007aa291c17daed4a4ad4c1","His paper introduces a system that processes clinical trials using a combination of natural language processing and database techniques. We process web-based clinical trial recruitment pages to extract semantic information reflecting eligibility criteria for potential participants. Prom this information we then formulate a query that can match criteria against medical data in patient records. The resulting system reflects a tight coupling of web-based information extraction, natural language processing, medical informatic approaches to clinical knowledge representation, and large-scale database technologies. We present an evaluation of the system and future directions for further system development. © Springer-Verlag Berlin Heidelberg 2006.",,"Database systems; Hospital data processing; Information retrieval; Knowledge acquisition; Linguistics; Medical applications; Semantics; World Wide Web; Clinical trials; Medical data; Natural language processing; Semantic information; Web-based information extraction; Query languages",2-s2.0-33746698345
"Sutcliffe R.F.E., White K., Slattery D., Gabbay I., Mulcahy M.","Cross-language French-English question answering using the DLT system at CLEF 2006",2006,"CEUR Workshop Proceedings",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922024067&partnerID=40&md5=017f614e7fe27ea0c32b45259f0d050e","The basic architecture of our factoid system is standard in nature and comprises query type identification, query analysis and translation, retrieval query formulation, document retrieval, text file parsing, named entity recognition and answer entity selection. Factoid classification into 69 query types is carried out using keywords. Associated with each type is a set of one or more Named Entities. Xelda is used to tag the French query for partof-speech and then shallow parsing is carried out over these in order to recognise thirteen different kinds of significant phrase. These were determined after a study of the constructions used in French queries together with their English counterparts. Our observations were that (1) Proper names usually only start with a capital letter with subsequent words un-capitalised, unlike English; (2) Adjective-Noun combinations either capitalised or not can have the status of compounds in French and hence need special treatment; (3) Certain noun-preposition-noun phrases are also of significance. The phrases are then translated into English by the engine WorldLingo and using the Grand Dictionnaire Terminologique, the results being combined. Each phrase has a weight assigned to it by the parser. A Boolean retrieval query is formulated consisting of an AND of all phrases in increasing order of weight. The corpus is indexed by sentence using Lucene. The Boolean query is submitted to the engine and if unsuccessful is re-submitted with the first (least significant) term removed. The process continues until the search succeeds. The documents (i.e. sentences) are retrieved and the NEs corresponding to the identified query type are marked. Significant terms from the query are also marked. Each NE is scored based on its distance from query terms and their individual weights. The answer returned is the highest-scoring NE. Temporarily Restricted Factoids are treated in the same way as Factoids. Definition questions are classified in three ways: organisation, person or unknown. This year Factoids had to be recognised automatically by an extension of the classifier. An IR query is formulated using the main term in the original question plus a disjunction of phrases depending on the identified type. All matching sentences are returned complete. Results this year were as follows: 32/150 (21%) of Factoids were R, 14/150 (9%) were X, 4/40 (10%) of Definitions were R and 2 List results were R (P@N = 0.2). Our ranking in Factoids relative to all thirteen runs was Fourth. However, scoring all systems over R&X together and including Definitions, our ranking would be Second Equal because we had more X scores than any other system. Last year our score on Factoids was 26/150 (17%) but the difference is probably the easier queries this year.","Question answering","Character recognition; Context free grammars; Digital libraries; Engines; Natural language processing systems; Syntactics; Definition questions; Document Retrieval; Entity selections; Named entity recognition; Question Answering; Retrieval query; Shallow parsing; Special treatments; Query processing",2-s2.0-84922024067
"Leveling J., Veiel D.","University of Hagen at GeoCLEF 2006: Experiments with metonymy recognition in documents",2006,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922023892&partnerID=40&md5=ae269570589088b9945df312b0a26af2","This paper describes the participation of the IICS group at the GeoCLEF task of the CLEF campaign 2006. We describe different retrieval experiments using a separate index for location names and identifying and indexing of metonymic location names differently. The setup of our GIR system is a modified variant of the setup for GeoCLEF 2005. We apply a classifier for the identification of metonymic location names for preprocessing the documents. This classifier is based on shallow features only and was trained on manually annotated data from the German CoNLL-2003 Shared Task corpus for Language-Independent Named Entity Recognition and from a subset of the GeoCLEF newspaper corpus. After preprocessing, documents contain additional information for location names that are to be indexed separately, i.e. LOC (all location names identified), LOCLIT (location names in their literal sense), and LOCMET (location names in their metonymic sense). To obtain an IR query from the topic title, description, and narrative, we employ two methods. In the first method, a semantic parser analyzes the query text and the resulting semantic net is transformed into database query. The second method uses a Boolean combination of a bag-of-words (consisting of topical search terms) with location names. The results of our experiments can be summarized as follows: excluding metonymic senses of location names improves mean average precision (MAP) for most of our experiments. For experiments in which this was not the case, a more detailed analysis showed that for some topics the precision increased. Our experiments show that the additional use of topic narratives decreases MAP. For almost all experiments with the topic narrative, lower values for MAP and for the number of relevant and retrieved documents were observed. However, query expansion and the use of separate indexes improves the performance of our GIR application.","Geographic information retrieval; Indexing location names; Metonymy recognition","Indexing (of information); Natural language processing systems; Query processing; Semantics; Boolean combinations; Database queries; Geographic information retrieval; Language independents; Metonymy recognition; Named entity recognition; Newspaper corpus; Retrieved documents; Digital libraries",2-s2.0-84922023892
[No author name available],"Mathematical Knowledge Management - 5th International Conference, MKM 2006, Proceedings",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941165757&partnerID=40&md5=47b354851a2c97d25c4b6e6f0bfbf3c5","The proceedings contain 22 papers. The topics discussed include: a dynamic poincaré principle; a proof-theoretic approach to tactics; proof transformation by CERES; verifying and invalidating textbook proofs using Scunak; capturing abstract matrices from paper; towards a parser for mathematical formula recognition; stochastic modeling of scientific terms distribution in publications; capturing the content of physics: systems, observables, and experiments; communities of practice in MKM: an extensional model; from notion to semantics: there and back again; managing informal mathematical knowledge: techniques from informal logic; from untyped to polymorphically typed objects in mathematical Web services; managing automatically formed mathematical theories; information retrieval and rendering with MML query; and integrating dynamic geometry software, deduction systems and theorem repositories.",,"Computational geometry; Computer software; Mathematical models; Query languages; Random processes; Semantics; Telecommunication services; Informal logic; Mathematical knowledge; Mathematical Web services; MML query; Knowledge acquisition",2-s2.0-84941165757
"Staworko S., Chomicki J.","Validity-sensitive querying of XML databases",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",28,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845248894&partnerID=40&md5=ad572371b4e085a129dfd67972364c39","We consider the problem of querying XML documents which are not valid with respect to given DTDs. We propose a framework for measuring the invalidity of XML documents and compactly representing minimal repairing scenarios. Furthermore, we present a validity-sensitive method of querying XML documents, which extracts more information from invalid XML documents than does the standard query evaluation. Finally, we provide experimental results which validate our approach. © Springer-Verlag Berlin Heidelberg 2006.",,"Database systems; Information analysis; Information technology; Optimization; Program documentation; Query languages; Information extraction; Standard query evaluation; XML",2-s2.0-33845248894
"Ruiz A.T., Puşcaşu G., Monteagudo L.M., Beviá R.I., Boró E.S.","University of Alicante at WiQA 2006",2006,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032257&partnerID=40&md5=0e9d9adadc85c9303bb4d8a7d58daede","This paper presents the participation of University of Alicante at the WiQA pilot task organized as part of the CLEF 2006 campaign. For a given set of topics, this task presupposes the discovery of important novel information distributed across different Wikipedia entries. The approach we adopted for solving this task uses Information Retrieval, query expansion by feedback, relevance and novelty re-ranking, as well as temporal ordering. Our system has participated both in the Spanish and English monolingual tasks. For each of the two participations the results are promising because, by employing a language independent approach, we obtain scores above the average. Moreover, in the case of Spanish, our result is very close to the best achieved score. Apart from introducing our system, the present paper also provides an in-depth result analysis, and proposes future lines of research, as well as follow-up experiments.","Information retrieval","Data mining; Information retrieval; Language independents; Monolingual tasks; Novel information; Pilot tasks; Query expansion; Re-ranking; Result analysis; Temporal order; Digital libraries",2-s2.0-84922032257
"Bernstein A., Kaufmann E.","GINO - A guided input natural language ontology editor",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",71,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845387691&partnerID=40&md5=9e1f0b025de023f6ee69ffb1d9bddc8b","The casual user is typically overwhelmed by the formal logic of the Semantic Web. The gap between the end user and the logic-based scaffolding has to be bridged if the Semantic Web's capabilities are to be utilized by the general public. This paper proposes that controlled natural languages offer one way to bridge the gap. We introduce GINO, a guided input natural language ontology editor that allows users to edit and query ontologies in a language akin to English. It uses a small static grammar, which it dynamically extends with elements from the loaded ontologies. The usability evaluation shows that GINO is well-suited for novice users when editing ontologies. We believe that the use of guided entry overcomes the habitability problem, which adversely affects most natural language systems. Additionally, the approach's dynamic grammar generation allows for easy adaptation to new ontologies. © Springer-Verlag Berlin Heidelberg 2006.",,"Formal languages; Natural language processing systems; Query languages; Semantics; World Wide Web; Grammar generation; Natural languages; Ontologies; Semantic Web; File editors",2-s2.0-33845387691
"Kotsakis E.","XML fuzzy ranking",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746255021&doi=10.1007%2f11766254_14&partnerID=40&md5=b3ffa9df976c5ba70299ca127ff9bee2","This paper proposes a method of ranking XML documents with respect to an Information Retrieval query by means of fuzzy logic. The proposed method allows imprecise queries to be evaluated against an XML document collection and it provides a model of ranking XML documents. In addition the proposed method enables sophisticated ranking of documents by employing proximity measures and the concept of editing (Levenshtein) distance between terms or XML paths. © Springer-Verlag Berlin Heidelberg 2006.",,"Database systems; Formal logic; Fuzzy sets; Information retrieval; Mathematical models; Fuzzy ranking; Proximity measures; XML documents; XML",2-s2.0-33746255021
"Lee S.G., Ng Y.C.","Hybrid case-based reasoning for on-line product fault diagnosis",2006,"International Journal of Advanced Manufacturing Technology",22,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-29344463446&doi=10.1007%2fs00170-004-2235-z&partnerID=40&md5=67bb6c5c1809bb1aff09d9c5bcaa2915","This paper presents a hybrid case-based reasoning system for on-line technical support of PC fault diagnosis. HyCase consists of a natural language (keyword) input and the graph-theoretic constraint-net. Natural language or keyword inputs are parsed and then generated into a constraint-net. The constraint-net is validated and its links rationalized and standardized to minimize ambiguity. Cases that partially match either the keywords or the constraint-net are ranked according to their matching scores based on four different preferences, and then retrieved and presented to the user. The case base, which was developed in Microsoft Access 2000, is updated by means of a keyword and constraint-net manager. HyCase was implemented on a PC with a Pentium III processor running at 500 MHz and 128 MB of SDRAM. Twenty typical queries from customers were tested on a collection of 174 cases based on different cut-off overall matching scores. The effectiveness of case retrieval was measured by the proportion of relevant cases retrieved from the case base (recall) and, of these, the proportion directly applicable to the problem at hand (precision). The accuracy of the natural language parser was ascertained to range between 62.5% and 87.7%, while a parsing accuracy of 60% is sufficient to ensure a reasonable recall and precision. A minimum overall matching score of about 0.5 ensures a precision of 0.60. The parsing time gets noticeably longer when there are more than 15 words. Except for minimum overall matching scores exceeding 0.1, the parsing time is largely independent of the minimum score. The merits and limitations of HyCase are also discussed.","Case based reasoning; Natural language parsing; PC fault diagnostics","Formal logic; Information retrieval; Natural language processing systems; Problem solving; Program processors; Standardization; Case based reasoning; Constraint-net; Natural language parsing; PC fault diagnostics; Computer aided software engineering",2-s2.0-29344463446
"Martínez-Fernández J.L., Villena J., García-Serrano A., Martínez P.","MIRACLE team report for ImageCLEF IR in CLEF 2006",2006,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922024060&partnerID=40&md5=a201717b5877cd579d9186dcf39cb931","The hypothesis which this paper tries to validate is that text based image retrieval could be improved by the use of semantic information, by means of an expansion algorithm and a module specifically designed to exclude common words and negated words from queries. The expansion algorithm applies specification marks to disambiguate words making use of WordNet [13]. An implementation of this algorithm has been developed for these experiments. On the other hand, the module in charge of removing common words and detecting negated words has also been specifically developed for this work. However, after an initial evaluation, none of these modules led to an improvement in the retrieval quality compared to the baseline experiment, which consists on the indexing of nouns present in image captions, without no further preprocessing.","Image retrieval; Linguistic engineering; Semantic expansion; Word sense disambiguation; WordNet","Digital libraries; Expansion; Natural language processing systems; Ontology; Search engines; Semantics; Image caption; Linguistic engineering; Retrieval quality; Semantic expansion; Semantic information; Text-based image retrievals; Word Sense Disambiguation; Wordnet; Image retrieval",2-s2.0-84922024060
"Zheleva E., Arslan A.N.","Fast motif search in protein sequence databases",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745926819&doi=10.1007%2f11753728_67&partnerID=40&md5=f4d10177c64c47bfb2092c0edec4fce7","Regular expression pattern matching is widely used in computational biology. Searching through a database of sequences for a motif (a simple regular expression), or its variations is an important interactive process which requires fast motif-matching algorithms. In this paper, we explore and evaluate various representations of the database of sequences using suffix trees for two types of query problems for a given regular expression: 1) Find the first match, and 2) Find all matches. Answering Problem 1 increases the level and effectiveness of interactive motif exploration. We propose a framework in which Problem 1 can be solved in a faster manner than existing solutions while not slowing down the solution of Problem 2. We apply several heuristics both at the level of suffix tree creation resulting in modified tree representations, and at the regular expression matching level in which we search subtrees in a given predefined order by simulating a deterministic finite automaton that we create from the given regular expression. The focus of our work is to develop a method for faster retrieval of PROSITE motif (a restricted regular expression) matches from a protein sequence database. We show empirically the effectiveness of our solution using several real protein data sets. © Springer-Verlag Berlin Heidelberg 2006.","Heuristic; Motif search; Preprocessing; PROSITE pattern; Regular expression matching; Suffix tree","Computer simulation; Finite automata; Information retrieval; Interactive computer systems; Pattern matching; Proteins; Search engines; Heuristic; Motif search; Motif-matching algorithms; Preprocessing; PROSITE pattern; Protein sequence databases; Regular expression matching; Suffix tree; Database systems",2-s2.0-33745926819
"Bos J., Nissim M.","Cross-lingual question answering by answer translation",2006,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922032113&partnerID=40&md5=f0ae91939874a77eb64f9efc16e3f7db","We approach cross-lingual question answering by using a mono-lingual QA system for the source language and by translating resulting answers into the target language. As far as we are aware, this is the first cross-lingual QA system in the history of CLEF that uses this method-all other cross-lingual QA systems known to us use translation of the question or query instead. We demonstrate the feasibility of this approach by using a mono-lingual QA system for English, and translating answers and finding appropriate documents in Italian and Dutch. For factoid and definition questions, we achieve overall accuracy scores ranging from 13% (EN→NL) to 17% (EN→IT) and lenient accuracy figures from 19% (EN→NL) to 25% (EN→IT). The advantage of this strategy to cross-lingual QA is that translation of answers is easier than translating questions-the disadvantage is that answers might be missing from the source corpus and additional effort is required for finding supporting documents of the target language.","Machine translation; Natural language processing; Question answering","Computational linguistics; Digital libraries; Natural language processing systems; Cross-lingual question answering; Definition questions; Machine translations; NAtural language processing; Overall accuracies; Question Answering; Source language; Target language; Translation (languages)",2-s2.0-84922032113
"Hovy E., Chalupsky H., Hobbs J., Lin C.-Y., Pantel P., Philpot A., Mulkar R., Pennacchiotti M.","Learning by reading: An experiment in text analysis",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750229203&partnerID=40&md5=aea11f9ca39d66e13ca6f2aecc68e134","It has long been a dream to build computer systems that learn automatically by reading text. This dream is generally considered infeasible, but some surprising developments in the US over the past three years have led to the funding of several short-term investigations into whether and how much the best current practices in Natural Language Processing and Knowledge Representation and Reasoning, when combined, actually enable this dream. This paper very briefly describes one of these efforts, the Learning by Reading project at ISI, which has converted a high school textbook of Chemistry into very shallow logical form and is investigating which semantic features can plausibly be added to support the kinds of inference required for answering standard high school text questions. © Springer-Verlag Berlin Heidelberg 2006.",,"Computer systems; Formal logic; Knowledge representation; Logic programming; Semantics; Text processing; Natural Language Processing; Semantic features; Text analysis; Learning systems",2-s2.0-33750229203
"Spencer B.S.","Incorporating the sense of smell into patient and haptic surgical simulators",2006,"IEEE Transactions on Information Technology in Biomedicine",20,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-30744464324&doi=10.1109%2fTITB.2005.856851&partnerID=40&md5=00bf48e7323676dc98c74aa1004badfe","It is widely recognized that the sense of smell plays an important role in the field of medicine. The sense of smell not only assists the physician in the diagnosis of certain disorders, but it also plays a surgical role as well. Historically, learning this skill was mostly contingent upon some level of clinical exposure to medically related odors. The advent of computerized scent production devices could change this. This article proposes a surgical simulation model that incorporates olfactory technologies into existing patient and haptic surgical simulators. If incorporated into virtual educational settings such as these, computerized scent production devices could be used not only as a novel way to enhance the virtual experience, but also as a way for medical students to begin to recognize the important role that the sense of smell plays during both diagnosis and surgery. © 2006 IEEE.","Olfaction; Simulation; Telemedicine; Virtual reality","Computer simulation; Diagnosis; Haptic interfaces; Sensory perception; Surgery; Virtual reality; Haptic surgical simulators; Olfaction; Surgical simulation model; Telemedicine; biological model; computer interface; computer simulation; education; feedback system; instrumentation; methodology; odor; review; surgery; teaching; telemedicine; touch; Computer Simulation; Computer-Assisted Instruction; Feedback; Models, Biological; Odors; Smell; Surgery; Telemedicine; Touch; User-Computer Interface",2-s2.0-30744464324
"Jalabert F., Ranwez S., Derozier V., Crampes M.","I2DEE: An integrated and interactive data exploration environment used for ontology design",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750691959&partnerID=40&md5=d889942b71f55d35dc78ab688b13c89a","Many communities need to organize and structure data to improve their utilization and sharing. Much research has been focused on this problem. Many solutions are based on a Terminological and Ontological Resource (TOR) which represents the domain knowledge for a given application. However TORs are often designed without taking into account heterogeneous data from specific resources. For example, in the biomedical domain, these sources may be medical reports, bibliographical resources or biological data extracted from GOA, Gene Ontology or KEGG. This paper presents an integrated visual environment for knowledge engineering. It integrates heterogeneous data from domain databases. Relevant concepts and relations are thus extracted from data resources, using several analysis and treatment processes. The resulting ontology embryo is visualized through a user friendly adaptive interface displaying a knowledge map. The experiments and evaluations dealt with in this paper concern biological data. © Springer-Verlag Berlin Heidelberg 2006.",,"Data acquisition; Data reduction; Data structures; Database systems; Interfaces (computer); Interactive data exploration; Ontology; Ontology design; Terminological and Ontological Resource (TOR); Knowledge based systems",2-s2.0-33750691959
"Ramakrishnan C., Kochut K.J., Sheth A.P.","A framework for schema-driven relationship discovery from unstructured text",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845412311&partnerID=40&md5=1dd33fed301753ac66a099c2b21f2c7d","We address the issue of extracting implicit and explicit relationships between entities in biomedical text. We argue that entities seldom occur in text in their simple form and that relationships in text relate the modified, complex forms of entities with each other. We present a rule-based method for (1) extraction of such complex entities and (2) relationships between them and (3) the conversion of such relationships into RDF. Furthermore, we present results that clearly demonstrate the utility of the generated RDF in discovering knowledge from text corpora by means of locating paths composed of the extracted relationships. © Springer-Verlag Berlin Heidelberg 2006.","Knowledge-driven text mining; Relationship extraction","Biomedical engineering; Relational database systems; Schematic diagrams; Textbooks; Biomedical texts; Knowledge-driven text mining; Relationship extraction; Unstructured text; Knowledge based systems",2-s2.0-33845412311
"Dubey A., Jalote P., Aggarwal S.K.","Inferring grammar rules of programming language dialects",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750296442&partnerID=40&md5=82efe37bb809e8693a21ddee5bab5891","In this paper we address the problem of grammatical inference in the programming language domain. The grammar of a programming language is an important asset because it is used in developing many software engineering tools. Sometimes, grammars of languages are not available and have to be inferred from the source code; especially in the case of programming language dialects. We propose an approach for inferring the grammar of a programming language when an incomplete grammar along with a set of correct programs is given as input. The approach infers a set of grammar rules such that the addition of these rules makes the initial grammar complete. A grammar is complete if it parses all the input programs successfully. We also proposes a rule evaluation order, i.e. an order in which the rules are evaluated for correctness. A set of rules are correct if their addition makes the grammar complete. Experiments show that the proposed rule evaluation order improves the process of grammar inference. © Springer-Verlag Berlin Heidelberg 2006.","Dialects; Minimum description length principle; Programming language grammars","Computer programming languages; Problem solving; Set theory; Dialects; Minimum description length principle; Programming language grammars; Context free grammars",2-s2.0-33750296442
"Nytun J.P., Prinz A., Tveit M.S.","Automatic generation of modelling tools",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746424850&doi=10.1007%2f11787044_21&partnerID=40&md5=73eb44d3aab9f5217ea2b4b21101f400","Higher-level modelling is considered to be the answer to many of the problems computer science is faced with. In order to do modelling, it is necessary to use proper tools. This article is about modelling tools and how they can be generated automatically out of (modelling) language descriptions. Language descriptions in turn are given in meta-models. In this article, we define a terminology for aspects of meta-models and check how they are supported by existing meta-modelling tools. In particular we look at semantic aspects of the meta-models. © Springer-Verlag Berlin Heidelberg 2006.",,"Computer programming languages; Computer science; Computer simulation; Mathematical models; Problem solving; Semantics; Higher-level modeling; Language descriptions; Meta-modeling; Modeling tools; Computer aided software engineering",2-s2.0-33746424850
"Avgustinov P., Christensen A.S., Hendren L., Kuzins S., Lhoták J., Lhoták O., De Moor O., Sereni D., Sittampalam G., Tibbie J.","abc: An extensible AspectJ compiler",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",30,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149011933&partnerID=40&md5=97d8fd68c5b201c20bb40847aa07f41d","Research in the design of aspect-oriented programming languages requires a workbench that facilitates easy experimentation with new language features and implementation techniques. In particular, new features for AspectJ have been proposed that require extensions in many dimensions: syntax, type checking and code generation, as well as data flow and control flow analyses. The AspectBench Compiler (abc) is an implementation of such a workbench. The base version of abc implements the full AspectJ language. Its front end is built using the Polyglot framework, as a modular extension of the Java language. The use of Polyglot gives flexibility of syntax and type checking. The back end is built using the Soot framework, to give modular code generation and analyses. In this paper, we outline the design of abc, focusing mostly on how the design supports extensibility. We then provide a general overview of how to use abc to implement an extension. We illustrate the extension mechanisms of abc through a number of small, but nontrivial, examples. We then proceed to contrast the design goals of abc with those of the original AspectJ compiler, and how these different goals have led to different design decisions. Finally, we review a few examples of projects by others that extend abc in interesting ways. ©Springer-Verlag Berlin Heidelberg 2006.",,"Decision theory; Feature extraction; Java programming language; Program compilers; Software design; Syntactics; Aspect-oriented programming languages; AspectBench Compiler (abc); Language features; Modular extension; Object oriented programming",2-s2.0-37149011933
"Khoo C.S.G., Na J.-C.","Semantic relations in information science",2006,"Annual Review of Information Science and Technology",49,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28844443320&partnerID=40&md5=57797705b526061ca1ba40dfa495691d",[No abstract available],,,2-s2.0-28844443320
"Schultz T., Kirchhoff K.","Multilingual Speech Processing",2006,"Multilingual Speech Processing",76,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013700737&doi=10.1016%2fB978-0-12-088501-5.X5000-8&partnerID=40&md5=8cca307671bea3138e94b7db9cc7e39c","Tanja Schultz and Katrin Kirchhoff have compiled a comprehensive overview of speech processing from a multilingual perspective. By taking this all-inclusive approach to speech processing, the editors have included theories, algorithms, and techniques that are required to support spoken input and output in a large variety of languages. This book presents a comprehensive introduction to research problems and solutions, both from a theoretical as well as a practical perspective, and highlights technology that incorporates the increasing necessity for multilingual applications in our global community. Current challenges of speech processing and the feasibility of sharing data and system components across different languages guide contributors in their discussions of trends, prognoses and open research issues. This includes automatic speech recognition and speech synthesis, but also speech-to-speech translation, dialog systems, automatic language identification, and handling non-native speech. The book is complemented by an overview of multilingual resources, important research trends, and actual speech processing systems that are being deployed in multilingual human-human and human-machine interfaces. Researchers and developers in industry and academia with different backgrounds but a common interest in multilingual speech processing will find an excellent overview of research problems and solutions detailed from theoretical and practical perspectives. © 2006 Elsevier Inc.",,,2-s2.0-85013700737
"Ramanath M., Zhang L., Freire J., Haritsa J.R.","IMAX: Incremental maintenance of schema-based XML statistics",2005,"Proceedings - International Conference on Data Engineering",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28444460689&doi=10.1109%2fICDE.2005.75&partnerID=40&md5=88a09958724ffabb450447f96230160d","Current approaches for estimating the cardinality of XML queries are applicable to a static scenario wherein the underlying XML data does not change subsequent to the collection of statistics on the repository. However, in practice, many XML-based applications are dynamic and involve frequent updates to the data. In this paper, we investigate efficient strategies for incrementally maintaining statistical summaries as and when updates are applied to the data. Specifically, we propose algorithms that handle both the addition of new documents as well as random insertions in the existing document trees. We also show, through a detailed performance evaluation, that our incremental techniques are significantly faster than the naive recomputation approach; and that estimation accuracy can be maintained even with a fixed memory budget. © 2005 IEEE.",,"Document trees; Random insertions; XML based application; XML queries; Algorithms; Computation theory; Data handling; Parameter estimation; Query languages; Statistical methods; XML",2-s2.0-28444460689
"Fang W.-D., Zhang L., Wang Y.-X., Dong S.-B.","Toward a semantic search engine based on ontologies",2005,"2005 International Conference on Machine Learning and Cybernetics, ICMLC 2005",23,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28444451890&partnerID=40&md5=495b564a99af93ab46dc07014df9c4c3","Semantic search requires a search engine to properly interpret the meaning of a user's query and the inherent relations among the terms that a document contains with respect to a specific domain. We present the framework of such a search engine based on domain ontologies. In this framework, a search request, which can be either a keyword list as in traditional search methods or a query in complex form containing various restrictions to the search, is first processed by a query parser which then finds qualified RDF triples in domain ontologies. Web documents relevant to the requested concepts and individuals specified in these triples are then retrieved by a document retriever. And finally, the retrieved documents are ranked according to their relevance to the user's query. An extended term-document matrix is built to reflect the relevance between documents, concepts/individuals, and terms. Such a matrix makes it possible for the search engine to work even in case that there are no available domain ontologies for user requests. © 2005 IEEE.","Information Retrieval (IR); Knowledge Representation; OWL-QL; Semantic search; Semantic Web, Ontology","Computer networks; Information retrieval; Knowledge representation; Query languages; Semantics; OWL-QL; Semantic search; Semantic Web, ontology; Search engines",2-s2.0-28444451890
"Stonebraker M., Çetintemel U.","""One size fits all"": An idea whose time has come and gone",2005,"Proceedings - International Conference on Data Engineering",141,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28444458809&doi=10.1109%2fICDE.2005.1&partnerID=40&md5=28c28e47e174fc35104abe5ebab58f6a","The last 25 years of commercial DBMS development can be summed up in a single phrase: ""One size fits all"". This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products. © 2005 IEEE.",,"Database engines; DBMS architecture; Stream-processing markets; Computer applications; Computer architecture; Data processing; Database systems",2-s2.0-28444458809
"Ge R., Mooney R.J.","A Statistical semantic parser that integrates syntax and semantics",2005,"CoNLL 2005 - Proceedings of the Ninth Conference on Computational Natural Language Learning",67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859016776&partnerID=40&md5=05663cd5f1168565c6800be7bae3ccc8","We introduce a learning semantic parser, SCISSOR, that maps natural-language sentences to a detailed, formal, meaningrepresentation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches. © 2005 Association for Computational Linguistics.",,"Database interfaces; Learning semantics; Non-terminal nodes; Parse trees; Robotic soccer; Semantic labels; Semantic representation; Statistical parser; Two domains; Forestry; Natural language processing systems; Syntactics; Tools; Semantics",2-s2.0-84859016776
"Wang L., Wang C., Xie X., Forman J., Lu Y., Ma W.-Y., Li Y.","Detecting dominant locations from search queries",2005,"SIGIR 2005 - Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",59,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885605841&doi=10.1145%2f1076034.1076107&partnerID=40&md5=9600059943a7d44372cf719b610625f9","Accurately and effectively detecting the locations where search queries are truly about has huge potential impact on increasing search relevance. In this paper, we define a search query's dominant location (QDL) and propose a solution to correctly detect it. QDL is geographical location(s) associated with a query in collective human knowledge, i.e., one or few prominent locations agreed by majority of people who know the answer to the query. QDL is a subjective and collective attribute of search queries and we are able to detect QDLs from both queries containing geographical location names and queries not containing them. The key challenges to QDL detection include false positive suppression (not all contained location names in queries mean geographical locations), and detecting implied locations by the context of the query. In our solution, a query is recursively broken into atomic tokens according to its most popular web usage for reducing false positives. If we do not find a dominant location in this step, we mine the top search results and/or query logs (with different approaches discussed in this paper) to discover implicit query locations. Our large-scale experiments on recent MSN Search queries show that our query location detection solution has consistent high accuracy for all query frequency ranges. © 2005 ACM.","information retrieval; local search; query's dominant location; search; search query location; search relevance","Geographical locations; Large scale experiments; Local search; Location detection; Query frequencies; search; Search queries; search relevance; Information retrieval; Online searching; Query processing",2-s2.0-84885605841
"Goldsmith S., O'Callahan R., Aiken A.","Relational queries over program traces",2005,"ACM SIGPLAN Notices",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745184246&doi=10.1145%2f1103845.1094841&partnerID=40&md5=f77007e20f8f6fec629dcf70372e87b1","Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, PARTIQLE. Given a PTQL query and a Java program, PARTIQLE instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance. Copyright 2005 ACM.","Partiqle; Program trace query language; PTQL; Relational","IBM (CO); Partiqle; Program Trace Query Language (PTQL); Relational; Benchmarking; Computer programming; Encoding (symbols); Java programming language; Online systems; Program debugging; Software prototyping; Query languages",2-s2.0-33745184246
"Goldsmith S., O'Callahan R., Aiken A.","Relational queries over program traces",2005,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",66,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-31744449292&partnerID=40&md5=dcca933672fcce3067c3b3592607ece0","Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, PARTIQLE. Given a PTQL query and a Java program, PARTIQLE instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance. Copyright 2005 ACM.","Partiqle; Program trace query language; PTQL; Relational","Program trace query language; Relational queries; Codes (standards); Java programming language; Program compilers; Program debugging; Software prototyping; XML; Query languages",2-s2.0-31744449292
"Woodley A., Geva S.","Applying transformation-based error-driven learning to structured natural language queries",2005,"Proceedings - 2005 International Conference on Cyberworlds, CW 2005",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745155036&doi=10.1109%2fCW.2005.19&partnerID=40&md5=381553ca64dc182c22a9644aee759d0e","XML information retrieval (XML-IR) systems aim to provide users with highly exhaustive and highly specific results. To interact with XML-IR systems, users must express both their content and structural requirement, in the form of a structured query. Traditionally, these structured queries have been formatted using formal languages such as XPath or NEXI. Unfortunately, formal query languages are very complex and too difficult to be used by experienced, let alone casual users. Therefore, recent research has investigated the idea of specifying users' content and structural needs via natural language queries (NLQs). In previous research we developed NLPX, a natural language interface to an XML-IR system. Here we present additions we have made to NLPX. The additions involve the application of transformation-based error-driven learning (TBL) to structured NLQs, to derive special connotations and group words into an atomic unit of information. TBL has successfully been applied to other areas of natural language processing; however, this paper presents the first time it has been applied to structured NLQs. Here, we investigate the applicability of TBL to NLQs and compare the TBL-based system, with our previous system and a system with a formal language interference. Our results show that TBL is effective for structured NLQs, and that structured NLQs a viable interface tor XML-IR systems. © 2005 IEEE.",,"Natural language queries; Structured query; Transformation-based error-driven learning (TBL); Data structures; Error detection; Information retrieval; Interfaces (computer); Learning systems; Natural language processing systems; Requirements engineering; XML; Query languages",2-s2.0-33745155036
"Tiedemann J.","Integrating linguistic knowledge in passage retrieval for question answering",2005,"HLT/EMNLP 2005 - Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749594771&partnerID=40&md5=3c08e37319ee28576f81b49d2c9afa0f","In this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system. We use annotation produced by a deep syntactic dependency parser for Dutch, Alpino, to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index. Similar annotation is produced for natural language questions to be answered by the system. From this we extract query terms to be sent to the enriched retrieval index. We use a genetic algorithm to optimize the selection of features and syntactic units to be included in a query. This algorithm is also used to optimize further parameters such as keyword weights. The system is trained on questions from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). We could show an improvement of about 15% in mean total reciprocal rank compared to traditional information retrieval using plain text keywords (including stemming and stop word removal). © 2005 Association for Computational Linguistics.",,"Cross-language evaluation forums; Dependency parser; Linguistic features; Linguistic knowledge; Natural language questions; Passage retrieval; Plain text; Query terms; Question Answering; Question answering systems; Stop word; Algorithms; Computational linguistics; Feature extraction; Information retrieval; Optimization; Syntactics; Natural language processing systems",2-s2.0-33749594771
"Shi Z., Gu B., Popowich F., Sarkar A.","Synonym-based query expansion and boosting-based re-ranking: A two-phase approach for genomic information retrieval",2005,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873527164&partnerID=40&md5=63ea7a243115ff42ffdea6c36f1d41b0","We describe in this paper the design and evaluation of the system built at Simon Fraser University for the TREC 2005 adhoc retrieval task in the Genomics track. The main approach taken in our system was to expand synonyms by exploiting a fusion of a set of biomedical and general ontology sources, and apply machine learning and natural language processing techniques to re-rank retrieved documents. In our system, we integrated EntrezGene, HUGO, Eugenes, ARGH, GO, MeSH, UMLSKS and WordNet into a large reference database and then used a conventional Information Retrieval (IR) toolkit, the Lemur toolkit (Lemur, 2005), to build an IR system. In the postprocessing phase, we applied a boosting algorithm (Kudo and Matsumoto, 2004) that captured natural language substructures embedded in texts to re-rank the retrieved documents. Experimental results show that the boosting algorithm worked well in cases where a conventional IR system performs poorly, but this re-ranking approach was not robust enough when applied to broad coverage task typically associated with IR.",,"Ad-hoc retrieval tasks; Boosting algorithm; Design and evaluations; Genomics; Ir systems; NAtural language processing; Natural languages; Query expansion; Re-ranking; Reference database; Retrieved documents; Simon Fraser University; Wordnet; Learning algorithms; Natural language processing systems; Query processing; Semantics; Information retrieval",2-s2.0-84873527164
"Le Nguyen M., Shimazu A., Phan H.X.","A structured SVM semantic parser augmented by semantic tagging with conditional random field",2005,"PACLIC 19 - Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864291127&partnerID=40&md5=e3a0585eedcd0e230f2638ea44dcb1b0","This paper presents a novel method of semantic parsing that maps a natural language (NL) sentence to a logical form. We propose a semantic parsing method by conducting separately two steps as follows; 1) The first step is to predict semantic tags for a given input sentence. 2) The second step is to build a semantic representation structure for the sentence using the sequence of semantic tags. We formulate the problem of semantic tagging as a sequence learning using a conditional random field models (CRFs). We then represent a tree structure of a given sentence in which syntactic and semantic information are integrated in that tree. The learning problem is to map a given input sentence to a tree structure using a structure support vector model. Experimental results on the CLANG corpus show that the semantic tagging performance achieved a sufficiently high result. In addition, the precision and recall of mapping NL sentences to logical forms i.e. the meaning representation in CLANG show an improvement in comparison with the previous work.",,"Conditional random field; Learning problem; Logical forms; Natural languages; Precision and recall; Semantic information; Semantic parsing; Semantic representation; Semantic tagging; Semantic tags; Sequence learning; Support vector; Tree structures; Forestry; Natural language processing systems; Random processes; Semantics; Trees (mathematics); Context free grammars",2-s2.0-84864291127
"Colucci S., Di Noia T., Di Sciascio E., Donini F.M., Ragone A.","Knowledge elicitation for query refinement in a semantic-enabled e-marketplace",2005,"ACM International Conference Proceeding Series",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547261414&doi=10.1145%2f1089551.1089673&partnerID=40&md5=973090a40adba310fad80a063bac1c06","In this paper we present a knowledge-based approach to the elicitation of information from advertisements, in the framework of a semantic-enabled marketplace. The elicited information can be used for advertisements enriching and refining, without requiring users thorough knowledge of the domain, and to determine a logicbased exact match. The approach exploits non-standard inference services in Description Logics, namely Abduction and Contraction, to tackle a typical problem of semantic-enabled marketplaces, that is the difficulty the average or casual user has in exploiting all the knowledge expressed in an e-commerce domain, which appears necessary to issue requests. We present an algorithm, which returns the set of concepts not included in the request-that can be used for query refinement- and more interesting what is still missing for each available supply, to obtain an exact, bidirectional, match. Copyright 2005 ACM.","concept abduction; concept contraction; description logics; e-commerce; knowledge management; matchmaking; semantic web","Concept abduction; Concept contraction; Description logic; E-Commerce; E-commerce domains; E-marketplaces; Exact match; Knowledge elicitation; Knowledge-based approach; Query refinement; Still missing; Concentration (process); Data description; Electronic commerce; Knowledge acquisition; Knowledge based systems; Knowledge management; Refining; Semantics; Semantic Web",2-s2.0-34547261414
"Becker M., Osborne M.","A two-stage method for active learning of statistical grammars",2005,"IJCAI International Joint Conference on Artificial Intelligence",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880710859&partnerID=40&md5=70a54d1f8d6ac6585b203480ff7803df","Active learning reduces the amount of manually annotated sentences necessary when training state-of-the-art statistical parsers. One popular method, uncertainty sampling, selects sentences for which the parser exhibits low certainty. However, this method does not quantify confidence about the current statistical model itself. In particular, we should be less confident about selection decisions based on low frequency events. We present a novel two-stage method which first targets sentences which cannot be reliably selected using uncertainty sampling, and then applies standard uncertainty sampling to the remaining sentences. An evaluation shows that this method performs better than pure uncertainty sampling, and better than an ensemble method based on bagged ensemble members only.",,"Current statistical model; Ensemble members; Ensemble methods; Selection decisions; Standard uncertainty; Statistical parser; Two-stage methods; Uncertainty samplings; Artificial intelligence",2-s2.0-84880710859
"Yokoyama S., Ohta M., Katayama K., Ishikawa H.","An access control method based on the prefix labeling scheme for XML repositories",2005,"Conferences in Research and Practice in Information Technology Series",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873305594&partnerID=40&md5=f1a40c06d7ac008f20409faadaa04891","This paper describes an access control method of the XML repository system, SAXOPHONE, which was implemented at Tokyo Metropolitan University. The main feature of our research is a novel account identifier that is based on the prefix-labeling scheme to realize a hierarchical authorization. SAXOPHONE uses relational databases for XML document storage. Using it, any valid or well-formed XML document is decomposed into events of the SAX parser and is then stored into relational tables using a fixed scheme. Consequently, users can handle the system as a normal SAX parser. This study also illustrates how to realize an access control method of XML documents on relational databases using the account identifier. © 2005, Australian Computer Society, Inc.","Access control; Relational database; SAX parser; XML database; XML repository","Document storage; Labeling scheme; Relational Database; Relational tables; SAX parser; Tokyo Metropolitan University; XML database; XML repositories; Access control; Acoustic devices; Database systems; XML",2-s2.0-84873305594
"Ragone A., Coppi S., Di Noia T., Di Sciascio E., Donini F.M.","Natural language processing for a semantic enabled resource retrieval scenario",2005,"Proceedings of the International Conference on Information Technology Interfaces, ITI",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745269771&doi=10.1109%2fITI.2005.1491159&partnerID=40&md5=b189565dd0894269f384b01e9ef03979","We present an extended approach to resource retrieval using Natural Language Processing techniques. The aim is to ease user interaction with semantic-based facilitators. The approach allows the automated mapping of the semantics of a natural Language sentence with respect to an ontology modeled using Description Logics. The approach is embedded in the MAMAS matchmaking framework and exploits non-standard inference services for Description Logics.","Description logics; E-commerce; Natural language processing; Parser; Semantic web","Embedded systems; Formal logic; Information retrieval; Semantics; Standards; Description logics; Natural language processing; Parser; Semantic web; Formal languages",2-s2.0-33745269771
"Kwok K.-L., Grunfeld L., Deng P.","Improving weak ad-hoc retrieval by Web assistance and data fusion",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646147124&doi=10.1007%2f11562382_2&partnerID=40&md5=90d929aec27ca432a7b59adf6da2e6f5","Users experience frustration when their reasonable queries retrieve no relevant documents. We call these weak queries and retrievals. Improving their effectiveness is an important issue in ad-hoc retrieval and will be most rewarding for these users. We offer an explanation (with experimental support) why data fusion of sufficiently different retrieval lists can improve weak query results. This approach requires sufficiently different retrieval lists for an ad-hoc query. We propose various ways of selecting salient terms from longer queries to probe the web, and define alternate queries from web results. Target retrievals by the original and alternate queries are combined. When compared with normal ad-hoc retrieval, web assistance and data fusion can improve weak query effectiveness by over 100%. Another benefit of this approach is that other queries also improve along with weak ones, unlike pseudo-relevance feedback which works mostly for non-weak queries. © Springer-Verlag Berlin Heidelberg 2005.",,"Data processing; Feedback control; Query languages; World Wide Web; Ad-hoc queries; Data fusion; Queries; Web assistance; Information retrieval",2-s2.0-33646147124
"Oldakowski R., Bizer C., Westphal D.","RAP: RDF API for PHP",2005,"CEUR Workshop Proceedings",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883641798&partnerID=40&md5=5d0f0b09c10fe0190246b0286bb8cd07","RAP - RDF API for PHP is a Semantic Web toolkit for PHP developers. It offers features for parsing, manipulating, storing, querying, serving, and serializing RDF graphs. RAP was started as an open source project by the Freie Universität Berlin in 2002 and has been extended with code contributions from the Semantic Web community. Its latest release (V0.9.1) includes among others: a statement-centric API for manipulating RDF graphs as a set of statements, a resource-centric API for manipulating RDF graphs as a set of resources with properties, an up-to-date RDF/XML parser and serializer, support for other serialization techniques (i.e. N-Triple, Notation3, RDF embedded in XHTML), in-memory or database model storage, an inference engine supporting RDF-Schema reasoning and some OWL entailments, an integrated RDF server, and support for the RDQL query language.",,"Database modeling; Open source projects; RDF graph; RDF/XML; Serializers; Web community; Java programming language; Open systems; Query processing; Query languages",2-s2.0-84883641798
"Carletta J., Evert S., Heid U., Kilgour J.","The NITE XML toolkit: Data model and query language",2005,"Language Resources and Evaluation",25,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746060532&doi=10.1007%2fs10579-006-9001-9&partnerID=40&md5=40e3c79c3bfb376c871c48acaf6904c7","The NITE XML Toolkit (NXT) is open source software for working with language corpora, with particular strengths for multimodal and heavily cross-annotated data sets. In NXT, annotations are described by types and attribute value pairs, and can relate to signal via start and end times, to representations of the external environment, and to each other via either an arbitrary graph structure or a multi-rooted tree structure characterized by both temporal and structural orderings. Simple queries in NXT express variable bindings for n-tuples of objects, optionally constrained by type, and give a set of conditions on the n-tuples combined with boolean operators. The defined operators for the condition tests allow full access to the timing and structural properties of the data model. A complex query facility passes variable bindings from one query to another for filtering, returning a tree structure. In addition to describing NXT's core data handling and search capabilities, we explain the stand-off XML data storage format that it employs and illustrate its use with examples from an early adopter of the technology.","Linguistic annotation; Multi-modal language corpora; Software tools",,2-s2.0-33746060532
"Schuhart H., Linnemann V.","Valid updates for persistent XML objects",2005,"BTW 2005 - Datenbanksysteme in Business, Technologie und Web, 11th Fachtagung des GI-Fachbereichs ""Datenbanken und Informationssysteme"" (DBIS)",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873909343&partnerID=40&md5=b7f0b68e665911773b1c57d34f474dd1","XML has emerged as the industry standard for representing and exchanging data and is already predominant in several applications today. Business, analytic and structered data will be exchanged as XML between applications and web services. XQuery is a language designed and developed for querying, filtering and generating XML structured data and is currently being standardized by the World Wide Web Consortium(W3C). XQuery seems to become the query language in context of (native) XML databases. Moreover in the context of document management XQuery seems suitable for querying large collections of documents with more irregular and deeply nested data structures. Despite these promising features XQuery or more precisely its FLWOR expression lacks of any update capability. In this paper we present important results concerning the development of XOBEDBPL (XML OBjEcts DataBase Programming Language). XOBEDBPL is the successor of the XOBE project. XOBE integrates XML and XPath into the Java programming language. In XOBEDBPL XML objects can become persistent. Moreover, a new feature in XOBEDBPL is the integration of xFLWOR(extended FLWOR) expressions for updating and querying XML objects. XML updates and queries in XOBEDBPL are statically typechecked. Finally we perform experiments with the XOBEDBPL prototype showing that the performance of low level API-based interfaces can be improved, as well as the performance of related approaches.",,"Database programming languages; Document management; FLWOR expressions; In contexts; Industry standards; Low level; Structured data; World wide web consortiums; XML database; XML update; Application programming interfaces (API); Data structures; Information services; Query languages; Query processing; Web services; XML",2-s2.0-84873909343
"Huang C.-H., Chuang T.-R., Lee H.-M.","Prefiltering techniques for efficient XML document processing",2005,"Proceedings of the 2005 ACM Symposium on Document Engineering",6,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745829080&partnerID=40&md5=af744aa233ff376270f423a0658ca911","Document Object Model (DOM) and Simple API for XML (SAX) are the two major programming models for XML document processing. Each, however, has its own efficiency limitation. DOM assumes an in-core representation of XML documents which can be problematic for large documents. SAX needs to scan over the document in a linear manner in order to locate the interesting fragments. Previously, we have used tree-to-table mapping and indexing techniques to help answer structural queries to large, or large collections of, XML documents. In this paper, we generalize the previous techniques into a prefiltering framework where repeated access to large XML documents can be efficiently carried out within the existing DOM and SAX models. The prefiltering framework essentially uses a tiny search engine to locate useful fragments in the target XML documents by approximately executing the user's queries. Those fragments are gathered into a candidate-set XML document, and is returned to the user's DOM- or SAX-based applications for further processing. This results in a practical and efficient model of XML processing, especially when the XML documents are large and infrequently updated, but are frequently being queried. Copyright 2005 ACM.","DOM; Prefiltering; SAX; Structural Query; Two-phased XML processing model","Computer programming; Data acquisition; Indexing (of information); Mathematical models; Query languages; XML; DOM; Prefiltering; SAX; Structural Query; Two-phased XML processing model; Information retrieval systems",2-s2.0-33745829080
"Noh S.-Y., Gadia S.K.","An XML-based Framework for Temporal Database Implementation",2005,"Proceedings of the International Workshop on Temporal Representation and Reasoning",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749148378&doi=10.1109%2fTIME.2005.12&partnerID=40&md5=0c7fe5f7e75335c420a9bf98fe774c08","This paper presents an XML-based approach to implementing the parametric model of temporal databases. In the parametric model, attribute values are functions of time and the entire history of an object is modeled in terms of a single tuple. This property makes it difficult to adapt the parametric model within the conventional databases. However, XML is an attractive option for implementation because data boundaries are not problematic in XML. Native XML storage with pagination is used for storing temporal data and the DOM parser for the paginated XML storage is used for data access. The primitives for the query language are implemented using the DOM parser. Many artifacts, such as parse tree and expression tree are also represented in XML. We hope that this paradigm offers an elegant approach for implementation of complex data models. © 2005 IEEE.",,"Attribute values; Complex data; Conventional database; Data access; Parametric models; Parse trees; Temporal Data; Temporal Database; XML storage; Biological materials; Data processing; Query languages; XML; Markup languages",2-s2.0-66749148378
"Hernández Y.R., Rodríguez A.F., Cortés J.P., Martínez J.N.","Indexing entities to improve information retrieval performance",2005,"Proceedings of the 2nd Indian International Conference on Artificial Intelligence, IICAI 2005",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872091653&partnerID=40&md5=2a04c3c5f1e358e81e0a70a33942493d","We propose a model for monolingual Information Retrieval in English and Spanish languages. This model uses Natural Language Processing techniques and resources (a POS-tagger. a Partial Parser, an Anaphora Resolver. and WordNet) in order to improve the precision of traditional IR systems. indexing the entities and the relations between these entities in the documents. In this paper we report the result from evaluate this model on the Spanish and English CLEF corpora, applying it to short and long queries. Copyright © IICAI 2005.",,"Long queries; Monolingual information retrieval; NAtural language processing; Spanish language; Wordnet; Artificial intelligence; Indexing (of information); Learning algorithms; Natural language processing systems; Information retrieval",2-s2.0-84872091653
"Yang J.O., Yu U., Kim S.","WIGED: Web-based data integration system for analysis of gene expression in disease",2005,"Key Engineering Materials",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646405471&partnerID=40&md5=cd5c8bab7e566dba22969739ccc0df6a","Gene expression patterns are useful for understanding the causes of diseases. Accordingly, the current paper presents a Web-based data Integration system for the analysis of Gene Expression in Diseases, called WIGED. WIGED provides an integrated search service over several databases. Specifically, it provides concurrent access to the SMD, SAGE Genie, and OMIM databases and integrates the results from all these databases. WIGED consists of three components: Data Accessor, ResultParser, and Visualizer. The Data Accessor launches multiple threads to send queries to several databases concurrently. The ResultParser then analyzes and integrates the query results from each database in HTML format. This process is fast as it is implemented using simple regular expressions rather than full-fledged HTML parsers. Finally, the Visualizer displays the integrated results. As a result, the combined results from these various databases can provide comprehensive gene expression patterns in a number of organs and tissues with reviewed descriptive information. Even though the results from this bioinformatic tool still require expert review, the whole decision-making process is certainly accelerated and a better opportunity provided to comprehend biological patterns in diseases.","Gene expression in disease; Genie; HTMLParser and integration; OMIM; SAGE; SMD; WIGED","Gene expression in disease; Genie; HTMLParser and integration; OMIM; SAGE; WIGED; Biological organs; Database systems; Decision making; Diseases; HTML; Query languages; Tissue; World Wide Web; Genes",2-s2.0-33646405471
"Yuan C., Wang C.","Parsing model for answer extraction in Chinese Question Answering system",2005,"Proceedings of 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering, IEEE NLP-KE'05",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847263982&doi=10.1109%2fNLPKE.2005.1598741&partnerID=40&md5=2e34316e97092f043279c50b8f5f0fe9","This paper presents a novel approach to answer extraction in Chinese Question Answering (QA) system. An extended lexicalized context-free grammar parser is used for question and answer understanding, the exact answer is extracted with the clue of named entity (NE) and question classification (QC). It shows that the performance of QA will be greatly improved with more comprehensive language understanding technologies. © 2005 IEEE.",,"Classification (of information); Context free grammars; Knowledge engineering; Mathematical models; Query languages; Named entity (NE); Parsing model; Question classification (QC); Natural language processing systems",2-s2.0-33847263982
"Dezsényi C., Dobrowiecki T.P., Mészáros T.","Adaptive document analysis with planning",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646129922&partnerID=40&md5=b9f7e85b9eacebaa4bcb5c2ea8aeaab7","Autonomous web information systems frequently have to answer queries by extracting information written in non-constrained natural language. This task is modeled as a planning problem. Elementary document processing modules are organized into query dependent information-processing graphs that are the tools of scheduling and controlling the execution and provide semantic fusion of heterogeneous information chunks. © Springer-Verlag Berlin Heidelberg 2005.",,"Autonomous agents; Data processing; Graph theory; Problem solving; Query languages; Semantics; Autonomous web information systems; Information chunks; Non-constrained natural language; Semantic fusion; Adaptive systems",2-s2.0-33646129922
"Hartel P.H.","A trace semantics for positive core xpath",2005,"Proceedings of the International Workshop on Temporal Representation and Reasoning",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750064916&doi=10.1109%2fTIME.2005.9&partnerID=40&md5=fb61d72d793223c8dd44a4f3d13732e7","We provide a novel trace semantics for positive core XPath that exposes all intermediate nodes visited by the query engine. This enables a detailed analysis of all information relevant to the query. We give two examples of such analyses in the form of access control policies. We translate positive core XPath into Linear Temporal Logic, showing that branching structures can be linearised effectively. We use the SPIN model checker in a proof of concept implementation to resolve the queries, and to perform access control. The performance of the implementation is competitive. © 2005 IEEE.",,"Access control policies; Branching structures; Intermediate node; Linear temporal logic; Proof of concept; Query engines; Spin models; Trace semantics; Model checking; Security systems; Semantics; Temporal logic; Access control",2-s2.0-33750064916
"White J., Kolpackov B., Natarajan B., Schmidt D.C.","Reducing application code complexity with vocabulary-specific XML language bindings",2005,"Proceedings of the Annual Southeast Conference",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953737454&doi=10.1145%2f1167253.1167316&partnerID=40&md5=569d0bcf3ba753d231d4ddcc75e7f533","The eXtensible Markup Language (XML) has become a ubiquitous data exchange and storage format. A variety of tools are available for incorporating XML-based data into applications. The most common XML tools (such as parsers for SAX and DOM) provide low-level vocabulary-independent interfaces, which can make it hard to develop and debug robust applications. This paper examines tools for generating vocabulary-specific XML-to-C++ language mappings and shows how they can reduce key sources of complexity associated with developing object-oriented XMLbased applications. The paper also presents criteria for evaluating tools that generate vocabulary-specific language mappings and applies these criteria to compare five tools for this purpose: XML Spy, Xbinder, Object Link, Liquid XML Data Binding Wizard, and XML Schema Compiler (XSC). Our results show that XSC is the only tool that provides a complete vocabulary-specific mapping, alignment with the C++ Standard Library, and code portability, while also providing the most manageable generated code base. Copyright 2005 ACM.","C++; DOM; SAX; Vocabulary-specific language binding; W3C XML schema; XML","Application codes; C++ language; Code portability; DOM; Extensible markup language; Object oriented; Robust application; Specific languages; Standard libraries; Storage formats; Ubiquitous data; XML data; XML languages; XML schemas; Linguistics; Machinery; Mapping; Markup languages; Object oriented programming; Query languages; XML; Hypertext systems",2-s2.0-77953737454
"Sung W.K.","A hybrid approach to storage scheme for Semantic Web metadata",2005,"Proceedings - International Conference on Next Generation Web Services Practices, NWeSP 2005",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847090831&doi=10.1109%2fNWESP.2005.4&partnerID=40&md5=e7aae87bcec823e2f9cd96a8f0d358b7","In the near future, it is anticipated that enormous Semantic Web metadata described in RDF will be appeared. Thus, some researches on the management for the Semantic Web metadata have been proposed. In most previous approaches, the Semantic Web metadata as a form of triple is stored in a large relational table. Since it always requires the whole table to be scanned for processing a query, it may degrade retrieval performance. In this paper, we distinguish some frequently appeared properties in the Semantic Web metadata and/or frequently used properties in a user query. RDF data with these distinguished properties are independently treated and stored in corresponding property-based tables respectively, For processing a query having a specific property, we can avoid full scanning the whole data and only have to access a corresponding table. Finally, we partially implement and evaluate the proposed scheme. The initial results show that the proposed scheme achieves better performance for queries having a specific property. © 2005 IEEE.",,"Computer simulation; Data reduction; Database systems; Query languages; Semantics; Data scanning; Relational tables; Metadata",2-s2.0-33847090831
"Kate R.J., Wong Y.W., Mooney R.J.","Learning to transform natural to formal languages",2005,"Proceedings of the National Conference on Artificial Intelligence",45,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-29344433086&partnerID=40&md5=bc67f42d290b4adab5192b19d1b65a72","This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora, one which maps English instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps English U.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains. Copyright © 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,"Context free grammars; Database systems; Formal languages; Query languages; Syntactics; Trees (mathematics); Command languages; Formal grammar; Syntactic parse tree; Target representation languages; Natural language processing systems",2-s2.0-29344433086
"Lioudakis G.V., Foukarakis I.E., Prezerakos G.N., Kaklamani D.I., Venieris I.S.","eDocuments intelligent enrichment from distributed knowledge resources",2005,"EUROCON 2005 - The International Conference on Computer as a Tool",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947319201&partnerID=40&md5=f4761a297e789dc5c65c83284ef1e20e","When reading A text document, current knowledge workers have to ""manually"" look for additional information: they decide on concepts to further explore, access a web search engine, form a query and compose the search results with the original document. In this paper, we propose a framework aiming at transforming electronic documents from passive pieces of information into dynamic entities that search and retrieve relevant information, becoming active partners in the knowledge worker's tasks. For this purpose, recent trends in natural language processing, semantic knowledge representation and agent-based technologies are combined. © 2005 IEEE.","Agents; Grid; Knowledge representation; Semantics; Web services","Electronic document exchange; Grid computing; Knowledge acquisition; Knowledge management; Knowledge representation; Natural language processing systems; Query processing; Semantics; Web services; Agent based technologies; Electronic documents; Knowledge workers; Text documents; Distributed database systems",2-s2.0-33947319201
"Buehrer G., Weide B.W., Sivilotti P.A.G.","Using parse tree validation to prevent SQL injection attacks",2005,"SEM 2005 - Proceedings of the 5th International Workshop on Software Engineering and Middleware",174,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953855187&doi=10.1145%2f1108473.1108496&partnerID=40&md5=4024ef742c2d51370784a7a17181085b","An SQL injection attack targets interactive web applications that employ database services. Such applications accept user input, such as form fields, and then include this input in database requests, typically SQL statements. In SQL injection, the attacker provides user input that results in a different database request than was intended by the application programmer. That is, the interpretation of the user input as part of a larger SQL statement, results in an SQL statement of a different form than originally intended. We describe a technique to prevent this kind of manipulation and hence eliminate SQL injection vulnerabilities. The technique is based on comparing, at run time, the parse tree of the SQL statement before inclusion of user input with that resulting after inclusion of input. Our solution is efficient, adding about 3 ms overhead to database query costs. In addition, it is easily adopted by application programmers, having the same syntactic structure as current popular record set retrieval methods. For empirical analysis, we provide a case study of our solution in J2EE. We implement our solution in a simple static Java class, and show its effectiveness and scalability. Copyright 2005 ACM.",,"Application programmers; Attack target; Database queries; Database service; Empirical analysis; Interactive web applications; Java class; Parse trees; Retrieval methods; Runtimes; SQL injection; Syntactic structure; User input; Computer crime; Middleware; Software engineering; Database systems",2-s2.0-77953855187
"Kwon J., Rao P., Moon B., Lee S.","FiST: Scalable XML document filtering by sequencing twig patterns",2005,"VLDB 2005 - Proceedings of 31st International Conference on Very Large Data Bases",67,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745591965&partnerID=40&md5=94cd7b87054e0f76aba36e8a7835cd63","In recent years, publish-subscribe (pub-sub) systems based on XML document filtering have received much attention. In a typical pubsub system, subscribed users specify their interest in profiles expressed in the XPath language, and each new content is matched against the user profiles so that the content is delivered to only the interested subscribers. As the number of subscribed users and their profiles can grow very large, the scalability of the system is critical to the success of pub-sub services. In this paper, we propose a novel scalable filtering system called FiST (Filtering by Sequencing Twigs) that transforms twig patterns expressed in XPath and XML documents into sequences using Prüfer's method. As a consequence, instead of matching linear paths of twig patterns individually and merging the matches during post-processing, FiST performs holistic matching of twig patterns with incoming documents. FiST organizes the sequences into a dynamic hash based index for efficient filtering. We demonstrate that our holistic matching approach yields lower filtering cost and good scalability under various situations.",,"Indexing (of information); Information retrieval; Pattern matching; Program processors; Query languages; XML; Scalability; Twig patterns; XML document filtering; XPath language; Data processing",2-s2.0-33745591965
"Tiedemann J.","Improving passage retrieval in question answering using NLP",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",8,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744819447&doi=10.1007%2f11595014_62&partnerID=40&md5=7e5b8c5b1cc562573e46927a09982f95","This paper describes an approach for the integration of linguistic information in passage retrieval in an open-source question answering system for Dutch. Annotation produced by the wide-coverage dependency parser Alpino is stored in multiple index layers to be matched with natural language question that have been analyzed by the same parser. We present a genetic algorithm to select features to be included in retrieval queries and for optimizing keyword weights. The system is trained on questions annotated with their answers from the competition on Dutch question answering within the Cross-Language Evaluation Forum (CLEF). The optimization yielded a significant improvement of about 19% in mean reciprocal rank scores on unseen evaluation data compared to the base-line using traditional information retrieval with plain text keywords. © Springer-Verlag Berlin Heidelberg 2005.",,"Computational methods; Formal languages; Genetic algorithms; Information retrieval; Optimization; Text processing; Language Evaluation Forum; Linguistic informations; Passage retrieval; Question answering systems; Natural language processing systems",2-s2.0-33744819447
"Daumke P., Schulz S., Markó K.","Searching multilingual medical content in the web",2005,"Technology and Health Care",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-32444449586&partnerID=40&md5=e7968d802a3f792fa80dcd394c9e11c2","Introduction: Medical document retrieval presents a unique combination of challenges for the design and implementation of retrieval engines. We introduce a method to meet these challenges by implementing a multilingual retrieval interface for biomedical content in the World Wide Web. To this end we developed an automated method for interlingual query construction by which a standard Web search engine is enabled to process queries in user's native language from the biomedical domain in order to retrieve documents in user desired target languages (currently, English, German, Portuguese, Spanish and Swedish are supported). Morpho-Semantic Indexing: To cope with various morphological processes in different languages we developed a term normalization methodology, called Morpho-Semantic Indexing (henceforth, MSI). MSI uses a special type of dictionary with entries consisting of sub-words, i.e. semantically minimal units. Sub-words are grouped into equivalence classes (represented by Morpheme identifiers, MIDs) which capture intralingual as well as interlingual synonymy. A morpho-syntactic parser extracts sub-words from texts and assigns MIDs in a three step MSI procedure (cf. Fig. 1). The result is a morpho-semantically normalized expression in a language independent representation. Its usefulness for cross-language indexing and retrieval has already been proven [1]. Retrieval Interface: Using domain and language specific corpora (cf. Fig. 2) we created a target list (B) of data including surface words, word bigrams and trigrams (data) as well as their frequencies freq(data) within these corpora. All target queries are subsequently translated to a set of MIDs (C). This data is encoded in a table FreqTab (D), each record being a quadruple (data; freq(data); MSI(data); language(data)). A user can choose his favoured target language on a web interface (http://www.morphosaurus.net→WebTools→Morphoogle). These user queries (E) are sent to our query construction tool. Again, these queries are initially transformed to a set of corresponding MIDs (F). We now further process these MIDs to a list of possible MID combinations - called partitions - which consist of sub-queries (G). All sub-queries are compared to MSI(data) in FreqTab at which all matching records are returned (H, I). Out of these records we generate possible target queries by a set of combination rules, together with a score which can be used as a measure for its lexical relevance (J). Finally, these target queries are sent separately to a standard Web search engine and merged by a simple frequency heuristics. Discussion and Related Work: Test runs on our retrieval interface using the OHSUMED collection [2] are still due. Future work will focus on problems in the query formulation process such as matching problems due to a lack of coherence of the underlying lexicon and language specific problems [3].",,"computer interface; conference paper; Internet; language; medical documentation; methodology; priority journal",2-s2.0-32444449586
"Mao J., Chen Q., Gao F., Guo R., Lu R.","SHTQS: A telephone-based Chinese spoken dialogue system",2005,"Journal of Systems Engineering and Electronics",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644662103&partnerID=40&md5=c32db348cbb264178edc057be032568e","SHTQS is an intelligent telephone-based spoken dialogue system providing the information about the best route between two sites in Shanghai. Instead of separated parts of speech decoding and language parsing, a close cooperation is carried out in SHTQS by integrating automatic speech recognizer (ASR), language understanding, dialogue management and speech generator. In such a way, the erroneous analysis and uncertainty happening in the preceding stages would be recovered and determined accurately with high-level knowledge. Moreover, instead of shallow word-level analysis or simply keyword or key phrase matching, a deeper analysis is performed in our system by integrating a robust parser and a semantic interpreter. The robust parser is particularly important for spontaneous speech inputs because most of the inquiry sentences/phrases are ill-formed. In addition, in designing a mixed-initiative dialogue system, understanding users' inquiries is essential; however, simply matching keywords and/or key phrases can hardly achieve this. Therefore, a semantic interpreter is incorporated in our system. The performance of SHTQS is also evaluated. The dialogue efficiency is 4.4 sentences per query on an average and the case precision rate of language understanding module is up to 81%. The results are satisfactory.","ASR; Natural language understanding; NLG; Spoken dialogue system; TTS","Automatic speech recognizer (ASR); Intelligent telephone; Language parsing; Language understanding module; Semantic interpreter; Speech decoding; Speech generator; Spoken dialogue system; Decoding; Natural language processing systems; Semantics; Telephone; Speech recognition",2-s2.0-33644662103
"Zettlemoyer L.S., Michael C.","Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",2005,"Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, UAI 2005",194,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348934176&partnerID=40&md5=e7a13861569d8b5acea57ab738b86ef8","This paper addresses the problem of mapping natural language sentences to lambda-calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains.",,"Benchmark database; Categorial grammar; Encodings; Lambda-calculus; Logical forms; Loglinear model; Natural language interfaces; Natural languages; Semantic analysis; Training sets; Artificial intelligence; Differentiation (calculus); Learning algorithms; Regression analysis; Semantics; Calculations",2-s2.0-36348934176
"Nowack B.","ARC: Appmosphere RDF classes for PHP developers",2005,"CEUR Workshop Proceedings",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883648972&partnerID=40&md5=1b9826a62110c550ab325602cf73eb9a","ARC is an open source collection of lightweight PHP scripts optimized for RDF development on hosted Web servers. It currently consists of a non-validating RDF/XML parser, an N-Triples Serializer, and a ""Simple Model"" class providing common methods for working with resource descriptions. The three main classes are stand-alone, single-file scripts, thus facilitating the bundling with existing PHP-based applications. By partly using arrays instead of objects, ARC offers speed improvements compared to toolkits that follow approaches completely based on PHP objects.",,"Open sources; RDF/XML; Resource description; Serializers; Simple modeling; Speed improvement; Web servers",2-s2.0-84883648972
"Desmarais M.C.","Web log session analyzer: Integrating parsing and logic programming into a data mart architecture",2005,"Proceedings - 2005 IEEE/WIC/ACM InternationalConference on Web Intelligence, WI 2005",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748873958&doi=10.1109%2fWI.2005.159&partnerID=40&md5=fc8b28ae85ed1bc6b7337ca4a3b847b3","Navigation and interaction patterns of Web users can be relatively complex, especially for sites with interactive applications that support user sessions and profiles. We describe such a case for an interactive virtual garment dressing room. The application is distributed over many web sites, supports personnalization and user profiles, and the notion of a multi-site user session. It has its own data logging system that generates approximately 5GB of complex data per month. The analysis of those logs requires more sophisticated processing than is typically done using a relational language. Even the use of procedural languages and DBMS can prove tedious and inefficient. We show an approach to the analysis of complex log data based on a parallel stream processing architecture and the use of specialized languages, namely a grammatical parser and a logic programming module, that offers an efficient, flexible, and powerful solution. © 2005 IEEE.",,"Computer architecture; Computer programming languages; Context free grammars; Data processing; Logic programming; User interfaces; Virtual reality; Websites; Dressing room; Parallel stream processing architecture; Parsing; Web log session analyzer; World Wide Web",2-s2.0-33748873958
"Li P., Wang X., Guan Y., Zhao Y.","Extracting answers to natural language questions from large-scale corpus",2005,"Proceedings of 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering, IEEE NLP-KE'05",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847313618&doi=10.1109%2fNLPKE.2005.1598824&partnerID=40&md5=5a5843722373a9a118e9ccd8fadfc157","This paper provides a novel and tractable method for extracting exact textual answers from the returned documents that are retrieved by traditional IR system in large-scale collection of texts. In our approach, WordNet and Web information are employed to improve the performance as external auxiliary resources, then some NLP technologies are used to constitute the empirical answer ranking formula, such as POS tagging, Named Entity recognition, and parser etc. The method involves automatically ranking passages with System Similarity Model, automatically downloading related Web pages by means of Web crawler, and automatically mining answers with empirical formula from candidate answer sets. The series of experimental results show that the overall performance of our system is good and the structure of the system is reasonable. © 2005 IEEE.",,"Documents; Downloading; Natural languages; Web information; Computational methods; Data mining; Information analysis; Information retrieval systems; Natural language processing systems; Text processing; Websites; Information retrieval",2-s2.0-33847313618
"Vidal J., García De Jalón J.","Description of multibody systems in MechXML (mechanism extensible markup language) including control",2005,"Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference - DETC2005",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244467903&partnerID=40&md5=f65ac8a407dc9a8021118162546735fa","MechXML is an XML-based language aimed at describing multibody systems and their simulations (see a more detailed description in [1]). With the appropriate easy-to-build parsers, a data file written in this language can be executed with little or no modification at all in many commercial and research-oriented programs. The purpose of this paper is twofold: to introduce briefly this language and to describe with more detail the new developments that allow the definition of continuous control subsystems. The definition of controls is based on block diagrams. The user defined functions are a general way to customize multibody simulation software so as to enable it to deal with arbitrary forces and constraints. They are described here because they can be used to describe the actions of control systems, too. At the end, a file written according to the rules of MechXML is parsed and run in two programs as different as MSC.ADAMS and SimMechanics. Copyright © 2005 by ASME.","Control; Mechanisms; Multibody; Simulation; XML","Computer programming; Computer simulation; Constraint theory; Control; Mechanisms; User interfaces; Block diagrams; Control subsystems; MechXML; Multibody; XML",2-s2.0-33244467903
"Sansonnet J.-P., Martin J.-C., Leguern K.","A software engineering approach combining rational and conversational agents for the design of assistance applications",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646053833&doi=10.1007%2f11550617_10&partnerID=40&md5=89085ba8185594c78fb1c221908b8d7f","A Conversational Agent can be useful for providing assistance to naïve users on how to use a graphical interface. Such an assistant requires three features: understanding users' requests, reasoning, and intuitive output. In this paper we introduce the DAFT-LEA architecture for enabling assistant agents to reply to questions asked by naive users about the structure and functioning of graphical interlaces. This architecture integrates via a unified software engineering approach a linguistic parser for the understanding the user's requests, a rational agent for the reasoning about the graphical application, and a 2D cartoon like agenl for the multimodal output. We describe how it has been applied to three different assistance application contexts, and how it was incrementally defined via the collection of a corpus of users' requests for assistance. Such an approach can be useful for the design of other assistance applications since it enables a clear separation between the original graphical application, its abstract DAFT model and the linguistic processing of users' requests. © Springer-Verlag Berlin Heidelberg 2005.",,"Animation; Computer architecture; Computer graphics; Graph theory; Linguistics; Logic design; Conversational Agent; DAFT-LEA architecture; Intuitive output; Linguistic processing; Software engineering",2-s2.0-33646053833
"Mayfield J., McNamee P.","JHU/APL at TREC 2005: QA retrieval and robust tracks",2005,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873554329&partnerID=40&md5=7bbb6eceab866ecfab774cd0a911d1e3","The Johns Hopkins University Applied Physics Laboratory (JHU/APL) focused on the Robust and Question Answering Information Retrieval (QAIR) Tracks at the 2005 TREC conference. For the Robust Track, we attempted to use the difference in retrieval scores between the top retrieved and the 100th document to predict performance; the result was not competitive. For QAIR, we augmented each query with terms that appeared frequently in documents that contained answers to questions from previous question sets; the results showed modest gains from the technique.",,"Johns Hopkins University Applied Physics laboratories; Question Answering; Information retrieval",2-s2.0-84873554329
"Talhami H., Kamel I.","Identifying semantically similar arabic words using a large vocabulary speech recognition system",2005,"Proceedings of the IASTED International Conference on Internet and Multimedia Systems and Applications, EuroIMSA",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-29844440879&partnerID=40&md5=3e5604c57e910e4fdf20eb42d447e5a1","Users search digital libraries for book references using one or more attributes such as keywords, subject and author name. Some book titles might contain the keyword that the user specified and thus these titles will directly qualify as candidate results. On the other hand there are other titles that are relevant but do not contain the same exact search keyword. A user expects to retrieve all titles that are relevant to a specified keyword. Similarly when searching for an author name, the system should be able to retrieve the different forms of the name. The library science community developed a mechanism called authority control that allows the user to do a comprehensive search and retrieve all the records that are relevant to the query keyword. In this paper we propose an approach that allows the user to query an Arabic audio library using voice. We use a combination of class-based language models and robust interpretation to recognize and identify the spoken keywords. The mechanism uses a Large Vocabulary Recognition System (LVCSR) to implement the functionality of the authority control system. A series of experiments were performed to assess the accuracy and the robustness of the proposed approach: restricted grammar recognition with semantic interpretation, class-based statistical language models (CB_SLM) with robust interpretation, and generalized CB-SLM. The results have shown that the combination of CB-SLM and robust interpretation provides better accuracy and robustness than the traditional grammar-based parsing.","Arabic; Indexing; Language processing; Speech recognition","Indexing (of information); Mathematical models; Robustness (control systems); Semantics; Speech recognition; Statistical methods; Arabic; Language processing; Digital libraries",2-s2.0-29844440879
"Kiselyov O.","Implementing Metcast in scheme",2005,"Higher-Order and Symbolic Computation",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28244466701&doi=10.1007%2fs10990-005-4880-9&partnerID=40&md5=23f58596d9d1647892fdda6d970a77ca","This paper describes the experience of implementing a core component of an operationally deployed large distributed system in Scheme. Metcast is a request-reply and subscription system for the dissemination of real-time weather information. The system stores vast amounts of weather observation reports, forecasts, gridded data produced by weather models, and satellite imagery. A Metcast server delivers a subset of these data in response to a query formulated in a domain-specific declarative language. Decoders of World Meteorological Organization's data feed, the Metcast application server, XML encoders and decoders, auxiliary and monitoring CGI scripts are all written in a mostly pure functional subset of Scheme. This paper describes three examples that demonstrate the benefits of our choice of the implementation language: parsing of the data feed; XML transformations and Web services; a modular interpreter for the extensible and expressive request language. We also discuss general-purpose extensions to Scheme developed in the project. © 2005 Springer Science + Business Media, Inc.","Application server; Applications and experience with symbolic computing; Scheme; SXML; Weather observations; WMO; XML","Application servers; Applications and experience with symbolic computing; Schemes; SXML; Weather observations; WMO; Decoding; Distributed parameter control systems; Encoding (symbols); Information dissemination; Mathematical models; Meteorology; Real time systems; Servers; Weather forecasting; Weather information services; XML; Computation theory",2-s2.0-28244466701
"Shimbo M., Yamasaki T., Matsumoto Y.","Sentence role identification in Medline abstracts: Training classifier with structured abstracts",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844496113&partnerID=40&md5=cfceaf9f31d04c14f89d10a0fba5a33d","The abstract of a scientific paper typically consists of sentences describing the background of study, its objective, experimental method and results, and conclusions. We discuss the task of identifying which of these ""structural roles"" each sentence in abstracts plays, with a particular focus on its application in building a literature retrieval system. By annotating sentences in an abstract collection with role labels, we can build a literature retrieval system in which users can specify the roles of the sentences in which query terms should be sought. We argue that this facility enables more goal-oriented search, and also makes it easier to narrow down search results when adding extra query terms does not work. To build such a system, two issues need to be addressed: (1) how we should determine the set of structural roles presented to users from which they can choose the target search area, and (2) how we should classify each sentence in abstracts by their structural roles, without relying too much on human supervision. We view the task of role identification as that of text classification based on supervised machine learning. Our approach is characterized by the use of structured abstracts for building training data. In structured abstracts, which is a format of abstracts popular in biomedical domains, sections are explicitly marked with headings indicating their structural roles, and hence they provide us with an inexpensive way to collect training data for sentence classifiers. Statistics on the structured abstracts contained in Medline give an insight on determining the set of sections to be presented to users as well. © Springer-Verlag Berlin Heidelberg 2005.","Information retrieval; Medline; Structured abstracts; Text classification","Biomedical engineering; Computational methods; Data structures; Information retrieval; Medical applications; Text processing; Medline; Structured abstracts; Text classification; Abstracting",2-s2.0-26844496113
"Yamamoto Y., Takagi T.","A sentence classification system for multi biomedical literature summarization",2005,"Proceedings - International Workshop on Biomedical Data Engineering, BMDE2005",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947168262&doi=10.1109%2fICDE.2005.170&partnerID=40&md5=a9919b4ef24284d6ccc2484606e272bf","A PubMed search often returns a long list of query-related papers that a researcher cannot cope with in a short time. As a first step to address this issue by summarizing retrieved papers, we developed a system to classify sentences of abstracts obtained from the MEDLINE database into five rhetorical statuses: background, purpose, method, result, or conclusion. We used Support Vector Machine (SVM) classifiers and trained each of them for a different rhetorical status on structured abstracts. A structured abstract is one that has labels indicating rhetorical statuses of the sentences, while an unstructured abstract does not. The classifiers were tested on both structured and unstructured abstracts. The former were randomly obtained from the MEDLINE database and the latter were manually labeled by humans. We compared our method with a previously reported one. In addition, we combined them and evaluated the combined method. Our method outperformed the previously reported one, and the combined method showed even better results. Classified abstracts can be used for multi-document summarization that provides researchers with a way of learning a research topic efficiently and effectively. © 2005 IEEE.",,"Abstracting; Bioinformatics; Database systems; Information retrieval; Learning systems; Support vector machines; Biomedical literature; MEDLINE database; Rhetorical status; Structured abstract; Pattern recognition systems",2-s2.0-33947168262
"Kwok K.L., Grunfeld L., Dinstl N., Deng P.","TREC2005 robust track experiments using PIRCS",2005,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873564993&partnerID=40&md5=5080fee96d3cd9c8d83a2e7af9abc6e6",[No abstract available],,,2-s2.0-84873564993
"Zhao Y., Xu Z., Guan Y., Li P.","Insun05QA on QA track of TREC2005",2005,"NIST Special Publication",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549072&partnerID=40&md5=32e925a3baaeeff8b0e56506995356fd",[No abstract available],,,2-s2.0-84873549072
"Nguyen M.L., Shimazu A., Phan H.X.","A maximum entropy model for transforming sentences to logical form",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745611561&doi=10.1007%2f11589990_84&partnerID=40&md5=6a4d3e3c2168a372af1f550ae8ccd4ca","We formulate the problem of transformation natural language sentences as the determination of sequence of actions that transforms an input sentence to its logical form. The model to determine a sequence of actions for a corresponding sentence is automatically estimated from a corpus of sentences and their logical forms with a MEM framework. Experimental results show that the MEM framework are suitable for the transformation problem and archived a comparable result in comparison with other methods. © Springer-Verlag Berlin Heidelberg 2005.",,"Formal logic; Problem solving; Maximum entropy model; MEM framework; Natural language processing systems",2-s2.0-33745611561
"Anderson N., Klein M., Lankshear C.","Redressing the gender imbalance in ICT professions: Toward state-level strategic approaches",2005,"Australian Educational Computing",9,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644626828&partnerID=40&md5=143696800835a04871ce0969d93a8694","This paper reports preliminary work in an ARC Linkage Project involving collaboration among James Cook University, Education Queensland, and Technology One (a Queensland based company). The project aims to identify and interpret factors associated with low female participation rates in Information and Communications Technology (ICT) professional level occupations and education pathways. Data is being collected using qualitative and quantitative methods. A survey is being administered to over 6,000 year 11 and 12 female high school students in Queensland to provide a comprehensive database that will be augmented with individual and focus group interviews. An online survey will be administered to women working in ICT industries. Reference groups representing EQ and ICT industries have been established to provide consultative advice and feedback throughout the project, and to function as working groups during the data analysis and interpretation phases. Project results will inform the development of strategic response options for education systems and ICT industries and suggest further research and industry-based initiatives designed to enhance female participation rates.",,,2-s2.0-33644626828
"Hinrichs H.","Application of open source software in teaching using the example of a lecture on XML technologies [Einsatz von Open Source-Software in der Lehre am Beispiel einer Vorlesung zu XML-Technologien]",2005,"INFORMATIK 2005 - Informatik LIVE!, Beitrage der 35. Jahrestagung der Gesellschaft fur Informatik e.V. (GI)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877351217&partnerID=40&md5=85ec5cafeb71c68f19d1552b84d31d0a",[No abstract available],,,2-s2.0-84877351217
"Yamanaka M., Niimura K., Kamada T.","A programming environment for demand-driven processing of network XML data and its performance evaluation",2005,"Proceedings of the 2005 ACM Symposium on Document Engineering",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745812427&doi=10.1145%2f1096601.1096651&partnerID=40&md5=db311f4c460809abf2c6a3f6698643c1","This paper proposes a programming environment for Java that processes network XML data in a demand-driven manner to return quick initial responses. Our system provides a data binding tool and a tree operation package, and the programmer can easily handle network XML data as tree-based operations using these facilities. For efficiency, demand-driven data binding allows the application to start the processing of a network XML document before the arrival of the whole data, and our tree operators are also designed to start the calculation using the initially accessible part of the input data. Our system uses multithread technology for implementation with optimization techniques to reduce runtime overheads. It can return initial responses quickly, and often shortens the total execution time due to the effects of latency hiding and the reduction of memory usage. Compared with an ordinary tree-based approach, our system shows a highly improved response and a 1-28% reduction of total execution time on the benchmark programs. It only needs 1-4% runtime overheads against the event-driven programs. Copyright 2005 ACM.","Data binding; Demand-Driven; Multithreading; XML","Data binding; Demand-Driven; Event-driven programs; Multithreading; Benchmarking; Computer programming; Optimization; Storage allocation (computer); Trees (mathematics); XML; Data processing",2-s2.0-33745812427
"Tilak S., Chiu K., Abu-Ghazaleh N.B., Fountain T.","Dynamic resource discovery for sensor networks",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744898816&doi=10.1007%2f11596042_81&partnerID=40&md5=d54c109dee8ad166ec1ebc3e5d197ef3","As sensor networks mature the current generation of sensor networks that are application-specific and exposed only to a limited set of users will give way to heterogeneous sensor networks that are used dynamically by users that need them. The available sensors are likely to be dynamic (e.g., due to mobility) and heterogeneous in terms of their capabilities and software elements. They may provide different types of services and allow different, configurability and access. A critical component in realizing such a vision is dynamic resource discovery. In this paper, we develop a resource discovery protocol for sensor networks, outline some of the challenges involved, and explore solutions to some of the most important ones. Specifically, we first discuss the problem of what resources to track and at what granularity: in addition to the individual sensor capabilities, some resources and services are associated with sensor networks as a whole, or with regions within the network. We also consider the design of the resource discovery protocol, and the inherent tradeoff between interoperability and energy efficiency. © IFIP International Federation for Information Processing 2005.",,"Granularity; Resource discovery; Sensor networks; Application specific; Critical component; Current generation; Dynamic resources; Heterogeneous sensor networks; Resource discovery; Sensor capability; Software elements; Computer software; Energy efficiency; Interoperability; Network protocols; Resource allocation; Sensors; Embedded systems; Energy efficiency; Intelligent buildings; Radio frequency identification (RFID); Sensor networks; Computer networks; Ubiquitous computing",2-s2.0-33744898816
"Cohen W.W., Minkov E., Tomasic A.","Learning to understand web site update requests",2005,"IJCAI International Joint Conference on Artificial Intelligence",12,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746062171&partnerID=40&md5=0b1e4a076c10ad9df926f8aa099c5300","Although Natural Language Processing (NLP) for requests for information has been well-studied, there has been little prior work on understanding requests to update information. In this paper, we propose an intelligent system that can process natural language website update requests semi-automatically. In particular, this system can analyze requests, posted via email, to update the factual content of individual tuples in a database-backed website. Users' messages are processed using a scheme decomposing their requests into a sequence of entity recognition and text classification tasks. Using a corpus generated by human-subject experiments, we experimentally evaluate the performance of this system, as well as its robustness in handling request types not seen in training, or user-specific language styles not seen in training.",,"Entity recognition; NAtural language processing; Natural languages; Requests for information; Text classification; Classification (of information); Intelligent systems; Natural language processing systems; Text processing; Websites",2-s2.0-33746062171
"Chen S.-K., Walher M., Bhaskaran K., Lei H., Chang H., Frank J.","A model driven XML transformation framework for business performance management",2005,"Proceedings - ICEBE 2005: IEEE International Conference on e-Business Engineering",3,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748305718&doi=10.1109%2fICEBE.2005.10&partnerID=40&md5=d8857a43e7c4c659c3efd9f1a4e3f6ad","As XML formats have been widely adopted for representing business documents both within and across enterprises, XML to XML translation becomes a common and critical component for business process integration. Due to limitations of popular approaches such as XSLT for XML translations, we designed a model driven development framework for XML to XML translation with the additional benefits of code re-use and strong built-in model validation. We further applied this framework to the domain of business performance management, converting documents from human-readable XML format to machine-readable XMI format. © 2005 IEEE.",,"Business documents; Business performance management; Human-readable XML format; Machine-readable XMI format; Computer simulation; Industrial management; Mathematical models; Online systems; XML; Electronic commerce",2-s2.0-33748305718
"Unold O., Cielecki L.","How to use crowding selection in grammar-based classifier system",2005,"Proceedings - 5th International Conference on Intelligent Systems Design and Applications 2005, ISDA '05",5,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847000932&doi=10.1109%2fISDA.2005.50&partnerID=40&md5=f3031e766a17c9a2217917393ee1cdc5","Grammar-based classifier system (GCS) is a new version of Learning Classifier Systems (LCS) in which classifiers are represented by context-free grammar in Chomsky Normal Form. GCS evolves one grammar during induction (the Michigan approach) what gives it an ability to find the proper set of rules very quickly. However it is quite sensitive to any variations of learning parameters. This paper investigates the role of crowding selection in GCS. To evaluate the performance of GCS depending on crowding factor and crowding subpopulation we used context-free language in the form of so-called toy language. The set of experiments was performed to obtain the answer for the raised question in the title. © 2005 IEEE.",,"Classification (of information); Context free grammars; Context free languages; Logic programming; Parameter estimation; Context-free language; Crowding factors; Crowding subpopulation; Learning systems",2-s2.0-33847000932
"Crysandt H.","Improving MPEG-7 sound classification",2005,"Audio Engineering Society - 119th Convention Fall Preprints 2005",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866350829&partnerID=40&md5=5d3695fba44388d9b6f1f7a6e3db6161","This paper describes a mechanism to improve the sound classification algorithm included in the MPEG- 7 standard without modifying or extending it. The sequential classification is turned into a hierarchical classification. Thereby it is possible to adopt the classification algorithm more exible to the characteristics of the sound classes. This paper also gives a detailed view on how the algorithm is implemented using a XML database to store and request content information of the audio signals and model descriptions of sound classes using the MPEG-7 standard.",,"Audio signal; Classification algorithm; Content information; Hierarchical classification; Model description; Sound classification; XML database; Algorithms; Motion Picture Experts Group standards",2-s2.0-84866350829
"Fernandas R., Raghavachari M.","Inflatable XML processing",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",2,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646772547&partnerID=40&md5=f2df510eb25cb8c5f828461b7b26f32e","The past few years have seen the widespread adoption of XML as a data representation format in various middleware: databases, Web Services, messaging systems, etc. One drawback of XML has been the high cost of XML processing. We present in this paper InflateX, a system that supports efficient XML processing. InflateX advances the state of the art in two ways. First, it uses a novel representation of XML, called inflatable trees, that supports lazy construction of an XML document in-memory in response to client requests, as well as, more efficient serialization of results. Second, it incorporates a novel algorithm, based on the idea of projection [8], for efficiently constructing an inflatable tree given a set of XPath expressions. The projection algorithm presented in this paper, unlike previous work, can handle all axes in XPath, including complex axes such as ancestor. While we describe the algorithm in terms of our inflatable tree representation, it is portable to other representations of XML. We provide experiments that validate the utility of our inflatable tree representation and our projection algorithm. © IFIP International Federation for Information Processing 2005.","Performance; Projection; XML; XPath","Messaging systems; Projections; XML processing; XPath; Data representations; Messaging system; Performance; Projection; Projection algorithms; Tree representation; XPath; XPath expressions; Algorithms; Data recording; Middleware; Performance; Relational database systems; Storage allocation (computer); Trees (mathematics); XML; Algorithms; Forestry; Middleware; Trees (mathematics); Web services; Parallel processing systems; XML; Algorithms; Data Bases; Forests",2-s2.0-33646772547
"Bansal A., Patel K., Gupta G., Raghavachari B., Harris E.D., Staves J.C.","Towards intelligent services: A case study in chemical emergency response",2005,"Proceedings - 2005 IEEE International Conference on Web Services, ICWS 2005",10,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749068045&doi=10.1109%2fICWS.2005.122&partnerID=40&md5=18f3f7d7e654b36623d985bf1745d463","In a short period the Web has become an important part of our lives. However, the full potential of the web is still not realized. Two recent developments - web services and the semantic web - are steps in the direction of utilizing the full potential of the Web. Web services allow applications to utilize the web for automatically extracting (and updating) information while the semantic web enterprise promises to provide the infrastructure that allows intelligent web services to be rapidly created and deployed. However, with this comes the task of transforming the traditional web-based systems to web-services over the semantic web. In this paper, we demonstrate how an existing successful web-based system for providing help to first responders of chemically hazardous emergencies (called E-plan) can be converted into a web-services based model using the semantic web and intelligent reasoning technologies. Our efforts can be regarded as a case study in converting monolithic web-based applications to a more agile, rapidly deployable intelligent web-services model.",,"Chemical emergency response; Chemically hazardous emergencies; Intelligent web services; Web based systems; Feature extraction; Semantics; World Wide Web; Information services",2-s2.0-33749068045
"Aminian M.","Active learning for reducing bias and variance of a classifier using Jensen-Shannon divergence",2005,"Proceedings - ICMLA 2005: Fourth International Conference on Machine Learning and Applications",4,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847270297&doi=10.1109%2fICMLA.2005.7&partnerID=40&md5=76b4e369d37c6ffe9cf888c170df98fc","We consider reducing loss of a classifier by decreasing its bias and variance. Embarking upon classification of scarcely labeled data, we use active learning approach in semi-supervised learning, and show that we can speed up convergence to a desired level of loss. Our focus, in this paper, is on the best instance selection for labeling the unlabeled data; we use Jensen-Shannon divergence as one selection criterion. We show that our single instance selection approaches are superior to multiple selection approach. Empirical results indicate that this method can decrease classification loss significantly. © 2005 IEEE.",,"Classification (of information); Convergence of numerical methods; Data processing; Feature extraction; Classification loss; Jensen-Shannon divergence; Selection criterion; Semi-supervised learning; Learning systems",2-s2.0-33847270297
"Cortés J., Siméon T.","Sampling-based motion planning under kinematic loop-closure constraints",2005,"Springer Tracts in Advanced Robotics",11,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880177767&partnerID=40&md5=b6f507b934514745cef5edabb1e898ca","Kinematic loop-closure constraints significantly increase the difficulty of motion planning for articulated mechanisms. Configurations of closed-chain mechanisms do not form a single manifold, easy to parameterize, as the configurations of open kinematic chains. In general, they are grouped into several subsets with complex and a priori unknown topology. Sampling-based motion planning algorithms cannot be directly applied to such closed-chain systems. This paper describes our recent work [7] on the extension of sampling-based planners to treat this kind of mechanisms. © Springer-Verlag Berlin Heidelberg 2005.",,"Closed chain mechanism; Kinematic chain; Loop closure; Sampling-based; Sampling-based motion planning; Chains; Kinematics; Mechanisms; Motion planning",2-s2.0-84880177767
"Bavoil L., Callahan S.P., Crossno P.J., Freire J., Scheidegger C.E., Silva C.T., Vo H.T.","VisTrails: Enabling interactive multiple-view visualizations",2005,"Proceedings of the IEEE Visualization Conference",113,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749434347&doi=10.1109%2fVIS.2005.113&partnerID=40&md5=fd3059b91b565939e75e4d33d59eeb2e","VisTrails is a new system that enables interactive multiple-view visualizations by simplifying the creation and maintenance of visualization pipelines, and by optimizing their execution. It provides a general infrastructure that can be combined with existing visualization systems and libraries. A key component of VisTrails is the visualization trail (vistrail), a formal specification of a pipeline. Unlike existing dataflow-based systems, in VisTrails there is a clear separation between the specification of a pipeline and its execution instances. This separation enables powerful scripting capabilities and provides a scalable mechanism for generating a large number of visualizations. VisTrails also leverages the vistrail specification to identify and avoid redundant operations. This optimization is especially useful while exploring multiple visualizations. When variations of the same pipeline need to be executed, substantial speedups can be obtained by caching the results of overlapping subsequences of the pipelines. In this paper, we describe the design and implementation of VisTrails, and show its effectiveness in different application scenarios. © 2005 IEEE.","Caching; Coordinated views; Dataflow; Interrogative visualization","Caching; Dataflow; Interrogative visualization; Visualization systems; Cache memory; Data communication systems; Data transfer; Digital libraries; Optimization; Interactive computer graphics",2-s2.0-33749434347
"Ferragina P., Luccio F., Manzini G., Muthukrishnan S.","Structuring labeled trees for optimal succinctness, and beyond",2005,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",62,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748626529&doi=10.1109%2fSFCS.2005.69&partnerID=40&md5=14d8ca6a6a81e6d8dbb42bde4591b875","Consider an ordered, static tree T on t nodes where each node has a label from alphabet set ∼. Tree T may be of arbitrary degree and of arbitrary shape. Say, we wish to support basic navigational operations such as find the parent of node u, the ith child of u, and any child of u with label α. In a seminal work over fifteen years ago, Jacobson [15] observed that pointer-based tree representations are wasteful in space and introduced the notion of succinct data structures. He studied the special case of unlabeled trees and presented a succinct data structure of 2t+o(t) bits supporting navigational operations in O(1) time. The space used is asymptotically optimal with the information-theoretic lower bound averaged over all trees. This led to a slew of results on succinct data structures for arrays, trees, strings and multisets. Still, for the fundamental problem of structuring labeled trees succinctly, few results, if any, exist even though labeled trees arise frequently in practice, e.g. in the data as in markup text (XML) or in augmented data structures. We present a novel approach to the problem of succinct manipulation of labeled trees by designing what we call the xbw transform of the tree, in the spirit of the well-known Burrows-Wheeler transform for strings, xbw transform uses path-sorting and grouping to linearize the labeled tree T into two coordinated arrays, one capturing the structure and the other the labels. Using the properties of the xbw transform, we (i) derive the first-known (near-)optimal results for succinct representation of labeled trees with O(1) time for navigation operations, (ii) optimally support the powerful subpath search operation for the first time, and (iii) introduce a notion of tree entropy and present linear time algorithms for compressing a given labeled tree up to its entropy beyond the information-theoretic lower bound averaged over all tree inputs. Our xbw transform is simple and likely to spur new results in the theory of tree compression and indexing, and may have some practical impact in XML data processing. © 2005 IEEE.",,"Augmented data structures; Labeled trees; Path sorting; Algorithms; Data processing; Data structures; Mathematical transformations; Optimal systems; Problem solving; Set theory; XML; Trees (mathematics)",2-s2.0-33748626529
"Avgustinov P., Christensen A.S., Hendren L., Kuzins S., Lhoták J., Lhoták O., De Morr O., Sereni D., Sittampalam G., Tibble J.","Abc An extensible AspectJ compiler",2005,"AOSD 2005: 4th International Conference on Aspect-Oriented Software Development - Conference Proceedings",135,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33244496661&doi=10.1145%2f1052898.1052906&partnerID=40&md5=bba1e026da51ca8ce74d71e65a0fabd7","Research in the design of aspect-oriented programming languages requires a workbench that facilitates easy experimentation with new language features and implementation techniques. In particular, new features for AspectJ have been proposed that require extensions in many dimensions: syntax, type checking and code generation, as well as data flow and control flow analyses. The AspectBench Compiler (abc) is an implementation of such a workbench. The base version of abc implements the full AspectJ language. Its frontend is built, using the Polyglot framework, as a modular extension of the Java language. The use of Polyglot gives flexibility of syntax and type checking. The backend is built using the Soot framework, to give modular code generation and analyses. In this paper, we outline the design of abc, focusing mostly on how the design supports extensibility. We then provide a general overview of how to use abc to implement an extension. Finally, we illustrate the extension mechanisms of abc through a number of small, but non-trivial, examples, abc is freely available under the GNU LGPL. Copyright 2005 ACM.",,"Codes (symbols); Data reduction; Java programming language; Object oriented programming; Aspect-oriented programming languages; Code generation; Data flow; Type checking; Program compilers",2-s2.0-33244496661
"Weeds J., Weir D.","Co-occurrence retrieval: A flexible framework for lexical distributional similarity",2005,"Computational Linguistics",89,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646389801&doi=10.1162%2f089120105775299122&partnerID=40&md5=00429d27682cc002ac33f0fdd2e9c9e6","Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing. For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events. In other applications, distributional similarity is taken to be an approximation to semantic similarity. However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted. In this work, a flexible, parameterized framework for calculating distributional similarity is proposed. Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval. As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework. In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity. First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship? Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity? Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type? Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity. © 2006 Association for Computational Linguistics.",,,2-s2.0-33646389801
"Draheim D., Weber G.","Form-oriented analysis: A new methodology to model form-based applications",2005,"Form-Oriented Analysis: A New Methodology to Model Form-Based Applications",61,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892129751&doi=10.1007%2f3-540-26893-6&partnerID=40&md5=a41e35c7122b596fadf1819970658c96","Form-based applications range from simple web shops to complex enterprise resource planning systems. Draheim and Weber adapt well-established basic modeling techniques in a novel way to achieve a modeling framework optimized for this broad application domain. They introduce new modeling artifacts, such as page diagrams and form storyboards, and separate dialogue patterns to allow for reuse. In their implementation they have developed new constructs such as typed server pages, and tools for forward and reverse engineering of presentation layers. The methodology is explained using an online bookshop as a running example in which the user can experience the modeling concepts in action. The combination of theoretical achievements and hands-on practical advice and tools makes this book a reference work for both researchers in the areas of software architectures and submit-response style user interfaces, and professionals designing and developing such applications. More information and additional material is also available online. © Springer-Verlag Berlin Heidelberg 2005.",,,2-s2.0-84892129751
"Glenstrup A.J., Jones N.D.","Termination analysis and specialization-point insertion in offline partial evaluation",2005,"ACM Transactions on Programming Languages and Systems",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745338876&doi=10.1145%2f1108970.1108973&partnerID=40&md5=b7063d8623209b3b697444fa36da8760","Recent research suggests that the goal of fully automatic and reliable program generation for a broad range of applications is coming nearer to feasibility. However, several interesting and challenging problems remain to be solved before it becomes a reality. Solving them is also necessary, if we hope ever to elevate software engineering from its current state (a highly developed handiwork) into a successful branch of engineering, capable of solving a wide range of new problems by systematic, well-automated and well-founded methods. A key problem in all program generation is termination of the generation process. This article focuses on off-line partial evaluation and describes recent progress towards automatically solving the termination problem, first for individual programs, and then for specializers and ""generating extensions,"" the program generators that most offline partial evaluators produce. The technique is based on size-change graphs that approximate the changes in parameter sizes at function calls. We formulate a criterion, bounded anchoring, for detecting parameters known to be bounded during specialization: a bounded parameter can act as an anchor for other parameters. Specialization points necessary for termination are computed by adding a parameter that tracks call depth, and then selecting a specialization point in every call loop where it is unanchored. By generalizing all unbounded parameters, we compute a binding-time division which together with the set of specialization points guarantees termination. Contributions of this article include a proof, based on the operational semantics of partial evaluation with memoization, that the analysis guarantees termination; and an in-depth description of safety of the increasing size approximation operator required for termination analysis in partial evaluation. Initial experiments with a prototype shows that the analysis overall yields binding-time divisions that can achieve a high degree of specialization, while still guaranteeing termination. The article ends with a list of challenging problems whose solution would bring the community closer to the goal of broad-spectrum, fully automatic and reliable program generation. © 2005 ACM.","Binding-time analysis; Quasitermination; Size-change graphs; Termination","Binding-time analysis; Quasitermination; Size-change graphs; Termination; Computation theory; Computer applications; Computer simulation; Problem solving; Semantics; Software engineering; Computer programming",2-s2.0-33745338876
"Zhang Q., Veretnik S., Bourne P.E.","Overview of structural bioinformatics",2005,"Bioinformatics Technologies",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651547727&doi=10.1007%2f3-540-26888-X_2&partnerID=40&md5=d28506bd07174625bf1562cdcea064fb","If we define bioinformatics as the development of algorithms and databases for understanding biological systems, then structural bioinformatics represents the subset that deals, directly or indirectly, with the structure of macromolecules. Structural bioinformatics includes study of the structures of DNA, RNA, and proteins. In this chapter we will be focusing primarily on the resources associated with protein structures. Knowledge of the protein structure allows us to investigate biological processes more directly and with much higher resolution and finer detail. For example, strikingly more details of protein-protein interactions can be obtained from the structure of the protein complex than from that of yeast two-hybrid assay. However, determination of protein structures is experimentally expensive and time consuming; this may explain why at the present time scientists are largely dependent on sequence rather than structure to infer the function of the protein. With the advent of structural genomics, we expect to systematically and rapidly solve a large number of macromolecular structures. Knowledge of a large number of protein structures gives us a bird's-eye view of protein fold space, as can be seen from Fig. 2.1, and is helpful to understand the evolutionary principles behind structure, which architectures and topologies are observed (and why), which topologies are prevalent or avoided, and how the structure of the protein affects its function. A well populated protein universe might be the most important resource for assigning structures to sequences without solving them crystallographically, rather than by using predictive methods. As structure determination will be lagging far behind genomic sequencing for considerable time, the predicting protein structures will remain an important and valuable ability. At present, structural bioinformatics is in its renaissance, with large amounts of structural data, and well developed (and ever increasing) arsenal of algorithms, applications and databases. Its contribution to the advancement of understanding biological systems can hardly be overestimated.",,,2-s2.0-78651547727
"Asveld P.R.J.","Fuzzy context-free languages - Part 2: Recognition and parsing algorithms",2005,"Theoretical Computer Science",13,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27844602863&doi=10.1016%2fj.tcs.2005.06.013&partnerID=40&md5=98f13bda73260fb8b88f0f045281e1f1","In a companion paper [P.R.J. Asveld, Fuzzy context-free languages - Part 1: Generalized fuzzy context-free grammars, Theoret. Comput. Sci., (2005).] we used fuzzy context-free grammars in order to model grammatical errors resulting in erroneous inputs for robust recognizing and parsing algorithms for fuzzy context-free languages. In particular, this approach enables us to distinguish between small errors (""tiny mistakes"") and big errors (""capital blunders""). In this paper, we present some algorithms to recognize fuzzy context-free languages: particularly, a modification of Cocke-Younger-Kasami's algorithm and some recursive descent algorithms. Then we extend these recognition algorithms to corresponding parsing algorithms for fuzzy context-free languages. These parsing algorithms happen to be robust in some very elementary sense. © 2005 Elsevier B.V. All rights reserved.","Formal language; Fuzzy context-free grammar; Grammatical error; Parsing algorithm; Recognition algorithm; Robust parsing","Algorithms; Context free grammars; Error analysis; Fuzzy contexts; Fuzzy language; Computer science",2-s2.0-27844602863
"Breaux T.D., Antón A.I.","Analyzing goal semantics for rights, permissions, and obligations",2005,"Proceedings of the IEEE International Conference on Requirements Engineering",48,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644461562&partnerID=40&md5=a183161b7cd9a61370388f931700e97f","Software requirements, rights, permissions, obligations, and operations of policy enforcing systems are often misaligned Our goal is to develop tools and techniques that help requirements engineers and policy makers bring policies and system requirements into better alignment. Goals from requirements engineering are useful for distilling natural language policy statements into structured descriptions of these interactions; however, they are limited in that they are not easy to compare with one another despite sharing common semantic features. In this paper, we describe a process called semantic parameterization that we use to derive semantic models from goals mined from privacy policy documents. We present example semantic models that enable comparing policy statements and present a template method for generating natural language policy statements (and ultimately requirements) from unique semantic models. The semantic models are described by a context-free grammar called KTL that has been validated within the context of the most frequently expressed goals in over 100 Internet privacy policy documents. KTL is supported by a policy analysis tool that supports queries and policy statement generation. © 2005 IEEE.",,"Natural language policy; Policy statement generation; Privacy policy documents; Software requirements; Context free grammars; Internet; Mathematical models; Parameter estimation; Public policy; Requirements engineering; Semantics",2-s2.0-27644461562
"Colucci S., Di Noia T., Di Sciascio E., Donini F.M., Mongiello M.","Concept abduction and contraction for semantic-based discovery of matches and negotiation spaces in an e-marketplace",2005,"Electronic Commerce Research and Applications",76,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644565226&doi=10.1016%2fj.elerap.2005.06.004&partnerID=40&md5=ee66e0834fa8964196f917d75d922297","In this paper, we present a Description Logic approach - fully compliant with the Semantic web vision and technologies - to extended matchmaking between demands and supplies in a semantic-enabled Electronic Marketplace, which allows the semantic-based treatment of negotiable and strict requirements in the demand/supply descriptions. To this aim, we exploit two novel non-standard Description Logic inference services, Concept Contraction - which extends satisfiability - and Concept Abduction - which extends subsumption. Based on these services, we devise algorithms, which allow to find negotiation spaces and to determine the quality of a possible match, also in the presence of a distinction between strictly required and optional elements. Both the algorithms and the semantic-based approach are novel, and enable a mechanism to boost logic-based discovery and negotiation stages within an e-marketplace. A set of simple experiments confirm the validity of the approach. © 2005 Elsevier B.V. All rights reserved.","Concept abduction; Concept contraction; Description logics; E-commerce; Matchmaking; Negotiable constraints; Semantic web; Semantics","Algorithms; Constraint theory; Industrial economics; Marketing; Semantics; World Wide Web; Concept abduction; Concept contraction; Description logics; Matchmaking; Negotiable constraints; Semantic web; Electronic commerce",2-s2.0-27644565226
"Stone R.G.","Validation of dynamic Web pages generated by an embedded scripting language",2005,"Software - Practice and Experience",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-27544443722&doi=10.1002%2fspe.670&partnerID=40&md5=7382642687dc65d865204fd513e2db48","This paper attempts to provide insight as to how to guarantee a statement such as My PHP script produces WML. To expand a little, the emphasis is to ensure that a script always produces a valid WML page. The context is where pages in a Web site are being created by an embedded scripting language (such as PHP, ASP, Perl) and also that the resulting pages are to conform to a strict tagged mark-up scheme such as WML or XHTML. Although there are validators for static pages, there is nothing available to check that a page containing embedded scripting will (always) generate valid documents. What is required is a validator for dynamic Web pages. Copyright © 2005 John Wiley & Sons, Ltd.","DTD; Dynamic Web pages; PHP; Validation; WML; XHTML","Embedded systems; HTML; Information retrieval systems; Query languages; DTD; Dynamic web pages; PHP; Validation; WML; XHTML; Websites",2-s2.0-27544443722
"Chen Y.-J., Chen Y.-M., Wang C.-B., Chu H.-C., Tsai T.-N.","Developing a multi-layer reference design retrieval technology for knowledge management in engineering design",2005,"Expert Systems with Applications",34,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-25844450322&doi=10.1016%2fj.eswa.2005.06.005&partnerID=40&md5=cb4ceee89419492a3bbbadd932054e19","Engineering design is a knowledge-intensive process that encompasses conceptual design, detailed design, engineering analysis, assembly design, process design, and performance evaluation. Each of these tasks involves various areas of knowledge and experience. The sharing of such knowledge and experience is critical to increasing the capacity for developing products and to increasing their quality. It is also critical to reducing the duration and cost of the development cycle. Accordingly, offering engineering designers various methods for retrieving engineering knowledge is one of the most important tasks in managing engineering knowledge. This study develops a multi-layer reference design retrieval technology for engineering knowledge management to provide engineering designers with easy access to relevant design and related knowledge. The tasks performed in this research include (i) designing a multi-layer reference design retrieval process, (ii) developing techniques associated with multi-layer reference design retrieval technology, and (iii) implementing a multi-layer reference design retrieval mechanism. The retrieval process contains three main phases - 'customer requirement-based reference design retrieval', 'functional requirement-based reference design retrieval' and 'functional feature-based reference design retrieval'. This technology involves (1) customer requirement-based reference design retrieval, which involves a structured query model for customer requirements, a case-based representation of designed entities, a customer requirement-based index structure for historical design cases, and customer requirement-based case searching, matching and ranking mechanisms, (2) functional requirement-based reference design retrieval, which includes a structured query model for functional requirements, a functional requirement-based index structure for historical design cases, and functional requirement-based case searching, matching and ranking mechanisms, and (3) functional feature-based reference design retrieval, which is a binary code-based representation for functional features, an ART1 neural network for functional feature-based case clustering and functional feature-based case ranking. © 2005 Elsevier Ltd. All rights reserved.","ART1 neural network; Engineering design; Knowledge management; Multi-layer; Reference design retrieval","Costs; Customer satisfaction; Design aids; Knowledge acquisition; Mathematical models; Multilayers; Neural networks; Search engines; ART1 neural network; Engineering design; Knowledge management; Reference design retrieval; Information retrieval",2-s2.0-25844450322
"Howe B., Maier D.","Algebraic manipulation of scientific datasets",2005,"VLDB Journal",14,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745323590&doi=10.1007%2fs00778-005-0157-5&partnerID=40&md5=57372a0f121382d31e74a15a4e68242f","We investigate algebraic processing strategies for large numeric datasets equipped with a (possibly irregular) grid structure. Such datasets arise, for example, in computational simulations, observation networks, medical imaging, and 2-D and 3-D rendering. Existing approaches for manipulating these datasets are incomplete: The performance of SQL queries for manipulating large numeric datasets is not competitive with specialized tools. Database extensions for processing multidimensional discrete data can only model regular, rectilinear grids. Visualization software libraries are designed to process arbitrary gridded datasets efficiently, but no algebra has been developed to simplify their use and afford optimization. Further, these libraries are data dependent - physical changes to data representation or organization break user programs. In this paper, we present an algebra of gridfields for manipulating arbitrary gridded datasets, algebraic optimization techniques, and an implementation backed by experimental results. We compare our techniques to those of Geographic Information Systems (GIS) and visualization software libraries, using real examples from an Environmental Observation and Forecasting System. We find that our approach can express optimized plans inaccessible to other techniques, resulting in improved performance with reduced programming effort.","Algebra; Mesh; Scientific Databases",,2-s2.0-33745323590
"Zhou Z.-H., Li M.","Tri-training: Exploiting unlabeled data using three classifiers",2005,"IEEE Transactions on Knowledge and Data Engineering",417,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-28244448186&doi=10.1109%2fTKDE.2005.186&partnerID=40&md5=c03f6ad2ee64eb60d31540f0a4a4462d","In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance. © 2005 IEEE.","Co-training, tri-training; Data mining; Learning from unlabeled data; Machine learning; Semi-supervised learning; Web page classification","Data sets; Learning from unlabeled data; Semi-supervized learning; Classification (of information); Data mining; Learning algorithms; Websites; Data reduction",2-s2.0-28244448186
"Sinha K., Zhang X., Jin R., Agrawal G.","Learning layouts of biological datasets semi-automatically",2005,"Lecture Notes in Bioinformatics (Subseries of Lecture Notes in Computer Science)",,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444481637&partnerID=40&md5=8c8b84608b90616757c5704d64d71844","A key challenge associated with the existing approaches for data integration and workflow creation for bioinformatics is the effort required to integrate a new data source. As new data sources emerge, and data formats and contents of existing data sources evolve, wrapper programs need to be written or modified. This can be extremely time consuming, tedious, and error-prone. This paper describes our semi-automatic approach for learning the layout of a flat-file bioinformatics dataset. Our approach involves three key steps. The first step is to use a number of heuristics to infer the delimiters used in the program. Specifically, we have developed a metric that uses information on the frequency and starting position of sequences. Based on this metric, we are able to find a superset of delimiters, and then we can seek user input to eliminate the incorrect ones. Our second step involves generating a layout descriptor based on the relative order in which the delimiters occur. Our final step is to generate a parser based on the layout descriptor. Our heuristics for finding the delimiters has been evaluated using three popular flat-file biological datasets. © Springer-Verlag Berlin Heidelberg 2005.",,"Bioinformatics; Data integration; Delimiters; Flat file bioinformatics dataset; Wrapper programs; Data acquisition; Error analysis; Heuristic methods; Error detection; Set theory; Learning systems; Database systems",2-s2.0-26444481637
"Chen K.-J., Hsieh Y.-M.","Chinese treebanks and grammar extraction",2005,"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)",1,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444582892&partnerID=40&md5=f346e6ab2975f15955cef19a23e69850","Preparation of knowledge bank is a very difficult task. In this paper, we discuss the knowledge extraction from the manually examined Sinica Tree-bank. Categorical information, word-to-word relation, word collocations, new syntactic patterns and sentence structures are obtained. A searching system for Chinese sentence structure was developed in this study. By using pre-extracted data and SQL commands, the system replies the user's queries efficiently. We also analyze the extracted grammars to study the tradeoffs between the granularity of the grammar rules and their coverage as well as ambiguities. It provides the information of knowing how large a treebank is sufficient for the purpose of grammar extraction. Finally, we also analyze the tradeoffs between grammar coverage and ambiguity by parsing results from the grammar rules of different granularity. © Springer-Verlag Berlin Heidelberg 2005.","Ambiguities; Grammar coverage; Knowledge extraction; Parsing; Treebanks","Ambiguities; Grammar coverage; Knowledge extraction; Parsing; Treebanks; Artificial intelligence; Data reduction; Information analysis; Knowledge acquisition; Natural language processing systems",2-s2.0-26444582892
"Stratica N., Kosseim L., Desai B.C.","Using semantic templates for a natural language interface to the CINDI virtual library",2005,"Data and Knowledge Engineering",24,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-23344454520&doi=10.1016%2fj.datak.2004.12.002&partnerID=40&md5=566911bbd742afe34ae4127f5abb7f4e","In this paper, we present our work in building a template-based system for translating English sentences into SQL queries for a relational database system. The input sentences are syntactically parsed using the Link Parser, and semantically parsed through the use of domain-specific templates. The system is composed of a pre-processor and a run-time module. The pre-processor builds a conceptual knowledge base from the database schema using WordNet. This knowledge base is then used at run time to semantically parse the input and create the corresponding SQL query. The system is meant to be domain independent and has been tested with the CINDI database that contains information on a virtual library. © 2004 Elsevier B.V. All rights reserved.","CINDI; Natural language interface; Natural language processing; Relational data base; Semantic analysis; Syntactic analysis","CINDI; Natural language interface; Natural language processing; Relational data base; Semantic analysis; Syntactic analysis; Template-based systems; Data reduction; Digital libraries; Interfaces (computer); Program processors; Relational database systems; Semantics; Knowledge engineering",2-s2.0-23344454520
